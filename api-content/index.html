{"posts":[{"title":"The Nature of Lisp","content":" The Nature of Lisp Introduction When I first stumbled into Lisp advocacy on various corners of the web I was already an experienced programmer. At that point I had grokked what seemed at the time a wide range of programming languages. I was proud to have the usual suspects (C++, Java, C#, etc.) on my service record and was under impression that I knew everything there is to know about programming languages. I couldn't have possibly been more wrong. My initial attempt to learn Lisp came to a crashing halt as soon as I saw some sample code. I suppose the same thought ran through my mind that ran through thousands of other minds who were ever in my shoes: &quot;Why on Earth would anyone want to use a language with such horrific syntax?!&quot; I couldn't be bothered to learn a language if its creators couldn't be bothered to give it a pleasant syntax. After all, I was almost blinded by the infamous Lisp parentheses! The moment I regained my sight I communicated my frustrations to some members of the Lisp sect. Almost immediately I was bombarded by a standard set of responses: Lisp's parentheses are only a superficial matter, Lisp has a huge benefit of code and data being expressed in the same manner (which, obviously, is a huge improvement over XML), Lisp has tremendously powerful metaprogramming facilities that allow programs to write code and modify themselves, Lisp allows for creation of mini-languages specific to the problem at hand, Lisp blurs the distinction between run time and compile time, Lisp, Lisp, Lisp... The list was very impressive. Needless to say none of it made sense. Nobody could illustrate the usefulness of these features with specific examples because these techniques are supposedly only useful in large software systems. After many hours of debating that conventional programming languages do the job just fine, I gave up. I wasn't about to invest months into learning a language with a terrible syntax in order to understand obscure features that had no useful examples. My time has not yet come. For many months the Lisp advocates pressed on. I was baffled. Many extremely intelligent people I knew and had much respect for were praising Lisp with almost religious dedication. There had to be something there, something I couldn't afford not to get my hands on! Eventually my thirst for knowledge won me over. I took the plunge, bit the bullet, got my hands dirty, and began months of mind bending exercises. It was a journey on an endless lake of frustration. I turned my mind inside out, rinsed it, and put it back in place. I went through seven rings of hell and came back. And then I got it. The enlightenment came instantaneously. One moment I understood nothing, and the next moment everything clicked into place. I've achieved nirvana. Dozens of times I heard Eric Raymond's statement quoted by different people: &quot;Lisp is worth learning for the profound enlightenment experience you will have when you finally get it; that experience will make you a better programmer for the rest of your days, even if you never actually use Lisp itself a lot.&quot; I never understood this statement. I never believed it could be true. And finally, after all the pain, it made sense! There was more truth to it than I ever could have imagined. I've achieved an almost divine state of mind, an instantaneous enlightenment experience that turned my view of computer science on its head in less than a single second. That very second I became a member of the Lisp cult. I felt something a ninjitsu master must feel: I had to spread my newfound knowledge to at least ten lost souls in the course of my lifetime. I took the usual path. I was rehashing the same arguments that were given to me for years (only now they actually made sense!), hoping to convert unsuspecting bystanders. It didn't work. My persistence sparked a few people's interest but their curiosity dwindled at the mere sight of sample Lisp code. Perhaps years of advocacy would forge a few new Lispers, but I wasn't satisfied. There had to be a better way. I gave the matter careful thought. Is there something inherently hard about Lisp that prevents very intelligent, experienced programmers from understanding it? No, there isn't. After all, I got it, and if I can do it, anybody can. Then what is it that makes Lisp so hard to understand? The answer, as such things usually do, came unexpectedly. Of course! Teaching anybody anything involves building advanced concepts on top of concepts they already understand! If the process is made interesting and the matter is explained properly the new concepts become as intuitive as the original building blocks that aided their understanding. That was the problem! Metaprogramming, code and data in one representation, self-modifying programs, domain specific mini-languages, none of the explanations for these concepts referenced familiar territory. How could I expect anyone to understand them! No wonder people wanted specific examples. I could as well have been speaking in Martian! I shared my ideas with fellow Lispers. &quot;Well, of course these concepts aren't explained in terms of familiar territory&quot;, they said. &quot;They are so different, they're unlike anything these people have learned before.&quot; This was a poor excuse. &quot;I do not believe this to be true&quot;, I said. The response was unanimous: &quot;Why don't you give it a try?&quot; So I did. This article is a product of my efforts. It is my attempt to explain Lisp in familiar, intuitive concepts. I urge brave souls to read on. Grab your favorite drink. Take a deep breath. Prepare to be blown away. Oh, and may the Force be with you. XML Reloaded A thousand mile journey starts with a single step. A journey to enlightenment is no exception and our first step just happens to be XML. What more could possibly be said about XML that hasn't already been said? It turns out, quite a bit. While there's nothing particularly interesting about XML itself, its relationship to Lisp is fascinating. XML is the all too familiar concept that Lisp advocates need so much. It is our bridge to conveying understanding to regular programmers. So let's revive the dead horse, take out the stick, and venture into XML wilderness that no one dared venture into before us. It's time to see the all too familiar moon from the other side. Superficially XML is nothing more than a standardized syntax used to express arbitrary hierarchical data in human readable form. To-do lists, web pages, medical records, auto insurance claims, configuration files are all examples of potential XML use. Let's use a simple to-do list as an example (in a couple of sections you'll see it in a whole new light): &lt;todo name=&quot;housework&quot;&gt; &lt;item priority=&quot;high&quot;&gt;Clean the house.&lt;/item&gt; &lt;item priority=&quot;medium&quot;&gt;Wash the dishes.&lt;/item&gt; &lt;item priority=&quot;medium&quot;&gt;Buy more soap.&lt;/item&gt; &lt;/todo&gt; What happens if we unleash our favorite XML parser on this to-do list? Once the data is parsed, how is it represented in memory? The most natural representation is, of course, a tree - a perfect data structure for hierarchical data. After all is said and done, XML is really just a tree serialized to a human readable form. Anything that can be represented in a tree can be represented in XML and vice versa. I hope you understand this idea. It's very important for what's coming next. Let's take this a little further. What other type of data is often represented as a tree? At this point the list is as good as infinite so I'll give you a hint at what I'm getting at - try to remember your old compiler course. If you have a vague recollection that source code is stored in a tree after it's parsed, you're on the right track. Any compiler inevitably parses the source code into an abstract syntax tree. This isn't surprising since source code is hierarchical: functions contain arguments and blocks of code. Blocks of code contain expressions and statements. Expressions contain variables and operators. And so it goes. Let's apply our corollary that any tree can easily be serialized into XML to this idea. If all source code is eventually represented as a tree, and any tree can be serialized into XML, then all source code can be converted to XML, right? Let's illustrate this interesting property by a simple example. Consider the function below: int add(int arg1, int arg2) { return arg1 + arg2; } Can you convert this function definition to its XML equivalent? Turns out, it's reasonably simple. Naturally there are many ways to do this. Here is one way the resulting XML can look like: &lt;define-function return-type=&quot;int&quot; name=&quot;add&quot;&gt; &lt;arguments&gt; &lt;argument type=&quot;int&quot;&gt;arg1&lt;/argument&gt; &lt;argument type=&quot;int&quot;&gt;arg2&lt;/argument&gt; &lt;/arguments&gt; &lt;body&gt; &lt;return&gt; &lt;add value1=&quot;arg1&quot; value2=&quot;arg2&quot; /&gt; &lt;/return&gt; &lt;/body&gt; &lt;/define&gt; We can go through this relatively simple exercise with any language. We can turn any source code into XML, and we can transform the resulting XML back to original source code. We can write a converter that turns Java into XML and a converter that turns XML back to Java. We could do the same for C++. (In case you're wondering if anyone is crazy enough to do it, take a look at GCC-XML). Furthermore, for languages that share common features but use different syntax (which to some extent is true about most mainstream languages) we could convert source code from one language to another using XML as an intermediary representation. We could use our Java2XML converter to convert a Java program to XML. We could then run an XML2CPP converter on the resulting XML and turn it into C++ code. With any luck (if we avoid using features of Java that don't exist in C++) we'll get a working C++ program. Neat, eh? All this effectively means that we can use XML for generic storage of source code. We'd be able to create a whole class of programming languages that use uniform syntax, as well as write transformers that convert existing source code to XML. If we were to actually adopt this idea, compilers for different languages wouldn't need to implement parsers for their specific grammars - they'd simply use an XML parser to turn XML directly into an abstract syntax tree. By now you're probably wondering why I've embarked on the XML crusade and what it has to do with Lisp (after all, Lisp was created about thirty years before XML). I promise that everything will become clear soon enough. But before we take our second step, let's go through a small philosophical exercise. Take a good look at the XML version of our &quot;add&quot; function above. How would you classify it? Is it data or code? If you think about it for a moment you'll realize that there are good reasons to put this XML snippet into both categories. It's XML and it's just information encoded in a standardized format. We've already determined that it can be generated from a tree data structure in memory (that's effectively what GCC-XML does). It's lying around in a file with no apparent way to execute it. We can parse it into a tree of XML nodes and do various transformations on it. It's data. But wait a moment! When all is said and done it's the same &quot;add&quot; function written with a different syntax, right? Once parsed, its tree could be fed into a compiler and we could execute it. We could easily write a small interpreter for this XML code and we could execute it directly. Alternatively, we could transform it into Java or C++ code, compile it, and run it. It's code. So, where are we? Looks like we've just arrived to an interesting point. A concept that has traditionally been so hard to understand is now amazingly simple and intuitive. Code is also always data! Does it mean that data is also always code? As crazy as this sounds this very well might be the case. Remember how I promised that you'll see our to-do list in a whole new light? Let me reiterate on that promise. But we aren't ready to discuss this just yet. For now let's continue walking down our path. A little earlier I mentioned that we could easily write an interpreter to execute our XML snippet of the add function. Of course this sounds like a purely theoretical exercise. Who in their right mind would want to do that for practical purposes? Well, it turns out quite a few people would disagree. You've likely encountered and used their work at least once in your career, too. Do I have you out on the edge of your seat? If so, let's move on! Ant Reloaded Now that we've made the trip to the dark side of the moon, let's not leave quite yet. We may still learn something by exploring it a little more, so let's take another step. We begin by closing our eyes and remembering a cold rainy night in the winter of 2000. A prominent developer by the name of [James Duncan Davidson][1] was hacking his way through Tomcat servlet container. As the time came to build the changes he carefully saved all his files and ran make. Errors. Lots of errors. Something was wrong. After careful examination James exclaimed: &quot;Is my command not executing because I have a space in front of my tab?!&quot; Indeed, this was the problem. Again. James has had enough. He could sense the full moon through the clouds and it made him adventurous. He created a fresh Java project and quickly hacked together a simple but surprisingly useful utility. This spark of genius used Java property files for information on how to build the project. James could now write the equivalent of the makefile in a nice format without worrying about the damned spaces ever again. His utility did all the hard work by interpreting the property file and taking appropriate actions to build the project. It was neat. Another Neat Tool. Ant. After using Ant to build Tomcat for a few months it became clear that Java property files are not sufficient to express complicated build instructions. Files needed to be checked out, copied, compiled, sent to another machine, and unit tested. In case of failure e-mails needed to be sent out to appropriate people. In case of success &quot;Bad to the Bone&quot; needed to be played at the highest possible volume. At the end of the track volume had to be restored to its original level. Yes, Java property files didn't cut it anymore. James needed a more flexible solution. He didn't feel like writing his own parser (especially since he wanted an industry standard solution). XML seemed like a reasonable alternative. In a couple of days Ant was ported to XML. It was the best thing since sliced bread. So how does Ant work? It's pretty simple. It takes an XML file with specific build instructions (you decide if they're data or code) and interprets them by running specialized Java code for each XML element. It's actually much simpler than it sounds. A simple XML instruction like the one below causes a Java class with an equivalent name to be loaded and its code to be executed. &lt;copy todir=&quot;../new/dir&quot;&gt; &lt;fileset dir=&quot;src_dir&quot;/&gt; &lt;/copy&gt; The snippet above copies a source directory to a destination directory. Ant locates a &quot;copy&quot; task (a Java class, really), sets appropriate parameters (todir and fileset) by calling appropriate Java methods and then executes the task. Ant comes with a set of core tasks and anyone can extend it with tasks of their own simply by writing Java classes that follow certain conventions. Ant finds these classes and executes them whenever XML elements with appropriate names are encountered. Pretty simple. Effectively Ant accomplishes what we were talking about in the previous section: it acts as an interpreter for a language that uses XML as its syntax by translating XML elements to appropriate Java instructions. We could write an &quot;add&quot; task and have Ant execute it when it encounters the XML snippet for addition presented in the previous section! Considering that Ant is an extremely popular project, the ideas presented in the previous section start looking more sane. After all, they're being used every day in what probably amounts to thousands of companies! So far I've said nothing about why Ant actually goes through all the trouble of interpreting XML. Don't try to look for the answer on its website either - you'll find nothing of value. Nothing relevant to our discussion, anyway. Let's take another step. It's time to find out why. Why XML? Sometimes right decisions are made without full conscious understanding of all the issues involved. I'm not sure if James knew why he chose XML - it was likely a subconscious decision. At the very least, the reasons I saw on Ant's website for using XML are all the wrong reasons. It appears that the main concerns revolved around portability and extensibility. I fail to see how XML helps advance these goals in Ant's case. What is the advantage of using interpreted XML over simple Java source code? Why not create a set of classes with a nice API for commonly used tasks (copying directories, compiling, etc.) and using those directly from Java source code? This would run on every platform that runs Java (which Ant requires anyway), it's infinitely extensible, and it has the benefit of having a more pleasant, familiar syntax. So why XML? Can we find a good reason for using it? It turns out that we can (although as I mentioned earlier I'm not sure if James was consciously aware of it). XML has the property of being far more flexible in terms of introduction of semantic constructs than Java could ever hope to be. Don't worry, I'm not falling into the trap of using big words to describe incomprehensible concepts. This is actually a relatively simple idea, though it may take some effort to explain. Buckle your seat-belt. We're about to make a giant leap towards achieving nirvana. How can we represent 'copy' example above in Java code? Here's one way to do it: CopyTask copy = new CopyTask(); Fileset fileset = new Fileset(); fileset.setDir(&quot;src_dir&quot;); copy.setToDir(&quot;../new/dir&quot;); copy.setFileset(fileset); copy.execute(); The code is almost the same, albeit a little longer than the original XML. So what's different? The answer is that the XML snippet introduces a special semantic construct for copying. If we could do it in Java it would look like this: copy(&quot;../new/dir&quot;) { fileset(&quot;src_dir&quot;); } Can you see the difference? The code above (if it were possible in Java) is a special operator for copying files - similar to a for loop or a new foreach construct introduced in Java 5. If we had an automatic converter from XML to Java it would likely produce the above gibberish. The reason for this is that Java's accepted syntax tree grammar is fixed by the language specification - we have no way of modifying it. We can add packages, classes, methods, but we cannot extend Java to make addition of new operators possible. Yet we can do it to our heart's content in XML - its syntax tree isn't restricted by anything except our interpreter! If the idea is still unclear, consider introducing a special operator 'unless' to Java: unless(someObject.canFly()) { someObject.transportByGround(); } In the previous two examples we extend the Java language to introduce an operator for copying files and a conditional operator unless. We would do this by modifying the abstract syntax tree grammar that Java compiler accepts. Naturally we cannot do it with standard Java facilities, but we can easily do it in XML. Because our XML interpreter parses the abstract syntax tree that results from it, we can extend it to include any operator we like. For complex operators this ability provides tremendous benefits. Can you imagine writing special operators for checking out source code, compiling files, running unit testing, sending email? Try to come up with some. If you're dealing with a specialized problem (in our case it's building projects) these operators can do wonders to decrease the amount of code you have to type and to increase clarity and code reuse. Interpreted XML makes this extremely easy to accomplish because it's a simple data file that stores hierarchical data. We do not have this option in Java because it's hierarchical structure is fixed (as you will soon find out, we do have this option in Lisp). Perhaps this is one of the reasons why Ant is so successful? I urge you to take a look at recent evolution of Java and C# (especially the recently released specification for C# 3.0). The languages are being evolved by abstracting away commonly used functionality and adding it in the form of operators. New C# operators for built-in queries is one example. This is accomplished by relatively traditional means: language creators modify the accepted abstract syntax tree and add implementations of certain features. Imagine the possibilities if the programmer could modify the abstract syntax tree himself! Whole new sub-languages could be built for specialized domains (for example a language for building projects, like Ant). Can you come up with other examples? Think about these concepts for a bit, but don't worry about them too much. We'll come back to these issues after introducing a few more ideas. By then things will be a little more clear. Almost Lisp Let's forget about the operator business for the moment and try to expand our horizons beyond the constraints of Ant's design. I mentioned earlier that Ant can be extended by writing conventional Java classes. Ant interpreter then attempts to match XML elements to appropriately named Java classes and if the match is found the task is executed. An interesting question begs to be asked. Why not extend Ant in Ant itself? After all, core tasks contain a lot of conventional programming language constructs ('if' being a perfect example). If Ant provided constructs to develop tasks in Ant itself we'd reach a higher degree of portability. We'd be dependent on a core set of tasks (a standard library, if you will) and we wouldn't care if Java runtime is present: the core set could be implemented in anything. The rest of the tasks would be built on top of the core using Ant-XML itself. Ant would then become a generic, extensible, XML-based programming language. Consider the possibilities: &lt;task name=&quot;Test&quot;&gt; &lt;echo message=&quot;Hello World!&quot;/&gt; &lt;/task&gt; &lt;Test /&gt; If ant supported the &quot;task&quot; construct, the example above would print &quot;Hello World!&quot;. In fact, we could write a &quot;task&quot; task in Java and make Ant able to extend itself using Ant-XML! Ant would then be able to build more complicated primitives on top of simple ones, just like any other programming language! This is an example of &quot;XML&quot; based programming language we were talking about in the beginning of this tutorial. Not very useful (can you tell why?) but pretty damn cool. By the way, take a look at our 'Test' task once again. Congratulations. You're looking at Lisp code. What on Earth am I talking about? It doesn't look anything like Lisp? Don't worry, we'll fix that in a bit. Confused? Good. Let's clear it all up! A Better XML I mentioned in the previous section that self-extending Ant wouldn't be very useful. The reason for that is XML's verbosity. It's not too bad for data files but the moment you try writing reasonably complex code the amount of typing you have to do quickly starts to get in the way and progresses to becoming unusable for any real project. Have you ever tried writing Ant build scripts? I have, and once they get complex enough having to do it in XML becomes really annoying. Imagine having to type almost everything in Java twice because you have to close every element. Wouldn't that drive you nuts? The solution to this problem involves using a less verbose alternative to XML. Remember, XML is just a format for representing hierarchical data. We don't have to use XML's angle brackets to serialize trees. We could come up with many other formats. One such format (incidentally, the one Lisp uses) is called an s-expression. S-expressions accomplish the same goals as XML. They're just a lot less verbose, which makes them much better suited for typing code. I will explain s-expressions in a little while, but before I do I have to clear up a few things about XML. Let's consider our XML example for copying files: &lt;copy todir=&quot;../new/dir&quot;&gt; &lt;fileset dir=&quot;src_dir&quot;/&gt; &lt;/copy&gt; Think of what the parse tree of this snippet would look like in memory. We'd have a 'copy' node that contains a fileset node. But what about attributes? How do they fit into our picture? If you've ever used XML to describe data and wondered whether you should use an element or an attribute, you're not alone. Nobody can really figure this out and doing it right tends to be black magic rather than science. The reason for that is that attributes are really subsets of elements. Anything attributes can do, elements can do as well. The reason attributes were introduced is to curb XML's verbosity. Take a look at another version of our 'copy' snippet: &lt;copy&gt; &lt;todir&gt;../new/dir&lt;/todir&gt; &lt;fileset&gt; &lt;dir&gt;src_dir&lt;/dir&gt; &lt;/fileset&gt; &lt;/copy&gt; The two snippets hold exactly the same information. However, we use attributes to avoid typing the same thing more than once. Imagine if attributes weren't part of XML specification. Writing anything in XML would drive us nuts! Now that we got attributes out of the way, let's look at s-expressions. The reason we took this detour is that s-expressions do not have attributes. Because they're a lot less verbose, attributes are simply unnecessary. This is one thing we need to keep in mind when transforming XML to s-expressions. Let's take a look at an example. We could translate above snippet to s-expressions like this: (copy (todir &quot;../new/dir&quot;) (fileset (dir &quot;src_dir&quot;))) Take a good look at this representation. What's different? Angle brackets seem to be replaced by parentheses. Instead of enclosing each element into a pair of parentheses and then closing each element with a &quot;(/element)&quot; we simply skip the second parenthesis in &quot;(element&quot; and proceed. The element is then closed like this: &quot;)&quot;. That's it! The translation is natural and very simple. It's also a lot easier to type. Do parentheses blind first time users? Maybe, but now that we're understand the reasoning behind them they're a lot easier to handle. At the very least they're better than arthritis inducing verbosity of XML. After you get used to s-expressions writing code in them is not only doable but very pleasant. And they provide all the benefits of writing code in XML (many of which we're yet to explore). Let's take a look at our 'task' code in something that looks a lot more like lisp: (task (name &quot;Test&quot;) (echo (message &quot;Hello World!&quot;))) (Test) S-expressions are called lists in Lisp lingo. Consider our 'task' element above. If we rewrite it without a line break and with comas instead of spaces it's starting to look surprisingly like a list of elements and other lists (the formatting is added to make it easier to see nested lists): (task, (name, &quot;test&quot;), (echo, (message, &quot;Hello World!&quot;))) We could do the same with XML. Of course the line above isn't really a list, it's a tree, just like its XML-alternative. Don't let references to lists confuse you, it's just that lists that contain other lists and trees are effectively the same thing. Lisp may stand for List Processing, but it's really tree processing - no different than processing XML nodes. Whew. After much rambling we finally got to something that looks like Lisp (and is Lisp, really). By now the mysterious Lisp parentheses as well as some claims made by Lisp advocates should become more clear. But we still have a lot of ground to cover. Ready? Let's move on! C Macros Reloaded By now you must be tired of all the XML talk. I'm tired of it as well. It's time to take a break from all the trees, s-expressions, and Ant business. Instead, let's go back to every programmer's roots. It's time to talk about C preprocessor. What's C got to do with anything, I hear you ask? Well, we now know enough to get into metaprogramming and discuss code that writes other code. Understanding this tends to be hard since all tutorials discuss it in terms of languages that you don't know. But there is nothing hard about the concept. I believe that a metaprogramming discussion based on C will make the whole thing much easier to understand. So, let's see (pun intended). Why would anyone want to write a program that writes programs? How can we use something like this in the real world? What on Earth is metaprogramming, anyway? You already know all the answers, you just don't know it yet. In order to unlock the hidden vault of divine knowledge let's consider a rather mundane task of simple database access from code. We've all been there. Writing SQL queries all over the code to modify data within tables turns into repetitive hell soon enough. Even with the new C# 3.0 LINQ stuff this is a huge pain. Writing a full SQL query (albeit with a nice built in syntax) to get someone's name or to modify someone's address isn't exactly a programmer's idea of comfort. What do we do to solve these problems? Enter data access layers. The idea is simple enough. You abstract database access (at least trivial queries, anyway) by creating a set of classes that mirror the tables in the database and use accessor methods to execute actual queries. This simplifies development tremendously - instead of writing SQL queries we make simple method calls (or property assignments, depending on your language of choice). Anyone who has ever used even the simplest of data access layers knows how much time it can save. Of course anyone who has ever written one knows how much time it can kill - writing a set of classes that mirror tables and convert accessors to SQL queries takes a considerable chunk of time. This seems especially silly since most of the work is manual: once you figure out the design and develop a template for your typical data access class you don't need to do any thinking. You just write code based on the same template over and over and over and over again. Many people figured out that there is a better way - there are plenty of tools that connect to the database, grab the schema, and write code for you based on a predefined (or a custom) template. Anyone who has ever used such a tool knows what an amazing time saver it can be. In a few clicks you connect the tool to the database, get it to generate the data access layer source code, add the files to your project and voilà - ten minutes worth of work do a better job than hundreds of man-hours that were required previously. What happens if your database schema changes? Well, you just have to go through this short process again. Of course some of the best tools let you automate this - you simply add them as a part of your build step and every time you compile your project everything is done for you automatically. This is perfect! You barely have to do anything at all. If the schema ever changes your data access layer code updates automatically at compile time and any obsolete access in your code will result in compiler errors! Data access layers are one good example, but there are plenty of others. From boilerplate GUI code, to web code, to COM and CORBA stubs, to MFC and ATL, - there are plenty of examples where the same code is written over and over again. Since writing this code is a task that can be automated completely and a programmer's time is far more expensive than CPU time, plenty of tools have been created that generate this boilerplate code automatically. What are these tools, exactly? Well, they are programs that write programs. They perform a simple task that has a mysterious name of metaprogramming. That's all there is to it. We could create and use such tools in millions of scenarios but more often than not we don't. What it boils down to is a subconscious calculation - is it worth it for me to create a separate project, write a whole tool to generate something, and then use it, if I only have to write these very similar pieces about seven times? Of course not. Data access layers and COM stubs are written hundreds, thousands of times. This is why there are tools for them. For similar pieces of code that repeat only a few times, or even a few dozen times, writing code generation tools isn't even considered. The trouble to create such a tool more often than not far outweighs the benefit of using one. If only creating such tools was much easier, we could use them more often, and perhaps save many hours of our time. Let's see if we can accomplish this in a reasonable manner. Surprisingly C preprocessor comes to the rescue. We've all used it in C and C++. On occasion we all wish Java had it. We use it to execute simple instructions at compile time to make small changes to our code (like selectively removing debug statements). Let's look at a quick example: #define triple(X) X + X + X What does this line do? It's a simple instruction written in the preprocessor language that instructs it to replace all instances of triple(X) with X + X + X. For example all instances of 'triple(5)' will be replaced with '5 + 5 + 5' and the resulting code will be compiled by the C compiler. We're really doing a very primitive version of code generation here. If only C preprocessor was a little more powerful and included ways to connect to the database and a few more simple constructs, we could use it to develop our data access layer right there, from within our program! Consider the following example that uses an imaginary extension of the C preprocessor: #get-db-schema(&quot;127.0.0.1, un, pwd&quot;); #iterate-through-tables #for-each-table class #table-name { }; #end-for-each We've just connected to the database schema, iterated through all the tables, and created an empty class for each. All in a couple of lines right within our source code! Now every time we recompile the file where above code appears we'll get a freshly built set of classes that automatically update based on the schema. With a little imagination you can see how we could build a full data access layer straight from within our program, without the use of any external tools! Of course this has a certain disadvantage (aside from the fact that such an advanced version of C preprocessor doesn't exist) - we'd have to learn a whole new &quot;compile-time language&quot; to do this sort of work. For complex code generation this language would have to be very complex as well, it would have to support many libraries and language constructs. For example, if our generated code depended on some file located at some ftp server the preprocessor would have to be able to connect to ftp. It's a shame to create and learn a new language just to do this. Especially since there are so many nice languages already out there. Of course if we add a little creativity we can easily avoid this pitfall. Why not replace the preprocessor language with C/C++ itself? We'd have full power of the language at compile time and we'd only need to learn a few simple directives to differentiate between compile time and runtime code! &lt;% cout &lt;&lt; &quot;Enter a number: &quot;; cin &gt;&gt; n; %&gt; for(int i = 0; i &lt; &lt;%= n %&gt;; i++) { cout &lt;&lt; &quot;hello&quot; &lt;&lt; endl; } Can you see what happens here? Everything that's between &lt;% and %&gt; tags runs when the program is compiled. Anything outside of these tags is normal code. In the example above you'd start compiling your program in the development environment. The code between the tags would be compiled and then ran. You'd get a prompt to enter a number. You'd enter one and it would be placed inside the for loop. The for loop would then be compiled as usual and you'd be able to execute it. For example, if you'd enter 5 during the compilation of your program, the resulting code would look like this: for(int i = 0; i &lt; 5; i++) { cout &lt;&lt; &quot;hello&quot; &lt;&lt; endl; } Simple and effective. No need for a special preprocessor language. We get full power of our host language (in this case C/C++) at compile time. We could easily connect to a database and generate our data access layer source code at compile time in the same way JSP or ASP generate HTML! Creating such tools would also be tremendously quick and simple. We'd never have to create new projects with specialized GUIs. We could inline our tools right into our programs. We wouldn't have to worry about whether writing such tools is worth it because writing them would be so fast - we could save tremendous amounts of time by creating simple bits of code that do mundane code generation for us! Hello, Lisp! Everything we've learned about Lisp so far can be summarized by a single statement: Lisp is executable XML with a friendlier syntax. We haven't said a single word about how Lisp actually operates. It's time to fill this [gap][2]. Lisp has a number of built in data types. Integers and strings, for example, aren't much different from what you're used to. The meaning of 71 or &quot;hello&quot; is roughly the same in Lisp as in C++ or Java. What is of more interest to us are symbols, lists, and functions. I will spend the rest of this section describing these data types as well as how a Lisp environment compiles and executes the source code you type into it (this is called evaluation in Lisp lingo). Getting through this section in one piece is important for understanding true potential of Lisp's metaprogramming, the unity of code and data, and the notion of domain specific languages. Don't think of this section as a chore though, I'll try to make it fun and accessible. Hopefully you can pick up a few interesting ideas on the way. Ok. Let's start with Lisp's symbols. A symbol in Lisp is roughly equivalent to C++ or Java's notion of an identifier. It's a name you can use to access a variable (like currentTime, arrayCount, n, etc.) The difference is that a symbol in Lisp is a lot more liberal than its mainstream identifier alternative. In C++ or Java you're limited to alphanumeric characters and an underscore. In Lisp, you are not. For example + is a valid symbol. So is -, =, hello-world, hello+world, *, etc. (you can find the exact definition of valid Lisp symbols online). You can assign to these symbols any data-type you like. Let's ignore Lisp syntax and use pseudo-code for now. Assume that a function set assigns some value to a symbol (like = does in Java or C++). The following are all valid examples: set(test, 5) // symbol 'test' will equal an integer 5 set(=, 5) // symbol '=' will equal an integer 5 set(test, &quot;hello&quot;) // symbol 'test' will equal a string &quot;hello&quot; set(test, =) // at this point symbol '=' is equal to 5 // therefore symbol 'test' will equal to 5 set(*, &quot;hello&quot;) // symbol '*' will equal a string &quot;hello&quot; At this point something must smell wrong. If we can assign strings and integers to symbols like *, how does Lisp do multiplication? After all, * means multiply, right? The answer is pretty simple. Functions in Lisp aren't special. There is a data-type, function, just like integer and string, that you assign to symbols. A multiplication function is built into Lisp and is assigned to a symbol *. You can reassign a different value to * and you'd lose the multiplication function. Or you can store the value of the function in some other variable. Again, using pseudo-code: *(3, 4) // multiplies 3 by 4, resulting in 12 set(temp, *) // symbol '*' is equal to the multiply function // so temp will equal to the multiply function set(*, 3) // sets symbol '*' to equal to 3 *(3, 4) // error, symbol '*' no longer equals to a function // it's equal to 3 temp(3, 4) // temp equals to a multiply function // so Lisp multiplies 3 by 4 resulting in 12 set(*, temp) // symbol '*' equals multiply function again *(3, 4) // multiplies 3 by 4, resulting in 12 You can even do wacky stuff like reassigning plus to minus: set(+, -) // the value of '-' is a built in minus function // so now symbol '+' equals to a minus function +(5, 4) // since symbol '+' is equal to the minus function // this results in 1 I've used functions quite liberally in these examples but I didn't describe them yet. A function in Lisp is just a data-type like an integer, a string, or a symbol. A function doesn't have a notion of a name like in Java or C++. Instead, it stands on its own. Effectively it is a pointer to a block of code along with some information (like a number of parameters it accepts). You only give the function a name by assigning it to a symbol, just like you assign an integer or a string. You can create a function by using a built in function for creating functions, assigned to a symbol 'fn'. Using pseudo-code: fn [a] { return *(a, 2); } This returns a function that takes a single parameter named 'a' and doubles it. Note that the function has no name but you can assign it to a symbol: set(times-two, fn [a] { return *(a, 2); }) We can now call this function: times-two(5) // returns 10 Now that we went over symbols and functions, what about lists? Well, you already know a lot about them. Lists are simply pieces of XML written in s-expression form. A list is specified by parentheses and contains Lisp data-types (including other lists) separated by a space. For example (this is real Lisp, note that we use semicolons for comments now): () ; an empty list (1) ; a list with a single element, 1 (1 &quot;test&quot;) ; a list with two elements ; an integer 1 and a string &quot;test&quot; (test &quot;hello&quot;) ; a list with two elements ; a symbol test and a string &quot;hello&quot; (test (1 2) &quot;hello&quot;) ; a list with three elements, a symbol test ; a list of two integers 1 and 2 ; and a string &quot;hello&quot; When a Lisp system encounters lists in the source code it acts exactly like Ant does when it encounters XML - it attempts to execute them. In fact, Lisp source code is only specified using lists, just like Ant source code is only specified using XML. Lisp executes lists in the following manner. The first element of the list is treated as the name of a function. The rest of the elements are treated as functions parameters. If one of the parameters is another list it is executed using the same principles and the result is passed as a parameter to the original function. That's it. We can write real code now: (* 3 4) ; equivalent to pseudo-code *(3, 4). ; Symbol '*' is a function ; 3 and 4 are its parameters. ; Returns 12. (times-two 5) ; returns 10 (3 4) ; error: 3 is not a function (times-two) ; error, times-two expects one parameter (times-two 3 4) ; error, times-two expects one parameter (set + -) ; sets symbol '+' to be equal to whatever symbol '-' ; equals to, which is a minus function (+ 5 4) ; returns 1 since symbol '+' is now equal ; to the minus function (* 3 (* 2 2)) ; multiplies 3 by the second parameter ; (which is a function call that returns 4). ; Returns 12. Note that so far every list we've specified was treated by a Lisp system as code. But how can we treat a list as data? Again, imagine an Ant task that accepts XML as one of its parameters. In Lisp we do this using a quote operator ' like so: (set test '(1 2)) ; test is equal to a list of two integers, 1 and 2 (set test (1 2)) ; error, 1 is not a function (set test '(* 3 4)) ; sets test to a list of three elements, ; a symbol *, an integer 3, and an integer 4 We can use a built in function head to return the first element of the list, and a built in function tail to return the rest of the list's elements: (head '(* 3 4)) ; returns a symbol '*' (tail '(* 3 4)) ; returns a list (3 4) (head (tail '( * 3 4))) ; (tail '(* 3 4)) returns a list (3 4) ; and (head '(3 4)) returns 3. (head test) ; test was set to a list in previous example ; returns a symbol '*' You can think of built in Lisp functions as you think of Ant tasks. The difference is that we don't have to extend Lisp in another language (although we can), we can extend it in Lisp itself as we did with the times-two example. Lisp comes with a very compact set of built in functions - the necessary minimum. The rest of the language is implemented as a standard library in Lisp itself. Lisp Macros So far we've looked at metaprogramming in terms of a simple templating engine similar to JSP. We've done code generation using simple string manipulations. This is generally how most code generation tools go about doing this task. But we can do much better. To get on the right track, let's start off with a question. How would we write a tool that automatically generates Ant build scripts by looking at source files in the directory structure? We could take the easy way out and generate Ant XML by manipulating strings. Of course a much more abstract, expressive and extensible way is to work with XML processing libraries to generate XML nodes directly in memory. The nodes can then be serialized to strings automatically. Furthermore, our tool would be able to analyze and transform existing Ant build scripts by loading them and dealing with the XML nodes directly. We would abstract ourselves from strings and deal with higher level concepts which let us get the job done faster and easier. Of course we could write Ant tasks that allow dealing with XML transformations and write our generation tool in Ant itself. Or we could just use Lisp. As we saw earlier, a list is a built in Lisp data structure and Lisp has a number of facilities for processing lists quickly and effectively (head and tail being the simplest ones). Additionally Lisp has no semantic constraints - you can have your code (and data) have any structure you want. Metaprogramming in Lisp is done using a construct called a &quot;macro&quot;. Let's try to develop a set of macros that transform data like, say, a to-do list (surprised?), into a language for dealing with to-do lists. Let's recall our to-do list example. The XML looks like this: &lt;todo name=&quot;housework&quot;&gt; &lt;item priority=&quot;high&quot;&gt;Clean the house.&lt;/item&gt; &lt;item priority=&quot;medium&quot;&gt;Wash the dishes.&lt;/item&gt; &lt;item priority=&quot;medium&quot;&gt;Buy more soap.&lt;/item&gt; &lt;/todo&gt; The corresponding s-expression version looks like this: (todo &quot;housework&quot; (item (priority high) &quot;Clean the house.&quot;) (item (priority medium) &quot;Wash the dishes.&quot;) (item (priority medium) &quot;Buy more soap.&quot;)) Suppose we're writing a to-do manager application. We keep our to-do items serialized in a set of files and when the program starts up we want to read them and display them to the user. How would we do this with XML and some other language (say, Java)? We'd parse our XML files with the to-do lists using some XML parser, write the code that walks the XML tree and converts it to a Java data structure (because frankly, processing DOM in Java is a pain in the neck), and then use this data structure to display the data. Now, how would we do the same thing in Lisp? If we were to adopt the same approach we'd parse the files using Lisp libraries responsible for parsing XML. The XML would then be presented to us as a Lisp list (an s-expression) and we'd walk the list and present relevant data to the user. Of course if we used Lisp it would make sense to persist the data as s-expressions directly as there's no reason to do an XML conversion. We wouldn't need special parsing libraries since data persisted as a set of s-expressions is valid Lisp and we could use Lisp compiler to parse it and store it in memory as a Lisp list. Note that Lisp compiler (much like .NET compiler) is available to a Lisp program at runtime. But we can do better. Instead of writing code to walk the s-expression that stores our data we could write a macro that allows us to treat data as code! How do macros work? Pretty simple, really. Recall that a Lisp function is called like this: (function-name arg1 arg2 arg3) Where each argument is a valid Lisp expression that's evaluated and passed to the function. For example if we replace arg1 above with (+ 4 5), it will be evaluated and 9 would be passed to the function. A macro works the same way as a function, except its arguments are not evaluated. (macro-name (+ 4 5)) In this case, (+ 4 5) is not evaluated and is passed to the macro as a list. The macro is then free to do what it likes with it, including evaluating it. The return value of a macro is a Lisp list that's treated as code. The original place with the macro is replaced with this code. For example, we could define a macro plus that takes two arguments and puts in the code that adds them. What does it have to do with metaprogramming and our to-do list problem? Well, for one, macros are little bits of code that generate code using a list abstraction. Also, we could create macros named to-do and item that replace our data with whatever code we like, for instance code that displays the item to the user. What benefits does this approach offer? We don't have to walk the list. The compiler will do it for us and will invoke appropriate macros. All we need to do is create the macros that convert our data to appropriate code! For example, a macro similar to our triple C macro we showed earlier looks like this: (defmacro triple (x) '(+ ~x ~x ~x)) The quote prevents evaluation while the tilde allows it. Now every time triple is encountered in lisp code: (triple 4) it is replaced with the following code: (+ 4 4 4) We can create macros for our to-do list items that will get called by lisp compiler and will transform the to-do list into code. Now our to-do list will be treated as code and will be executed. Suppose all we want to do is print it to standard output for the user to read: (defmacro item (priority note) '(block (print stdout tab &quot;Priority: &quot; ~(head (tail priority)) endl) (print stdout tab &quot;Note: &quot; ~note endl endl))) We've just created a very small and limited language for managing to-do lists embedded in Lisp. Such languages are very specific to a particular problem domain and are often referred to as domain specific languages or DSLs. Domain Specific Languages In this article we've already encountered two domain specific languages: Ant (specific to dealing with project builds) and our unnamed mini-language for dealing with to-do lists. The difference is that Ant was written from scratch using XML, an XML parser, and Java while our language is embedded into Lisp and is easily created within a couple of minutes. We've already discussed the benefits of DSLs, mainly why Ant is using XML, not Java source code. Lisp lets us create as many DSLs as we need for our problem. We can create domain specific languages for creating web applications, writing massively multiplayer games, doing fixed income trading, solving the protein folding problem, dealing with transactions, etc. We can layer these languages on top of each other and create a language for writing web-based trading applications by taking advantage of our web application language and bond trading language. Every day we'd reap the benefits of this approach, much like we reap the benefits of Ant. Using DSLs to solve problems results in much more compact, maintainable, flexible programs. In a way we create them in Java by creating classes that help us solve the problem. The difference is that Lisp allows us to take this abstraction to the next level: we're not limited by Java's parser. Think of writing build scripts in Java itself using some supporting library. Compare it to using Ant. Now apply this same comparison to every single problem you've ever worked on and you'll begin to glimpse a small share of the benefits offered by Lisp. What's next? Learning Lisp is an uphill battle. Even though in Computer Science terms Lisp is an ancient language, few people to date figured out how to teach it well enough to make it accessible. Despite great efforts by many Lisp advocates, learning Lisp today is still hard. The good news is that this won't remain the case forever since the amount of Lisp-related resources is rapidly increasing. Time is on Lisp's side. Lisp is a way to escape mediocrity and to get ahead of the pack. Learning Lisp means you can get a better job today, because you can impress any reasonably intelligent interviewer with fresh insight into most aspects of software engineering. It also means you're likely to get fired tomorrow because everyone is tired of you constantly mentioning how much better the company could be doing if only its software was written in Lisp. Is it worth the effort? Everyone who has ever learned Lisp says yes. The choice, of course, remains yours. Comments? Whew. That's enough. I've been writing this article, on and off, for months. If you find it interesting, have any questions, comments, or suggestions, please drop a note at coffeemug@gmail.com. I'll be glad to hear your feedback. [1]: I have never met James, nor does he know about my existence. The story is entirely fictional and is based on a few postings about Ant's history I found on the internet. [2]: Lisp has many different dialects (the most popular of which are Common Lisp and Scheme). Each dialect deals with intricate details differently yet shares the same set of basic principles. Since the goal of this article is to give you an understanding of Lisp's principles I will use Blaise for examples (which at the time of this writing is vaporware). With some minor modifications these examples can be translated to other Lisp dialects. ","link":"https://faded.auspicious.space/post/the-nature-of-lisp/"},{"title":"Amazon Web Services","content":" Amazon Web Services More often than not, I’m using Amazon Web Services (AWS) as my “cloud”. Not only for my own projects, but almost all customers I’m working for use Amazon for hosting their applications. So over time you build up a lot of experience on AWS service: you know how to (correctly) setup VPC’s, know when to you ECS, EC2 or lambda to host code and even services like S3, SNS and SQS pose no challenges anymore. But there are a lot of AWS services available. And I do mean: a LOT. Currently, there are 163 (!) different services that are available from the Amazon Dashboard, each with their own way of working, difficulties, catches and best practises. Discovering AWS You might realise that it’s probably near impossible to dive into each service and completely understand how they work and most likely, you don’t really need to know the exact ins and outs. But, having a basic understanding on each service can be a major benefit as a developer, architect or administrator. It makes it easier to see if there is an already existing solution for your problem at hand. So, I dove into each and every service to figure out what it exactly was for and how it works in the basics. I tried to experiment with as many components as possible (time and money permitting, I didn’t want to spend 15.000$ on AWS Data Exchange). I tried to capture what the service does in a single one-liner to give you a global overview. I think most of them are correct enough but if you have any suggestions or corrections, please tell me! ","link":"https://faded.auspicious.space/post/amazon-web-services/"},{"title":"软件设计，那些你不知道的事","content":" 软件设计，那些你不知道的事 代码质量和产出是衡量一个程序员是否优秀最直接的标准。如何提高代码质量和产出？这就要从软件重构和 review 入手。市面上有很多关于重构和 review 的书籍，但是看完之后，代码能力并不能立竿见影显著提升，只能帮助我们解决表面的 bug 和规范点，无法帮助我们发现更深层次的设计问题。 从设计角度来考虑 review，识别代码坏味道可以可以有效减少技术债务。技术债务是指有意或无意的做出错误的或非最优的设计决策所引发的债务。债务越积越多，最后只能重新彻底重构项目才能解决问题，这也叫做技术破产。如何解决技术债务问题，就要从根源上明确引起技术债务的重要的原因——设计坏味和重构认识不足。 首先要明确软件设计原则 抽象原则： 通过精简和概括来简化实体：精简指的是删除不必要的细节，概括是找出并定义重要的通用特征。 非循环依赖原则： 包之间的关系不可形成循环。 不自我重复原则： 在详细设计中，设计实体和代码和重复可能表现为类型名重复和实现重复。 封装原则： 通过隐藏抽象的实现细节和隐藏变化等方法实现关注点分离和信息隐藏。 信息隐藏原则： 找出棘手或可能变化的设计决策，并创建合适的模块或类型来对其他模块或类型隐藏这些决策。 保持简单原则： 简洁是软件系统设计的重要目标，应避免引入不必要的复杂性。 里氏替换原则： 所有的子类型都必须至少提供超类型承诺的行为且对每个超类型的引用都可替换成子类型实例。 层次接口原则： 使用分类、概括、替换、排序等方法以层次方式组织对抽象。 模块化原则： 通过集中和分解等手法创建高内聚、低耦合的抽象。 开闭原则： 类型应对扩展开放，对修改关闭。具体是模块应该能够在不修改代码情况下支持新需求。 单一职责原则： 绝不应有多个导致类需要修改的原因，如修改一个成员可能影响类的其他不相关职责，导致类难以维护。 变化封装原则： 倡导一种信息隐藏方式，建议将可能发生变化的概念封装起来。很多设计模式都体现了这种设计原则，如策略模式、桥梁模式、观察者模式。 我们从设计的角度来看代码时，要遵循六要素 可理解性： 代码理解起来的难易程度。 可修改性： 在修改既有功能时，不会导致连锁反应。 可扩展性： 支持新功能，不会导致连锁反应。 可重用性： 可以在代码的其他地方引用其一块代码。 可测试性： 项目要能够支持单元测试。 可靠性： 在正确地实现了功能的同时，也能够考虑各种异常情况如何容错。 了解完设计原则和六要素后，我们再来看设计坏味 抽象型 封装型 层次结构 模块化型 本文中每种坏味我们只选其中一例做具体说明。 抽象型坏味 抽象原则倡导通过精简和概括来简化实体：精简指的是删除不必要的细节，而概括指的是找出并定义通过的重要特征。交通标志是用于交流的抽象示例，而数字符号和编程语言是用于解决问题的抽象示例。 缺失抽象 命令式抽象 多方面的抽象 不必要的抽象 未用的抽象 重复的抽象 缺失抽象 使用一系列数据或者编码字符串，而不创建类或者接口时会产生这种坏味。 概念 应用抽象原则的一种实现手法是创建概念边界清晰，身份唯一的实体。由于没有创建抽象来表示实体，而是使用基本数据类型或编码字符串等原始数据来表示它，这违反了抽象原则，将这种坏味称为缺失抽象（Missing Abstraction）。不必要的抽象也违反了模块化原则。 潜在原因 未做重复的设计分析 未重构 错误的将重点放在细微的性能改善上 示例 在 JDK1.0 中方法 printStackTrace() 以字符串的方式将栈跟踪打印到标准错误流。 在需要以编程方式访问栈跟踪元素的客户程序中，必须要编程代码来获取数据，如行号等，由于客户程度依赖这种字符串格式，JDK 设计人员只能在后续版本中兼容这种格式了。 public class Throwable { public void printStackTrace() { printStackTrace(System.err); } } 重构建议 从 JDK1.4 起对 JAVA 的 API 进行了改进，StackTraceElement 类就是原来设计中缺失的对象。 public class Throwable { public void printStackTrace() { printStackTrace(System.err); } public StackTraceElement[] getStackTrace() { return getOurStackTrace().clone(); } } /** * @since 1.4 * @author Josh Bloch */ public final class StackTraceElement implements java.io.Serializable { // Normally initialized by VM (public constructor added in 1.5) private String declaringClass; private String methodName; private String fileName; private int lineNumber; } 别名 基本类型偏执：使用基本类型对日期、金额进行编码，而不创建类时，将引发这种坏味。 数据泥团：在很多地方同事使用一系列数据项，而不创建类时，将引发这种坏味。 现实考虑 避免过度设计:有时候，实体只是数据元素，没有任何相关联的行为。这种情况下使用类或者接口来表示它们可能导致过度设计。 封装型坏味 封装原则倡导通过隐藏抽象的实现细节和隐藏变化等手法实现关注的分离和信息隐藏。比如开车必须知道发动机原理吗？ 不充分的封装 泄露的封装 缺失封装 未利用封装 不充分的封装 对于抽象的一个或多个成员，声明的访问权限超过了实际需求时，将导致这种坏味。例如，将字段声明为公有的类就存在「不充分封装」坏味。 概念 封装的原则是将接口和实现分离，以便能独立修改。这种关注点分离，让客户程序只依赖抽象的接口，而对它们隐藏具体实现。修改实现不影响客户程序。对抽象的内部隐藏的不充分称为不充分的封装（Deficient Encapsulation）。 潜在原因 为方便测试 在面向对象中采用过程思维 快速交付 示例 来看看 java.lang.System，in、out、err 都被声明成 final，但可以通过java.lang.System 的 setIn、setOut、setErr 分别赋值。任何代码都能很方便的使用它们，比如 System.out.println()； PrintStream 是 Java 1.0 就有的，只支持 8 位的 ASCII 值，Java1.1 出的 PrintWriter 支持 Unicode，然而就是因为应用程序都能直接使用 PrintStream 来访问 PrintStream 的方法，根本不能摒弃 PrintStream 类。 public final static InputStream in = null; public final static PrintStream out = null; public final static PrintStream err = null; public static void setIn(InputStream in) { checkIO(); setIn0(in); } 重构建议 Java 1.6 引入了 java.io.Console 类，他提供了用于访问基于字符的控制台的方法。reader()、writer() 来获取 Console 相关的 Writer 和 Reader 对象。 别名 可隐藏的公有属性、方法 未封装的类 包含未参数化方法的类 现实考虑 嵌套或匿名类中过于宽松的访问性 性能考虑：比如前面说的 java.lang.System 模块化坏味 模块化原则倡导利用集中和分解等手法创建高内聚、低耦合的抽象。 拆散的模块化 不充分的模块化 循环依赖式模块化 轮毂式模块化 拆散的模块化 应集中放在一个抽象中的数据和方法分散在多个抽象中时，将导致这种坏味。表现为类被用作数据容器没有任何方法、类的方法更多的被其他类的成员调用。 概念 一种重要的模块化实现手法是「将相关的数据和方法集中在一起」。如果抽象中只包含数据成员，而操作这些数据成员的方法位于其他抽象中，它就违反了这种实现手法，存在「拆散的模块化」坏味。称为拆散的模块化（Broken Modularization）。 潜在原因 以过程思维使用面向对象语言 不熟悉既有设计 重构建议 对于包含大量数据类的过程型设计，可采用重构手法“将过程型设计转换为对象”。 别名 被动地存储数据的类 数据类 数据记录 记录类 数据容器 错位的操作 依恋情结 错位的控制 现实考虑 自动生成的代码 数据传输对象 层次型坏味 层次结构原则倡导采用分类、归并、替换和排序等手法以层次方式组织抽象。比如地球上的 870 万种生物。 缺失层次结构 不必要层次结构 为归并的层次结构 过宽的层次结构 凭空想象的层次结构 过深的层次结构 叛逆型层次结构 支离破碎的层次结构 多路径层次结构 循环层次结构 缺失的层次结构 代码片段使用条件逻辑来显式管理行为变化，而原本可以创建一个层次目录，并使用它来封装这些变化，会产生这种坏味。 概念 基于类型码的 switch 语句（或串接的 if-else 语句）是最著名的设计坏味之一。 使用类型码来处理行为变化表明没有进行有意义的分类，导致设计中缺少相应的层次结构。称为缺失层次结构（Missing Hierarchy）。 潜在原因 错误的采用过于简单的设计 过程型设计思维 忽视了继承也是一种设计手法 示例 串接的 if else 语句显示的检查类型 AbstractButton，JToolBar 和 JTextCompont并在各种条件下调用方法 getMargin()，这种造成的情况是将来可能在代码中的其他地方也会出现。 public Inserts getBorderInserts(Component c, Inserts inserts) { if(c instanceof AbstractButton) { margin = ((AbstractButton)c).getMargin(); } else if(c instanceof JToolBar) { margin = ((JToolBar)c).getMargin(); } else if(c instanceof JTextComponent) { margin = ((JTextComponent)c).getMargin(); } } 重构建议 如果条件检查中的多个实现调用方法相同，可引入相关的接口来抽象共同的协议。 如果代码中包含可转换为类的条件语句，可采用重构手法“提取层次结构”来创建一个类层次结构，其中每个类都表示条件检查中的一种情形。 别名 标记类 继承缺位 紧缩的类型层次结构 内嵌功能 现实考虑 与外部交互 ","link":"https://faded.auspicious.space/post/software-design-things-you-dont-know/"},{"title":"你可能会错过的 CSS 伪选择器","content":" [译]你可能会错过的 CSS 伪选择器 （伪）选择器可以为文档中不一定具体存在的结构指定样式，或者为某些元素、文档的标记模式、甚至是文档本身的状态所指示的幻像类指定样式。 —— CSS 权威指南：Eric Meyer、Estelle Weyl 这篇文章鼓励构造 UI 时使用更多纯 CSS 和更少的 JS。熟悉所有的 CSS 是实现这个目标的一种方法 —— 另一种是实施最佳实践和尽可能的减少代码。 ::first-line | 选择首行文本 这个伪元素选择器选择换行之前文本的首行。 p:first-line { color: lightcoral; } ::first-letter | 选择首字母 这个伪元素选择器应用于元素中文本的首字母。 .innerDiv p:first-letter { color: lightcoral; font-size: 40px } ::selection | 选择高亮（被选中）的区域 应用于任何被用户选中的高亮区域。 通过 ::selection 伪元素选择器，我们可以将样式应用于高亮区域。 div::selection { background: yellow; } :root | 根元素 :root 伪类选中文档的根元素。在 HTML 中，为 HTML 元素。在 RSS 中，则为 RSS 元素. 这个伪类选择器应用于根元素，多用于存储全局 CSS 自定义属性。 :empty | 仅当元素为空时触发 这个伪类选择器将选中没有任何子项的元素。该元素必须为空。如果一个元素没有空格、可见的内容、后代元素，则为空元素。 div:empty { border: 2px solid orange; } &lt;div&gt;&lt;/div&gt; &lt;div&gt;&lt;/div&gt; &lt;div&gt; &lt;/div&gt; 这个规则将应用于空的 div 元素。这个规则将应用于第一个和第二个 div，因为他们是真为空，而第三个 div 包含空格。 :only-child | 选择仅有的子元素 匹配父元素中没有任何兄弟元素的子元素。 .innerDiv p:only-child { color: orangered; } :first-of-type | 选择第一个指定类型的子元素 .innerDiv p:first-of-type { color: orangered; } 这将应用于 .innerDiv 下的第一个 p 元素。 &lt;div class=&quot;innerDiv&quot;&gt; &lt;div&gt;Div1&lt;/div&gt; &lt;p&gt;These are the necessary steps&lt;/p&gt; &lt;p&gt;hiya&lt;/p&gt; &lt;p&gt; Do &lt;em&gt;not&lt;/em&gt; push the brake at the same time as the accelerator. &lt;/p&gt; &lt;div&gt;Div2&lt;/div&gt; &lt;/div&gt; 这个 p（“These are the necessary step”）将被选中。 :last-of-type | 选择最后一个指定类型的子元素 像 :first-of-type 一样，但是会应用于最后一个同类型的子元素。 .innerDiv p:last-of-type { color: orangered; } 这将应用于 innerDiv 下的最后一个 p 段落元素。 &lt;div class=&quot;innerDiv&quot;&gt; &lt;p&gt;These are the necessary steps&lt;/p&gt; &lt;p&gt;hiya&lt;/p&gt; &lt;div&gt;Div1&lt;/div&gt; &lt;p&gt; Do the same. &lt;/p&gt; &lt;div&gt;Div2&lt;/div&gt; &lt;/div&gt; 因此，这个 p 元素（“Do the same”）将被选中。 :nth-of-type() | 选择特定类型的子元素 这个选择器将从指定的父元素的孩子列表中选择某种类型的子元素。 .innerDiv p:nth-of-type(1) { color: orangered; } :nth-last-of-type() | 选择列表末尾中指定类型的子元素 这将选择最后一个指定类型的子元素。 .innerDiv p:nth-last-of-type() { color: orangered; } 这将选择 innerDiv 列表元素中包含的最后一个段落类型子元素。 &lt;div class=&quot;innerDiv&quot;&gt; &lt;p&gt;These are the necessary steps&lt;/p&gt; &lt;p&gt;hiya&lt;/p&gt; &lt;div&gt;Div1&lt;/div&gt; &lt;p&gt; Do the same. &lt;/p&gt; &lt;div&gt;Div2&lt;/div&gt; &lt;/div&gt; innerDiv 中最后一个段落子元素 p（“Do the same”）将会被选中。 :link | 选择一个未访问过的超链接 这个选择器应用于未被访问过的链接。常用于带有 href 属性的 a 锚元素。 a:link { color: orangered; } &lt;a href=&quot;/login&quot;&gt;Login&lt;a&gt; 这将选中未被点击过带有 href 的指定界面的 a 锚点元素，选中的元素中的文字将会显示为橙色。 :checked | 选择一个选中的复选框 这个应用于已经被选中的复选框。 input:checked { border: 2px solid lightcoral; } 这个规则应用到所有被选中的复选框。 :valid | 选择一个通过验证的元素 这主要用于可视化表单元素，以让用户判断是否验证通过。验证通过时，默认元素带有 valid 属性。 input:valid { boder-color: lightsalmon; } :invalid | 选择一个未通过验证的元素 像 :valid 一样，但是会应用到未通过验证的元素。 input[type=&quot;text&quot;]:invalid { border-color: red; } :lang() | 选择指定语言的元素 应用于指定了语言的元素。 可以通过以下两种方式使用： p:lang(fr) { background: yellow; } 或者 p[lang|=&quot;fr&quot;] { background: yellow; } &lt;p lang=&quot;fr&quot;&gt;Paragraph 1&lt;/p&gt; :not() | 对于选择取反（这是一个运算符） 否定伪类选择器选中相反的。 让我们看一个示例： .innerDiv :not(p) { color: lightcoral; } &lt;div class=&quot;innerDiv&quot;&gt; &lt;p&gt;Paragraph 1&lt;/p&gt; &lt;p&gt;Paragraph 2&lt;/p&gt; &lt;div&gt;Div 1&lt;/div&gt; &lt;p&gt;Paragraph 3&lt;/p&gt; &lt;div&gt;Div 2&lt;/div&gt; &lt;/div&gt; Div 1 和 Div 2 会被选中，因为他们不是 p 元素。 ","link":"https://faded.auspicious.space/post/css-pseudo-classes-you-might-have-missed/"},{"title":"通过 JavaScript 获取内网 IP 和外网 IP 的简单方法","content":" 通过js获取内网ip和外网ip的简单方法 ... 获取内网 IP function getIP(callback) { let recode = {}; let RTCPeerConnection = window.RTCPeerConnection || window.mozRTCPeerConnection || window.webkitRTCPeerConnection; // 如果不存在则使用一个iframe绕过 if (!RTCPeerConnection) { // 因为这里用到了iframe，所以在调用这个方法的script上必须有一个iframe标签 // &lt;iframe id=&quot;iframe&quot; sandbox=&quot;allow-same-origin&quot; style=&quot;display:none;&quot;&gt;&lt;/iframe&gt; let win = iframe.contentWindow; RTCPeerConnection = win.RTCPeerConnection || win.mozRTCPeerConnection || win.webkitRTCPeerConnection; } //创建实例，生成连接 let pc = new RTCPeerConnection(); // 匹配字符串中符合ip地址的字段 function handleCandidate(candidate) { let ip_regexp = /([0-9]{1,3}(\\.[0-9]{1,3}){3}|([a-f0-9]{1,4}((:[a-f0-9]{1,4}){7}|:+[a-f0-9]{1,4}){6}))/; let ip_isMatch = candidate.match(ip_regexp)[1]; if (!recode[ip_isMatch]) { callback(ip_isMatch); recode[ip_isMatch] = true; } } //监听icecandidate事件 pc.onicecandidate = (ice) =&gt; { if (ice.candidate) { handleCandidate(ice.candidate.candidate); } }; //建立一个伪数据的通道 pc.createDataChannel(''); pc.createOffer((res) =&gt; { pc.setLocalDescription(res); }, () =&gt; { }); //延迟，让一切都能完成 setTimeout(() =&gt; { let lines = pc.localDescription.sdp.split('\\n'); lines.forEach(item =&gt; { if (item.indexOf('a=candidate:') === 0) { handleCandidate(item); } }) }, 1000); } 调用该函数： getIP(function (ip) { console.log(ip); }) // 192.168.1.191 // 2001::2841:aa90:2843:1983:e4d1:a9b8 获取公网 IP &lt;!--引入接口文件--&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;http://pv.sohu.com/cityjson?ie=utf-8&quot;&gt;&lt;/script&gt; &lt;!--返回结果为 var returnCitySN = {&quot;cip&quot;: &quot;27.46.86.71&quot;, &quot;cid&quot;: &quot;440000&quot;, &quot;cname&quot;: &quot;广东省&quot;}; 在下面的js中，通过调用returnCitySN.cip就可以获取 外网的ip--&gt; ","link":"https://faded.auspicious.space/post/a-simple-way-to-access-both- intranet-and-extranet-ip-through-javascript/"},{"title":"一篇文章掌握 14 种 UML 图","content":" UML科普文，一篇文章掌握14种UML图 什么是 UML？ UML 是 Unified Model Language 的缩写，中文是统一建模语言，是由一整套图表组成的标准化建模语言。 为什么要用 UML？ 通过使用 UML 使得在软件开发之前， 对整个软件设计有更好的可读性，可理解性，从而降低开发风险。同时，也能方便各个开发人员之间的交流。 UML 提供了极富表达能力的建模语言，可以让软件开发过程中的不同人员分别得到自己感兴趣的信息。 Page-Jones 在《Fundamental Object-Oriented Design in UML》 一书中总结了 UML 的主要目的，如下： 为用户提供现成的、有表现力的可视化建模语言，以便他们开发和交换有意义的模型。 为核心概念提供可扩展性（Extensibility）和特殊化（Specialization）机制。 独立于特定的编程语言和开发过程。 为了解建模语言提供一个正式的基础。 鼓励面向对象工具市场的发展。 支持更高层次的开发概念，如协作，框架，模式和组件。 整合最佳的工作方法（Best Practices）。 UML 图有哪些？ UML 图分为结构图和行为图。 结构图分为类图、轮廓图、组件图、组合结构图、对象图、部署图、包图。 行为图又分活动图、用例图、状态机图和交互图。 交互图又分为序列图、时序图、通讯图、交互概览图。 UML 图概览 UML 类型 目的 版本 类图 描述了系统中对象的类型以及他们之间存在的各种静态关系。 UML 1.x 组件图 描绘了系统中组件提供的、需要的接口、端口等，以及它们之间的关系。 UML 1.x，UML 2.0 重新定义了 对象图 对象图是类图的一个实例，是系统在某个时间点的详细状态的快照。 UML 1.x 轮廓图 轮廓图提供了一种通用的扩展机制，用于为特定域和平台定制 UML 模型。 UML 2.0 组合结构图 描述了一个“组合结构”的内部结构，以及它们之间的关系。 UML 2.0 部署图 描绘了系统在包层面上的结构设计。 UML 1.x 包图 描绘了系统在包层面上的结构设计。 UML 2.0 用例图 指由参与者、用例、边界以及它们之间的关系构成的用于描述系统功能的视图。 UML 1.x 活动图 描述了具体业务用例的实现流程。 UML 1.x 状态机图 描述了对象在它的整个生命周期里，响应不同事件时，执行相关事件的顺序。 UML 1.x 序列图 描述了在用例的特定场景中，对象如何与其他对象交互。 UML 1.x 时序图 时序图被用来显示随时间变化，一个或多个元素的值或状态的更改。 UML 2.0 交互概览图 交互概览图与活动图类似，但是它的节点是交互图。 UML 2.0 通讯图 描述了收发消息的对象的组织关系，强调对象之间的合作关系而不是时间顺序。 UML 1.x 叫协作图，UML 2.0 改名了 什么是类图？ 概念 类图是一切面向对象方法的核心建模工具。类图描述了系统中对象的类型以及它们之间存在的各种静态关系。 目的 用来表示类、接口以及它们之间的静态结构和关系。 关系 在类图中，常见的有以下几种关系。 泛化（Generalization） 【泛化关系】是一种继承关系，表示子类继承父类的所有特征和行为。 【箭头指向】带三角箭头的实线，箭头指向父类。 实现（Realization） 【实现关系】是一种类与接口的关系，表示类是接口所有特征和行为的实现。 【箭头指向】带三角箭头的虚线，箭头指向接口。 关联（Association） 【关联关系】是一种拥有关系，它使得一个类知道另一个类的属性和方法。 【代码体现】成员变量 【箭头指向】带普通箭头的实线，指向被拥有者。双向的关联可以有两个箭头，或者没有箭头。单向的关联有一个箭头。 自己买的车，想什么时候开就开。但是车是车，人是人，没有整体与部分的关系。 聚合（Aggregation） 【聚合关系】是一种整体与部分的关系。且部分可以离开整体而单独存在。聚合关系是关联关系的一种，是强的关联关系；关联和聚合在语法上无法区分，必须考察具体的逻辑关系。 【代码体现】成员变量 【箭头指向】带空心菱形的实线，空心菱形指向整体。 电脑有键盘才能输入信息，电脑是整体，键盘是部分，键盘也可以离开电脑，单纯的拿去敲。所以是聚合。 组合（Composition） 【组合关系】是一种整体与部分的关系。但部分不能离开整体而单独存在，组合关系是关联关系的一种，是比聚合关系还要强的关系。 【代码体现】成员变量 【箭头指向】带实心菱形和普通箭头的实线，实心菱形指向整体。 鸟是整体，翅膀是部分。鸟死了，翅膀也就不能飞了。所以是组合。我们再看一下，下面的一组经典的聚合组合关系的例子。 一个公司拥有多个部门，公司和部门之间是组合关系，公司破产了，部门就不复存在了。部门和员工是聚合关系，部门被裁掉，员工就换下家了。 依赖（Dependency） 【依赖关系】是一种使用关系，即一个类的实现需要另一个类的协助。 【箭头指向】带普通箭头的虚线，普通箭头指向被使用者。 老司机只管开车，车是谁的不重要，给什么车开什么车。 什么是组件图？ 【概念】描绘了系统中组件提供的、需要的接口、端口等，以及它们之间的关系。 【目的】用来展示各个组件之间的依赖关系。 订单系统组件依赖于客户资源库和库存系统组件。中间的虚线箭头表示依赖关系。另外两个符号，表示组件连接器，一个提供接口，一个需要接口。 什么是部署图？ 【概念】描述了系统内部的软件如何分布在不同的节点上。 【目的】用来表示软件和硬件的映射关系。 图中简单的表示，不同机器上面部署的不同软件。 什么是对象图？ 【概念】对象图是类图的一个实例，是系统在某个时间点的详细状态的快照。 【目的】用来表示两个或者多个对象之间在某一时刻之间的关系。 图中就是描述的，某时间点 bat 这个公司有一个研发部，一个销售部，两个部门只有一个人 iisheng。 什么是包图？ 【概念】描绘了系统在包层面上的结构设计。 【目的】用来表示包和包之间的依赖关系。 《Use》关系表示使用依赖，Web Shopping 依赖 Payment 《Merge》关系表示合并，Web Shopping 合并了 Shopping Cart 就拥有了 Shopping Cart 的功能 《Access》关系表示私有引入，比如代码中的指定包名类名 《Import》关系表示公共引入，比如 Java 中的 import 之后，就可以直接使用 import 包中的类了。 什么是组合结构图？ 【概念】描述了一个“组合结构”的内部结构，以及他们之间的关系。这个“组合结构”可以是系统的一部分，或者一个整体。 【目的】用来表示系统中逻辑上的“组合结构”。 图中描述了 Car 是由车轴连接着的两个前面轮子、两个后面轮子，和引擎组合的。 什么是轮廓图？ 【概念】轮廓图提供了一种通用的扩展机制，用于为特定域和平台定制 UML 模型。 【目的】用于在特定领域中构建 UML 模型。 图中我们定义了一个简易的 EJB 的概要图。Bean 是从 Component 扩展来的。Entity Bean 和 Session Bean 继承了 Bean。EJB 拥有 Remote 和 Home 接口，和 JAR 包。 什么是用例图？ 【概念】用例图是指由参与者、用例，边界以及它们之间的关系构成的用于描述系统功能的视图。 【目的】用来描述整个系统的功能。 用例图中包含以下三种关系： 包含关系使用符号《include》，想要查看订单列表，前提是需要先登录。 扩展关系使用符号《extend》，基于查询订单列表的功能，可以增加一个导出数据的功能 泛化关系，子用例继承父用例所有结构、行为和关系。 什么是活动图？ 【概念】描述了具体业务用例的实现流程。 【目的】用来表示用例实现的工作流程。 图中简单描述了，从开始到登录到查看订单列表，或者登录失败直接结束。 什么是状态机图？ 【概念】状态机图对一个单独对象的行为建模，指明对象在它的整个生命周期里，响应不同事件时，执行相关事件的顺序。 【目的】用来表示指定对象，在整个生命周期，响应不同事件的不同状态。 图中描述了，门在其生命周期内所经历的状态。 什么是序列图？ 【概念】序列图根据时间序列展示对象如何进行协作。它展示了在用例的特定场景中，对象如何与其他对象交互。 【目的】通过描述对象之间发送消息的时间顺序显示多个对象之间的动态协作。 图中展示的是支付宝条码支付场景的序列图。其中，loop 是循环，alt 是选择，序列图的其他关系这里就不介绍了。 什么是通讯图？ 【概念】描述了收发消息的对象的组织关系，强调对象之间的合作关系而不是时间顺序。 【目的】用来显示不同对象的关系。 图中展示了一个线上书店的通讯图，方框和小人表示生命线，不同生命线之间可以传递消息，消息前面的数字可以表达序列顺序。 什么是交互概览图？ 【概念】交互概览图与活动图类似，但是它的节点是交互图。 【目的】提供了控制流的概述。 图中表示一个调度系统的交互概览图，跟活动图很像。其中 sd 的框代表具体的交互流程，ref 框代表使用交互。 什么是时序图？ 【概念】时序图被用来显示随时间变化，一个或多个元素的值或状态的更改。也显示时控事件之间的交互和管理它们的时间和期限约束。 【目的】用来表示元素状态或者值随时间的变化而变化的视图。 图中展示了老年痴呆病人随着时间的变化病情的变化。 总结 学习 UML，我们没必要纠结比如像聚合关系是带箭头还是不带箭头，这样的问题。更重要的是 UML 图所给我们带来的画图思想，让我们画 UML 图或者其他图能让其他人更好的理解我们的设计思想。 参考文献： [1]:《Learning UML 2.0》 [2]: https://www.uml-diagrams.org/ [3]: https://www.visual-paradigm.com/guide/ [4]: https://sparxsystems.com/resources/tutorials/ ","link":"https://faded.auspicious.space/post/mastering-14-uml-diagrams-in-a-single-article/"},{"title":"关于数据库使用和开发的一些建议","content":" https://medium.com/@rakyll/things-i-wished-more-developers-knew-about-databases-2d0178464f78 You are lucky if 99.999% of the time network is not a problem. ACID has many meanings. Each database has different consistency and isolation capabilities. Optimistic locking is an option when you can't hold a lock. There are anomalies other than dirty reads and data loss. My database and I don't always agree on ordering. Application-level sharding can live outside the application. AUTOINCREMENT'ing can be harmful. Stale data can be useful and lock-free. Clock skews happen between any clock sources. Latency has many meanings. Evaluate performance requirements per transaction. Nested transactions can be harmful. Transactions shouldn't maintain application state. Query planners can tell a lot about databases. Online migrations are complex but possible. Significant database growth introduces unpredictability. ","link":"https://faded.auspicious.space/post/things-i-wished-more-developers-knew-about-databases/"},{"title":"A letter to myself as a fresh software engineer","content":" A letter to myself as a fresh software engineer Dear Self, You just graduated and you are ready to start your career in the IT field. I cannot spoiler anything, but I assure you it will be an interesting ride. I'm writing you this letter because I want to give you some advice that will help you be a better professional. Nothing you won't learn by yourself in the next few years, but it is something that I wish someone had told me when I started my career. They are not ordered by any means and are all equally important. Run a marathon, not a sprint. The road to becoming a good software engineer is a long one. Don't rush on stuff, and don't give up just because you are not getting an easy and fast win. Take your time to learn and become good in the topics you are interested in. Remember that this is a marathon, not a sprint. Be humble, not stupid. It is good — sorry, it is fundamental — to be humble. There is always something to learn from others, even when you are an experienced professional. But this doesn't mean that everyone is better than you. You have to respect yourself and your skills. When you don't respect yourself you become stupid, not humble. Compare with yourself, not others. There is no point in comparing yourself with others. There will always be someone better than you in your job. And there will always be someone better than the one that is better than you. And there will… ok, you got the point. Just do your best. If you think someone is a better engineer than you are, learn from him/her. Keep doing your best, and eventually, you will be a reference for someone else. Respect people, not titles. During your career, you will work with exceptional professional. Most important, you will meet exceptional human beings. Respect people for who they are, not for the title they have. If foo is &quot;Principal Senior Lead Engineering Chief Architect&quot; doesn't mean that he deserves more respect than bar that is a junior software developer. Choose the challenge, not comfort. The road will be full of crossroads. There may be multiple choices, but everything boils down to a choice between your comfort zone, or go outside your comfort zone. There may be a moment in your life — hopefully after decades of work — when you will feel the need to cool down a bit because you will be satisfied with what you achieved. Until that moment, try to go out of your comfort zone. It will make you a better professional and you will feel more satisfied with your career. Remember that the best things often happen outside the comfort zone. Jump on the whiteboard, not on the keyboard. When you have to design a new feature or a new system, don't jump on the keyboard to start coding. The &quot;muscle&quot; you have to train and use as an engineer is your brain, not your fingers. Always think before act. For this reason, jump on the whiteboard instead of the keyboard, and start thinking of what you should implement. Better if you have a sparring partner to challenge your thoughts. Oh, when I say &quot;the whiteboard&quot; I mean &quot;every object that can help you think&quot;, be it pen and paper, a notebook application, draw.io, etc. Deliver value, not code. Please don't be affected by the NIH syndrome. There is no point in reinventing the wheel. Avoid wasting time in something that is already out there. If you can achieve your goal simply glueing together some tools, just do it. What you should deliver as a software engineer is value to your business, not lines of code. Choose life, not work. In the IT field, it is easy to focus too much on work. After all, for most of us, it is not just a job, it is passion. Remember that work is important, but life is more. Live a meaningful and rich life. Play sports, read books, find hobbies, travel and see the beautiful world we are living in. Hangout with friends, find a partner for your life and give to your partner all the love, attention, and support that you can. You'll be surprised how much having a rich life will improve you as a professional. That's all I can tell you right now. I still have a lot to learn. One last thing: enjoy the ride! 🚀 With love, (a more experienced) You. ","link":"https://faded.auspicious.space/post/a-letter-to-myself-as-a-fresh-software-engineer/"},{"title":"SQLite vs MySQL vs PostgreSQL: A Comparison Of Relational Database Management Systems","content":" SQLite vs MySQL vs PostgreSQL: A Comparison Of Relational Database Management Systems Introduction The relational data model, which organizes data in tables of rows and columns, predominates in database management tools. Today there are other data models, including NoSQL and NewSQL, but relational database management systems (RDBMSs) remain dominant for storing and managing data worldwide. This article compares and contrasts three of the most widely implemented open-source RDBMSs: SQLite, MySQL, and PostgreSQL. Specifically, it will explore the data types that each RDBMS uses, their advantages and disadvantages, and situations where they are best optimized. A Bit About Database Management Systems Databases are logically modelled clusters of information, or data. A database management system (DBMS), on the other hand, is a computer program that interacts with a database. A DBMS allows you to control access to a database, write data, run queries, and perform any other tasks related to database management. Although database management systems are often referred to as &quot;databases&quot;, the two terms are not interchangeable. A database can be any collection of data, not just one stored on a computer, while a DBMS is the software that allows you to interact with a database. All database management systems have an underlying model that structures how data is stored and accessed. A relational database management system is a DBMS that employs the relational data model. In this model, data are organized into tables, which in the context of RDBMSs are more formally referred to as relations. A relation is a set of tuples, or rows in a table, with each tuple sharing a set of attributes, or columns: Most relational databases use structured query language (SQL) to manage and query data. However, many RDBMSs use their own particular dialect of SQL, which may have certain limitations or extensions. These extensions typically include extra features that allow users to perform more complex operations than they otherwise could with standard SQL. Note: The term &quot;standard SQL&quot; comes up several times throughout this guide. SQL standards are jointly maintained by the American National Standards Institute (ANSI), the International Organization for Standardization (ISO), and the International Electrotechnical Commission (IEC). Whenever this article mentions &quot;standard SQL&quot; or &quot;the SQL standard&quot;, it's referring to the current version of the SQL standard published by these bodies. It should be noted that the full SQL standard is large and complex: full core SQL:2011 compliance requires 179 features. Because of this, most RDBMSs don't support the entire standard, although some do come closer to full compliance than others. Each column is assigned a data type which dictates what kind of entries are allowed in that column. Different RDBMSs implement different data types, which aren't always directly interchangeable. Some common data types include dates, strings, integers, and Booleans. Numeric data types can either be signed, meaning they can represent both positive and negative numbers, or unsigned, which means they can only represent positive numbers. For example, MySQL's tinyint data type can hold 8 bits of data, which equates to 256 possible values. The signed range of this data type is from -128 to 127, while the unsigned range is from 0 to 255. Sometimes, a database administrator will impose a constraint on a table to limit what values can be entered into it. A constraint typically applies to one particular column, but some constraints can also apply to an entire table. Here are some constraints that are commonly used in SQL: UNIQUE: Applying this constraint to a column ensures that no two entries in that column are identical. NOT NULL: This constraint ensures that a column doesn't have any NULL entries. PRIMARY KEY: A combination of UNIQUE and NOT NULL, the PRIMARY KEY constraint ensures that no entry in the column is NULL and that every entry is distinct. FOREIGN KEY: A FOREIGN KEY is a column in one table that refers to the PRIMARY KEY of another table. This constraint is used to link two tables together: entries to the FOREIGN KEY column must already exist in the parent PRIMARY KEY column for the write process to succeed. CHECK: This constraint limits the range of values that can be entered into a column. For example, if your application is intended only for residents of Alaska, you could add a CHECK constraint on a ZIP code column to only allow entries between 99501 and 99950. DEFAULT: This provides a default value for a given column. Unless another value is specified, SQLite enters the default value automatically. INDEX: Used to help retrieve data from a table more quickly, this constraint is similar to an index in a textbook: instead of having to review every entry in a table, a query only has to review entries from the indexed column to find the desired results. If you'd like to learn more about database management systems, check out our article on Understanding SQL and NoSQL Databases and Different Database Models. Now that we've covered relational database management systems generally, let's move onto the first of the three open-source relational databases this article will cover: SQLite. SQLite SQLite is a self-contained, file-based, and fully open-source RDBMS known for its portability, reliability, and strong performance even in low-memory environments. Its transactions are ACID-compliant, even in cases where the system crashes or undergoes a power outage. The SQLite project's website describes it as a &quot;serverless&quot; database. Most relational database engines are implemented as a server process in which programs communicate with the host server through an interprocess communication that relays requests. With SQLite, though, any process that accesses the database reads from and writes to the database disk file directly. This simplifies SQLite's setup process, since it eliminates any need to configure a server process. Likewise, there's no configuration necessary for programs that will use the SQLite database: all they need is access to the disk. SQLite is free and open-source software, and no special license is required to use it. However, the project does offer several extensions — each for a one-time fee — that help with compression and encryption. Additionally, the project offers various commercial support packages, each for an annual fee. SQLite's Supported Data Types SQLite allows a variety of data types, organized into the following storage classes: Data Type Explanation null Includes any NULL values. integer Signed integers, stored in 1, 2, 3, 4, 6, or 8 bytes depending on the magnitude of the value. real Real numbers, or floating point values, stored as 8-byte floating point numbers. text Text strings stored using the database encoding, which can either be UTF-8, UTF-16BE or UTF-16LE. blob Any blob of data, with every blob stored exactly as it was input. In the context of SQLite, the terms &quot;storage class&quot; and &quot;data type&quot; are considered interchangeable. If you'd like to learn more about SQLite's data types and SQLite type affinity, check out SQLite's official documentation on the subject. Advantages of SQLite Small footprint: As its name implies, the SQLite library is very lightweight. Although the space it uses varies depending on the system where it's installed, it can take up less than 600KiB of space. Additionally, it's fully self-contained, meaning there aren't any external dependencies you have to install on your system for SQLite to work. User-friendly: SQLite is sometimes described as a &quot;zero-configuration&quot; database that's ready for use out of the box. SQLite doesn't run as a server process, which means that it never needs to be stopped, started, or restarted and doesn't come with any configuration files that need to be managed. These features help to streamline the path from installing SQLite to integrating it with an application. Portable: Unlike other database management systems, which typically store data as a large batch of separate files, an entire SQLite database is stored in a single file. This file can be located anywhere in a directory hierarchy, and can be shared via removable media or file transfer protocol. Disadvantages of SQLite Limited concurrency: Although multiple processes can access and query an SQLite database at the same time, only one process can make changes to the database at any given time. This means SQLite supports greater concurrency than most other embedded database management systems, but not as much as client/server RDBMSs like MySQL or PostgreSQL. No user management: Database systems often come with support for users, or managed connections with predefined access privileges to the database and tables. Because SQLite reads and writes directly to an ordinary disk file, the only applicable access permissions are the typical access permissions of the underlying operating system. This makes SQLite a poor choice for applications that require multiple users with special access permissions. Security: A database engine that uses a server can, in some instances, provide better protection from bugs in the client application than a serverless database like SQLite. For example, stray pointers in a client cannot corrupt memory on the server. Also, because a server is a single persistent process, a client-server database cancontrol data access with more precision than a serverless database, allowing for more fine-grained locking and better concurrency. When To Use SQLite Embedded applications: SQLite is a great choice of database for applications that need portability and don't require future expansion. Examples include single-user local applications and mobile applications or games. Disk access replacement: In cases where an application needs to read and write files to disk directly, it can be beneficial to use SQLite for the additional functionality and simplicity that comes with using SQL. Testing: For many applications it can be overkill to test their functionality with a DBMS that uses an additional server process. SQLite has an in-memory mode which can be used to run tests quickly without the overhead of actual database operations, making it an ideal choice for testing. When Not To Use SQLite Working with lots of data: SQLite can technically support a database up to 140TB in size, as long as the disk drive and filesystem also support the database's size requirements. However, the SQLite website recommends that any database approaching 1TB be housed on a centralized client-server database, as an SQLite database of that size or larger would be difficult to manage. High write volumes: SQLite allows only one write operation to take place at any given time, which significantly limits its throughput. If your application requires lots of write operations or multiple concurrent writers, SQLite may not be adequate for your needs. Network access is required: Because SQLite is a serverless database, it doesn't provide direct network access to its data. This access is built into the application, so if the data in SQLite is located on a separate machine from the application it will require a high bandwidth engine-to-disk link across the network. This is an expensive, inefficient solution, and in such cases a client-server DBMS may be a better choice. MySQL According to the DB-Engines Ranking, MySQL has been the most popular open-source RDBMS since the site began tracking database popularity in 2012. It is a feature-rich product that powers many of the world's largest websites and applications, including Twitter, Facebook, Netflix, and Spotify. Getting started with MySQL is relatively straightforward, thanks in large part to its exhaustive documentation and large community of developers, as well as the abundance of MySQL-related resources online. MySQL was designed for speed and reliability, at the expense of full adherence to standard SQL. The MySQL developers continually work towards closer adherence to standard SQL, but it still lags behind other SQL implementations. It does, however, come with various SQL modes and extensions that bring it closer to compliance. Unlike applications using SQLite, applications using a MySQL database access it through a separate daemon process. Because the server process stands between the database and other applications, it allows for greater control over who has access to the database. MySQL has inspired a wealth of third-party applications, tools, and integrated libraries that extend its functionality and help make it easier to work with. Some of the more widely-used of these third-party tools are phpMyAdmin, DBeaver, and HeidiSQL. MySQL's Supported Data Types MySQL's data types can be organized into three broad categories: numeric types, date and time types, and string types. Numeric types: Data Type Explanation tinyint A very small integer. The signed range for this numeric data type is -128 to 127, while the unsigned range is 0 to 255. smallint A small integer. The signed range for this numeric type is -32768 to 32767, while the unsigned range is 0 to 65535. mediumint A medium-sized integer. The signed range for this numeric data type is -8388608 to 8388607, while the unsigned range is 0 to 16777215. int or integer A normal-sized integer. The signed range for this numeric data type is -2147483648 to 2147483647, while the unsigned range is 0 to 4294967295. bigint A large integer. The signed range for this numeric data type is -9223372036854775808 to 9223372036854775807, while the unsigned range is 0 to 18446744073709551615. float A small (single-precision) floating-point number. double, double precision, or real A normal sized (double-precision) floating-point number. dec, decimal, fixed, or numeric A packed fixed-point number. The display length of entries for this data type is defined when the column is created, and every entry adheres to that length. bool or boolean A Boolean is a data type that only has two possible values, usually either true or false. bit A bit value type for which you can specify the number of bits per value, from 1 to 64. Date and time types: Data Type Explanation date A date, represented as YYYY-MM-DD. datetime A timestamp showing the date and time, displayed as YYYY-MM-DD HH:MM:SS. timestamp A timestamp indicating the amount of time since the Unix epoch (00:00:00 on January 1, 1970). time A time of day, displayed as HH:MM:SS. year A year expressed in either a 2 or 4 digit format, with 4 digits being the default. String types: Data Type Explanation char A fixed-length string; entries of this type are padded on the right with spaces to meet the specified length when stored. varchar A string of variable length. binary Similar to the char type, but a binary byte string of a specified length rather than a nonbinary character string. varbinary Similar to the varchar type, but a binary byte string of a variable length rather than a nonbinary character string. blob A binary string with a maximum length of 65535 (2^16 - 1) bytes of data. tinyblob A blob column with a maximum length of 255 (2^8 - 1) bytes of data. mediumblob A blob column with a maximum length of 16777215 (2^24 - 1) bytes of data. longblob A blob column with a maximum length of 4294967295 (2^32 - 1) bytes of data. text A string with a maximum length of 65535 (2^16 - 1) characters. tinytext A text column with a maximum length of 255 (2^8 - 1) characters. mediumtext A text column with a maximum length of 16777215 (2^24 - 1) characters. longtext A text column with a maximum length of 4294967295 (2^32 - 1) characters. enum An enumeration, which is a string object that takes a single value from a list of values that are declared when the table is created. set Similar to an enumeration, a string object that can have zero or more values, each of which must be chosen from a list of allowed values that are specified when the table is created. Advantages of MySQL Popularity and ease of use: As one of the world's most popular database systems, there's no shortage of database administrators who have experience working with MySQL. Likewise, there's an abundance of documentation in print and online on how to install and manage a MySQL database, as well as a number of third-party tools — such as phpMyAdmin — that aim to simplify the process of getting started with the database. Security: MySQL comes installed with a script that helps you to improve the security of your database by setting the installation's password security level, defining a password for the root user, removing anonymous accounts, and removing test databases that are, by default, accessible to all users. Also, unlike SQLite, MySQL does support user management and allows you to grant access privileges on a user-by-user basis. Speed: By choosing not to implement certain features of SQL, the MySQL developers were able to prioritize speed. While more recent benchmark tests show that other RDBMSs like PostgreSQL can match or at least come close to MySQL in terms of speed, MySQL still holds a reputation as an exceedingly fast database solution. Replication: MySQL supports a number of different types of replication, which is the practice of sharing information across two or more hosts to help improve reliability, availability, and fault-tolerance. This is helpful for setting up a database backup solution or horizontally scaling one's database. Disadvantages of MySQL Known limitations: Because MySQL was designed for speed and ease of use rather than full SQL compliance, it comes with certain functional limitations. For example, it lacks support for FULL JOIN clauses. Licensing and proprietary features: MySQL is dual-licensed software, with a free and open-source community edition licensed under GPLv2 and several paid commercial editions released under proprietary licenses. Because of this, some features and plugins are only available for the proprietary editions. Slowed development: Since the MySQL project was acquired by Sun Microsystems in 2008, and later by Oracle Corporation in 2009, there have been complaints from users that the development process for the DBMS has slowed down significantly, as the community no longer has the agency to quickly react to problems and implement changes. When To Use MySQL Distributed operations: MySQL's replication support makes it a great choice for distributed database setups like primary-secondary or primary-primary architectures. Websites and web applications: MySQL powers many websites and applications across the internet. This is, in large part, thanks to how easy it is to install and set up a MySQL database, as well as its overall speed and scalability in the long run. Expected future growth: MySQL's replication support can help facilitate horizontal scaling. Additionally, it's a relatively straightforward process to upgrade to a commercial MySQL product, like MySQL Cluster, which supports automatic sharding, another horizontal scaling process. When Not To Use MySQL SQL compliance is necessary: Since MySQL does not try to implement the full SQL standard, this tool is not completely SQL compliant. If complete or even near-complete SQL compliance is a must for your use case, you may want to use a more fully compliant DBMS. Concurrency and large data volumes: Although MySQL generally performs well with read-heavy operations, concurrent read-writes can be problematic. If your application will have many users writing data to it at once, another RDBMS like PostgreSQL might be a better choice of database. PostgreSQL PostgreSQL, also known as Postgres, bills itself as &quot;the most advanced open-source relational database in the world&quot;. It was created with the goal of being highly extensible and standards compliant. PostgreSQL is an object-relational database, meaning that although it's primarily a relational database it also includes features — like table inheritance and function overloading — that are more often associated with object databases. Postgres is capable of efficiently handling multiple tasks at the same time, a characteristic known as concurrency. It achieves this without read locks thanks to its implementation of Multiversion Concurrency Control (MVCC), which ensures the atomicity, consistency, isolation, and durability of its transactions, also known as ACID compliance. PostgreSQL isn't as widely used as MySQL, but there are still a number of third-party tools and libraries designed to simplify working with with PostgreSQL, including pgAdmin and Postbird. PostgreSQL's Supported Data Types PostgreSQL supports numeric, string, and date and time data types like MySQL. In addition, it supports data types for geometric shapes, network addresses, bit strings, text searches, and JSON entries, as well as several idiosyncratic data types. Numeric types: Data Type Explanation bigint A signed 8 byte integer. bigserial An autoincrementing 8 byte integer. double precision An 8 byte double precision floating-point number. integer A signed 4 byte integer. numeric or decimal An number of selectable precision, recommended for use in cases where exactness is crucial, such as monetary amounts. real A 4 byte single precision floating-point number. smallint A signed 2 byte integer. smallserial An autoincrementing 2 byte integer. serial An autoincrementing 4 byte integer. Character types: Data Type Explanation character A character string with a specified fixed length. character varying or varchar A character string with a variable but limited length. text A character string of a variable, unlimited length. Date and time types: Data Type Explanation date A calendar date consisting of the day, month, and year. interval A time span. time or time without time zone A time of day, not including the time zone. time with time zone A time of day, including the time zone. timestamp or timestamp without time zone A date and time, not including the time zone. timestamp with time zone A date and time, including the time zone. Geometric types: Data Type Explanation box A rectangular box on a plane. circle A circle on a plane. line An infinite line on a plane. lseg A line segment on a plane. path A geometric path on a plane. point A geometric point on a plane. polygon A closed geometric path on a plane. Network address types: Data Type Explanation cidr An IPv4 or IPv6 network address. inet An IPv4 or IPv6 host address. macaddr A Media Access Control (MAC) address. Bit string types: Data Type Explanation bit A fixed-length bit string. bit varying A variable-length bit string. Text search types: Data Type Explanation tsquery A text search query. tsvector A text search document. JSON types: Data Type Explanation json Textual JSON data. jsonb Decomposed binary JSON data. Other data types: Data Type Explanation boolean A logical Boolean, representing either true or false. bytea Short for &quot;byte array&quot;, this type is used for binary data. money An amount of currency. pg_lsn A PostgreSQL Log Sequence Number. txid_snapshot A user-level transaction ID snapshot. uuid A universally unique identifier. xml XML data. Advantages of PostgreSQL SQL compliance: More so than SQLite or MySQL, PostgreSQL aims to closely adhere to SQL standards. According to the official PostgreSQL documentation, PostgreSQL supports 160 out of the 179 features required for full core SQL:2011 compliance, in addition to a long list of optional features. Open-source and community-driven: A fully open-source project, PostgreSQL's source code is developed by a large and devoted community. Similarly, the Postgres community maintains and contributes to numerous online resources that describe how to work with the DBMS, including the official documentation, the PostgreSQL wiki, and various online forums. Extensible: Users can extend PostgreSQL programmatically and on the fly through its catalog-driven operation and its use of dynamic loading. One can designate an object code file, such as a shared library, and PostgreSQL will load it as necessary. Disadvantages of PostgreSQL Memory performance: For every new client connection, PostgreSQL forks a new process. Each new process is allocated about 10MB of memory, which can add up quickly for databases with lots of connections. Accordingly, for simple read-heavy operations, PostgreSQL is typically less performant than other RDBMSs, like MySQL. Popularity: Although more widely used in recent years, PostgreSQL historically lagged behind MySQL in terms of popularity. One consequence of this is that there are still fewer third-party tools that can help to manage a PostgreSQL database. Similarly, there aren't as many database administrators with experience managing a Postgres database compared to those with MySQL experience. When To Use PostgreSQL Data integrity is important: PostgreSQL has been fully ACID-compliant since 2001 and implements multiversion currency control to ensure that data remains consistent, making it a strong choice of RDBMS when data integrity is critical. Integration with other tools: PostgreSQL is compatible with a wide array of programming languages and platforms. This means that if you ever need to migrate your database to another operating system or integrate it with a specific tool, it will likely be easier with a PostgreSQL database than with another DBMS. Complex operations: Postgres supports query plans that can leverage multiple CPUs in order to answer queries with greater speed. This, coupled with its strong support for multiple concurrent writers, makes it a great choice for complex operations like data warehousing and online transaction processing. When Not To Use PostgreSQL Speed is imperative: At the expense of speed, PostgreSQL was designed with extensibility and compatibility in mind. If your project requires the fastest read operations possible, PostgreSQL may not be the best choice of DBMS. Simple setups: Because of its large feature set and strong adherence to standard SQL, Postgres can be overkill for simple database setups. For read-heavy operations where speed is required, MySQL is typically a more practical choice. Complex replication: Although PostgreSQL does provide strong support for replication, it's still a relatively new feature and some configurations — like a primary-primary architecture — are only possible with extensions. Replication is a more mature feature on MySQL and many users see MySQL's replication to be easier to implement, particularly for those who lack the requisite database and system administration experience. Conclusion Today, SQLite, MySQL, and PostgreSQL are the three most popular open-source relational database management systems in the world. Each has its own unique features and limitations, and excels in particular scenarios. There are a quite a few variables at play when deciding on an RDBMS, and the choice is rarely as simple as picking the fastest one or the one with the most features. The next time you're in need of a relational database solution, be sure to research these and other tools in depth to find the one that best suits your needs. If you'd like to learn more about SQL and how to use it to manage a relational database, we encourage you to refer to our How To Manage an SQL Database cheat sheet. On the other hand, if you'd like to learn about non-relational (or NoSQL) databases, check out our Comparison Of NoSQL Database Management Systems. ","link":"https://faded.auspicious.space/post/sqlite-vs-mysql-vs-postgresql-a-comparison-of-relational-database-management-systems/"},{"title":"innodb 逻辑存储结构","content":" MySQL相关（番外篇）- innodb 逻辑存储结构 前言 前面已经写了有两篇章长度的文章，第三篇我一直在寻思着要写什么（其实并没有），按照脑图来的话，这篇文章我们该来讲讲关于索引的知识了，这可是 MySQL 性能优化很关键的知识点，千万千万不要错过，不过我这里会相对比较深入地探究，相信大家读完之后多少会有点收获。 先送上两张飞机票🛬还没读过前面文章的伙伴可以先前往阅读，由浅入深： MySQL相关（一）- 一条查询语句是如何执行的 MySQL相关（二）- 一条更新语句是如何执行的 由于索引的知识点比较多，官网的内容也很多，如果大家想详细了解可以到官网，想先通读了解的话可以先看看我对索引的总结，这一章节分为三部分来讲： innodb 逻辑存储结构需要了解，作为番外篇 MySQL相关（番外篇）- innodb 逻辑存储结构； 索引的数据结构也作为另外的篇章，通过对查询算法的数据模型进行演算分析 MySQL相关（三）- 索引数据模型推演及 B+Tree 的详细介绍； 对索引的使用及优化规则也会作为单独的篇章 MySQL相关（四）- 性能优化关键点索引 正文 innodb 逻辑存储结构 https://dev.mysql.com/doc/refman/5.7/en/innodb-disk-management.html https://dev.mysql.com/doc/refman/5.7/en/innodb-file-space.html MySQL 的存储结构分为 5 级：表空间、段、簇、页、行。 表空间 Table Space 上一篇文章讲磁盘结构的时候提到过，表空间可以看做是 InnoDB 存储引擎逻辑结构的最高层，所有的数据都存放在表空间中。分为：系统表空间、独占表空间、通用表空间、 临时表空间、Undo 表空间。 段 Segment 表空间是由各个段组成的，常见的段有数据段、索引段、回滚段等，段是一个逻辑的概念。一个 ibd 文件（独立表空间文件）里面会由很多个段组成。 创建一个索引会创建两个段，一个是索引段：leaf node segment，一个是数据段：non-leaf node segment。索引段管理非叶子节点的数据。数据段管理叶子节点的数据。 也就是说，一个表的段数，就是索引的个数乘以 2。 簇 Extent 一个段（Segment）又由很多的簇（也可以叫区）组成，每个区的大小是 1MB（64个连续的页）。 每一个段至少会有一个簇，一个段所管理的空间大小是无限的，可以一直扩展下去，但是扩展的最小单位就是簇。 页 page 为了高效管理物理空间，对簇进一步细分，就得到了页。簇是由连续的页（Page）组成的空间，一个簇中有 64 个连续的页。 （1MB／16KB = 64）。这些页面在物理上和逻辑上都是连续的。 跟大多数数据库一样，InnoDB 也有页的概念（也可以称为块），每个页默认 16KB。 页是 InnoDB 存储引擎磁盘管理的最小单位，通过 innodb_page_size 设置。 一个表空间最多拥有 2^32 个页，默认情况下一个页的大小为 16KB，也就是说一个表空间最多存储 64TB 的数据。 注意，文件系统中，也有页的概念。 操作系统和内存打交道，最小的单位是页 Page。文件系统的内存页通常是 4K。 SHOW VARIABLES LIKE 'innodb_page_size'; 假设一行数据大小是 1K，那么一个数据页可以放 16 行这样的数据。 举例：一个页放 3 行数据。 往表中插入数据时，如果一个页面已经写完，产生一个新的叶页面。如果一个簇的所有的页面都被用完，会从当前页面所在段新分配一个簇。 如果数据不是连续的，往已经写满的页中插入数据，会导致叶页面分裂： 行 Row（仅供了解） InnoDB 存储引擎是面向行的（row-oriented），也就是说数据的存放按行进行存放。 https://dev.mysql.com/doc/refman/5.7/en/innodb-row-format.html Antelope[ˈæntɪləʊp]（羚羊）是 InnoDB 内置的文件格式，有两种行格式： REDUNDANT[rɪˈdʌndənt] Row Format COMPACT Row Format（5.6 默认） Barracuda[ˌbærəˈkjuːdə]（梭子鱼）是 InnoDB Plugin 支持的文件格式，新增了 两种行格式： DYNAMIC Row Format（5.7 默认） COMPRESSED Row Format 文件格式 行格式 描述 Antelope （Innodb-base） ROW_FORMAT=COMPACT ROW_FORMAT=REDUNDANT Compact 和 redumdant 的区别在就是在于首部的存 存内容区别。 compact 的存储格式为首部为一个非 NULL 的变长字 段长度列表 redundant 的存储格式为首部是一个字段长度偏移 列表（每个字段占用的字节长度及其相应的位移）。 在 Antelope 中对于变长字段，低于 768 字节的，不 会进行 overflow page 存储，某些情况下会减少结果 集 IO. Barracuda (innodb-plugin) ROW_FORMAT=DYNAMIC ROW_FORMAT=COMPRESSED 这两者主要是功能上的区别功能上的。 另外在行 里的变长字段和 Antelope 的区别是只存 20 个字节， 其它的 overflow page 存储。 另外这两都需要开启 innodb_file_per_table=1 innodb_file_format 在配置文件中指定；row_format 则在创建数据表时指定。 SHOW VARIABLES LIKE &quot;%innodb_file_format%&quot;; SET GLOBAL innodb_file_format=Barracuda; 在创建表的时候可以指定行格式。 CREATE TABLE tf1 (c1 INT PRIMARY KEY) ROW_FORMAT=COMPRESSED KEY_BLOCK_SIZE=8; 查看行格式： SHOW TABLE STATUS LIKE 'student' \\G; 这一块的内容主要是让大家了解页 page 的概念。 接下来我们可以到MySQL相关（三）- 索引数据模型推演及 B+Tree 的详细介绍查看关于索引数据结构的演算。 ","link":"https://faded.auspicious.space/post/innodb-logical-storage-structure/"},{"title":"this 到底指向啥","content":" this到底指向啥？看完这篇就知道了！ JS 中的 this 是一个老生常谈的问题了，因为它并不是一个确定的值，在不同情况下有不同的指向，所以也经常使人困惑。本篇文章会谈谈我自己对 this 的理解。 this 到底是啥 其实 this 就是一个指针，它指示的就是当前的一个执行环境，可以用来对当前执行环境进行一些操作。因为它指示的是执行环境，所以在定义这个变量时，其实是不知道它真正的值的，只有运行时才能确定他的值。同样一段代码，用不同的方式执行，他的 this 指向可能是不一样的。我们来看看如下代码： function func() { this.name = &quot;小小飞&quot;; console.log(this); // 看一下this是啥 } 这个方法很简单，只是给 this 添加了一个 name 属性，我们把这个方法复制到 Chrome 调试工具看下结果： 上图中我们直接调用了 func()，发现 this 指向的是 window，name 属性添加到了 window 上。下面我们换一种调用方式，我们换成 new func() 来调用： 我们看到输出了两个 func {name: &quot;小小飞&quot;}，一个是我们 new 返回的对象，另一个是方法里面的 console。这两个值是一样的，说明这时候方法里面 this 就指向了 new 返回的对象，而不是前面例子的 window 了。这是因为当你使用 new 去调用一个方法时，这个方法其实就作为构造函数使用了，这时候的 this 指向的是 new 出来的对象。 下面我们分别讲解下几种情况： 使用 new 调用时，this 指向 new 出来的对象 这个规则其实是 JS 面向对象的一部分，JS 使用了一种很曲折的方式来支持面向对象。当你用 new 来执行一个函数时，这个函数就变成了一个类，new 关键字会返回一个类的实例给你，这个函数会充当构造函数的角色。作为面向对象的构造函数，必须要有能够给实例初始化属性的能力，所以构造函数里面必须要有某种机制来操作生成的实例，这种机制就是 this。让 this 指向生成的实例就可以通过 this 来操作实例了。关于 JS 的面向对象更详细的解释可以看这篇文章。 this 的这种特性还有一些妙用。一个函数可以直接调用，也可以用 new 调用，那假如我只想使用者通过 new 调用有没有办法呢？下图截取自 Vue 源码： Vue 巧妙利用了 this 的特性，通过检查 this 是不是 Vue 的一个实例来检测使用者是通过 new 调用的还是直接调用的。 没有明确调用者时，this 指向 window 这个其实在最开始的例子就讲过了，那里没有明确调用者，this 指向的是 window。我们这里讲另外一个例子，函数里面的函数，this 指向谁？ function func() { function func2() { console.log('this:', this); // 这里的this指向谁？ } func2(); } 我们执行一下看看： 直接执行： 使用 new 执行： 我们发现无论是直接执行，还是使用 new 执行，this 的值都指向的 window。直接执行时很好理解，因为没有明确调用者，那 this 自然就是 window。需要注意的是使用 new 时，只有被 new 的 func 才是构造函数，他的 this 指向 new 出来的对象，他里面的函数的 this 还是指向 window。 有明确调用者时，this 指向调用者 看这个例子： var obj = { myName: &quot;小小飞&quot;, func: function() { console.log(this.myName); } } obj.func(); // 小小飞 上述例子很好理解，因为调用者是 obj，所以 func 里面的 this 就指向 obj，this.myName 就是 obj.myName。其实这一条和上一条可以合在一起，没有明确调用者时其实隐含的调用者就是 window，所以经常有人说 this 总是指向调用者。 下面我们将这个例子稍微改一下： var myName = &quot;大飞哥&quot;; var obj = { myName: &quot;小小飞&quot;, func: function() { console.log(this.myName); } } var anotherFunc = obj.func; anotherFunc(); // 输出是啥？ 这里的输出应该是“大飞哥”，因为虽然 anotherFunc 的函数体跟 obj.func 一样，但是他的执行环境不一样，他其实没有明确的调用者，或者说调用者是 window。这里的 this.myName 其实是 window.myName，也就是“大飞哥”。 我们将这个例子再改一下： let myName = &quot;大飞哥&quot;; var obj = { myName: &quot;小小飞&quot;, func: function() { console.log(this.myName); } } var anotherFunc = obj.func; anotherFunc(); // 输出是啥？ 这次我们只是将第一个 var 改成了 let，但是我们的输出却变成了 undefined。这是因为 let，const 定义变量，即使在最外层也不会变成 window 的属性，只有 var 定义的变量才会成为 window 的属性。 箭头函数并不会绑定 this 这句话的意思是箭头函数本身并不具有 this，箭头函数在被申明确定 this，这时候他会直接将当前作用域的 this 作为自己的 this。还是之前的例子我们将函数改为箭头函数： var myName = &quot;大飞哥&quot;; var obj = { myName: &quot;小小飞&quot;, func: () =&gt; { console.log(this.myName); } } var anotherFunc = obj.func; obj.func(); // 大飞哥 anotherFunc(); // 大飞哥 上述代码里面的 obj.func() 输出也是“大飞哥”，是因为 obj 在创建时申明了箭头函数，这时候箭头函数会去寻找当前作用域，因为 obj 是一个对象，并不是作用域，所以这里的作用域是 window，this 也就是 window 了。 再来看一个例子： var myName = &quot;大飞哥&quot;; var obj = { myName: &quot;小小飞&quot;, func: function () { return { getName: () =&gt; { console.log(this.myName); } } } } var anotherFunc = obj.func().getName; obj.func().getName(); // 小小飞 anotherFunc(); // 小小飞 两个输出都是“小小飞”，obj.func().getName() 输出“小小飞”很好理解，这里箭头函数是在 obj.func() 的返回值里申明的，这时他的 this 其实就是 func() 的 this，因为他是被 obj 调用的，所以 this 指向 obj。 那为什么 anotherFunc() 输出也是“小小飞”呢？这是因为 anotherFunc() 输出的 this，其实在 anotherFunc 赋值时就确定了： var anotherFunc = obj.func().getName; 其实是先执行了 obj.func()； 执行 obj.func() 的时候 getName 箭头函数被申明； 这时候箭头函数的 this 应该是当前作用域的 this，也就是 func() 里面的 this； func() 因为是被 obj 调用，所以 this 指向 obj； 调用 anotherFunc 时，其实 this 早就确定了，也就是 obj，最终输出的是 obj.myName。 再来看一个构造函数里面的箭头函数，前面我们说了构造函数里面的函数，直接调用时，他的 this 指向 window，但是如果这个函数时箭头函数呢： var myName = &quot;大飞哥&quot;; function func() { this.myName = &quot;小小飞&quot;; const getName = () =&gt; { console.log(this.myName); } getName(); } new func(); // 输出啥？ 这里输出的是“小小飞”，原理还是一样的，箭头函数在申明时 this 确定为当前作用域的 this，在这里就是 func 的作用域，跟 func 的 this 一样指向 new 出来的实例。如果不用 new，而是直接调用，这里的 this 就指向 window。 DOM 事件回调里面，this 指向绑定事件的对象 function func(e) { console.log(this === e.currentTarget); // 总是true console.log(this === e.target); // 如果target等于currentTarget,这个就为true } const ele = document.getElementById('test'); ele.addEventListener('click', func); currentTarget 指的是绑定事件的 DOM 对象，target 指的是触发事件的对象。DOM 事件回调里面 this 总是指向 currentTarget，如果触发事件的对象刚好是绑定事件的对象，即 target === currentTarget，this 也会顺便指向 target。如果回调是箭头函数，this 是箭头函数申明时作用域的 this。 严格模式下 this 是 undefined function func() { &quot;use strict&quot; console.log(this); } func(); // 输出是undefined 注意这里说的严格模式下 this 是 undefined 是指在函数体内部，如果本身就在全局作用域，this 还是指向 window。 &lt;html&gt; ... &lt;script&gt; &quot;use strict&quot; console.log(this); // window &lt;/script&gt; ... &lt;/html&gt; this 能改吗 this 是能改的，call 和 apply 都可以修改 this，ES6 里面还新增了一个 bind 函数。 使用 call 和 apply 修改 this const obj = { myName: &quot;大飞哥&quot;, func: function(age, gender) { console.log(`我的名字是${this.myName}, 我的年龄是${age}，我是一个${gender}`); } } const obj2 = { myName: &quot;小小飞&quot; } obj.func.call(obj2, 18, &quot;帅哥&quot;); // 我的名字是小小飞, 我的年龄是18，我是一个帅哥 注意上面输出的名字是“小小飞”，也就是 obj2.myName。正常直接调用 obj.func() 输出的名字应该是 obj.myName，也就是“大飞哥”。但是如果你使用 call 来调用，call 的第一个参数就是手动指定的 this。我们将它指定为 obj2，那在函数里面的 this.myName 其实就是 obj2.myName 了。 apply 方法跟 call 方法作用差不多，只是后面的函数参数形式不同，使用 apply 调用应该这样写，函数参数应该放到一个数组或者类数组里面： obj.func.apply(obj2, [18, &quot;帅哥&quot;]); // 我的名字是小小飞, 我的年龄是18，我是一个帅哥 之所以有 call 和 apply 两个方法实现了差不多的功能，是为了让大家使用方便，如果你拿到的参数是一个一个的，那就使用 call 吧，但是有时候拿到的参数是 arguments，这是函数的一个内置变量，是一个类数组结构，表示当前函数的所有参数，那就可以直接用 apply，而不用将它展开了。 使用 bind 修改 this bind 是 ES5 引入的一个方法，也可以修改 this，但是调用它并不会立即执行方法本身，而是会返回一个修改了 this 的新方法： const obj = { myName: &quot;大飞哥&quot;, func: function(age, gender) { console.log(`我的名字是${this.myName}, 我的年龄是${age}，我是一个${gender}`); } } const obj2 = { myName: &quot;小小飞&quot; } const func2 = obj.func.bind(obj2); // 返回一个this改为obj2的新方法 func2(18, &quot;帅哥&quot;); // 我的名字是小小飞, 我的年龄是18，我是一个帅哥 bind 和 call，apply 最大的区别就是 call，apply 会立即执行方法，而 bind 并不会立即执行，而是会返回一个新方法供后面使用。 bind 函数也可以接收多个参数，第二个及以后的参数会作为新函数的参数传递进去，比如前面的 bind 也可以这样写： const func3 = obj.func.bind(obj2, 18); // 注意我们这里已经传了一个年龄参数 func3(&quot;帅哥&quot;); //注意这里只传了性别参数，年龄参数已经在func3里面了，输出还是：我的名字是小小飞, 我的年龄是18，我是一个帅哥 自己写一个 call 知道了 call 的作用，我们自己来写一个 call： Function.prototype.myCall = function(...args) { // 参数检查 if(typeof this !== &quot;function&quot;) { throw new Error('Must call with a function'); } const realThis = args[0] || window; const realArgs = args.slice(1); const funcSymbol = Symbol('func'); realThis[funcSymbol] = this; // 这里的this是原方法，保存到传入的第一个参数上 //用传入的参数来调方法，方法里面的this就是传入的参数了 const res = realThis[funcSymbol](...realArgs); delete realThis[funcSymbol]; // 最后删掉临时存储的原方法 return res; // 将执行的返回值返回 } 自己写一个 apply apply 方法跟 call 方法很像，区别只是在取调用参数上： Function.prototype.myApply = function(...args) { if(typeof this !== &quot;function&quot;) { throw new Error('Must call with a function'); } const realThis = args[0] || window; // 直接取第二个参数，是一个数组 const realArgs = args[1]; const funcSymbol = Symbol('func'); realThis[funcSymbol] = this; const res = realThis[funcSymbol](...realArgs); delete realThis[funcSymbol]; return res; } 自己写一个 bind 自己写一个 bind 需要用到前面的 apply，注意他的返回值是一个方法： Function.prototype.myBind = function(...args) { if(typeof this !== &quot;function&quot;) { throw new Error('Must call with a function'); } const _func = this; // 原方法 const realThis = args[0] || window; // 绑定的this const otherArgs = args.slice(1); // 取出后面的参数作为新函数的默认参数 return function(...args2) { // 返回一个方法 return _func.apply(realThis, [...otherArgs,...args2]); // 拼接存储参数和新参数，然后用apply执行 } } 总结 函数外面的 this，即全局作用域的 this 指向 window。 函数里面的 this 总是指向直接调用者。如果没有直接调用者，隐含的调用者是 window。 使用 new 调用一个函数，这个函数即为构造函数。构造函数里面的 this 是和实例对象沟通的桥梁，他指向实例对象。 箭头函数里面的 this 在它申明时确定，跟他当前作用域的 this 一样。 DOM 事件回调里面，this 指向绑定事件的对象（currentTarget），而不是触发事件的对象（target）。当然这两个可以是一样的。如果回调是箭头函数，请参考上一条，this 是它申明时作用域的 this。 严格模式下，函数里面的 this 指向 undefined，函数外面（全局作用域）的 this 还是指向 window。 call 和 apply 可以改变 this，这两个方法会立即执行原方法，他们的区别是参数形式不一样。 bind 也可以修改 this，但是他不会立即执行，而是返回一个修改了 this 的函数。 ","link":"https://faded.auspicious.space/post/what-does-this-point-to-in-javascript/"},{"title":"分布式事务基础篇","content":" 后端程序员必备：分布式事务基础篇 数据库事务 数据库事务（简称：事务），是数据库管理系统执行过程中的一个逻辑单位，由一个有限的数据库操作序列构成，这些操作要么全部执行,要么全部不执行，是一个不可分割的工作单位。 数据库事务的几个典型特性：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持久性（Durabilily），简称就是 ACID。 原子性： 事务作为一个整体被执行，包含在其中的对数据库的操作要么全部被执行，要么都不执行。 一致性： 指在事务开始之前和事务结束以后，数据不会被破坏，假如 A 账户给 B 账户转 10 块钱，不管成功与否，A 和 B 的总金额是不变的。 隔离性： 多个事务并发访问时，事务之间是相互隔离的，即一个事务不影响其它事务运行效果。简言之，就是事务之间是进水不犯河水的。 持久性： 表示事务完成以后，该事务对数据库所作的操作更改，将持久地保存在数据库之中。 事务的实现原理 本地事务 传统的单服务器，单关系型数据库下的事务，就是本地事务。本地事务由资源管理器管理，JDBC事务就是一个非常典型的本地事务。 事务日志 innodb 事务日志包括 redo log 和 undo log。 redo log（重做日志） redo log 通常是物理日志，记录的是数据页的物理修改，而不是某一行或某几行修改成怎样，它用来恢复提交后的物理数据页。 undo log（回滚日志） undo log 是逻辑日志，和 redo log 记录物理日志的不一样。可以这样认为，当 delete 一条记录时，undo log 中会记录一条对应的 insert 记录，当 update 一条记录时，它记录一条对应相反的 update 记录。 事务 ACID 特性的实现思想 原子性：是使用 undo log 来实现的，如果事务执行过程中出错或者用户执行了 rollback，系统通过 undo log 日志返回事务开始的状态。 持久性：使用 redo log 来实现，只要 redo log 日志持久化了，当系统崩溃，即可通过 redo log 把数据恢复。 隔离性：通过锁以及 MVCC，使事务相互隔离开。 一致性：通过回滚、恢复，以及并发情况下的隔离性，从而实现一致性。 分布式事务 分布式事务：就是指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点之上。简单来说，分布式事务指的就是分布式系统中的事务，它的存在就是为了保证不同数据库节点的数据一致性。 为什么需要分布式事务？接下来分两方面阐述： 微服务架构下的分布式事务 随着互联网的快速发展，轻盈且功能划分明确的微服务，登上了历史舞台。比如，一个用户下订单，购买直播礼物的服务，被拆分成三个 service，分别是金币服务（coinService），下订单服务（orderService）、礼物服务（giftService）。这些服务都部署在不同的机器上（节点），对应的数据库（金币数据库、订单数据库、礼物数据库）也在不同节点上。 用户下单购买礼物，礼物数据库、金币数据库、订单数据库在不同节点上，用本地事务是不可以的，那么如何保证不同数据库（节点）上的数据一致性呢？这就需要分布式事务啦~ 分库分表下的分布式事务 随着业务的发展，数据库的数据日益庞大，超过千万级别的数据，我们就需要对它分库分表（以前公司是用 mycat 分库分表，后来用 sharding-jdbc）。一分库，数据又分布在不同节点上啦，比如有的在深圳机房，有的在北京机房你再想用本地事务去保证，已经无动于衷啦还是需要分布式事务啦。 比如 A 转 10 块给 B，A 的账户数据是在北京机房，B 的账户数据是在深圳机房。流程如下： CAP 理论 &amp; BASE 理论 学习分布式事务，当然需要了解 CAP 理论和BASE 理论。 CAP 理论 CAP 理论作为分布式系统的基础理论，指的是在一个分布式系统中，Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），这三个要素最多只能同时实现两点。 一致性（C：Consistency）： 一致性是指数据在多个副本之间能否保持一致的特性。例如一个数据在某个分区节点更新之后，在其他分区节点读出来的数据也是更新之后的数据。 可用性（A：Availability）： 可用性是指系统提供的服务必须一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果。这里的重点是“有限时间内”和“返回结果”。 分区容错性（P:Partition tolerance）: 分布式系统在遇到任何网络分区故障的时候，仍然需要能够保证对外提供满足一致性和可用性的服务。 选择 说明 CA 放弃分区容错性，加强一致性和可用性，其实就是传统的单机数据库的选择 AP 放弃一致性，分区容错性和可用性，这是很多分布式系统设计时的选择 CP 放弃可用性，追求一致性和分区容错性，网络问题会直接让整个系统不可用 BASE 理论 BASE 理论， 是对 CAP 中 AP 的一个扩展，对于我们的业务系统，我们考虑牺牲一致性来换取系统的可用性和分区容错性。BASE 是 Basically Available（基本可用），Soft state（软状态）和 Eventually consistent（最终一致性）三个短语的缩写。 Basically Available 基本可用：通过支持局部故障而不是系统全局故障来实现的。如将用户分区在 5 个数据库服务器上，一个用户数据库的故障只影响这台特定主机那 20% 的用户，其他用户不受影响。 Soft State 软状态，状态可以有一段时间不同步 Eventually Consistent 最终一致，最终数据是一致的就可以了，而不是时时保持强一致。 分布式事务的几种解决方案 分布式事务解决方案主要有以下这几种： 2PC（二阶段提交）方案 TCC（Try、Confirm、Cancel） 本地消息表 最大努力通知 Saga事务 二阶段提交方案 二阶段提交方案是常用的分布式事务解决方案。事务的提交分为两个阶段：准备阶段和提交执行方案。 二阶段提交成功的情况 准备阶段：事务管理器向每个资源管理器发送准备消息，如果资源管理器的本地事务操作执行成功，则返回成功。 提交执行阶段：如果事务管理器收到了所有资源管理器回复的成功消息，则向每个资源管理器发送提交消息，RM 根据 TM 的指令执行提交。如图： 二阶段提交失败的情况 准备阶段：事务管理器向每个资源管理器发送准备消息，如果资源管理器的本地事务操作执行成功，则返回成功，如果执行失败，则返回失败。 提交执行阶段：如果事务管理器收到了任何一个资源管理器失败的消息，则向每个资源管理器发送回滚消息。资源管理器根据事务管理器的指令回滚本地事务操作，释放所有事务处理过程中使用的锁资源。 二阶段提交优缺点 2PC 方案实现起来简单，成本较低，但是主要有以下缺点： 单点问题：如果事务管理器出现故障，资源管理器将一直处于锁定状态。 性能问题：所有资源管理器在事务提交阶段处于同步阻塞状态，占用系统资源，一直到提交完成，才释放资源，容易导致性能瓶颈。 数据一致性问题：如果有的资源管理器收到提交的消息，有的没收到，那么会导致数据不一致问题。 TCC（补偿机制） TCC 采用了补偿机制，其核心思想是：针对每个操作，都要注册一个与其对应的确认和补偿（撤销）操作。 TCC（Try-Confirm-Cancel）模型 TCC（Try-Confirm-Cancel）是通过对业务逻辑的分解来实现分布式事务。针对一个具体的业务服务，TCC 分布式事务模型需要业务系统都实现一下三段逻辑： Try 阶段： 尝试去执行，完成所有业务的一致性检查，预留必须的业务资源。 Confirm 阶段：该阶段对业务进行确认提交，不做任何检查，因为 Try 阶段已经检查过了，默认 Confirm 阶段是不会出错的。 Cancel 阶段：若业务执行失败，则进入该阶段，它会释放 Try 阶段占用的所有业务资源，并回滚 Confirm 阶段执行的所有操作。 TCC 分布式事务模型包括三部分：主业务服务、从业务服务、业务活动管理器。 主业务服务：主业务服务负责发起并完成整个业务活动。 从业务服务：从业务服务是整个业务活动的参与方，实现 Try、Confirm、Cancel 操作，供主业务服务调用。 业务活动管理器：业务活动管理器管理控制整个业务活动，包括记录事务状态，调用从业务服务的 Confirm 操作，调用从业务服务的 Cancel 操作等。 下面再拿用户下单购买礼物作为例子来模拟 TCC 实现分布式事务的过程： 假设用户 A 余额为 100 金币，拥有的礼物为 5 朵。A 花了 10 个金币，下订单，购买 10 朵玫瑰。余额、订单、礼物都在不同数据库。 TCC 的 Try 阶段： 生成一条订单记录，订单状态为待确认。 将用户 A 的账户金币中余额更新为 90，冻结金币为 10（预留业务资源）。 将用户的礼物数量为 5，预增加数量为 10。 Try 成功之后，便进入 Confirm 阶段。 Try 过程发生任何异常，均进入 Cancel 阶段。 TCC 的 Confirm 阶段： 订单状态更新为已支付。 更新用户余额为 90，可冻结为 0。 用户礼物数量更新为 15，预增加为 0。 Confirm 过程发生任何异常，均进入 Cancel 阶段。 Confirm 过程执行成功，则该事务结束。 TCC 的 Cancel 阶段： 修改订单状态为已取消。 更新用户余额回 100。 更新用户礼物数量为 5。 TCC 优缺点 TCC 方案让应用可以自定义数据库操作的粒度，降低了锁冲突，可以提升性能，但是也有以下缺点： 应用侵入性强，try、confirm、cancel 三个阶段都需要业务逻辑实现。 需要根据网络、系统故障等不同失败原因实现不同的回滚策略，实现难度大，一般借助 TCC 开源框架，ByteTCC，TCC-transaction，Himly。 本地消息表 ebay 最初提出本地消息表这个方案，来解决分布式事务问题。业界目前使用这种方案是比较多的，它的核心思想就是将分布式事务拆分成本地事务进行处理。可以看一下基本的实现流程图： 基本实现思路 发送消息方： 需要有一个消息表，记录着消息状态相关信息。 业务数据和消息表在同一个数据库，即要保证它俩在同一个本地事务。 在本地事务中处理完业务数据和写消息表操作后，通过写消息到 MQ 消息队列。 消息会发到消息消费方，如果发送失败，即进行重试。 消息消费方： 处理消息队列中的消息，完成自己的业务逻辑。 此时如果本地事务处理成功，则表明已经处理成功了。 如果本地事务处理失败，那么就会重试执行。 如果是业务上面的失败，给消息生产方发送一个业务补偿消息，通知进行回滚等操作。 生产方和消费方定时扫描本地消息表，把还没处理完成的消息或者失败的消息再发送一遍。如果有靠谱的自动对账补账逻辑，这种方案还是非常实用的。 优点 &amp; 缺点： 该方案的优点是很好地解决了分布式事务问题，实现了最终一致性。缺点是消息表会耦合到业务系统中。 最大努力通知 什么是最大通知 最大努力通知也是一种分布式事务解决方案。下面是企业网银转账一个例子： 企业网银系统调用前置接口，跳转到转账页 企业网银调用转账系统接口 转账系统完成转账处理，向企业网银系统发起转账结果通知，若通知失败，则转账系统按策略进行重复通知。 企业网银系统未接收到通知，会主动调用转账系统的接口查询转账结果。 转账系统会遇到退汇等情况，会定时回来对账。 最大努力通知方案的目标，就是发起通知方通过一定的机制，最大努力将业务处理结果通知到接收方。最大努力通知实现机制如下： 最大努力通知解决方案 要实现最大努力通知，可以采用 MQ 的 ack 机制。 方案： 发起方将通知发给 MQ。 接收通知方监听 MQ 消息。 接收通知方收到消息后，处理完业务，回应 ack。 接收通知方若没有回应 ack，则 MQ 会间隔 1 min、5 min、10 min 等重复通知。 接受通知方可用消息校对接口，保证消息的一致性。 转账业务实现流程图： 交互流程如下： 用户请求转账系统进行转账。 转账系统完成转账，将转账结果发给 MQ。 企业网银系统监听 MQ，接收转账结果通知，如果接收不到消息，MQ 会重复发送通知。接收到转账结果，更新转账状态。 企业网银系统也可以主动查询转账系统的转账结果查询接口，更新转账状态。 Saga 事务 Saga 事务由普林斯顿大学的 Hector Garcia-Molina 和 Kenneth Salem 提出，其核心思想是将长事务拆分为多个本地短事务，由 Saga 事务协调器协调，如果正常结束那就正常完成，如果某个步骤失败，则根据相反顺序一次调用补偿操作。 Saga 简介 Saga = Long Live Transaction（LLT，长活事务） LLT = T1 + T2 + T3 + ... + Ti（Ti 为本地短事务） 每个本地事务 Ti 有对应的补偿 Ci Saga 的执行顺序 正常情况：T1 T2 T3 ... Tn 异常情况：T1 T2 T3 C3 C2 C1 Saga 两种恢复策略 向后恢复，如果任意本地子事务失败，补偿已完成的事务。如异常情况的执行顺序 T1 T2 Ti Ci C2 C1。 向前恢复，即重试失败的事务，假设最后每个子事务都会成功。执行顺序：T1, T2, ..., Tj(失败), Tj(重试),..., Tn。 举个例子，假设用户下订单，花 10 块钱购买了 10 朵玫瑰，则有： T1=下订单 ，T2=扣用户 10 块钱，T3=用户加 10 朵玫瑰， T4=库存减 10 朵玫瑰 C1=取消订单 ，C2= 给用户加 10 块钱，C3 =用户减 10 朵玫瑰， C4=库存加 10 朵玫瑰 假设事务执行到 T4 发生异常回滚，在 C4 的要把玫瑰给库存加回去的时候，发现用户的玫瑰都用掉了，这是 Saga 的一个缺点，由于事务之间没有隔离性导致的问题。 可以通过以下方案解决这个问题： 在应用层面加入逻辑锁的逻辑。 Session 层面隔离来保证串行化操作。 业务层面采用预先冻结资金的方式隔离此部分资金。 业务操作过程中通过及时读取当前状态的方式获取更新。 参考与感谢 干货 | 一篇文章带你学习分布式事务 再有人问你分布式事务，把这篇扔给他 聊聊分布式事务，再说说解决方案 Mysql事务实现原理 详细分析MySQL事务日志(redo log和undo log) 《Saga分布式事务解决方案与实践》 分布式事务解决方案之最大努力通知 ","link":"https://faded.auspicious.space/post/fundamentals-of-distributed-transactions/"},{"title":"RSA 加密","content":" 找出 2 个质数，ppp, qqq (只有自己和1能够整除) 假设 p=3,q=7p=3, q = 7p=3,q=7； n=p×q→n=21n = p\\times q \\to n = 21n=p×q→n=21； f(n)=(p−1)×(q−1)f(n) = (p-1)\\times(q-1)f(n)=(p−1)×(q−1) 欧拉函数 f(n)=18f(n) = 18f(n)=18； 公钥 eee, 1&lt;e&lt;f(n)1 \\lt e \\lt f(n)1&lt;e&lt;f(n) 的整数，且 e,f(n)e, f(n)e,f(n) 互质（无公共因子），e=13e = 13e=13 私钥 d:e×dd: e\\times dd:e×d 除以 f(n)f(n)f(n) 余数为 1； 加密算法，mem^eme 除以 nnn，求余数 =c= c=c； 解密方式：cdc^dcd 除以 nnn，求余数 =m= m=m。 ","link":"https://faded.auspicious.space/post/rsa-algorithm/"},{"title":"如何降低程序员的工资","content":" 如何降低程序员的工资？ 开发软件需要程序员，但不幸的是他们工资高，又很懒，还难以控制。 不管他们写的软件能不能工作，你每个月都得给他们付工资。如果能少付一点工资就好了！可是这些程序员有时候会发现他们的薪水低，居然辞职了！这该怎么办？ 很可惜，现在不能对他们使用暴力了，唉！ 但是总归有一些办法，我来分享一下。 工资一定要保密 工资必须保密，坚决不能让他们去讨论薪水，入职的时候要警告他们，或者签署保密协议禁止讨论任何薪水，奖金，福利之类的东西。 必须让程序员感觉到，这些东西是有毒的，不能和别人随意聊。由于他们不知道同事拿多少钱，他们很长时间都不会提出加薪的需求了。 禁止参加外部会议 不能让他们去面基，参加外部会议，因为在那里他们会发现自己的薪水和别人比起来不公平，还可能遇到猎头。 所以一定要灌输一种参加会议就是浪费时间的想法，想开会就在办公室搞，不能让他们遇到别的公司的程序员， 他们知道得越少，你就越安全。 画大饼 经常给他们洗脑，告诉他们你的公司多么伟大，公司的目标是星辰大海，他们的贡献是多么重要。相比于他们要去占领的数十亿美元的市场，他们工资单上的那点儿数字就不那么重要了。 这种招数很有用，那些受到激励的程序员会努力奉献很长时间。 给他们承诺 你不需要遵守承诺，但是你还是要对程序员承诺：承诺不久以后就会加薪，比如完成某一轮投资，完成了某个大项目，大合同。 注意：你需要拿捏好这个“恰当”的时机，让你的承诺建立在一些你不可控制的事件上，这样当不可控的事情发生时，你也是无辜的“受害者”。 监视他们 严密地监视他们，确保他们使用你的邮件服务器，电脑，服务器，甚至手机。 在这些设备上安装监视软件，看看他们都发了什么样的消息。 理想情况下，你应该有个安全的部门来监视所有人，并定期通知你关于他们的异常/可疑的行为（办公室如果有视像头就更好了！） 怀疑他们和外边公司的任何联系，当然你一定得让程序员知道，你在监视他们，额外的恐惧总是有用的。 给他们响亮的头衔 叫他们副总裁！比如工程副总裁！技术副总裁！或者其他什么副总裁。 对你来说是头衔是个不值钱的小事，对他们确是大事，相比薪水，能够放在 LinkedIn 上的头衔对他们更重要！ 如果你的副总裁头衔用完了，可以试试高级架构师，首席科学家...... 禁止在家工作 他们每天必须来办公室工作，这里有桌子，椅子，电脑，还有订书机，这里应该成为他们的第二个家，不，最好是第一个家。他们在这里可以从早干到晚，不愿离开——不管工资有多低。 严禁在家远程办公，因为这会让他们考虑找一个有更高薪水的新家。 随机地涨工资 涨工资应该完全凭你的喜好来做，不要依赖什么系统，不要依赖这个人的表现，要让你的决定完全不可预测。不可预测性就会带来恐惧，程序员就会害怕，就不会抱怨工资低了。 和竞争对手达成协议 联系你的主要竞争对手，商量好大家都不互挖墙角。如果他们不同意，那就付双倍的工资挖几个他们的核心程序员。 当然，你不会真正地雇佣他们，关键是这可以震慑竞争对手，让他们害怕，不敢去碰你的奴隶，哦不，程序员。 极限施压 不能让程序员放松下来，确保他们每天都有复杂的问题需要解决，进度紧张，最后期限就在眼前，让他们尽可能多地对失败负责，这样他们就有沉重的负罪感，不好意思请求加薪。 在公司建立家庭氛围 搞点公司聚会，周五啤酒日，打打保龄球，给员工过生日，用这些方法让他们觉得公司就像他们的家庭一样。既然已经在家里了，讨论工资不妥吧？升值加薪就像是对家庭价值观的背叛，程序员们是不好意思这么做的。 给他们买带坐垫的椅子和乒乓球桌 一点点办公室的小恩小惠就能让你获得巨大的回报，一个精致的、专业的咖啡机会花掉你 $1000，同时可以每个月在每个程序员身上节省 $200 到 $300，你自己算算这笔帐！ 给自己立个规矩，与其给某个人加工资，还不如买一个全新的 Play Station 游戏机放到办公室。 还有，要允许他们带宠物到办公室，这样他们会在办公室工作更长时间，并且薪水更少。 帮助他们生存 在理财方面，大部分程序员都是笨蛋，他们根本不知道怎么买保险，怎么交税，怎么计划退休的基金。这方面你得好好地帮助他们，你是“家长”，他们是“孩子”，这样他们在你的手上才会感到安全，才不会离开你，更别说要求加薪了！ 这种方法很老套，但是很有效。 要做正确的培训 不能培训他们学习、使用最新的热门技术，培训的内容只能和工作相关，只要是能提升他们价值的东西，像证书了，好的培训课程了，都不让他们参加。 所有的这些都是确保他们不会变成“高级人士”，这样外边的企业对他们就不会有需求。 和他们成为朋友 这应该是终极杀招了，你一定得和你的程序员们成为朋友。和朋友协商钱的问题是很难的，他们不会轻易这么做。 由于你们是好朋友，他们会一直为你工作，即使薪水少一点。 怎么样和程序员成为朋友呢？嗯，经常和他们的家人会面，邀请他们到你的家里吃晚饭，给他们准备生日礼物，所有的这些小伎俩，都会帮你节省很多钱。 我能想到的就这么多了，还有别的吗？ ","link":"https://faded.auspicious.space/post/how-to-lower-programmers-wages/"},{"title":"从 JavaScript 中看设计模式","content":" 从JavaScript中看设计模式(总结) 概念 设计模式（Design Pattern）是一套被反复使用、多数人知晓的、经过分类的、代码设计经验的总结。 任何事情都有套路，设计模式就是写代码中常见的套路，有些写法我们日常都在使用，下面我们来介绍一下。 订阅/发布模式(观察者) pub/sub 这个应该大家用到的最广的设计模式了 在这种模式中，并不是一个对象调用另一个对象的方法，而是一个对象订阅另一个对象 特定活动并在状态改变后获得通知，订阅者因此也成为观察者，而被观察的对象成为发布者或主题。当发生了一个重要事件的时候发布者会通知（调用）所有订阅者并且可能经常以事件对象的形式传递消息。 自己实现一个简单的发布订阅设计模式。 // 创建EventBus class EventBus { constructor() { // 储存事件 this.tasks = {}; } // 绑定事件 $on(eName, cb) { typeof cb == &quot;function&quot; ? this.tasks[eName] || (this.tasks[eName] = []) : this.Error(cb, &quot;is not a function&quot;); this.tasks[eName].some(fn =&gt; fn == cb) ? true : this.tasks[eName].push(cb); // 避免重复绑定 } // 触发事件 $emit(eName, ...arg) { let taskQueue; this.tasks[eName] &amp;&amp; this.tasks[eName].length &gt; 0 ? (taskQueue = this.tasks[eName]) : this.Error(eName, &quot;is not defined or is a array of having empty callback&quot;); taskQueue.forEach(fn =&gt; { fn(...arg); }); } // 触发一次 $once(eName, cb) { let fn = (...arg) =&gt; { this.$off(eName, fn); cb(...arg); }; typeof cb == &quot;function&quot; &amp;&amp; this.$on(eName, fn); } // 卸载事件 $off(eName, cb) { let taskQueue; this.tasks[eName] &amp;&amp; this.tasks[eName].length &gt; 0 ? (taskQueue = this.tasks[eName]) : this.Error(eName, &quot;is not exist&quot;); if (typeof cb === &quot;function&quot;) { let index = taskQueue.findIndex(v =&gt; (v == cb)); index != -1 &amp;&amp; taskQueue.splice( taskQueue.findIndex(v =&gt; (v == cb)), 1 ); } if (typeof cb === &quot;undefined&quot;) { taskQueue.length = 0; } } // 异常处理 Error(node, errorMsg) { throw Error(`${node} ${errorMsg}`); } } 下面我们针对自己的模式进行简单的使用： // 首先定义一个事件池 const EventSinks = { add(x, y) { console.log(&quot;总和: &quot; + x + y); }, multip(x, y) { console.log(&quot;乘积: &quot; + x * y); }, onceEvent() { console.log(&quot;我执行一次后就自动卸载&quot;); } }; // 实例化对象 let bus = new EventBus(); bus.$on(&quot;operator&quot;, EventSinks.add); // 监听operator事件, 增加一个EventSinks.add bus.$on(&quot;operator&quot;, EventSinks.add); // 当事件名和回调函数相同时，跳过，避免重复绑定 bus.$on(&quot;operator&quot;, EventSinks.multip); // 给operator事件增加一个EventSinks.multip回调函数 bus.$once(&quot;onceEvent&quot;, EventSinks.onceEvent); // 触发一次后卸载 console.log(bus.tasks); // { operator: [ [Function: add], [Function: multip] ], onceEvent: [ [Function: fn] ]} bus.$emit(&quot;operator&quot;, 3, 5); // 总和:8 乘积:15 bus.$emit(&quot;onceEvent&quot;); // 我就执行一次 console.log(bus.tasks); // { operator: [ [Function: add], [Function: multip] ], onceEvent: [] } bus.$off(&quot;operator&quot;, EventSinks.add); // 卸载掉operator事件中的EventSinks.add函数体 console.log(bus.tasks); // { operator: [ [Function: multip] ], onceEvent: [] } bus.$off(&quot;operator&quot;); // 卸载operator事件的所有回调函数 console.log(bus.tasks); // { operator: [], onceEvent: [] } bus.$emit(&quot;onceEvent&quot;); // onceEvent is not defined or is a array of having empty callback 单例模式 单例模式的定义：保证一个类仅有一个实例，并提供一个访问它的全局访问点。实现的方法为先判断实例存在与否，如果存在则直接返回，否则就创建实例再返回，这就保证了一个类只实例化一次 使用场景：一个单一对象。比如：弹窗，无论点击多少次，弹窗只应该被创建一次，实现起来也很简单，用一个变量缓存起来即可。可以参考 ElementUI 模态框的实现。 模仿一下单例模式（只要有个变量确保实例只创建一次）。 class Singleton { constructor() { } } Singleton.getInstance = (function () { let instance return function () { if (!instance) { instance = new Singleton() } return instance } })() let s1 = Singleton.getInstance() let s2 = Singleton.getInstance() console.log(s1 === s2) // true 当我们再次创建时，如果实例化了，就不在实例化，直接返回，上面可以看出，二者相同。 策略模式 策略模式的定义：定义一系列的算法，把他们一个个封装起来，并且使他们可以互相替换。 策略模式的目的就是将算法的使用算法的实现分离出来。 一个基于策略模式的程序至少由两部分组成。第一部分是一组策略类（可变），策略类封装了具体的算法，并负责具体的计算过程。第二部分是环境类 Context（不变），Context 接受客户的请求，随后将请求委托给某一个策略类。要做到这一点，说明 Context中 要维持对某个策略对象的引用。 举个表单校验栗子： // 普通写法 const form = document.querySelector(&quot;#form&quot;); form.onsubmit = () =&gt; { if (form.username.value == &quot;&quot;) { console.log(&quot;用户名不能为空&quot;); return false; } if (form.username.password.length &lt; 10) { console.log('密码长度不能小于10') return false } } 简单的策略模式： // 创建校验器 const checker = { isEmpty(v, errorMsg) { if (value === '') { return errorMsg } }, minLength(v, length, errorMsg) { if (value.length &lt; length) { return errorMsg } } } const validator = () =&gt; { // 校验规则存储 this.cache = [] } validator.prototype.add = (...rule) =&gt; { let arr = rule.split(',') this.cache.push(() =&gt; { let valit = arr.shift() arr.unshift(dom.value) arr.push(errorMsg) return checker[valit].apply(dom, arr) }) } validator.prototype.start = () =&gt; { for (let i = 0, validatorFunc; validatorFunc = this.cache[i++];) { // 开始校验，并取得校验后的返回值 let msg = validatorFunc() if (msg) { return msg } } } const validatorFunc = () =&gt; { // 创建一个validator对象 let valit = new validator() valit.add(form.username, 'isEmpty', '用户名不能为空') valit.add(form.password, 'minLength', '密码长度不能小于10') // 获得校验结果 let errorMsg = valit.start() return errorMsg } // 再次登录 const form = document.querySelector(&quot;#form&quot;); form.onsubmit = () =&gt; { let errorMsg = validatorFunc() if (errorMsg) { console.error(errorMsg) return false } } 当创建校验器后，校验规则清晰明了，可以动态增改，便于维护。 代理模式 代理模式的定义：为一个对象提供一个代用品或占位符，以便控制它的访问。 常用的虚拟代理形式：某一个花销很大的操作，可以通过虚拟代理的方式延迟这种需要他的时候才去创建（例：使用虚拟代理实现图片懒加载）。 图片懒加载的方式：先通过一张 loading 图占位，然后通过异步的方式加载图片，等图片加载好了再把请求成功的图片加载到 img 标签上。 栗子： const imgFunc = (() =&gt; { const imgNode = document.createElement('img') document.body.appendChild(imgNode) return { setSrc: function (src) { imgNode.src = src } } })() const proxyImage = (() =&gt; { let img = new Image() img.onload = function () { imgFunc.setSrc(this.src) } return { setSrc: function (src) { imgFunc.setSrc('./loading.gif') img.src = src } } })() proxyImage.setSrc('./pic.png')() 上面的栗子实现了加载图片时，在图片加载成功前，指定特定的图片，加载完成后替换成真是的数据。 在我们生活中常用的事件代理、节流防抖函数其实都是代理模式的实现。 装饰器模式 装饰器模式的定义：在不改变对象自身的基础上，在程序运行期间给对象动态地添加方法，注解也可以理解为装饰器。常见应用：React 的高阶组件，或者 React-Redux 中的 @connect 或者自己定义一些高阶组件。 简单实现： import React from 'react' const withLog = Component =&gt; { // 完好无损渲染出来, 只是添加了两个生命周期函数 class NewComponent extends React.Component { // 1 componentWillMount() { console.time('ComponentRender') console.log('准备完毕了') } render() { // 完好无损渲染出来 return &lt;Component {...this.props}&gt;&lt;/Component&gt; } // 2 componentDidMount() { console.timeEnd('ComponentRender') console.log('渲染完毕了') } } return NewComponent } export { withLog } @withLog class xxx 在 Redux 中可以找出装饰器的方式，其实 Vue 中的 v-input，v-checkbox 也可以认为是装饰器模式，对原生 input 和 checkbox 做一层装饰。 装饰器模式和代理模式的结构看起来非常相似，这两种模式都描述了怎样为对象提供一定程度上的间接引用，并且向那个对象发送请求。代理模式和装饰器模式最重要的区别在于它们的意图和设计目的。代理模式的目的是：当直接访问本体不方便或者不符合需要时，为这个本体提供一个替代者。装饰模式目的是：为对象动态加入的行为，本体定义了关键功能，而装饰器提供或拒绝它的访问，或者在访问本体前做一些额外的事。 外观模式 外观模式的定义：即在内部让多个方法一起被调用。 涉及到兼容性，参数支持多格式，有很多这种代码，对外暴露统一 API，比如上面的 $on 支持数组，$off 参数支持多种情况，对面只用一个函数，内部判断实现。 举个简单的栗子： // 封装一些事件，让其兼容各个浏览器 const myEvent = { stopBubble(e) { if (typeof e.preventDefault() === 'function') { e.preventDefault() } if (typeof e.stopPropagation() === 'function') { e.stopPropagation() } // for IE if (typeof e.returnValue === 'boolean') { e.returnValue = false } if (typeof e.cancelBubble === 'boolean') { e.cancelBubble = false } }, addEvent(dom, type, cb) { if (dom.addEventListener) { dom.addEventListener(type, cb, false) } else if (dom.attachEvent) { dom.attachEvent('on' + type, cb) } else { dom['on' + type] = cb } } } 以上就用外观模式封装了两个基本事件，让其兼容各种浏览器，调用者不需要知道内部的构造，只要知道这个方法怎么用就行了。 工厂模式 工厂模式的定义：提供创建对象的接口，把成员对象的创建工作转交给一个外部对象，好处就是消除对象直接的耦合（也就是相互影响）。 常见的栗子，我们的弹窗 message，对外部提供 API，都是调用 API，然后新建一个弹窗或者 message 的实例，就是典型的工程模式 简单的栗子： class Man { constructor(name) { this.name = name } say() { console.log(`我的名字 ` + this.name) } } const p = new Man('JavaScript') p.say() // 我的名字 JavaScript 当然工厂模式并不仅仅是用来 new 出实例 可以想象一个场景。假设有一份很复杂的代码需要用户去调用，但是用户并不关心这些复杂的代码，只需要你提供给我一个接口去调用，用户只负责传递需要的参数，至于这些参数怎么使用，内部有什么逻辑是不关心的，只需要你最后返回我一个实例。这个构造过程就是工厂。 再比如下面 Vue 这个例子： const Notification = function (options) { if (Vue.prototype.$isServer) return; options = options || {}; let userOnClose = options.onClose; let id = &quot;notification_&quot; + seed++; let postion = options.postion || &quot;top-right&quot;; options.onClose = function () { Notification.close(id, userOnClose); }; instance = new NotificationConstructor({ data: options }); if (isVNode(options.message)) { instance.$slots.default = [options.message] options.message = 'REPLACED_BY_VNODE' } instance.id = id instance.$mount() document.body.appendChild(instance.$el) instance.visible = true instance.dom = instance.$el instance.dom.style.zIndex = PopupManager.nextZIndex() let verticalOffset = options.offset || 0 instances.filter(item =&gt; { verticalOffset += item.$el.offsetHeight + 16 }) verticalOffset += 16 instance.verticalOffset = verticalOffset instances.push(instance) return instance }; 在上述代码中，我们可以调用它封装好的方法就可以创建对象实例，至于它内部的实现原理我们并不关心。 建造者模式 Builder 建造者模式的定义：和工厂者模式相比，参与了更多创建过程或者更加复杂。 const Person = function (name, work) { // 创建应聘者缓存对象 let _person = new Human() // 创建应聘者姓名解析对象 _person.name = new NamedNodeMap(name) // 创建应聘者期望职位 _person.work = new Worker(work) return _person } const p = new Person('小明', 'Java') console.log(p) 迭代器模式 迭代器模式定义：指提供一种方法顺序访问一个聚合对象中的各个元素，而又不需要暴露该对象的内部表示。迭代器模式可以把迭代的过程从业务逻辑中分离出来，在使用迭代器模式之后，即使不关心对象的内部构造，也可以按顺序访问其中的每个元素。 比如常用的：every、map、filter、forEach 等等。 const each = function (arr, callback) { if (!Array.isArray(arr)) { throw Error(`${arr} is not a Array`) } for (let i = 0, l = arr.length; i &lt; l; i++) { callback.call(arr[i], i, arr[i]) } } each([1, 2, 4], function (i, n) { console.log([i, n]) }) 享元模式 享元（flyweight）模式的定义：一种用于性能优化的模式，fly 在这里是苍蝇的意思，意为蝇量级。享元模式的核心是运用共享技术来有效支持大量细粒度的对象。如果系统中因为创建了大量类似的对象而导致内存占用过高，享元模式就是非常有用了。在 JavaScript 中，浏览器特别是移动端的浏览器分配的内存并不多，如何节省内存就成了一件非常有意义的事情。 假设有个内衣工厂，目前的产品有 50 中男衣和 50 中女士内衣，为了推销产品，工厂决定生产一些塑料模特来穿上他们的内衣拍成广告照片。正常情况下需要 50 个男模特和 50 个女模特，然后让他们每人分别穿上一件内衣来拍照。 普通的做法： const Model = function (sex, underwear) { this.sex = sex this.underwear = underwear } Model.prototype.takePhoto = function () { console.log('sex=' + this.sex + ' underwear=' + this.underwear) } for (let i = 1; i &lt;= 50; i++) { let maleModel = new Model('male', 'underwear' + i) maleModel.takePhoto() } for (let join = 1; join &lt;= 50; join++) { let femaleModel = new Model('female', 'underwear' + join) femaleModel.takePhoto() } 采用享元模式： const Model = function (sex) { this.sex = sex } Model.prototype.takePhoto = function () { console.log('sex=' + this.sex + ' underwear=' + this.underwear) } // 分别创建一个男模特和一个女模特对象 let maleModel = new Model('male'), femaleModel = new Model('female') // 给男模特依次穿上所有的男装，并进行拍照 for (let i = 1; i &lt;= 50; i++) { maleModel.underwear = 'underwear' + i maleModel.takePhoto() } // 给女模特依次穿上所有的女装，并进行拍照 for (let j = 1; j &lt;= 50; j++) { femaleModel.underwear = 'underwear' + j femaleModel.takePhoto() } 内部状态存储于对象内部； 内部状态可以被一些对象共享； 内部状态独立于具体的场景，通常不会改变； 外部状态取决于具体的场景，并根据场景而变化，外部状态不能被共享。 职责链模式 职责链模式的定义：使多个对象都有机会处理请求，从而避免请求发送者和接受者之间的耦合关系，将这些对象连成一条链，并沿着这条链传递该请求，知道有一个对象处理它为止。职责链模式的名字非常形象，一系列可能会处理请求的对象被连成一条链，请求在这些对象之间依次传递，知道遇到一个可以处理它的对象，我们把这些对象称为链中的节点。 简单的栗子：假设我们负责一个售卖手机的电商网站，分别经过缴纳 500 元定金和 200 元定金的两轮预定后（订单已在此时生成），现在已经到了正式购买的阶段。公司针对支付过预定金的用户有一定的优惠政策。在正式购买后，已经支付过 500 元定金的用户会受到 100 元的商城优惠券，200 元定金的用户可以收到 50 元的优惠券，而之前没有支付定金的用户只能进入普通购买模式，也就是没有优惠券，且在存库有限的情况下不一定保证买到。 let order500 = function (orderType, pay, stock) { if (orderType === 1 &amp;&amp; pay === true) { console.log(&quot;500元定金预购，得到100元优惠券&quot;); } else { // 我不知道下一个节点是谁，反正把请求往后面传递 return &quot;nextSuccessor&quot;; } }; let order200 = function (orderType, pay, stock) { if (orderType === 2 &amp;&amp; pay === true) { console.log(&quot;200元定金预购，得到50元优惠券&quot;); } else { return &quot;nextSuccessor&quot;; } }; let orderNormal = function (orderType, pay, stock) { if (stock &gt; 0) { console.log(&quot;普通购买， 无优惠券&quot;); } else { console.log(&quot;库存不足&quot;); } }; let Chain = function (fn) { this.fn = fn; this.successor = null; }; // Chain.prototype.setNextSuccessor 指定在链中的下一个节点 Chain.prototype.setNextSuccessor = function (successor) { return (this.successor = successor); }; // Chain.prototype.passRequest 传递请求给某个节点 Chain.prototype.passRequest = function () { let ret = this.fn.apply(this, arguments); if (ret === &quot;nextSuccessor&quot;) { return ( this.successor &amp;&amp; this.successor.passRequest.apply(this.successor, arguments) ); } return ret; }; let chainOrder500 = new Chain(order500) let chainOrder200 = new Chain(order200) let chainOrderNormal = new Chain(orderNormal) chainOrder500.setNextSuccessor(chainOrder200) chainOrder200.setNextSuccessor(chainOrderNormal) // 500元定金预购，得到100元优惠券 chainOrder500.passRequest(1, true, 500) // 200元定金预购，得到50元优惠券 chainOrder500.passRequest(2, true, 500) // 普通购买，无优惠券 chainOrder500.passRequest(3, true, 500) // 库存不足 chainOrder500.passRequest(1, false, 0) 适配器模式 适配器模式定义：解决两个软件实体间的接口不兼容的问题。使用适配器模式之后，原本由于接口不兼容而不能工作的两个软件实体可以一起工作。适配器的别名是包装器（wrapper），这是一个相对简单的模式。在程序开发过程中有许多这样的场景：当我们试图调用模块或者对象的某个接口时，却发现这个接口的格式并不符合目前需求。这时候有两种解决办法，第一种是修改原来的接口实现，但如果原来的模板很复杂，或者我们拿到模块是一段别人编写的经过压缩的代码，修改原接口就显得不太现实了。第二种方法是创建一个适配器，将原接口转换为客户希望的另一个接口，客户只需要和适配器打交道。 let googleMap = { show: function () { console.log('开始渲染谷歌地图') } } let baiduMap = { display: function () { console.log('开始渲染百度地图') } } let baiduMapAdapter = { show: function () { return baiduMap.display() } } renderMap(googleMap) // 开始渲染谷歌地图 renderMap(baiduMapAdapter) // 开始渲染百度地图 适配器模式主要用来解决两个已有接口不匹配的问题，它不考虑这接口时怎么实现的，也不考虑他们将来可能会如何演化。适配器模式不需要改变已有的接口，就能够使他们协同作用。 装饰模式和代理模式也不会改变原有对象的接口，但装饰器模式的作用是为了给对象增加功能。装饰器模式常常形成一条长的装饰链，适配器模式通常只包装一次。代理模式为了控制对对象的访问，通常也只包装一次。 我们设计很多插件，有默认值，也算是适配器的一种应用，Vue 的 prop 校验，default 也算是适配器的应用了。 外观模式的作用倒是和适配器比较相似，有人把外观模式看成一组对象的适配器，但外观模式最显著的特点是定义了一个新的接口。 模板方法模式 模板方法模式定义：在一个方法中定义一个算法骨架，而将一些步骤的实现延迟到子类中。模板方法使得子类可以在不改变算法结构的情况下，重新定义算法中某些步骤的具体实现。 我们常用的有很多，Vue 中的 slot，React 中的 children。 class Parent { constructor() { } render() { &lt;div&gt; &lt;div name=&quot;tom&quot;&gt;&lt;/div&gt; {/* 算法过程：children要渲染在name为joe的div中 */} &lt;div name=&quot;joe&quot;&gt;{this.props.children}&lt;/div&gt; &lt;/div&gt; } } class Stage { constructor() { } render() { // 在parent中已经设定了children的渲染位置算法 &lt;Parent&gt; // children的具体实现 &lt;div&gt;child&lt;/div&gt; &lt;/Parent&gt; } } &lt;template&gt; &lt;div&gt; &lt;div name=&quot;tom&quot;&gt;&lt;/div&gt; &lt;div name=&quot;joe&quot;&gt; &lt;slot /&gt; &lt;/div&gt; &lt;/div&gt; &lt;/template&gt; &lt;template&gt; &lt;div&gt; &lt;parent&gt; &lt;!-- children的具体实现 --&gt; &lt;div&gt;child&lt;/div&gt; &lt;/parent&gt; &lt;/div&gt; &lt;/template&gt; 中介者模式 中介者模式的定义：通过一个中介者对象，其他所有的相关对象都通过该中介者来通信，而不是相互引用，当其中的一个对象发生改变时，只需要通知中介者对象即可。通过中介者模式可以解除对象与对象之间的紧耦合关系（目的就是减少耦合）。 栗子：现实生活中，航线上的飞机只需要和机场的塔台通信就能确定航线和飞行状态，而不需要和所有飞机通信。同时塔台作为中介者，知道每架飞机的飞行状态，所以可以安排所有飞机的起降和航信安排。 中介者模式使用场景：例如购物车需求，存在商品选择表单、颜色选择表单、购买数量表单等等，都会触发 change 事件，那么可以通过中介者来转发处理这些事件，实现各个事件间的解耦，仅仅维护中介者对象即可。 Redux、Vuex 都属于中介者模式的实际应用，我们把共享的数据，抽离成一个单独的 store，每个都通过 tore 这个中介者来操作对象。 推荐设计模式书籍 Head First 设计模式 大话设计模式 设计模式（可复用面向对象软件的基础）(初学者不适) JavaScript 设计模式和开发实践 图解设计模式 总结 创建设计模式：工厂，单例、建造者、原型； 结构化设计模式：外观，适配器，代理，装饰器，享元，桥接，组合； 行为型模式：策略、模板方法、观察者、迭代器、责任链、命令、备忘录、状态、访问者、终结者、解释器。 ","link":"https://faded.auspicious.space/post/design-patterns-from-javascript/"},{"title":"127 个常用的 JS 代码片段","content":" 127个常用的JS代码片段，每段代码花30秒就能看懂（一） 127个常用的JS代码片段，每段代码花30秒就能看懂（二） 127个常用的JS代码片段，每段代码花30秒就能看懂（三） 127个常用的JS代码片段，每段代码花30秒就能看懂（四） 1 all 如果数组所有元素满足函数条件，则返回 true。调用时，如果省略第二个参数，则默认传递布尔值。 const all = (arr, fn = Boolean) =&gt; arr.every(fn); all([4, 2, 3], x =&gt; x &gt; 1); // true all([1, 2, 3]); // true 2 allEqual 判断数组中的元素是否都相等。 const allEqual = arr =&gt; arr.every(val =&gt; val === arr[0]); allEqual([1, 2, 3, 4, 5, 6]); // false allEqual([1, 1, 1, 1]); // true 3 approximatelyEqual 此代码示例检查两个数字是否近似相等，差异值可以通过传参的形式进行设置。 const approximatelyEqual = (v1, v2, epsilon = 0.001) =&gt; Math.abs(v1 - v2) &lt; epsilon; approximatelyEqual(Math.PI / 2.0, 1.5708); // true 4 arrayToCSV 此段代码将没有逗号或双引号的元素转换成带有逗号分隔符的字符串即 CSV 格式识别的形式。 const arrayToCSV = (arr, delimiter = ',') =&gt; arr.map(v =&gt; v.map(x =&gt; `&quot;${x}&quot;`).join(delimiter)).join('\\n'); arrayToCSV([['a', 'b'], ['c', 'd']]); // '&quot;a&quot;,&quot;b&quot;\\n&quot;c&quot;,&quot;d&quot;' arrayToCSV([['a', 'b'], ['c', 'd']], ';'); // '&quot;a&quot;;&quot;b&quot;\\n&quot;c&quot;;&quot;d&quot;' 5 arrayToHtmlList 此段代码将数组元素转换成 &lt;li&gt; 标记，并将此元素添加至给定的 ID 元素标记内。 const arrayToHtmlList = (arr, listID) =&gt; (el =&gt; ( (el = document.querySelector('#' + listID)), (el.innerHTML += arr.map(item =&gt; `&lt;li&gt;${item}&lt;/li&gt;`).join('')) ))(); arrayToHtmlList(['item 1', 'item 2'], 'myListID'); 6 attempt 此段代码执行一个函数，将剩余的参数传回函数当参数，返回相应的结果，并能捕获异常。 const attempt = (fn, ...args) =&gt; { try { return fn(...args); } catch (e) { return e instanceof Error ? e : new Error(e); } }; var elements = attempt(function (selector) { return document.querySelectorAll(selector); }, '&gt;_&gt;'); if (elements instanceof Error) elements = []; // elements = [] 7 average 此段代码返回两个或多个数的平均数。 const average = (...nums) =&gt; nums.reduce((acc, val) =&gt; acc + val, 0) / nums.length; average(...[1, 2, 3]); // 2 average(1, 2, 3); // 2 8 averageBy 一个 map() 函数和 reduce() 函数结合的例子，此函数先通过 map() 函数将对象转换成数组，然后在调用 reduce() 函数进行累加，然后根据数组长度返回平均值。 const averageBy = (arr, fn) =&gt; arr.map(typeof fn === 'function' ? fn : val =&gt; val[fn]).reduce((acc, val) =&gt; acc + val, 0) / arr.length; averageBy([{ n: 4 }, { n: 2 }, { n: 8 }, { n: 6 }], o =&gt; o.n); // 5 averageBy([{ n: 4 }, { n: 2 }, { n: 8 }, { n: 6 }], 'n'); // 5 9 bifurcate 此函数包含两个参数，类型都为数组，依据第二个参数的真假条件，将一个参数的数组进行分组，条件为真的放入第一个数组，其它的放入第二个数组。这里运用了 Array.prototype.reduce() 和 Array.prototype.push() 相结合的形式。 const bifurcate = (arr, filter) =&gt; arr.reduce((acc, val, i) =&gt; (acc[filter[i] ? 0 : 1].push(val), acc), [[], []]); bifurcate(['beep', 'boop', 'foo', 'bar'], [true, true, false, true]); // [ ['beep', 'boop', 'bar'], ['foo'] ] 10 bifurcateBy 此段代码将数组按照指定的函数逻辑进行分组，满足函数条件的逻辑为真，放入第一个数组中，其它不满足的放入第二个数组 。这里运用了 Array.prototype.reduce() 和 Array.prototype.push() 相结合的形式，基于函数过滤逻辑，通过 Array.prototype.push() 函数将其添加到数组中。 const bifurcateBy = (arr, fn) =&gt; arr.reduce((acc, val, i) =&gt; (acc[fn(val, i) ? 0 : 1].push(val), acc), [[], []]); bifurcateBy(['beep', 'boop', 'foo', 'bar'], x =&gt; x[0] === 'b'); // [ ['beep', 'boop', 'bar'], ['foo'] ] 11 bottomVisible 用于检测页面是否滚动到页面底部。 const bottomVisible = () =&gt; document.documentElement.clientHeight + window.scrollY &gt;= (document.documentElement.scrollHeight || document.documentElement.clientHeight); bottomVisible(); // true 12 byteSize 此代码返回字符串的字节长度。这里用到了 Blob 对象，Blob（Binary Large Object）对象代表了一段二进制数据，提供了一系列操作接口。其他操作二进制数据的 API（比如 File 对象），都是建立在 Blob 对象基础上的，继承了它的属性和方法。生成 Blob 对象有两种方法：一种是使用 Blob 构造函数，另一种是对现有的 Blob 对象使用 slice 方法切出一部分。 const byteSize = str =&gt; new Blob([str]).size; byteSize('😀'); // 4 byteSize('Hello World'); // 11 13 capitalize 将字符串的首字母转成大写，这里主要运用到了 ES6 的展开语法在数组中的运用。 const capitalize = ([first, ...rest]) =&gt; first.toUpperCase() + rest.join(''); capitalize('fooBar'); // 'FooBar' capitalize('fooBar', true); // 'FooBar' 14 capitalizeEveryWord 将一个句子中每个单词首字母转换成大写字母，这里中要运用了正则表达式进行替换。 const capitalizeEveryWord = str =&gt; str.replace(/\\b[a-z]/g, char =&gt; char.toUpperCase()); capitalizeEveryWord('hello world!'); // 'Hello World!' 15 castArray 此段代码将非数值的值转换成数组对象。 const castArray = val =&gt; (Array.isArray(val) ? val : [val]); castArray('foo'); // ['foo'] castArray([1]); // [1] 16 compact 将数组中移除值为 false 的内容。 const compact = arr =&gt; arr.filter(Boolean); compact([0, 1, false, 2, '', 3, 'a', 'e' * 23, NaN, 's', 34]); // [ 1, 2, 3, 'a', 's', 34 ] 17 countOccurrences 统计数组中某个值出现的次数。 const countOccurrences = (arr, val) =&gt; arr.reduce((a, v) =&gt; (v === val ? a + 1 : a), 0); countOccurrences([1, 1, 2, 1, 2, 3], 1); // 3 18 Create Directory 此代码段使用 existSync() 检查目录是否存在，然后使用 mkdirSync() 创建目录（如果不存在）。 const fs = require('fs'); const createDirIfNotExists = dir =&gt; (!fs.existsSync(dir) ? fs.mkdirSync(dir) : undefined); createDirIfNotExists('test'); // creates the directory 'test', if it doesn't exist 19 currentURL 返回当前访问的 URL 地址。 const currentURL = () =&gt; window.location.href; currentURL(); // 'https://medium.com/@fatosmorina' 20 dayOfYear 返回当前是今年的第几天。 const dayOfYear = date =&gt; Math.floor((date - new Date(date.getFullYear(), 0, 0)) / 1000 / 60 / 60 / 24); dayOfYear(new Date()); // 272 21 decapitalize 将字符串的首字母转换成小写字母。 const decapitalize = ([first, ...rest]) =&gt; first.toLowerCase() + rest.join('') decapitalize('FooBar'); // 'fooBar' 22 deepFlatten 通过递归的形式，将多维数组展平成一维数组。 const deepFlatten = arr =&gt; [].concat(...arr.map(v =&gt; (Array.isArray(v) ? deepFlatten(v) : v))); deepFlatten([1, [2], [[3], 4], 5]); // [1,2,3,4,5] 23 default 去重对象的属性，如果对象中含有重复的属性，以前面的为准。 const defaults = (obj, ...defs) =&gt; Object.assign({}, obj, ...defs.reverse(), obj); defaults({ a: 1 }, { b: 2 }, { b: 6 }, { a: 3 }); // { a: 1, b: 2 } 24 defer 延迟函数的调用，即异步调用函数。 const defer = (fn, ...args) =&gt; setTimeout(fn, 1, ...args); defer(console.log, 'a'), console.log('b'); // logs 'b' then 'a' 25 degreesToRads 此段代码将标准的度数，转换成弧度。 const degreesToRads = deg =&gt; (deg * Math.PI) / 180.0; degreesToRads(90.0); // ~1.5708 26 difference 此段代码查找两个给定数组的差异，查找出前者数组在后者数组中不存在元素。 const difference = (a, b) =&gt; { const s = new Set(b); return a.filter(x =&gt; !s.has(x)); }; difference([1, 2, 3], [1, 2, 4]); // [3] 27 differenceBy 通过给定的函数来处理需要对比差异的数组，查找出前者数组在后者数组中不存在元素。 const differenceBy = (a, b, fn) =&gt; { const s = new Set(b.map(fn)); return a.filter(x =&gt; !s.has(fn(x))); }; differenceBy([2.1, 1.2], [2.3, 3.4], Math.floor); // [1.2] differenceBy([{ x: 2 }, { x: 1 }], [{ x: 1 }], v =&gt; v.x); // [ { x: 2 } ] 28 differenceWith 此段代码按照给定函数逻辑筛选需要对比差异的数组，查找出前者数组在后者数组中不存在元素。 const differenceWith = (arr, val, comp) =&gt; arr.filter(a =&gt; val.findIndex(b =&gt; comp(a, b)) === -1); differenceWith([1, 1.2, 1.5, 3, 0], [1.9, 3, 0], (a, b) =&gt; Math.round(a) === Math.round(b)); // [1, 1.2] 29 digitize 将输入的数字拆分成单个数字组成的数组。 const digitize = n =&gt; [...`${n}`].map(i =&gt; parseInt(i)); digitize(431); // [4, 3, 1] 30 distance 计算两点之间的距离。 const distance = (x0, y0, x1, y1) =&gt; Math.hypot(x1 - x0, y1 - y0); distance(1, 1, 2, 3); // 2.23606797749979 31 drop 此段代码将给定的数组从左边开始删除 n 个元素。 const drop = (arr, n = 1) =&gt; arr.slice(n); drop([1, 2, 3]); // [2,3] drop([1, 2, 3], 2); // [3] drop([1, 2, 3], 42); // [] 32 dropRight 此段代码将给定的数组从右边开始删除 n 个元素。 const dropRight = (arr, n = 1) =&gt; arr.slice(0, -n); dropRight([1, 2, 3]); // [1,2] dropRight([1, 2, 3], 2); // [1] dropRight([1, 2, 3], 42); // [] 33 dropRightWhile 此段代码将给定的数组按照给定的函数条件从右开始删除，直到当前元素满足函数条件为 True 时，停止删除，并返回数组剩余元素。 const dropRightWhile = (arr, func) =&gt; { while (arr.length &gt; 0 &amp;&amp; !func(arr[arr.length - 1])) arr = arr.slice(0, -1); return arr; }; dropRightWhile([1, 2, 3, 4], n =&gt; n &lt; 3); // [1, 2] 34 dropWhile 按照给的的函数条件筛选数组，不满足函数条件的将从数组中移除。 const dropWhile = (arr, func) =&gt; { while (arr.length &gt; 0 &amp;&amp; !func(arr[0])) arr = arr.slice(1); return arr; }; dropWhile([1, 2, 3, 4], n =&gt; n &gt;= 3); // [3,4] 35 elementContains 接收两个 DOM 元素对象参数，判断后者是否是前者的子元素。 const elementContains = (parent, child) =&gt; parent !== child &amp;&amp; parent.contains(child); elementContains(document.querySelector('head'), document.querySelector('title')); // true elementContains(document.querySelector('body'), document.querySelector('body')); // false 36 filterNonUnique 移除数组中重复的元素。 const filterNonUnique = arr =&gt; [ …new Set(arr)]; filterNonUnique([1, 2, 2, 3, 4, 4, 5]); // [1, 2, 3, 4, 5] 37 findKey 按照给定的函数条件，查找第一个满足条件对象的键值。 const findKey = (obj, fn) =&gt; Object.keys(obj).find(key =&gt; fn(obj[key], key, obj)); findKey( { barney: { age: 36, active: true }, fred: { age: 40, active: false }, pebbles: { age: 1, active: true } }, o =&gt; o['active'] ); // 'barney' 38 findLast 按照给定的函数条件筛选数组，将最后一个满足条件的元素进行删除。 const findLast = (arr, fn) =&gt; arr.filter(fn).pop(); findLast([1, 2, 3, 4], n =&gt; n % 2 === 1); // 3 39 flatten 按照指定数组的深度，将嵌套数组进行展平。 const flatten = (arr, depth = 1) =&gt; arr.reduce((a, v) =&gt; a.concat(depth &gt; 1 &amp;&amp; Array.isArray(v) ? flatten(v, depth - 1) : v), []); flatten([1, [2], 3, 4]); // [1, 2, 3, 4] flatten([1, [2, [3, [4, 5], 6], 7], 8], 2); // [1, 2, 3, [4, 5], 6, 7, 8] 40 forEachRight 按照给定的函数条件，从数组的右边往左依次进行执行。 const forEachRight = (arr, callback) =&gt; arr .slice(0) .reverse() .forEach(callback); forEachRight([1, 2, 3, 4], val =&gt; console.log(val)); // '4', '3', '2', '1' 41 forOwn 此段代码按照给定的函数条件，支持三个参数作为输入（值、键、对象本身），进行迭代对象。 const forOwn = (obj, fn) =&gt; Object.keys(obj).forEach(key =&gt; fn(obj[key], key, obj)); forOwn({ foo: 'bar', a: 1 }, v =&gt; console.log(v)); // 'bar', 1 42 functionName 此段代码输出函数的名称。 const functionName = fn =&gt; (console.debug(fn.name), fn); functionName(Math.max); // max (logged in debug channel of console) 43 getColonTimeFromDate 此段代码从 Date 对象里获取当前时间。 const getColonTimeFromDate = date =&gt; date.toTimeString().slice(0, 8); getColonTimeFromDate(new Date()); // &quot;08:38:00&quot; 44 getDaysDiffBetweenDates 此段代码返回两个日期之间相差多少天。 const getDaysDiffBetweenDates = (dateInitial, dateFinal) =&gt; (dateFinal - dateInitial) / (1000 * 3600 * 24); getDaysDiffBetweenDates(new Date('2019-01-13'), new Date('2019-01-15')); // 2 45 getStyle 此代码返回 DOM 元素节点对应的属性值。 const getStyle = (el, ruleName) =&gt; getComputedStyle(el)[ruleName]; getStyle(document.querySelector('p'), 'font-size'); // '16px' 46 getType 此段代码的主要功能就是返回数据的类型。 const getType = v =&gt; v === undefined ? 'undefined' : v === null ? 'null' : v.constructor.name.toLowerCase(); getType(new Set([1, 2, 3])); // 'set' 47 hasClass 此段代码返回 DOM 元素是否包含指定的 Class 样式。 const hasClass = (el, className) =&gt; el.classList.contains(className); hasClass(document.querySelector('p.special'), 'special'); // true 48 head 此段代码输出数组的第一个元素。 const head = arr =&gt; arr[0]; head([1, 2, 3]); // 1 49 hide 此段代码隐藏指定的 DOM 元素。 const hide = (...el) =&gt; [...el].forEach(e =&gt; (e.style.display = 'none')); hide(document.querySelectorAll('img')); // Hides all &lt;img&gt; elements on the page 50 httpsRedirect 此段代码的功能就是将 http 网址重定向 https 网址。 const httpsRedirect = () =&gt; { if (location.protocol !== 'https:') location.replace('https://' + location.href.split('//')[1]); }; httpsRedirect(); // If you are on http://mydomain.com, you are redirected to https://mydomain.com 51 indexOfAll 此代码可以返回数组中某个值对应的所有索引值，如果不包含该值，则返回一个空数组。 const indexOfAll = (arr, val) =&gt; arr.reduce((acc, el, i) =&gt; (el === val ? [...acc, i] : acc), []); indexOfAll([1, 2, 3, 1, 2, 3], 1); // [0,3] indexOfAll([1, 2, 3], 4); // [] 52 initial 此段代码返回数组中除最后一个元素的所有元素。 const initial = arr =&gt; arr.slice(0, -1); initial([1, 2, 3]); // [1,2]const initial = arr =&gt; arr.slice(0, -1); initial([1, 2, 3]); // [1,2] 53 insertAfter 此段代码的功能主要是在给定的 DOM 节点后插入新的节点内容。 const insertAfter = (el, htmlString) =&gt; el.insertAdjacentHTML('afterend', htmlString); insertAfter(document.getElementById('myId'), '&lt;p&gt;after&lt;/p&gt;'); // &lt;div id=&quot;myId&quot;&gt;...&lt;/div&gt; &lt;p&gt;after&lt;/p&gt; 54 insertBefore 此段代码的功能主要是在给定的 DOM 节点前插入新的节点内容。 const insertBefore = (el, htmlString) =&gt; el.insertAdjacentHTML('beforebegin', htmlString); insertBefore(document.getElementById('myId'), '&lt;p&gt;before&lt;/p&gt;'); // &lt;p&gt;before&lt;/p&gt; &lt;div id=&quot;myId&quot;&gt;...&lt;/div&gt; 55 intersection 此段代码返回两个数组元素之间的交集。 const intersection = (a, b) =&gt; { const s = new Set(b); return a.filter(x =&gt; s.has(x)); }; intersection([1, 2, 3], [4, 3, 2]); // [2, 3] 56 intersectionBy 按照给定的函数处理需要对比的数组元素，然后根据处理后的数组，找出交集，最后从第一个数组中将对应的元素输出。 const intersectionBy = (a, b, fn) =&gt; { const s = new Set(b.map(fn)); return a.filter(x =&gt; s.has(fn(x))); }; intersectionBy([2.1, 1.2], [2.3, 3.4], Math.floor); // [2.1] 57 intersectionWith 按照给定的函数对比两个数组的差异，然后找出交集，最后从第一个数组中将对应的元素输出。 const intersectionWith = (a, b, comp) =&gt; a.filter(x =&gt; b.findIndex(y =&gt; comp(x, y)) !== -1); intersectionWith([1, 1.2, 1.5, 3, 0], [1.9, 3, 0, 3.9], (a, b) =&gt; Math.round(a) === Math.round(b)); // [1.5, 3, 0] 58 is 此段代码用于判断数据是否为指定的数据类型，如果是则返回 true。 const is = (type, val) =&gt; ![, null].includes(val) &amp;&amp; val.constructor === type; is(Array, [1]); // true is(ArrayBuffer, new ArrayBuffer()); // true is(Map, new Map()); // true is(RegExp, /./g); // true is(Set, new Set()); // true is(WeakMap, new WeakMap()); // true is(WeakSet, new WeakSet()); // true is(String, ''); // true is(String, new String('')); // true is(Number, 1); // true is(Number, new Number(1)); // true is(Boolean, true); // true is(Boolean, new Boolean(true)); // true 59 isAfterDate 接收两个日期类型的参数，判断前者的日期是否晚于后者的日期。 const isAfterDate = (dateA, dateB) =&gt; dateA &gt; dateB; isAfterDate(new Date(2010, 10, 21), new Date(2010, 10, 20)); // true 60 isAnagram 用于检测两个单词是否相似。 const isAnagram = (str1, str2) =&gt; { const normalize = str =&gt; str .toLowerCase() .replace(/[^a-z0-9]/gi, '') .split('') .sort() .join(''); return normalize(str1) === normalize(str2); }; isAnagram('iceman', 'cinema'); // true 61 isArrayLike 此段代码用于检测对象是否为类数组对象,是否可迭代。 const isArrayLike = obj =&gt; obj != null &amp;&amp; typeof obj[Symbol.iterator] === 'function'; isArrayLike(document.querySelectorAll('.className')); // true isArrayLike('abc'); // true isArrayLike(null); // false 62 isBeforeDate 接收两个日期类型的参数，判断前者的日期是否早于后者的日期。 const isBeforeDate = (dateA, dateB) =&gt; dateA &lt; dateB; isBeforeDate(new Date(2010, 10, 20), new Date(2010, 10, 21)); // true 63 isBoolean 此段代码用于检查参数是否为布尔类型。 const isBoolean = val =&gt; typeof val === 'boolean'; isBoolean(null); // false isBoolean(false); // true 64 getColonTimeFromDate 用于判断程序运行环境是否在浏览器，这有助于避免在 node 环境运行前端模块时出错。 const isBrowser = () =&gt; ![typeof window, typeof document].includes('undefined'); isBrowser(); // true (browser) isBrowser(); // false (Node) 65 isBrowserTabFocused 用于判断当前页面是否处于活动状态（显示状态）。 const isBrowserTabFocused = () =&gt; !document.hidden; isBrowserTabFocused(); // true 66 isLowerCase 用于判断当前字符串是否都为小写。 const isLowerCase = str =&gt; str === str.toLowerCase(); isLowerCase('abc'); // true isLowerCase('a3@$'); // true isLowerCase('Ab4'); // false 67 isNil 用于判断当前变量的值是否为 null 或 undefined 类型。 const isNil = val =&gt; val === undefined || val === null; isNil(null); // true isNil(undefined); // true 68 isNull 用于判断当前变量的值是否为 null 类型。 const isNull = val =&gt; val === null; isNull(null); // true 69 isNumber 用于检查当前的值是否为数字类型。 function isNumber(n) { return !isNaN(parseFloat(n)) &amp;&amp; isFinite(n); } isNumber('1'); // false isNumber(1); // true 70 isObject 用于判断参数的值是否是对象，这里运用了 Object 构造函数创建一个对象包装器，如果是对象类型，将会原值返回。 const isObject = obj =&gt; obj === Object(obj); isObject([1, 2, 3, 4]); // true isObject([]); // true isObject(['Hello!']); // true isObject({ a: 1 }); // true isObject({}); // true isObject(true); // false 71 isObjectLike 用于检查参数的值是否为 null 以及类型是否为对象。 const isObjectLike = val =&gt; val !== null &amp;&amp; typeof val === 'object'; isObjectLike({}); // true isObjectLike([1, 2, 3]); // true isObjectLike(x =&gt; x); // false isObjectLike(null); // false 72 isPlainObject 此代码段检查参数的值是否是由 Object 构造函数创建的对象。 const isPlainObject = val =&gt; !!val &amp;&amp; typeof val === 'object' &amp;&amp; val.constructor === Object; isPlainObject({ a: 1 }); // true isPlainObject(new Map()); // false 73 isPromiseLike 用于检查当前的对象是否类似 Promise 函数。 const isPromiseLike = obj =&gt; obj !== null &amp;&amp; (typeof obj === 'object' || typeof obj === 'function') &amp;&amp; typeof obj.then === 'function'; isPromiseLike({ then: function () { return ''; } }); // true isPromiseLike(null); // false isPromiseLike({}); // false 74 isSameDate 用于判断给定的两个日期是否是同一天。 const isSameDate = (dateA, dateB) =&gt; dateA.toISOString() === dateB.toISOString(); isSameDate(new Date(2010, 10, 20), new Date(2010, 10, 20)); // true 75 isString 用于检查当前的值是否为字符串类型。 const isString = val =&gt; typeof val === 'string'; isString('10'); // true 76 isSymbol 用于判断参数的值是否是 Symbol 类型。 const isSymbol = val =&gt; typeof val === 'symbol'; isSymbol(Symbol('x')); // true 77 isUndefined 用于判断参数的类型是否是 Undefined 类型。 const isUndefined = val =&gt; val === undefined; isUndefined(undefined); // true 78 isUpperCase 用于判断当前字符串的字母是否都为大写。 const isUpperCase = str =&gt; str === str.toUpperCase(); isUpperCase('ABC'); // true isLowerCase('A3@$'); // true isLowerCase('aB4'); // false 79 isValidJSON 用于判断给定的字符串是否是 JSON 字符串。 const isValidJSON = str =&gt; { try { JSON.parse(str); return true; } catch (e) { return false; } }; isValidJSON('{&quot;name&quot;:&quot;Adam&quot;,&quot;age&quot;:20}'); // true isValidJSON('{&quot;name&quot;:&quot;Adam&quot;,age:&quot;20&quot;}'); // false isValidJSON(null); // true 80 last 此函数功能返回数组的最后一个元素。 const last = arr =&gt; arr[arr.length - 1]; last([1, 2, 3]); // 3 81 matches 此函数功能用于比较两个对象，以确定第一个对象是否包含与第二个对象相同的属性与值。 const matches = (obj, source) =&gt; Object.keys(source).every(key =&gt; obj.hasOwnProperty(key) &amp;&amp; obj[key] === source[key]); matches({ age: 25, hair: 'long', beard: true }, { hair: 'long', beard: true }); // true matches({ hair: 'long', beard: true }, { age: 25, hair: 'long', beard: true }); // false 82 maxDate 此代码段查找日期数组中最大的日期进行输出。 const maxDate = (...dates) =&gt; new Date(Math.max.apply(null, ...dates)); const array = [ new Date(2017, 4, 13), new Date(2018, 2, 12), new Date(2016, 0, 10), new Date(2016, 0, 9) ]; maxDate(array); // 2018-03-11T22:00:00.000Z 83 maxN 此段代码输出数组中前 n 位最大的数。 const maxN = (arr, n = 1) =&gt; [...arr].sort((a, b) =&gt; b - a).slice(0, n); maxN([1, 2, 3]); // [3] maxN([1, 2, 3], 2); // [3,2] 84 minDate 此代码段查找日期数组中最早的日期进行输出。 const minDate = (...dates) =&gt; new Date(Math.min.apply(null, ...dates)); const array = [ new Date(2017, 4, 13), new Date(2018, 2, 12), new Date(2016, 0, 10), new Date(2016, 0, 9) ]; minDate(array); // 2016-01-08T22:00:00.000Z ","link":"https://faded.auspicious.space/post/127-ge-chang-yong-de-js-dai-ma-pian-duan/"},{"title":"处理 JavaScript 中的非预期数据","content":" [译] 处理 JavaScript 中的非预期数据 动态类型语言的最大问题就是无法保证数据流总是正确的，因为我们无法“强行控制”一个参数或变量，比方说，让它不为 null。当我们面对这些情况时的标准做法是简单地做一个判断： function foo (mustExist) { if (!mustExist) throw new Error('Parameter cannot be null') return ... } 这样做的问题在于会污染我们的代码，因为要随处做判断，并且实际上也无法保证每一位开发代码的人都像这样判断；我们甚至都不知道这样被传进来的一个参数是 undefined 还是 null，这在不同团队负责前后端的情况下司空见惯，也是大概率的情况。 如何以更好的方式让“非预期”数据造成的副作用最小化呢？作为一个 后端开发者，我想给出一些个人化的意见。 一切的源点 数据有多种来源，最主要的当然就是 用户输入。但是，也存在其它有缺陷数据的来源，比如数据库、函数返回值中的隐形空数据、外部 API 等。 我们稍后将展开讨论以如何不同的方式对待每一种的情况，要知道毕竟没什么灵丹妙药。大多数这些非预期数据的起源都是人为失误，当语言解析到 null 或 undefined 时，与之配套的逻辑却没准备好处理它们。 用户输入 在这种情况下，我们能做的不多，如果是用户输入的问题，我们通过称为 补水（Hydration） 的方式处理它。换句话说，我们得拿到用户发来的原始输入，比如一个 API 中的负载，并将其转换为我们可以无错应用的某些形式。 在后端，当使用 Express 这样的 Web 服务器时，我们可以通过标准的 JSON Schema (&lt;www.npmjs.com/package/ajv&gt;) 或是 Joi 这样的工具对来自前端的用户输入执行所有的操作。 关于我们能用 Express 和 AJV 对一个路由做什么的例子可能是下面这样： const Ajv = require('ajv') const Express = require('express') const bodyParser = require('body-parser') const app = Express() const ajv = new Ajv() app.use(bodyParser.json()) app.get('/foo', (req, res) =&gt; { const schema = { type: 'object', properties: { name: { type: 'string' }, password: { type: 'string' }, email: { type: 'string', format: 'email' } }, additionalProperties: false required: ['name', 'password', 'email'] } const valid = ajv.validate(schema, req.body) if (!valid) return res.status(422).json(ajv.errors) // ... }) app.listen(3000) 可见我们对一个路由中请求的 body 做了校验，默认情况下 body 是个从 body-parser 包中通过负载接收到的对象，在本例中将其传到一个 JSON-Schema 实例中校验，看看其中的某个属性是否有不同的类型或格式。 重要： 注意我们返回了一个 HTTP 422 Unprocessable Entity 状态码，意味着“无法处理的实体”。许多人对待像这样 body 或者 query 错误的请求，使用了表示整体错误的 400 Bad Request 报错；在这种情况中，请求本身并没有错，只是用户发送的数据不符合预期而已。 默认值的可选参数 我们之前做的校验的一个额外收获是，我们开启了一种可能性，那就是 如果一个可选域没有被传值，一个空值也能被传递进我们的应用 。例如，想象一个有 page 和 size 两个参数作为查询字符串的分页路由，但二者都不是必须的；如果它们都没收到的话，必须设定一个默认值。 理想的话，我们的控制器里应该有一个像这样的函数： function searchSomething (filter, page = 1, size = 10) { // ... } 注意：正如之前我们返回的 422 一样，对于分页查询，重要的是返回恰当的状态码，无论何时对于一个只在返回值中包含了部分数据的请求，都应该返回 HTTP 206 Partial Content，也就是“不完整的内容”；当用户到达最后一页且再没有更多数据时，才返回 200；如果用户尝试查询超出了总范围的页数，则返回一个 204 No Content。 这将会解决我们接受两个空值的案例，但这触碰到了在 JavaScript 中通常非常引起争论的一点。对于可选参数的默认值，只假设了 当且仅当 其为空的情况，而为 null 时就不灵了。所以如果我们这样操作： function foo(a = 10) { console.log(a) } foo(undefined) // 10 foo(20) // 20 foo(null) // null 因此，不能仅靠可选参数。对于这样的情况我们有两种处理方式： 前端控制器中的 if 语句，虽然看着有点啰嗦：function searchSomething(filter, page = 1, size = 10) { if (!page) page = 1 if (!size) size = 10 // ... } 直接用 JSON-Schema 处理路由： 可以再次使用 AJV 或 @expresso/validator 来校验数据：app.get('/foo', (req, res) =&gt; { const schema = { type: 'object', properties: { page: { type: 'number', default: 1 }, size: { type: 'number', default: 10 }, }, additionalProperties: false } const valid = ajv.validate(schema, req.params) if (!valid) return res.status(422).json(ajv.errors) // ... }) 应对 null 和 undefined 我个人对在 JavaScript 中用 null 还是 undefined 来表示空值这类争论兴趣不大。 如果你对这些概念仍有疑问，下图是个很好的比方： 现在我们知道了每种定义，而 JavaScript 在 2020 将新增了两个实验性的特性（译注：部分引自 MDN）。 空值合并运算符 ?? 空值合并运算符 ?? 是一个逻辑运算符。当左侧操作数为 null 或 undefined 时，其返回右侧的操作数。否则返回左侧的操作数。 let myText = ''; let notFalsyText = myText || 'Hello world'; console.log(notFalsyText); // Hello world let preservingFalsy = myText ?? 'Hi neighborhood'; console.log(preservingFalsy); // '' 可选链操作符 ?. ?. 运算符功能类似于 . 运算符，不同之处在于如果链条上的一个引用 null 或 undefined，. 操作符会引起一个错误，而 ?. 操作符则会按照短路计算的方式返回一个 undefined。 const adventurer = { name: 'Alice', cat: { name: 'Dinah' } }; const dogName = adventurer.dog?.name; console.log(dogName); // undefined console.log(adventurer.someNonExistentMethod?.()) // undefined 结合 空值合并运算符 ?? 使用： let customer = { name: &quot;Carl&quot;, details: { age: 82 } }; let customerCity = customer?.city ?? &quot;Unknown city&quot;; console.log(customerCity); // Unknown city 这两项新增特性将让事情简单得多，因为我们可以把焦点集中在 null 和 undefined 上从而作出恰当的操作了；用 ?? 而不是布尔值判断 !obj 更易于处理很多错误情况。 隐性 null 函数 这个暗中作祟的问题更加复杂。一些函数会假设要处理的数据都是正确填充的，但有时并不能如意： function foo(num) { return 23 * num } 若 num 为 null，则函数返回值会为 0（译注：如果操作值之一不是数值，则被隐式调用 Number() 进行转换），这不符合我们的期望。在这种情况下，我们能做的只有加上判断。可行的判断形式有两种，第一种可以简单地使用 if： function foo(num) { if (!num) throw new Error('Error') return 23 * num } 第二种办法是使用一个叫做 Either 的 Monad（译注：Monad 是一种对函数计算过程的通用抽象机制，关键是统一形式和操作模式，相当于是把值包装在一个 context 中。https://zhuanlan.zhihu.com/p/65449477）中。对于数据是不是 null 这种模棱两可的问题，这可是个好办法；因为 JavaScript 已经有了一个支持双动作流的原生的函数，即 Promise： function exists(value) { return x != null ? Promise.resolve(value) : Promise.reject(`Invalid value: ${value}`) } async function foo(num) { return exists(num).then(v =&gt; 23 * v) } 通过这种方式就可以把来自 exists 中的 catch 方法委派到调用 foo 的函数中： function init(n) { foo(n) .then(console.log) .catch(console.error) } init(12) // 276 init(null) // Invalid value: null 外部 API 和数据库记录 这也是相当常见的情况，特别是当系统是在先前创建和填充的数据库之上开发的时候。例如，一个沿用之前成功产品数据库的新产品、在不同系统间整合用户等等。 这里的大问题不在于不知道数据库，实际上则是我们不知道在数据库层面有什么已经被完成了，我们没法证明数据会不会是 null 或 undefined。另一个问题是缺乏文档，难以令人满意的数据库文档化还是会带来前面一个问题。 因为返回值数据量可能较大，这样的情况能施展的空间也不大，除了不得不对个别数据作出判断外，在对成组的数据进行正式操作之前用 map 或 filter 进行一遍过滤是个好的做法。 抛出 Errors 对于数据库和外部 API 中的服务器代码使用 断言函数（Assertion Functions） 也是个好的实践，基本上这些函数的做法就是如果数据存在就返回否则报错。这类函数的大多数常见情况，比方说有一个根据一个 id 搜索某种数据的 API： async function findById(id) { if (!id) throw new InvalidIDError(id) const result = await entityRepository.findById(id) if (!result) throw new EntityNotFoundError(id) return result } 实际应用中，应把 Entity 替换为符合情况的名字，如 UserNotFoundError。 该做法之所以好，是因为我们可以用这样一个函数找到的 user，可以被另外的函数用来检索位于其它数据库中的相关数据，比如用户的详细资料；而当我们调用后一个检索函数时，前置函数 findUser 已经 保证 了 user 的真实存在，因为如果出错就会抛出错误并可以据此直接在路由逻辑中找到问题。 async function findUserProfiles(userId) { const user = await findUser(userId) const profile = await profileRepository.findById(user.profileId) if (!profile) throw new ProfileNotFoundError(user.profileId) return profile } 路由逻辑会像这样： app.get('/users/{id}/profiles', handler) // --- // async function handler(req, res) { try { const userId = req.params.id const profile = await userService.findUserProfiles(userId) return res.status(200).json(profile) } catch (e) { if (e instanceof UserNotFoundError || e instanceof ProfileNotFoundError) return res.status(404).json(e.message) if (e instanceof InvalidIDError) return res.status(400).json(e.message) } } 总结 在必要的地方单独判断非预期数据； 设置可选参数的默认值； 用 ajv 等工具对可能不完整的数据进行补水处理； 恰当使用实验性的 空值合并运算符 ?? 和 可选链操作符 ?.； 用 Promise 包装隐性的空值、统一操作模式； 用前置的 map 或 filter 过滤成组数据中的非预期数据； 在职责明确的控制器函数中，各自抛出类型明确的错误。 用这些方法处理数据就能得到连续而可预测的信息流了。 ","link":"https://faded.auspicious.space/post/dealing-with-unexpected-data-in-javascript/"},{"title":"25 个数组 reduce 高级用法","content":" 25个你不得不知道的数组reduce高级用法 概况 reduce 作为 ES5 新增的常规数组方法之一，对比 forEach、filter 和 map，在实际使用上好像有些被忽略，发现身边的人极少使用它，导致这个如此强大的方法被逐渐埋没。 下面对 reduce 的语法进行简单说明，详情可查看 MDN 的 reduce() 的相关说明。 定义：对数组中的每个元素执行一个自定义的累计器，将其结果汇总为单个返回值 形式：array.reduce((t, v, i, a) =&gt; {}, initValue) 参数 allback：回调函数（必选） initValue：初始值（可选） 回调函数的参数 total(t)：累计器完成计算的返回值（必选） value(v)：当前元素（必选） index(i)：当前元素的索引（可选） array(a)：当前元素所属的数组对象（可选） 过程 以 t 作为累计结果的初始值，不设置 t 则以数组第一个元素为初始值 开始遍历，使用累计器处理 v，将 v 的映射结果累计到 t 上，结束此次循环，返回 t 进入下一次循环，重复上述操作，直至数组最后一个元素 结束遍历，返回最终的 t reduce 的精华所在是将累计器逐个作用于数组成员上，把上一次输出的值作为下一次输入的值。下面举个简单的栗子，看看 reduce 的计算结果。 const arr = [3, 5, 1, 4, 2]; const a = arr.reduce((t, v) =&gt; t + v); // 等同于 const b = arr.reduce((t, v) =&gt; t + v, 0); 代码不太明白没关系，贴一个 reduce 的作用动图应该就会明白了。 reduce 实质上是一个累计器函数，通过用户自定义的累计器对数组成员进行自定义累计，得出一个由累计器生成的值。另外 reduce 还有一个胞弟 reduceRight，两个方法的功能其实是一样的，只不过 reduce 是升序执行，reduceRight 是降序执行。 对空数组调用 reduce() 和 reduceRight() 是不会执行其回调函数的，可认为 reduce() 对空数组无效 高级用法 累加累乘 function Accumulation(...vals) { return vals.reduce((t, v) =&gt; t + v, 0); } function Multiplication(...vals) { return vals.reduce((t, v) =&gt; t * v, 1); } Accumulation(1, 2, 3, 4, 5); // 15 Multiplication(1, 2, 3, 4, 5); // 120 权重求和 const scores = [ { score: 90, subject: &quot;chinese&quot;, weight: 0.5 }, { score: 95, subject: &quot;math&quot;, weight: 0.3 }, { score: 85, subject: &quot;english&quot;, weight: 0.2 } ]; const result = scores.reduce((t, v) =&gt; t + v.score * v.weight, 0); // 90.5 代替 reverse function Reverse(arr = []) { return arr.reduceRight((t, v) =&gt; (t.push(v), t), []); } Reverse([1, 2, 3, 4, 5]); // [5, 4, 3, 2, 1] 代替 map 和 filter const arr = [0, 1, 2, 3]; // 代替map：[0, 2, 4, 6] const a = arr.map(v =&gt; v * 2); const b = arr.reduce((t, v) =&gt; [...t, v * 2], []); // 代替filter：[2, 3] const c = arr.filter(v =&gt; v &gt; 1); const d = arr.reduce((t, v) =&gt; v &gt; 1 ? [...t, v] : t, []); // 代替map和filter：[4, 6] const e = arr.map(v =&gt; v * 2).filter(v =&gt; v &gt; 2); const f = arr.reduce((t, v) =&gt; v * 2 &gt; 2 ? [...t, v * 2] : t, []); 代替 some 和 every const scores = [ { score: 45, subject: &quot;chinese&quot; }, { score: 90, subject: &quot;math&quot; }, { score: 60, subject: &quot;english&quot; } ]; // 代替some：至少一门合格 const isAtLeastOneQualified = scores.reduce((t, v) =&gt; t || v.score &gt;= 60, false); // true // 代替every：全部合格 const isAllQualified = scores.reduce((t, v) =&gt; t &amp;&amp; v.score &gt;= 60, true); // false 数组分割 function Chunk(arr = [], size = 1) { return arr.length ? arr.reduce((t, v) =&gt; (t[t.length - 1].length === size ? t.push([v]) : t[t.length - 1].push(v), t), [[]]) : []; } 数组过滤 function Difference(arr = [], oarr = []) { return arr.reduce((t, v) =&gt; (!oarr.includes(v) &amp;&amp; t.push(v), t), []); } const arr1 = [1, 2, 3, 4, 5]; const arr2 = [2, 3, 6] Difference(arr1, arr2); // [1, 4, 5] 数组填充 function Fill(arr = [], val = &quot;&quot;, start = 0, end = arr.length) { if (start &lt; 0 || start &gt;= end || end &gt; arr.length) return arr; return [ ...arr.slice(0, start), ...arr.slice(start, end).reduce((t, v) =&gt; (t.push(val || v), t), []), ...arr.slice(end, arr.length) ]; } const arr = [0, 1, 2, 3, 4, 5, 6]; Fill(arr, &quot;aaa&quot;, 2, 5); // [0, 1, &quot;aaa&quot;, &quot;aaa&quot;, &quot;aaa&quot;, 5, 6] 数组扁平 function Flat(arr = []) { return arr.reduce((t, v) =&gt; t.concat(Array.isArray(v) ? Flat(v) : v), []) } const arr = [0, 1, [2, 3], [4, 5, [6, 7]], [8, [9, 10, [11, 12]]]]; Flat(arr); // [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] 数组去重 function Uniq(arr = []) { return arr.reduce((t, v) =&gt; t.includes(v) ? t : [...t, v], []); } const arr = [2, 1, 0, 3, 2, 1, 2]; Uniq(arr); // [2, 1, 0, 3] 数组最大最小值 function Max(arr = []) { return arr.reduce((t, v) =&gt; t &gt; v ? t : v); } function Min(arr = []) { return arr.reduce((t, v) =&gt; t &lt; v ? t : v); } const arr = [12, 45, 21, 65, 38, 76, 108, 43]; Max(arr); // 108 Min(arr); // 12 数组成员独立拆解 function Unzip(arr = []) { return arr.reduce( (t, v) =&gt; (v.forEach((w, i) =&gt; t[i].push(w)), t), Array.from({ length: Math.max(...arr.map(v =&gt; v.length)) }).map(v =&gt; []) ); } const arr = [[&quot;a&quot;, 1, true], [&quot;b&quot;, 2, false]]; Unzip(arr); // [[&quot;a&quot;, &quot;b&quot;], [1, 2], [true, false]] 数组成员个数统计 function Count(arr = []) { return arr.reduce((t, v) =&gt; (t[v] = (t[v] || 0) + 1, t), {}); } const arr = [0, 1, 1, 2, 2, 2]; Count(arr); // { 0: 1, 1: 2, 2: 3 } 此方法是字符统计和单词统计的原理，入参时把字符串处理成数组即可 数组成员位置记录 function Position(arr = [], val) { return arr.reduce((t, v, i) =&gt; (v === val &amp;&amp; t.push(i), t), []); } const arr = [2, 1, 5, 4, 2, 1, 6, 6, 7]; Position(arr, 2); // [0, 4] 数组成员特性分组 function Group(arr = [], key) { return key ? arr.reduce((t, v) =&gt; (!t[v[key]] &amp;&amp; (t[v[key]] = []), t[v[key]].push(v), t), {}) : {}; } const arr = [ { area: &quot;GZ&quot;, name: &quot;YZW&quot;, age: 27 }, { area: &quot;GZ&quot;, name: &quot;TYJ&quot;, age: 25 }, { area: &quot;SZ&quot;, name: &quot;AAA&quot;, age: 23 }, { area: &quot;FS&quot;, name: &quot;BBB&quot;, age: 21 }, { area: &quot;SZ&quot;, name: &quot;CCC&quot;, age: 19 } ]; // 以地区area作为分组依据 Group(arr, &quot;area&quot;); // { GZ: Array(2), SZ: Array(2), FS: Array(1) } 数组成员所含关键字统计 function Keyword(arr = [], keys = []) { return keys.reduce((t, v) =&gt; (arr.some(w =&gt; w.includes(v)) &amp;&amp; t.push(v), t), []); } const text = [ &quot;今天天气真好，我想出去钓鱼&quot;, &quot;我一边看电视，一边写作业&quot;, &quot;小明喜欢同桌的小红，又喜欢后桌的小君，真TM花心&quot;, &quot;最近上班喜欢摸鱼的人实在太多了，代码不好好写，在想入非非&quot; ]; const keyword = [&quot;偷懒&quot;, &quot;喜欢&quot;, &quot;睡觉&quot;, &quot;摸鱼&quot;, &quot;真好&quot;, &quot;一边&quot;, &quot;明天&quot;]; Keyword(text, keyword); // [&quot;喜欢&quot;, &quot;摸鱼&quot;, &quot;真好&quot;, &quot;一边&quot;] 字符串翻转 function ReverseStr(str = &quot;&quot;) { return str.split(&quot;&quot;).reduceRight((t, v) =&gt; t + v); } const str = &quot;reduce最牛逼&quot;; ReverseStr(str); // &quot;逼牛最ecuder&quot; 数字千分化 function ThousandNum(num = 0) { const str = (+num).toString().split(&quot;.&quot;); const int = nums =&gt; nums.split(&quot;&quot;).reverse().reduceRight((t, v, i) =&gt; t + (i % 3 ? v : `${v},`), &quot;&quot;).replace(/^,|,$/g, &quot;&quot;); const dec = nums =&gt; nums.split(&quot;&quot;).reduce((t, v, i) =&gt; t + ((i + 1) % 3 ? v : `${v},`), &quot;&quot;).replace(/^,|,$/g, &quot;&quot;); return str.length &gt; 1 ? `${int(str[0])}.${dec(str[1])}` : int(str[0]); } ThousandNum(1234); // &quot;1,234&quot; ThousandNum(1234.00); // &quot;1,234&quot; ThousandNum(0.1234); // &quot;0.123,4&quot; ThousandNum(1234.5678); // &quot;1,234.567,8&quot; 异步累计 async function AsyncTotal(arr = []) { return arr.reduce(async(t, v) =&gt; { const at = await t; const todo = await Todo(v); at[v] = todo; return at; }, Promise.resolve({})); } const result = await AsyncTotal(); // 需要在async包围下使用 斐波那契数列 function Fibonacci(len = 2) { const arr = [...new Array(len).keys()]; return arr.reduce((t, v, i) =&gt; (i &gt; 1 &amp;&amp; t.push(t[i - 1] + t[i - 2]), t), [0, 1]); } Fibonacci(10); // [0, 1, 1, 2, 3, 5, 8, 13, 21, 34] URL 参数反序列化 function ParseUrlSearch() { return location.search.replace(/(^\\?)|(&amp;$)/g, &quot;&quot;).split(&quot;&amp;&quot;).reduce((t, v) =&gt; { const [key, val] = v.split(&quot;=&quot;); t[key] = decodeURIComponent(val); return t; }, {}); } // 假设URL为：https://www.baidu.com?age=25&amp;name=TYJ ParseUrlSearch(); // { age: &quot;25&quot;, name: &quot;TYJ&quot; } URL 参数序列化 function StringifyUrlSearch(search = {}) { return Object.entries(search).reduce( (t, v) =&gt; `${t}${v[0]}=${encodeURIComponent(v[1])}&amp;`, Object.keys(search).length ? &quot;?&quot; : &quot;&quot; ).replace(/&amp;$/, &quot;&quot;); } StringifyUrlSearch({ age: 27, name: &quot;YZW&quot; }); // &quot;?age=27&amp;name=YZW&quot; 返回对象指定键值 function GetKeys(obj = {}, keys = []) { return Object.keys(obj).reduce((t, v) =&gt; (keys.includes(v) &amp;&amp; (t[v] = obj[v]), t), {}); } const target = { a: 1, b: 2, c: 3, d: 4 }; const keyword = [&quot;a&quot;, &quot;d&quot;]; GetKeys(target, keyword); // { a: 1, d: 4 } 数组转对象 const people = [ { area: &quot;GZ&quot;, name: &quot;YZW&quot;, age: 27 }, { area: &quot;SZ&quot;, name: &quot;TYJ&quot;, age: 25 } ]; const map = people.reduce((t, v) =&gt; { const { name, ...rest } = v; t[name] = rest; return t; }, {}); // { YZW: {…}, TYJ: {…} } Redux Compose 函数原理 function Compose(...funs) { if (funs.length === 0) { return arg =&gt; arg; } if (funs.length === 1) { return funs[0]; } return funs.reduce((t, v) =&gt; (...arg) =&gt; t(v(...arg))); } 性能 另外，有些同学可能会问，reduce 的性能又如何呢？下面我们通过对 for、forEach、map 和 reduce 四个方法同时做 1~100000 的累加操作，看看四个方法各自的执行时间。 // 创建一个长度为100000的数组 const list = [...new Array(100000).keys()]; // for console.time(&quot;for&quot;); let result1 = 0; for (let i = 0; i &lt; list.length; i++) { result1 += i + 1; } console.log(result1); console.timeEnd(&quot;for&quot;); // forEach console.time(&quot;forEach&quot;); let result2 = 0; list.forEach(v =&gt; (result2 += v + 1)); console.log(result2); console.timeEnd(&quot;forEach&quot;); // map console.time(&quot;map&quot;); let result3 = 0; list.map(v =&gt; (result3 += v + 1, v)); console.log(result3); console.timeEnd(&quot;map&quot;); // reduce console.time(&quot;reduce&quot;); const result4 = list.reduce((t, v) =&gt; t + v + 1, 0); console.log(result4); console.timeEnd(&quot;reduce&quot;); 累加操作 执行时间 for 10.171875ms forEach 6.744140625ms map 8.202880859375ms reduce 3.65185546875ms 以上代码在 Surface Pro 5 8G 内存 Microsoft Edge 版本 83.0.478.28 (官方内部版本) beta (64 位) 下执行，不同的机器不同的环境下执行以上代码都有可能存在差异。 ","link":"https://faded.auspicious.space/post/25-advanced-uses-of-array-reduce/"},{"title":"开箱即用的 JS 干货","content":" 开箱即用的JS干货助力金三银四 浏览器相关 检查是否为浏览器环境 const isBrowser = () =&gt; ![typeof window, typeof document].includes('undefined') isBrowser() // true (browser) isBrowser() // false (Node) 判断手机类型 getMobile() { var u = navigator.userAgent var isAndroid = u.indexOf('Android') &gt; -1 || u.indexOf('Linux') &gt; -1 // g var isIOS = !!u.match(/\\(i[^;]+;( U;)? CPU.+Mac OS X/) // ios终端 if (isAndroid) { return 'Android' } if (isIOS) { return 'iOS' } } 判断微信 / QQ 浏览器 let url = navigator.userAgent.toLowerCase() //使用toLowerCase将字符串全部转为小写 方便我们判断使用 if (url.indexOf('15b202 qq') &gt; -1) { //单独判断QQ内置浏览器 alert('QQ APP 内置浏览器，做你想做的操作') } if (url.indexOf('micromessenger') &gt; -1) { //单独判断微信内置浏览器 alert('微信内置浏览器，做你想做的操作') } if (url.indexOf('15b202') &gt; -1) { //判断微信内置浏览器，QQ内置浏览器 alert('QQ和微信内置浏览器，做你想做的操作') } 判断手机开屏 / 息屏 document.addEventListener('visibilitychange', () =&gt; { console.log(document.visibilityState) if (document.visibilityState === 'hidden') { console.log('息屏时间') } else if (document.visibilityState === 'visible') { console.log('开屏时间') } }) 监听浏览器的联网状态 window.addEventListener(&quot;offline&quot;, function (e) { alert(&quot;offline&quot;); }) window.addEventListener(&quot;online&quot;, function (e) { alert(&quot;online&quot;); }) if (window.navigator.onLine == true) { alert(&quot;已连接&quot;) } else { alert(&quot;未连接&quot;) } JavaScript 检测手机是否横屏 window.addEventListener('resize', () =&gt; { if (window.orientation === 180 || window.orientation === 0) { // 正常方向或屏幕旋转180度 console.log('竖屏') } if (window.orientation === 90 || window.orientation === -90) { // 屏幕顺时钟旋转90度或屏幕逆时针旋转90度 console.log('横屏') } }) 字符串相关 首字母大写 const capitalize = ([first, ...rest]) =&gt; first.toUpperCase() + rest.join('') capitalize('fooBar') // 'FooBar' 单个单词首字母大写 const capitalizeEveryWord = str =&gt; str.replace(/\\b[a-z]/g, char =&gt; char.toUpperCase()) capitalizeEveryWord('hello world!') // 'Hello World!' 删除字符串中的 HTML 标签 const stripHTMLTags = str =&gt; str.replace(/&lt;[^&gt;]*&gt;/g, '') stripHTMLTags('&lt;p&gt;&lt;em&gt;Hello&lt;/em&gt; &lt;strong&gt;World&lt;/strong&gt;&lt;/p&gt;') // 'Hello World!' 字符串翻转 // 方法一 var arr = str.split('') var newArr = [] for (var i = 0; i &lt; arr.length; i++) { newArr[i] = arr[arr.length - i - 1] } var newStr = newArr.join('') console.log(str0) // 方法二 var newStr = '' for (var i = 0; i &lt; str.length; i++) { newStr += str.charAt(str.length - i - 1) } console.log(newStr) // 方法三 var newStr = str .split('') .reverse() .join('') console.log(newStr) // 方法四 var arr = str.split('') var obj = Array.from(new Set([...arr])) var newStr = '' for (i of obj) { newStr += obj[arr.length - i] } console.log(newStr) // 方法五 var arr = str.split('') var newArr = [] while (arr.length &gt; 0) { newArr.push(arr.pop()) } var newStr = newArr.join('') console.log(newStr) 统计字符串出现最多的字母和次数 var str = 'abcdeddd' var n = {} for (var i = 0; i &lt; str.length; i++) { var char = str.charAt(i) if (n[char]) { n[char]++ //计算出现的次数 } else { n[char] = 1 //第一次出现标记为1 } } console.log(n) var max = 0 var maxChar = null for (var key in n) { if (max &lt; n[key]) { max = n[key] maxChar = key } } console.log('最多的字符' + maxChar) //&quot;最多的字符d&quot; console.log('出现次数' + max) //&quot;出现次数4&quot; 数字相关 格式化金钱，每千分位加逗号 function format(str) { let s = '' let count = 0 for (let i = str.length - 1; i &gt;= 0; i--) { s = str[i] + s count++ if (count % 3 == 0 &amp;&amp; i != 0) { s = ',' + s } } return s } function format(str) { return str.replace(/(\\d)(?=(?:\\d{3})+$)/g, '$1,') } 文件单位显示转换 bytesToSize(bytes) { if (bytes === 0) return '0 B' var k = 1024 // or 1024 var sizes = ['B', 'KB', 'MB', 'GB', 'TB', 'PB', 'EB', 'ZB', 'YB'] var i = Math.floor(Math.log(bytes) / Math.log(k)) return (bytes / Math.pow(k, i)).toPrecision(3) + ' ' + sizes[i] } bytesToSize(12) // 12.0 B bytesToSize(683468) // 667 KB bytesToSize(4544) // 4.44 KB bytesToSize(98223445) // 93.7 MB bytesToSize(9822344566) // 9.15 GB 计算两点间的距离 const distance = (x0, y0, x1, y1) =&gt; Math.hypot(x1 - x0, y1 - y0) distance(1, 1, 2, 3) // 2.23606797749979 数组 / 对象相关 获取 URL 参数 function getQueryString(name) { var reg = new RegExp('(^|&amp;)' + name + '=([^&amp;]*)(&amp;|$)', 'i') var r = window.location.search.substr(1).match(reg) if (r != null) return unescape(r[2]) return null } 实现对五种 JS 数据类型的克隆 function clone(obj) { var copy switch (typeof obj) { case 'undefined': break case 'number': copy = obj - 0 break case 'string': copy = obj + '' break case 'boolean': copy = obj break case 'object': // object分为两种情况 对象（Object）和数组（Array） if (obj === null) { copy = null } else { if (object.prototype.toString.call(obj).slice(8, -1) === 'Array') { copy = [] for (var i = 0; i &lt; obj.length; i++) { copy.push(clone(obj[i])) } } else { copy = {} for (var j in obj) { copy[j] = clone(obj[j]) } } } break default: copy = obj break } return copy } 统计数组中出现的次数的对象 const nums = [3, 5, 6, 82, 1, 4, 3, 5, 82] const result = nums.reduce((tally, amt) =&gt; { tally[amt] ? tally[amt]++ : (tally[amt] = 1) return tally }, {}) console.log(result) //{ '1': 1, '3': 2, '4': 1, '5': 2, '6': 1, '82': 2 } 检测数值出现次数 const countOccurrences = (arr, val) =&gt; { arr.reduce((a, v) =&gt; (v === val ? a + 1 : a), 0) } countOccurrences([1, 1, 2, 1, 2, 3], 1) // 3 数组对象排序 单个属性排序 function compare(property) { return function (a, b) { let value1 = a[property] let value2 = b[property] return value1 - value2 } } 多个属性排序 function compare(name, minor) { return function (o, p) { var a, b if (o &amp;&amp; p &amp;&amp; typeof o === 'object' &amp;&amp; typeof p === 'object') { a = o[name] b = p[name] if (a === b) { return typeof minor === 'function' ? minor(o, p) : 0 } if (typeof a === typeof b) { return a &lt; b ? -1 : 1 } return typeof a &lt; typeof b ? -1 : 1 } else { thro('error') } } } 数组去重 基本数组去重 利用数组的 indexOf 下标属性来查询 function unique4(arr) { var newArr = [] for (var i = 0; i &lt; arr.length; i++) { if (newArr.indexOf(arr[i]) === -1) { newArr.push(arr[i]) } } return newArr } console.log(unique4([1, 1, 2, 3, 5, 3, 1, 5, 6, 7, 4])) // 结果是[1, 2, 3, 5, 6, 7, 4] 先将原数组排序，在与相邻的进行比较，如果不同则存入新数组 function unique2(arr) { var formArr = arr.sort() var newArr = [formArr[0]] for (let i = 1; i &lt; formArr.length; i++) { if (formArr[i] !== formArr[i - 1]) { newArr.push(formArr[i]) } } return newArr } console.log(unique2([1, 1, 2, 3, 5, 3, 1, 5, 6, 7, 4])) 利用对象属性存在的特性，如果没有该属性则存入新数组 function unique3(arr) { var obj = {} var newArr = [] for (let i = 0; i &lt; arr.length; i++) { if (!obj[arr[i]]) { obj[arr[i]] = 1 newArr.push(arr[i]) } } return newArr } console.log(unique2([1, 1, 2, 3, 5, 3, 1, 5, 6, 7, 4])) 利用数组原型对象上的 includes 方法 function unique5(arr) { var newArr = [] for (var i = 0; i &lt; arr.length; i++) { if (!newArr.includes(arr[i])) { newArr.push(arr[i]) } } return newArr } console.log(unique5([1, 1, 2, 3, 5, 3, 1, 5, 6, 7, 4])) 利用数组原型对象上的 filter 和 includes 方法 function unique6(arr) { var newArr = [] newArr = arr.filter(function (item) { return newArr.includes(item) ? '' : newArr.push(item) }) return newArr } console.log(unique6([1, 1, 2, 3, 5, 3, 1, 5, 6, 7, 4])) 利用 ES6 的 set 方法 function unique10(arr) { return Array.from(new Set(arr)) // 利用Array.from将Set结构转换成数组 } console.log(unique10([1, 1, 2, 3, 5, 3, 1, 5, 6, 7, 4])) 根据数组某个属性去重 // 方法一 function unique(arr) { const res = new Map() return arr.filter(item =&gt; !res.has(item.productName) &amp;&amp; res.set(item.productName, 1)) } // 方法二 function unique(arr) { let result = {} let obj = {} for (var i = 0; i &lt; arr.length; i++) { if (!obj[arr[i].key]) { result.push(arr[i]) obj[arr[i].key] = true } } } ","link":"https://faded.auspicious.space/post/js-dry-goods-out-of-the-box/"},{"title":"JavaScript 原型链","content":" JavaScript-原型链 每个“构造函数”都有一个默认的属性叫做 prototype，prototype 属性保存着一个对象,这个对象我们称为“原型对象”。 每个“原型对象”中都有个默认的属性叫做 constructor，constructor 指向当前“原型对象”对应的那个构造函数。 通过构造函数创建出来的对象我们称为“实例对象”，每个实例对象中都有个默认属性叫 proto 指向创建它的那个构造函数的“原型对象”。 function 函数是所有函数的祖先函数 所有构造函数都有一个 prototype 属性 所有原型对象都有一个 constructor 属性 所有函数都是对象 所有对象都有一个 proto 属性 function 函数的 proto 属性比较特殊,它指向自己的 function 原型对象。 而 Object 原型对象里的 proto 比较特殊是直接指向 null（空）。 注意点：为了不破坏原有的关系，在给 prototype 赋值的时候，需要在自定义对象中手动添加 constructor 属性，手动指定给需要指向的对象。 function Person(myName,myAge) { this.name = myName; this.age = myAge; // this.say = getSay; } Person.prototype = { constructor:Person, say:function () { } } 原型链查找顺序： 如果构造函数中有的属性，就会使用构造函数里的属性,如果构造函数没有就会去她的原型对象中查找,如果原型对象没有，一直查找到 Object，Object 没有那就报错。 属性注意点 在给一个对象不存在的属性赋值时，如果这个对象没有这个属性，那样这个赋值就等同于给这对象新增一个不存在的属性。 ","link":"https://faded.auspicious.space/post/javascript-prototype-chain/"},{"title":"InnoDB 事务加锁分析","content":" InnoDB 事务加锁分析 一般大家对数据库事务的了解可能停留在事务的 ACID 特性以及事务 4 种不同的隔离级别层面上，而对于事务 4 种不同隔离级别如何实现了解相对较少。 本文以 MySQL 数据库 InnoDB 引擎为例，为大家分析 InnoDB数据库引擎对默认的隔离级别可重复读（RR）的具体实现。 整文知识点介绍：事务 4 种隔离级别、不同隔离级别解决的问题、MVCC、锁的类型、加锁案例分析；阅读完整文相信大家对事务隔离级别的具体实现有了一定的认识。 事务的隔离级别 4 种隔离级别 未提交读（Read uncommitted）：一个事务读取到其他事务未提交的数据，是级别最低的隔离机制； 提交读（Read committed）：一个事务读取到其他事务提交后的数据； 可重复读（Repeatable read）：一个事务对同一份数据读取到的相同，不在乎其他事务对数据的修改； 序列化（Serializable）：事务串行化执行，隔离级别最高，牺牲了系统的并发性。 不同隔离级别解决的问题 若不考虑事务的隔离级别，则事务的并发会造成以下问题： 脏读：事务 A 读取了事务 B 更新的数据，然后 B 回滚操作，那么 A 读取到的数据是脏数据。 不可重复读：事务 A 多次读取同一数据，事务 B 在事务 A 多次读取的过程中，对数据作了更新并提交，导致事务 A 多次读取同一数据时，结果不一致。 幻读：同一事务中对同一范围的数据进行读取，结果却多出了数据或者少了数据，这就叫幻读。（如同一事务对 id &lt; 10 的范围进行 2 次查询，第一次出现 id = 8 / 9 的两条数据，第二次出现 id = 7 / 8 / 9 的 3 条数据）。 不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表。 不同的隔离级别针对上述 3 个问题的解决能力，如下表： 隔离级别 脏读（Dirty Read） 不可重复读（NonRepeatable Read） 幻读（Phantom Read） 未提交读（Read uncommitted） 可能 可能 可能 已提交读（Read committed） 不可能 可能 可能 可重复读（Repeatable read） 不可能 不可能 可能 串行化（Serializable） 不可能 不可能 不可能 MVCC 上文提到 InnoDB 默认的隔离级别是可重复读（RR），InnoDB 是通过 MVCC（多版本并发控制）来实现可重复读的，下面为大家介绍 MVCC。 概念 在 InnoDB 中，给每行增加两个隐藏字段来实现 MVCC，一个用来记录数据行的创建时间，另一个用来记录行的过期时间（删除时间）。在实际操作中，存储的并不是时间，而是事务的版本号，每开启一个新事务，事务的版本号就会递增。 于是乎，默认的隔离级别（REPEATABLE READ）下，增删查改变成了这样： SELECT 读取创建版本小于或等于当前事务版本号，并且删除版本为空或大于当前事务版本号的记录。这样可以保证在读取之前记录是存在的。 INSERT 将当前事务的版本号保存至行的创建版本号。 UPDATE 新插入一行，并以当前事务的版本号作为新行的创建版本号，同时将原记录行的删除版本号设置为当前事务版本号。 DELETE 将当前事务的版本号保存至行的删除版本号。 快照读和当前读 快照读：读取的是快照版本，也就是历史版本； 当前读：读取的是最新版本。 普通的 SELECT 就是快照读，而 UPDATE、DELETE、INSERT、SELECT ... LOCK IN SHARE MODE、SELECT ... FOR UPDATE 是当前读。 结论：如果隔离级别是 REPEATABLE READ，那么在同一个事务中的所有普通 SELECT 读读到的都是事务第一个读到的快照，如此实现了可重复读；而对于当前读（UPDATE、DELETE、INSERT、SELECT ... LOCK IN SHARE MODE、SELECT ... FOR UPDATE），InnoDB 通过加锁来实现可重复读，且 InnoDB 加锁同时解决了幻读问题。 锁的类型 InnoDB 引入以下三种锁类型： Record Locks（记录锁）：在索引记录上加锁，即行锁，锁住当前行。 Gap Locks（间隙锁）：在索引记录之间加锁，或者在第一个索引记录之前加锁，或者在最后一个索引记录之后加锁。 Next-Key Locks：在索引记录上加锁，并且在索引记录之前的间隙加锁。它相当于是 Record Locks 与 Gap Locks 的一个结合。 假设一个索引包含以下几个值：101010, 111111, 131313, 202020。那么这个索引的 next-key 锁将会覆盖以下区间：(−∞,10]、(10,11]、(11,13]、(13,20]、(20,+∞)(-\\infty, 10]、(10, 11]、(11, 13]、(13, 20]、(20, +\\infty)(−∞,10]、(10,11]、(11,13]、(13,20]、(20,+∞)。 MySQL InnoDB 通过间隙锁解决了幻读问题。以下通过实际的案例分析来介绍 InnoDB 是如果解决幻读问题的。 案例分析 在对 SQL 进行加锁分析前，需要明确表的结构和索引类型。在不知道索引的情况下直接给出一条 SQL 来分析如果加锁是没有任何意义的。 以下以用户表（t_user）为例（id 为主键，name 为唯一索引，age 为一般索引，address 无索引）分析不同索引条件的加锁表现。 CREATE TABLE `t_user` ( `id` bigint NOT NULL, `name` varchar(20), `age` int, `address` varchar(20), PRIMARY KEY (`id`), UNIQUE KEY `idx_name` (`name`) USING BTREE, KEY `idx_age` (`age`) ) ENGINE = InnoDB; 主键索引 例：DELETE FROM t_user WHERE id = 120; 条件为主键，此时锁住聚簇索引中对应的行记录：即 Record Locks 锁住 id=120 的行记录。 此种情况下，其他事务除了不能删除、更新此条记录外，其他插入其他行、更新其他行都行。 SQL验证： Time Session 1 Session 2 1 start transaction; start transaction; 2 DELETE FROM t_user WHERE id = 120; Query OK, 1 row affected (0.00 sec) 3 DELETE FROM t_user WHERE id = 120;...Lock wait timeout...(阻塞至超时)(聚簇索引 Record X-Lock 冲突) 唯一索引 例：DELETE FROM t_user WHERE name = 'n20'; 条件为唯一索引，锁住索引记录，同时锁住聚簇索引中的对应行记录： SQL验证： Time Session 1 Session 2 1 start transaction; start transaction; 2 DELETE FROM t_user WHERE name = 'n20'; Query OK, 1 row affected (0.00 sec) 3 DELETE FROM t_user WHERE name = 'n20';...Lock wait timeout...(阻塞至超时)(唯一索引 Record X-Lock 冲突) 一般索引 例：DELETE FROM t_user WHERE age = 20; 与主键和唯一索引不同的是，一般索引的记录是允许重复的；换句话说，如果我们单纯地给索引加记录锁时，其他事务依然可以插入，也就有可能出现幻读问题了。 所以除了给对应索引记录加上记录锁之外，还要给 Gap 加上锁。 从上面知识点我们可以预估这个操作一共需要的锁： age 索引记录锁（Record Lock）： 2012020_{120}20120​, 2013020_{130}20130​（以下均用 ageid\\text{age}_\\text{id}ageid​ 这种形式表示索引值） age 索引间隙锁（Gap X-Lock）： (10,20)(10, 20)(10,20)、(20,20)(20, 20)(20,20)、(20,40)(20, 40)(20,40) 聚簇索引上的记录锁（Record X-Lock）： id=120/130 对应的行记录 SQL验证： Time Session 1 Session 2 1 start transaction; start transaction; 2 DELETE FROM t_user WHERE age = 20; Query OK, 2 row affected （0.00 sec） 3 DELETE FROM t_user WHERE age = 20;...Lock wait timeout...（age 索引的 Record X-Lock 之间冲突） 4 DELETE FROM t_user WHERE id = 120;...Lock wait timeout...（聚簇索引的 Record X-Lock 之间冲突） 5 INSERT INTO t_user(age) VALUES(15);...Lock wait timeout...（与(10,20)的 Gap X-Lock 冲突） 6 INSERT INTO t_user(age) VALUES(25);...Lock wait timeout...（与(20,40)的 Gap X-Lock 冲突） 7 INSERT INTO t_user(age) VALUES(10);...Lock wait timeout...（阻塞了？难道边界的 10 和 40 也会加锁？） 8 INSERT INTO t_user(age) VALUES(40);Query OK，1 row affected (0.00 sec)（这个不阻塞？） 根据实际情况，3-6 均符合我们预期，然而 7 和 8 则超出了我们预期的锁范围。为什么会超出我们预期呢？此次我们进行分析一下： 从 7、8 插入语句来看，由于 id 为自增主键，会自动递增，语句 7 插入值预计为：1014110_{141}10141​； 语句 8 插入值预计为：4014140_{141}40141​，为什么只有后者能插入呢？ 其实我们可以将 B+ 树中的间隙理解得更加精准一点： age = 20 的三个间隙应该为：(10110,20120)(10_{110}, 20_{120})(10110​,20120​)、(20120,20130)(20_{120}, 20_{130})(20120​,20130​)、(20130,40140)(20_{130}, 40_{140})(20130​,40140​)； 从上图可以看出语句 7 插入值 1014110_{141}10141​ 无法插入，因为间隙被锁住了；而语句 8 插入 4014140_{141}40141​ 值因为在间隙之外了，无锁冲突，允许插入。 所以最终的加锁情况应该这样表示： age 索引记录锁（Record Lock）：2012020_{120}20120​, 2013020_{130}20130​ age 索引间隙锁（Gap X-Lock）：(10110,20120)(10_{110}, 20_{120})(10110​,20120​)、(20120,20130)(20_{120}, 20_{130})(20120​,20130​)、(20130,40140)(20_{130}, 40_{140})(20130​,40140​) 聚簇索引上的记录锁（Record X-Lock）：id=120 / 130 对应的行记录 无索引 DELETE FROM t_user WHERE address = 'a20';，因为无法精准定位，InnoDB 选择将聚簇索引中的所有行以及间隙都锁起来，功能上已经等于锁表了： SQL验证： Time Session 1 Session 2 1 start transaction; start transaction; 2 DELETE FROM t_user WHERE address = 'a20'; Query OK, 1 row affected （0.00 sec） 3 DELETE FROM t_user WHERE id = 110;...Lock wait timeout...（聚簇索引与 Record X-Lock 之间冲突） 4 DELETE FROM t_user WHERE age = 40;...Lock wait timeout...（一般索引与 Record X-Lock 之间冲突） 5 INSERT INTO t_user(age) VALUES(50);...Lock wait timeout...（与 Gap 锁冲突） 5、结论 InnoDB 在 RC（READ COMMITTED）隔离级别中，只会在对应的索引/行记录上加 Record Lock，而不会加 Gap 锁，原因也很简单，因为该隔离级别是允许存在幻读问题的。 在 RR 级别下的加锁方式称之为 Next-Key Locks，其实就是上述 Record Locks 和 Gap Locks 的结合。比如 Gap Lock 为 (10,20)(10,20)(10,20)，record lock 为 202020，结合的 Next-Key lock 为：(10,20](10, 20](10,20]。 分析 Next-Key Locks 其实就是要分析 Record Locks 和 Gap Locks。MySQL InnoDB 的可重复读并不保证避免幻读，需要应用使用加锁读来保证。而这个加锁读使用到的机制就是 next-key locks。 如果使用普通的读，会得到一致性的结果，如果使用了加锁的读，就会读到“最新的”“提交”读的结果。本身，可重复读和提交读是矛盾的。在同一个事务里，如果保证了可重复读， 就会看不到其他事务的提交，违背了提交读；如果保证了提交读，就会导致前后两次读到的结果不一致，违背了可重复读。可以这么讲，InnoDB 提供了这样的机制，在默认的可重复读的隔离级别里，可以使用加锁读去查询最新的数据。 ","link":"https://faded.auspicious.space/post/innodb-transaction-locking-analysis/"},{"title":"数据加密与 crypto 模块","content":" 数据加密与crypto模块 Node.js 中的 crypto 模块提供了各种各样加密算法的 API。这篇文章记录了常用加密算法的种类、特点、用途和代码实现。其中涉及算法较多，应用面较广，每类算法都有自己适用的场景。为了使行文流畅，列出了本文记录的几类常用算法： 内容摘要：散列（Hash）算法 内容摘要：HMac 算法 内容加解密：对称加密（AES）与非对称加密解密（RSA） 内容签名：签名和验证算法 散列（Hash）算法 散列函数（英语：Hash function）又称散列算法、哈希函数，是一种从任何一种数据中创建小的数字“指纹”的方法。基本原理是将任意长度数据输入，最后输出固定长度的结果。 Hash 算法具有以下特点： 不能从 hash 值倒推原数据 不同的输入，会有不同的输出 好的 hash 算法冲突概率更低 正因为 hash 算法的这些特点，因此 hash 算法主要用于：加密、数据检验、版本标识、负载均衡、分布式（一致性 hash）。 下面实现了一个获取文件标识的函数： const crypto = require(&quot;crypto&quot;); const fs = require(&quot;fs&quot;); function getFileHash(file, algorithm) { if (!crypto.getHashes().includes(algorithm)) { throw new Error(&quot;不支持此哈希函数&quot;); } return new Promise(resolve =&gt; { const hash = crypto.createHash(algorithm); const rs = fs.createReadStream(file); rs.on(&quot;readable&quot;, () =&gt; { const data = rs.read(); if (data) { hash.update(data); } }); rs.on(&quot;end&quot;, () =&gt; { resolve(hash.digest(&quot;hex&quot;)); }); }); } // 用法：获取文件md5 getFileHash(&quot;./db.json&quot;, &quot;md5&quot;).then(val =&gt; { console.log(val); }); HMac 算法 攻击者可以借助“彩虹表”来破解哈希表。应对彩虹表的方法，是给密码加盐值（salt），将 pwd 和 salt 一起计算 hash 值。其中，salt 是随机生成的，越长越好，并且需要和用户名、密码对应保存在数据表中。 虽然通过加盐，实现了哈希长度扩展，但是攻击者通过提交密码和哈希值也可以破解攻击。服务器会把提交的密码和 salt 构成字符串，然后和提交的哈希值对比。如果系统不能提交哈希值，不会受到此类攻击。 显然，没有绝对安全的方法。但是不推荐使用密码加盐，而是 HMac 算法。它可以使用任意的 Hash 函数，例如 md5 =&gt; HmacMD5、sha1 =&gt; HmacSHA1。 下面是利用 Hmac 实现加密数据的函数： const crypto = require(&quot;crypto&quot;); function encryptData(data, key, algorithm) { if (!crypto.getHashes().includes(algorithm)) { throw new Error(&quot;不支持此哈希函数&quot;); } const hmac = crypto.createHmac(algorithm, key); hmac.update(data); return hmac.digest(&quot;hex&quot;); } // output: 30267bcf2a476abaa9b9a87dd39a1f8d6906d1180451abdcb8145b384b9f76a5 console.log(encryptData(&quot;root&quot;, &quot;7(23y*&amp;745^%I&quot;, &quot;sha256&quot;)); 对称加密(AES)与非对称加密解密(RSA) 有很多数据需要加密存储，并且需要解密后进行使用。这和前面不可逆的哈希函数不同。此类算法一共分为两类： 对称加密（AES）：加密和解密使用同一个密钥 非对称加密解密（RSA）：公钥加密，私钥解密 对称加密（AES） 查看 Node.js 支持的所有加密算法： crypto.getCiphers(); Node.js 提供了 Cipher 类和 Decipher 类，分别用于加密和解密。两者都继承 Transfrom Stream，API 的使用方法和哈希函数的 API 使用方法类似。 下面是用 aes-256-cbc 算法对明文进行加密： const crypto = require(&quot;crypto&quot;); const secret = crypto.randomBytes(32); // 密钥 const content = &quot;hello world!&quot;; // 要加密的明文 const cipher = crypto.createCipheriv( &quot;aes-256-cbc&quot;, secret, Buffer.alloc(16, 0) ); cipher.update(content, &quot;utf8&quot;); // 加密后的结果：e2a927165757acc609a89c093d8e3af5 console.log(cipher.final(&quot;hex&quot;)); **注意：**在使用加密算法的时候，给定的密钥长度是有要求的，否则会爆出 this[kHandle].initiv(cipher, credential, iv, authTagLength); Error: Invalid key length... 的错误。以 aes-256-cbc 算法为例，需要 256 bits = 32 bytes 大小的密钥。同样地，AES 的 IV 也是有要求的，需要 128 bits。（请参考“参考链接”部分） 使用 32 个连续 I 作为密钥，用 aes-256-cbc 加密后的结果是 a061e67f5643d948418fdb150745f24d。下面是逆向解密的过程： const secret = &quot;I&quot;.repeat(32); const decipher = crypto.createDecipheriv( &quot;aes-256-cbc&quot;, secret, Buffer.alloc(16, 0) ); decipher.update(&quot;a061e67f5643d948418fdb150745f24d&quot;, &quot;hex&quot;); console.log(decipher.final(&quot;utf8&quot;)); // 解密后的结果：hello world! 非对称加密解密(RSA) 借助 openssl 生成私钥和公钥： # 生成私钥 openssl genrsa -out privatekey.pem 1024 # 生成公钥 openssl rsa -in privatekey.pem -pubout -out publickey.pem 对 hello world! 加密和解密的代码如下： const crypto = require(&quot;crypto&quot;); const fs = require(&quot;fs&quot;); const privateKey = fs.readFileSync(&quot;./privatekey.pem&quot;); const publicKey = fs.readFileSync(&quot;./publickey.pem&quot;); const content = &quot;hello world!&quot;; // 待加密的明文内容 // 公钥加密 const encodeData = crypto.publicEncrypt(publicKey, Buffer.from(content)); console.log(encodeData.toString(&quot;base64&quot;)); // 私钥解密 const decodeData = crypto.privateDecrypt(privateKey, encodeData); console.log(decodeData.toString(&quot;utf8&quot;)); 签名和验证算法 除了不可逆的哈希算法、数据加密算法，还有专门用于签名和验证的算法。这里也需要用 openssl 生成公钥和私钥。 代码示范如下： const crypto = require(&quot;crypto&quot;); const fs = require(&quot;fs&quot;); const assert = require(&quot;assert&quot;); const privateKey = fs.readFileSync(&quot;./privatekey.pem&quot;); const publicKey = fs.readFileSync(&quot;./publickey.pem&quot;); const data = &quot;传输的数据&quot;; // 第一步：用私钥对传输的数据，生成对应的签名 const sign = crypto.createSign(&quot;sha256&quot;); // 添加数据 sign.update(data, &quot;utf8&quot;); sign.end(); // 根据私钥，生成签名 const signature = sign.sign(privateKey, &quot;hex&quot;); // 第二步：借助公钥验证签名的准确性 const verify = crypto.createVerify(&quot;sha256&quot;); verify.update(data, &quot;utf8&quot;); verify.end(); assert.ok(verify.verify(publicKey, signature, &quot;hex&quot;)); 从前面这段代码可以看到，利用私钥进行加密，得到签名值；最后利用公钥进行验证。 ","link":"https://faded.auspicious.space/post/encryption-and-crypto-module/"},{"title":"深度解析 C 语言：指针","content":" 深度解析C语言：指针 指针是 C 语言的灵魂。并不是所有的高级语言都具有指针，C++ 虽然有指针，但是从设计上希望程序员尽可能少得使用指针，C 则不然。C 是追求效率的，所以有时不得不把安全问题交给程序员。正是指针赋予了 C“直接操作内存”的能力，也正是指针使一些复杂的数据结构成为可能。 流畅且准确地使用指针，是掌握 C 语言精髓的关键。 什么是指针 在程序运行的过程中，所有的数据都是放在主存（内存）里的，主存按字节编址，每个字节对应一个编号，称为地址，计算机通过地址可以精确地存取数据。 在 C 语言中有一类数据类型，它保存主存地址，并可以通过运算符对该地址的数据进行操作。如果它保存的是某个变量的地址，可以形象地说它指向了某个变量，所以这类数据类型称为指针。 指针的存储方式 主存从 0 开始顺序编址，所以指针的值是非负整数，实际上，指针的存储结构与 unsigned long 相同，在 32 位机上指针是 32 位，而在 64 位机上指针是 64 位。 指针类型 前面已经提到，指针是一类数据类型，这是一个大家族，拥有非常多的指针类型，尤其是当指针与数组结合以后，其类型描述会变得非常复杂。 也许会有疑问，既然存储方式已经确定，为什么还需要众多的指针类型？ 其实，指针类型描述的不是自身数据的类型，而是其指向数据的类型。想象一下，一个 int 变量一般是 4 个字节，而指针储存的是 1 个字节的地址，怎样才能让指针指向这个变量。C 的做法是，储存这个 int 变量的第一个字节的地址，即首地址。之后，我们根据指针存储的数据找到了该地址，这是 1 个字节的地址，该怎么表示这里有一个 4 字节的 int 变量。同样，我们把这个地址作为首地址，把这个字节和其后的 3 个字节一起作为一个 int 变量。显然，我们需要明确表示，这个指针指向一个 int 变量，否则无法使用指针进行任何操作。这就引出了指针类型。 同时，指针之间可以进行有限的运算，且指向不同类型数据的指针运算规则是不同的，这是指针类型的另一个用途。 &amp; 和 * 运算符 &amp; 运算符 又称为“取址运算符”，顾名思义，其作用是得到一个变量的地址。严格来说，它取的是变量的首地址，并且包含了变量类型信息，所以，它返回的是一个指针（请勿把指针和指针变量弄混），并且它是一个右值。&amp; 运算符为一元运算符，右结合，优先级大于算术运算符，运算对象必须是变量（暂不引入引用的概念，并姑且把主存中的固定数据都称为变量），而不能是例如 a + 1 这样的表达式。 * 运算符 取值运算符，也称“解引用运算符”，返回指针所指的变量。* 运算符返回左值，如果 a 指向 b，那么 *a 和 b 是相同的，完全可以混用。* 运算符也是一元运算符，右结合，优先级与 &amp; 运算符相同。* 运算符的运算对象可以是任何指针类型（除了 void）的值，而不一定是指针变量。 指针变量的声明 C 变量声明的设计理念是：声明方式应该与使用方式相同。 首先，我们需要指定指针所指变量的类型，以 int 为例。当我们使用指针时，需要在指针变量前使用 * 运算符，所以在声明时也应在变量名前加 * 。所以，声明一个指向 int 变量的指针应写为： int *var 若要声明一个“指向‘指向 char 变量的指针’的指针”，因为我们需要两次使用 * 运算符才能得到这个 char 变量，所以声明时也要写两个 * ，写为： char **var 值得一提的是，以下三种写法都是合法的： int *var int* var int * var 声明中 * 只对与它相邻的那个变量有效，也就是说，如果有以下声明： int* a, b 则声明了一个指针变量 a 和一个 int 变量 b，如果想同时声明两个相同类型的指针，需要在每个变量名前加 *，所以建议采用第一种写法。如下： int *a, *b 指针运算 指针运算赋予了指针更灵活的用法，在数组等操作中尤为明显。 指针运算旨在使指针偏移到相邻的对象，或指向一个确定的位置，也可以求出两个指针所指对象在主存中的距离。 赋值运算 将一个指针类型的值赋值给一个指针变量，最基本的用法如下： int var = 0; int *pt = &amp;var; 一般情况下，赋值号两边的指针类型必须相同。 int var = 0; int *pt1 = &amp;var; // Correct int *pt2 = pt1; // Correct char *pt3 = &amp;var; // Incorrect char *pt4 = pt1; // Incorrect char *pt5 = (char *)pt1; // Correct char *pt6 = (char *)1234; // Correct 当把一个int * 类型的值赋值给一个 char * 变量时，一个负责的编译器应当会报错。如果确实需要这么做，可以通过显式类型转换使赋值号两边类型一致。(char *) 1234 虽然合法，但通常并没有意义。 从上面的例子中我们可以发现，&amp; 运算符返回与运算对象相应的指针类型的值。对 int 变量取址则返回 int * 类型的指针。* 运算符同理。 算术运算 指针的算术运算与整数的算术运算不同，一般用在数组操作中，使指针偏移，或求两个对象的距离。 指针与整数的算术运算 指针可以与 long int 类型的整数进行 +，- 运算，与 = 结合可以进行 +=，-= 运算，如果是其他整数，则会提升或截断为 long int 再进行运算。 指针与整数运算的意义是使指针向整数个单位偏移。对于以下指针和数组： int array[10] = { 0 }; int *pt = &amp;array[3]; 此时 pt 指向 array 下标为 3 的元素，若执行 pt = pt + 2，则 pt 指向下标为 5 的元素；若再执行 pt = pt - 4，则 pt 指向下标为 1 的元素。 指针与整数的算术运算返回值为指针，类型与参与运算的指针一致。 运算表达式须满足以下条件： 进行 + 运算时，指针与整数的位置可以互换。 进行 - 运算时，必须指针在左，整数在右。 以上是从逻辑层面对指针与整数的运算进行解析，下面分析实现细节。 int 类型的变量一般占用 4 个字节，而指针指向其首地址。从 int 类型的数组 array 下标 3 的元素到下标 5 的元素，指针偏移 2 个 int 类型的变量，指针的值增加了8。可见对于 int * 类型的指针，每增加整数 1，指针的值就增加 4，也就是 int 变量的存储大小。对于 char * 类型的指针，每增加整数 1，指针的值就增加 1，因为 char 变量的存储大小为 1 个字节。其他指针类型同理。 这就是指针的算术运算与整数运算不同的地方，对于不同的指针类型，与相同的整数做运算，值的变化是不同的，这也体现了指针类型的意义。 指针与指针的算术运算 指针与指针只能进行 - 运算，且参与运算的指针类型必须相同，返回值为 long int 类型的整数。 指针与指针运算的意义是求两个指针所指对象的距离。对以下指针和数组： int array[10] = { 0 }; int *pt1 = &amp;array[3]; int *pt2 = &amp;array[7]; 此时 pt1 指向 array 下标为 3 的元素，pt2 指向 array 下标为 7 的元素。表达式 pt2 - pt1 的值为 4，表达式 pt1 - pt2 的值为 -4。运算结果与相应下标的算术运算结果相同。 对于以下变量： int var1 = 0; int var2 = 0; 表达式 &amp;var1 - &amp;var2 虽然合法，但通常并没有意义。 和指针与整数的算术运算相同，指针与指针的算术运算也不是简单的数值相减，需要再除以指向类型的存储大小，从而得到“下标距离”。对 int * 类型的指针来说，就是把值相减的结果再除以 4。 条件运算 相同类型的指针可以进行条件运算，结果为各自值进行相同条件运算的结果。比如对于以下指针： int *pt1 = (int *)3; int *pt2 = (int *)5; 表达式 pt1 &lt; pt2 为真，因为 3 小于 5，返回 1。 特别地，任何类型的指针可以与整数 0 进行 == 与 != 运算。 逻辑运算 指针可以作为整数参与逻辑运算，此时对类型没有要求，甚至另一个运算对象可以不是指针。比如对于以下指针： int *pt1 = (int *)1; int *pt2 = 0; (pt1 &amp;&amp; 1) == 1; (pt1 || 0) == 1; (pt2 &amp;&amp; 1) == 0; (pt2 || 1) == 1; (!pt2) == 1; 以上表达式全为真。 void 指针类型 void 是一种特殊的指针类型，表示没有指向类型。 因为没有指向类型，所以 void 指针不能进行算术运算与取值运算，只能进行赋值等运算。而且 void 类型的赋值运算有以下特点： 任何类型的指针都可以不经过显式类型转换直接赋值给 void 类型指针变量。 在 C 中，void 类型的指针可以不经过显式类型转换直接赋值给任何类型的指针变量，而 C++ 取消了这一规则。 因为 void 指针易于赋值的特点，它经常用作不同类型指针交流的中间变量，或者在不能确定类型的时候使用 void 类型。 以 malloc() 函数为例，其作用是申请一块给定大小的内存空间，并返回其首地址。然而函数不能确定调用者会如何使用这块空间，所以返回 void 类型。 顶层 const 与底层 const 对于普通变量，在申明时使用 const 关键字可以使其不允许修改。指针类似，但更为复杂，因为指针可以指定其所指对象的 const 属性。 对于以下申明： const int *pt1; int const *pt2; int *const pt3; const int *const pt4; int const *const pt5; pt1 与 pt2 相同，pt1 可以修改，而 *pt1 不可以修改，这种对其所指对象施加的 const 称为底层 const。 pt3 不可以修改，*pt3 可以修改，这种对指针施加的 const 称为顶层 const。 pt4 与 pt5 相同，pt4 与 *pt4 都不可以修改，同时施加了顶层 const 与底层 const。 区别顶层 const 与底层 const 的依据是 const 相对 * 的位置。对于多层指针，位于所有 * 右边，与变量名相邻的 const 是最顶层的 const，之后每向左跨过一个 *，便向下一层，位于所有 * 左边的 const 是最底层的 const。 对于以下指针： int ***const pt1; int **const *pt2; int *const **pt3; int const ***pt4; int const *const *const *const pt5; pt1 不可以修改，*pt1、**pt1、***pt1 可以修改。 pt2 可以修改，*pt2 不可以修改，**pt2、***pt2 可以修改。 t3、*pt3 可以修改，**pt3 不可以修改，***pt3 可以修改。 pt4、*pt4、**pt4 可以修改，***pt4 不可以修改 。 pt5、*pt5、**pt5、***pt5 都不可以修改。 简而言之，位于 n 个 * 前面的 const，就意味着对指针进行 n 次取值后不可修改。 底层（仅限次顶层）非 const 类型的值可以赋值给底层（仅限次顶层）const 类型的变量，反之不可以。对于以下指针： int *pt1 = 0; int const *pt2 = 0; pt2 = pt1; //Correct pt1 = pt2; //Incorrect 加强版： int ****pt1 = 0; int ***const *pt2 = 0; pt2 = pt1; //Correct pt1 = pt2; //Incorrect 此规则在函数参数传递时仍然适用，所以将形参指针申明为底层（仅限次顶层）const 类型可以增加函数的适用性。 指针申明中的 const 只对通过该指针访问变量有效。对于以下变量和指针： int var = 0; int const *pt = &amp;var; *pt = 5; //Incorrect var = 5; //Correct 指针与数组 首先需要说明的是，数组不是指针。 尽管数组名经常可以当作指针来使用，但二者是不同的。在全局位置申明和定义时混用数组和指针，会产生严重后果。 数组在内存上是一片连续的空间，数组名经常可以当作其第一个元素的指针来使用。在赋值运算、算术运算、取值运算等以及函数参数中，数组名的表现与顶层 const 指针一致。但是如果分别对指针和数组使用 sizeof 运算符，结果是不一样的。 int array[10] = { 0 }; int *pt = array; sizeof(array) == 40; sizeof(array) == 8; 对数组进行 sizeof 运算，返回值为整个数组占用的内存大小；对指针进行 sizeof 运算，返回值为指针的存储大小，在 64 位机上是 8 字节。 另外，对数组取地址，返回值类型为行指针；而对指针变量取地址，则返回该变量的指针。 数组名作为指针 数组的下标运算符 [] 其实是一种简写，array[n] 与 *(array + n) 的行为完全一致。（数组申明中的 [] 只是一种记号，不是运算符） int array[5]; array[2] == *(array + 2); 数组名可以当作其第一个元素的指针,对其直接取值就可得到第一个元素。 *array == *(array + 0); *(array + 0) == array[0]; *array == array[0]; 这就是数组下标从 0 开始的原因。 多维数组同理。[] 运算符为自左向右结合，所以 array[a][b][c] == ((array[a])[b])[c]; ((array[a])[b])[c] == *(*(*(array + a) + b) + c); array[a][b][c] == *(*(*(array + a) + b) + c); array[0][0][0] == ***array; 由于指针与整数相加时二者的位置可以互换，所以 array[3] == *(array + 3); *(array + 3) == *(3 + array) *(3 + array) == 3[array]; array[3] == 3[array]; 但是强烈不建议第二种可读性差的写法。 把数组名作为指针时，可以给相同类型的指针变量赋值。但是由于数组名的顶层 const 属性，无法给数组名赋值。 int array[5] = { 0 }; int *pt = 0; pt = array; //Correct array = pt; //Incorrect 指针作为数组名 前面说到 [] 运算符只是一种简写，所以也可以对指针使用 [] 运算符。 int var = 0; int *pt = &amp;var; *pt == pt[0]; 当然也可以用指针冒充数组。 int array[5]; int *pt = array; pt[3] == array[3]; 对于多维数组，情况复杂一些。 int **pt = array; 数组 array 表示“包含 3 个‘包含 5 个 int 变量的数组’的数组”（多维数组是元素为数组的数组），而指针 pt 表示“指向一个 int * 类型指针的指针”。 众所周知，多维数组在内存中是按列优先连续存储的，若 array 偏移 1 个单位,则相应的指针偏移 5 个 int 变量的距离；而 pt 偏移 1 个单位，只偏移 1 个 int * 变量的距离。 所以，如何申明这种拥有特殊偏移规则的指针？ C 引入了“行指针”以自定义偏移规则，根据声明方式应该与使用方式相同的原则，行指针应该如下申明： int (*p)[5]; 因为最左边的 [3] 不影响偏移规则，所以只需表明 [5]` 即可。 行指针可以冒充多维数组。 int array[3][5][7]; int (*pt)[5][7] = array; pt[1][2][3] == array[1][2][3]; 指针与数组在函数中的使用 除了结构体和联合体，函数在参数传递时只有值传递，所以不能将整个数组的副本传递给函数。 如果需要将数组传递给函数，只能将数组名作为第一个元素指针当作实参传递。同样，函数的形参也只能申明为指针，然后当作数组使用。 此外，因为值传递的限制，函数的返回值可以是指针，但不能是数组。 基本用法如下： int fn(int *array); int main(void) { int array[5] = { 0 }; fn(array); return 0; } int fn(int *array) { return array[3]; } 如果传递多维数组，只需把指针申明为相应的行指针。 int fn(int (*array)[5]); int main(void) { int array[3][5] = { 0 }; fn(array); return 0; } int fn(int (*array)[5]) { return array[1][3]; } 还有一种更加直观的申明方式： int fn(int (*array)[5][7]); int main(void) { int array[3][5][7] = { 0 }; fn(array); return 0; } int fn(int array[][5][7]) { return array[1][3][5]; } 在申明 int array[][5][7] 中，因为第一个 [] 的内容不影响偏移规则，所以可以为空或者任意值。这种申明方式更加直观，建议使用。 需要注意的是，即使函数形参如 int array[3][5][7] 看起来是数组，但它只是指针，对其使用 sizeof 运算符结果为 8，即一个指针的存储大小。甚至，可以在定义和申明函数时混用两种申明方式，因为他们本质上是一致的。 指针与函数 指针与函数的关系跟指针与数组的关系类似。 在早期的 C 中，函数指针与函数名的概念是相对独立的，对函数取地址以后才可以当作函数指针，对函数指针取值后才可以当作函数名使用。而在现代 C 标准中，函数名与函数指针经常可以混用。 函数名作为指针 根据现代 C 标准，函数名经常可以直接作为顶层 const 函数指针来使用，同时也保留了之前的用法，对函数取地址之后类型与值都不变。所以函数名有以下三种合法的使用方法： int fn(int); fn(5); (&amp;fn)(5); (*(&amp;fn))(5); 实际上，函数名当作函数指针时，其值就是函数入口地址，() 是函数调用运算符，可以对函数指针及括号内的实参进行运算，表现为调用函数。 函数名可以作为指针赋值给相同类型的函数指针。 如果需要向函数传递函数，可以将函数名作为函数指针当作实参。 指针作为函数名 根据早期 C 语言对函数指针的定义，以及声明方式应该与使用方式相同的原则，形成了函数指针的申明方式： int fn(int, char); int (*pt)(int, char) = fn; 以上分别申明了一个函数与其对应的函数指针。跟函数原型一样，申明函数指针时仅保留参数类型即可，形参名可以省略，当然也可以像函数申明一样完全省略参数。 现代 C 标准规定可以直接将函数指针当函数名使用，而无需先对其取值，同时也保留了之前的使用方法，对函数指针取值后类型和值都不变。对于上面申明的函数指针，以下两种用法都是合法的： pt(5, 'a'); (*pt)(5, 'a'); 如果需要向函数传递函数，只需将函数的形参申明为函数指针，并在函数调用时将函数指针作为实参。 比如以下程序： #include &lt;stdio.h&gt; void fn1(void); void fn2(void (*)(void)); int main(void) { fn2(fn1); return 0; } void fn1(void) { printf(&quot;hello, world\\n&quot;); } void fn2(void (*pt)(void)) { pt(); } 执行结果为输出字符串 &quot;hello, world\\n&quot;。 指针与结构体/联合体 因为结构体和联合体在使用方式上一致，所以以下全部使用“结构体”代替“结构体/联合体”。 结构体与指针的关系普普通通，中规中矩，如果结构体指针显得复杂，是因为结构体和指针二者本来分别就很复杂。结构体指针通常用于复杂的数据结构，所以通常很抽象。 结构体和指针唯一的火花是 -&gt; 运算符，看起来像一个箭头，与其作用相应。 对于以下结构体和指针： struct Type { int a; char b; } var; struct Type *pt = &amp;var; 如果要通过指针 pt 访问 var 的成员 a，按照指针的使用方法应写为 (*pt).a。因为 . 运算符的优先级高于 * 运算符，所以必须在 *pt 两边加括号。在数据结构的操作中，经常需要通过指针访问结构体成员，而上述写法过于繁琐，于是 C 提供了一种简写 pt-&gt;a。(*pt).a 与 pt-&gt;a 的行为完全一致。 同样，可以给结构体指针施加 const 属性，且规则与“顶层 const 与底层 const”一节相同。 对结构体指针施加顶层 const 只限定指针值不可以修改。 对结构体指针施加底层 const 限定不可以修改结构体及其成员。 你知道吗 整数 0 可以不经过显式类型转换直接赋值给任何类型的指针变量。 NULL 是宏，定义在头文件 stddef.h 中，常用的头文件如 stdio.h、stdlib.h 都包含此头文件。 在 C 中，NULL 展开为 ((void *)0)，而在 C++中展开为 0。 C++ 引入关键字 nullptr 替代 NULL，其为 (void *)0。 指针可以作为整数成为 if、while、for 等语句 () 内的判断条件。 ","link":"https://faded.auspicious.space/post/deep-analysis-c-pointers/"},{"title":"TCP 和 UDP 的区别","content":" TCP和UDP的区别 前言 我们都知道 TCP 和 UDP 作为传输协议，被广泛应用于网络通信。这些基础的网络知识也是需要了解，既然两种传输协议都可以用于通信，那它们两者又有何区别。本文将以下面 5 个点进行对比： 连接特点 交互通信 数据处理 传输服务 报头开销 UDP 和 TCP 在 TCP/IP 模型中的位置 在比较这两者的区别之前，有必要了解下 TCP/IP 模型，这有利于我们理解下面的内容。 网络模型并非一开始就有的，在网络发展初期，网络协议都是互联网公司自己定义的。由于各家公司的网络协议不同，没有统一标准的网络协议来规定，各个公司的协议都不能互通。这对于网络发展很不利，为了解决这个问题，国际标准化组织 1984 提出的模型标准，简称 OSI（Open Systems Interconnection Model），这是一个标准，并非实现。TCP/IP 协议就是基于此模型设计。 TCP/IP 模型是一个四层模型，自底而上分别是网络接口层、网络层、传输层和应用层： 网络接口层：实现网卡接口的网络驱动程序，以处理数据在物理媒介（比如以太网、令牌环等）上的传输。 网络层：实现数据包的选路和转发。 传输层：为主机的应用程序提供端到端的通信，传输层只关心通信的起始端和目的端，而不在乎数据包的中转过程。 应用层：负责处理应用程序的逻辑。 连接特点 UDP UDP 是一种无连接的传输层协议，因为在使用 UDP 发送报文段时，发送端和接收端的传输层实体之间没有进行握手。所谓的握手，就是发送端和接收端通过发送一些特定的报文段来互相确认，从而为发送做准备。由于 UDP 可以不用任何准备即可进行数据数据传输，因此 UDP 的数据传输速度会比 TCP 快。 TCP TCP 是一种面向连接的传输层协议，网络系统需要在两台计算机之间发送数据之前先建立连接。类似于我们打电话一样，通信之前需要呼叫和应答。其过程分为建立连接（三次握手）、使用连接（数据传输）、释放连接（四次挥手）三个过程。由于这些机制，TCP 数据传输会比 UDP 可靠，即确保双方都互通后再发送数据，保证数据包能够完整的发送过去。 交互通信 UDP UDP 是无连接的传输协议，不需要维护连接状态，包括收发状态，可以实现一对一，一对多，多对一和多对多的交互通信。 TCP TCP 是面向连接的传输协议，发送数据需要双方建立连接，属于端到端的通信，实现的是一对一的交互通信。 数据处理 UDP UDP 是面向报文的。发送端的传输层对应用层交下来的报文，在添加报头后就向下交付给 IP 层。既不拆分，也不合并，而是保留这些报文的边界。接收端传输层接收到报文会去掉报头，将数据部分交给应用层。 TCP TCP 是面向字节流的。发送端的应用层将数据字节流交付到传输层的缓存区，根据发送策略对字节流分片，添加报头发送 TCP 报文。接收端传输层收到报文后，去掉报头存储到接收缓存。接收缓存将字节流片段交给应用层，应用层再将字节流片段重组还原为可用的数据。 传输服务 UDP UDP 提供无连接的不可靠服务。在发送端到接收端的传递过程中出现数据包丢失或接收误码的情况，协议本身并不能做出任何检测或提示。UDP 只是尽可能快地把数据扔到网络上，并不保证数据包的完整性。因此 UDP 没有可靠性保证、顺序保证和流量控制字段。 TCP TCP 提供面向连接的可靠服务。在发送端到接收端的传递过程中出现数据包丢失或接收误码的情况，接收端在定时器超时后没有收到相应的确认，发送端会重新发送数据包。TCP 连接每一方的接收缓冲空间大小都固定，接收端只允许另一端发送接收端缓冲区所能接纳的数据，TCP 在此基础上提供流量控制，防止较快主机致使较慢主机的缓冲区溢出。 报头开销 UDP UDP 传输的段有 8 个字节的报头和有效载荷字段构成。UDP 报头由 4 个域组成，其中每个域各占用 2 个字节，具体包括源端口号、目标端口号、数据报长度、校验和。 端口号：使用端口号为不同的应用保留其各自的数据传输通道。 长度：数据报的长度是指包括报头和数据部分在内的总字节数。 校验和：使用报头中的校验值来保证数据的安全。 TCP TCP 传输的段有最小 20 字节的报头和有效载荷字段构成。TCP 具体组成包括端口号、序号、确认号、保留域、标志域、窗口、校验和、紧急指针构成，另外可扩展首部包括选项和填充。 总结 UDP 特点 无连接 支持一对一，一对多，多对一和多对多的通信 面向报文 不可靠传输，不使用流量控制和拥塞控制 报头开销小，仅8字节 应用场景 常用于实时应用。例如视频直播、IP 电话，QQ 语音和 QQ 视频就是使用 UDP 的协议。 TCP 特点 面向连接 一对一通信 面向字节流 可靠传输，使用流量控制和拥塞控制 报头最小20字节，最大60字节 应用场景 常用于对可靠性要求高的通信。例如文件传输。 ","link":"https://faded.auspicious.space/post/the-difference-between-tcp-and-udp/"},{"title":"原生实现前端路由","content":" 原生实现前端路由 在 Web SPA 中，前端路由描述的 URL 与 UI 之间的单向映射关系，即 URL 变化引起 UI 页面的更新（无需刷新页面）。 核心问题 上面我们提到，在前端路由中，当 URL 发生变化时，我们需要在不刷新页面的情况下，触发 UI 页面的更新。因此，在实现前端路由时，我们需要解决以下两个核心的问题。 如何检测 URL 是否变化？ 如何改变 URL 却不引起页面刷新？ 我们可以从 Hash 和 History 两种实现方式回答上述两个问题。 在 Hash 方式中 我们可以通过 hashchange 事件监听 URL 的变化，以下场景会触发 hashchange 事件：通过浏览器前进后退改变 URL 、通过标签改变 URL 、通过 window.location 改变 URL 。Hash 是 URL 中 # 及后面的部分，改变 URL 中的 Hash 部分不会引起页面刷新。 在 History 方式中 我们可以通过 popstate 事件监听 URL 的变化。我们可通过调用 pushState 和 replaceState 两种方法，改变 URL 而不引起页面刷新。值得注意的是，通过浏览器前进后退改变 URL 时会触发 popstate 事件，而通过 pushState、replaceState 或标签改变 URL 并不会触发 popstate 事件，因此我们需要手动拦截。 实现 代码地址：GitHub Gist 前端路由实现方式 为便于测试，我们使用了 github.com/zeit/serve 作为页面服务器，相关命令如下。 # 安装依赖。 yarn # 对 Vanilla Hash 进行演示。 yarn vanilla.hash # 对 Vanilla History 进行演示。 yarn vanilla.history Vanilla 现在，我们使用原生 HTML/JS 实现 Hash 和 History 两种模式的前端路由，不依赖任何框架。 Hash 页面 vanilla.hash.html 的具体代码如下。 &lt;!DOCTYPE html&gt; &lt;html lang=&quot;zh-CN&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot; /&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;ie=edge&quot; /&gt; &lt;title&gt;Hash Route - Vanilla&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;ul&gt; &lt;!-- 定义路由 --&gt; &lt;li&gt;&lt;a href=&quot;#/home&quot;&gt;home&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;#/about&quot;&gt;about&lt;/a&gt;&lt;/li&gt; &lt;!-- 渲染路由对应的 UI --&gt; &lt;div id=&quot;routeView&quot;&gt;&lt;/div&gt; &lt;/ul&gt; &lt;script src=&quot;hash.js&quot;&gt;&lt;/script&gt; &lt;/body&gt; &lt;/html&gt; 在 vanilla.hash.js 中，我们通过对 hashchange 事件进行监听，从而检测 URL 是否变化。当 URL 变化时，我们调用 onHashChange 函数，通过修改元素的 innerHTML 属性，在页面无刷新的情况下，实现页面视图的更新。 // 维护 UI 页面。 let routerView = null; // 路由变化时，根据路由渲染对应 UI 页面。 function onHashChange() { switch (window.location.hash) { case '': case '#/home': routerView.innerHTML = 'Home'; return; case '#/about': routerView.innerHTML = 'About'; break; default: } } // 页面加载完不会触发 hashchange，这里主动触发一次 hashchange 事件。 window.addEventListener('DOMContentLoaded', () =&gt; { routerView = document.querySelector('#routeView'); onHashChange(); }); // 监听路由变化。 window.addEventListener('hashchange', onHashChange) History 页面 vanilla.history.html 的具体代码如下。 &lt;!DOCTYPE html&gt; &lt;html lang=&quot;zh-CN&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot; /&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot; /&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;ie=edge&quot; /&gt; &lt;title&gt;History Route - Vanilla&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;ul&gt; &lt;!-- 定义路由 --&gt; &lt;li&gt;&lt;a href=&quot;/home&quot;&gt;home&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;/about&quot;&gt;about&lt;/a&gt;&lt;/li&gt; &lt;!-- 渲染路由对应的 UI --&gt; &lt;div id=&quot;routeView&quot;&gt;&lt;/div&gt; &lt;/ul&gt; &lt;script src=&quot;vanilla.history.js&quot;&gt;&lt;/script&gt; &lt;/body&gt; &lt;/html&gt; 在 vanilla.history.js 中，我们通过对 popstate 事件进行监听，从而检测 URL 是否变化。当 URL 变化时，我们调用 onPopState 函数，通过修改元素的 innerHTML 属性，在页面无刷新的情况下，实现页面视图的更新。 由于通过 pushState、replaceState 和标签改变 URL 时，并不会触发 popstate 事件，我们需要手动拦截。 // 维护 UI 页面。 let routerView = null; // 路由变化时，根据路由渲染对应 UI 页面。 function onPopState() { switch (window.location.pathname) { case '/': case '/home': routerView.innerHTML = 'Home'; return; case '/about': routerView.innerHTML = 'About'; break; default: } } // 页面加载完不会触发 hashchange，这里主动触发一次 hashchange 事件。 window.addEventListener('DOMContentLoaded', () =&gt; { routerView = document.querySelector('#routeView'); // 刷新页面。 onPopState(); // 拦截 &lt;a&gt; 标签点击事件默认行为， 点击时使用 pushState 修改 URL并更新手动 UI，从而实现点击链接更新 URL 和 UI 的效果。 const links = document.querySelectorAll('a[href]'); links.forEach(el =&gt; el.addEventListener('click', function handler(e) { e.preventDefault(); // 手动拦截。 window.history.pushState(null, '', el.getAttribute('href')); onPopState(); }), ); }); // 监听路由变化。 window.addEventListener('popstate', onPopState); 在使用 History 模式时，当我们刷新页面，会出现 404 页面。为解决这个问题，我们需要 rewrite 请求。 在 Nginx 中，我们需要添加以下设置：对于所有的请求，我们都响应相同的页面，让页面来接管路由。 location / { try_files $uri $uri/ /vanilla.history.html; } 在 serve 中，我们只需定义 rewrites 数组即可。 { &quot;rewrites&quot;: [ { &quot;source&quot;: &quot;**&quot;, &quot;destination&quot;: &quot;vanilla.history.html&quot; } ] } ","link":"https://faded.auspicious.space/post/the-native-implementation-of-frontend-route/"},{"title":"JavaScript 模块化进化史","content":" 你可能不知道的 JavaScript 模块化野史 对于 JavaScript 新手，看到“CommonJS vs AMD”、“Requirejs vs Seajs”、“Webpack vs Browserify”等这些可能会不知所措。 特别是在大部分浏览器都已经实现 ES6 模块化规范的今天，我们新开发的项目基本都是 ES6 搭配 Webpack ，这些 AMD、CMD、UMD、Requirejs、Seajs 都已经是过去式了，很多同学并没有使用过。 但模块化是 JavaScript 开发体系的一部分，了解它的历史还是很有必要的，至少不会在这方面与其他开发者失去对话能力，比如你的面试官。 Foreword 从 1995 年发布 JavaScript 开始，浏览器端加载 JS 模块就是使用简单的 script 标签。早在 1996 年，就涌现了很多 服务器端 JavaScript 实现， 例如 2009 年发布的 Node.js。无论是浏览器端还是服务端 JavaScript， 在 ES6 规范提出之前，JavaScript 本身一直没有模块体系。 那么什么是模块？ 优秀的作者把他们的书分成章节，优秀的程序员把他们的程序分成模块。好的模块是高度独立的，具有特定功能的，可以根据需要对它们进行修改，删除或添加，而不会破坏整个系统。 模块化有什么好处？ 模块化带来的好处主要是这些： 命名空间 在 JavaScript 中，每个 JS 文件的接口都暴露在全局作用域中，每个人都可以访问它们，并且容易造成命名空间污染。模块化可以为变量创建私有空间来避免命名空间污染。 可重用性 有没有曾经在某个时候将之前编写的代码复制到新的项目中呢？如果将此代码模块化，则可以反复使用，且在需要修改时只需要修改此模块，而不需要在项目中的每个此代码处做修改。 可维护性 模块应该是独立的，一个设计良好的模块应尽可能减少对部分代码库的依赖，从而使其能够独立地删减和修改。当模块与其他代码片段分离时，更新单个模块要容易得多，还可以对每次修改的内容做版本管理。 传统的模块化开发方式 当多个 JS 文件为变量和方法取相同名称而造成命名冲突时，可以采用 Java 中的命名空间的方式。 // 代码来自：https://github.com/seajs/seajs/issues/547 var org = {}; org.CoolSite = {}; org.CoolSite.Utils = {}; org.CoolSite.Utils.each = function (arr) { // 实现代码 }; org.CoolSite.Utils.log = function (str) { // 实现代码 }; 类似于 Java 或 Python 等其他编程语言中使用类的方式，可以将公共以及私有的方法和变量存储在单个对象中。将要公开给全局作用域的方法写在闭包外，将私有变量和方法封装在闭包范围内，这样就可以解决变量都暴露在全局作用域的问题。 // 全局作用局可访问 var global = 'Hello World'; (function() { // 只能在闭包内访问 var a = 2; })() 虽然这种方法有其好处，但也有其缺点。 通过立即执行的工厂函数定义的模块（IIFE: Immediately Invoked Function Expression）。 对依赖项的引用是通过通过 HTML 脚本标记加载的全局变量名完成的。 依赖关系是非常弱的：开发人员需要知道正确的依赖顺序。例如，使用 Backbone 的文件不能在 jQuery 标记之前。 需要额外的工具来将一组脚本标记替换为一个标记以优化部署。 这在大型项目上很难管理，特别是当脚本以重叠和嵌套的方式具有许多依赖关系时。手写脚本标记的可伸缩性不高，而且它没有按需加载脚本的能力。 那是否有方法，可以不用在全局范围内请求依赖的模块，而是在模块内部请求依赖的模块呢？CommonJS、AMD、CMD、UMD 等应运而生，这些模块化规范告诉开发者： 如何引入模块的依赖（imports） 如何定义模块（code） 如何导出模块的接口（exports） 从模块化开发思想提出以来，无论是浏览器端还是服务端 JavaScript 开发，开发者们一直在探索满足实际需求的模块化规范及其实现，它们要解决的问题是相同的,即模块化开发和模块依赖的问题，但它们发起的原因却各有不同。 CommonJS Mozilla 工程师 Kevin Dangoor 于 2009 年 1 月发起 ServerJS 项目，旨在规范化 JavaScript 在服务端使用时的模块化，以及 Filesystem API、I/O Streams、Socket IO 等服务端开发领域所涉及内容的标准化。 他在 what-server-side-javascript-needs 中提到了服务端 JavaScript 需要什么： a cross-interpreter standard library a handful of standard interfaces a standard way to include other modules a way to package up code for deployment and distribution and further to install packages 并希望这些在尽可能多的操作系统和解释器上工作，包括三个主要的操作系统（Windows、Mac、Linux）和四个主要的解释器（SpiderMonkey、Rhino、v8、JavaScriptCore），另外还有“浏览器”（本身就是一个独特的环境）。 为了展示其定义的 API 可以广泛适用，在 2009 年 8 月 ，ServerJS 被改名为 CommonJS。后来的很多开发吐槽，认为 CommonJS 的模块格式对浏览器很不友好(不支持异步写法)，把浏览器当第二类公民，它更适合 ServerJS 这个名称。 I also feel like CommonJS has treated browser use as a second class citizen, which may have made more sense when it was ServerJS. As it stands today, the CommonJS module format is unfriendly to the browser. NodeJS 同年 5 月 31，美国程序员 Ryan Dahl 实现了 Node.js 项目（New server-side js project: Node），并在同年 11 月 8 日在 JSConf 大会上首次介绍 Node.js（Ryan Dahl at JSConf EU 2009 Video）。 直接使用 CommonJS 规范实现模块体系的 Node.js 广受欢迎，相信绝大部分 Web 开发者至今都管 Node.js 的模块体系叫 CommonJS 规范： //math.js exports.sum = function(...nums){ return nums.reduce((result, num) =&gt; result + num, 0) } //index.js var math = require('math') exports.result = math.sum(1,3); 事实上，两者的关系并非我们认为的标准制定者和标准执行者的角色。在 2011 年 5 月， Ryan 应 r/node 的版主要求开了个问答的帖子, 在回答问题时说到 CommonJS 已死，不值得我们去讨论，那已经是 2009 年的事情： Consider CommonJS extinct - not worth thinking further about. That was a 2009 thing. 到 2013 年 3 月， brettz9 就此在 Node.js 社区发问： What is the reason for the indifference to CommonJS? I understand you are no longer looking to adhere to it.* Are all contributors abandoning it or just you? npm 创始人 Isaac Schlueter 对此做出了回应： A few good things came out of CommonJS. The module system we have now is basically indistinguishable from the original &quot;securable modules&quot; proposal that Kris Kowal originally came up with. (Of course, we went pretty far off the reservation in v0.4, with the whole node_modules folder thing, and loading packages via their &quot;main&quot; field. Maybe we should remove those features, I'm sure that Node users would appreciate us being more spec-compliant!) 评论中认为，CommonJS 标准已经成为小众服务端 JS（Server Side JS）方案的文档集中地，而 Node.js 已经赢得服务端 JS 的竞争，如同 Node.js 创始人 Ryan Dahl 所说： &quot;Forget CommonJS. It's dead. We are server side JavaScript.&quot; Node.js 就是服务端 JavaScript。更为重要的是，Isaac 更看重真实用户的声音而不是所谓标准制定者的意见，而当时 CommonJS 工作组所提出的新标准更多的是添乱（比如所谓的 Package 标准）。到 2013 年的时候，其实 Node.js Modules 就已经自成一家了。 Module Loader 回到 2009 年，网页开发者们正对着一堆 &lt;script&gt; 标签发愁。如何在浏览器中管理依赖，是一个很让人头疼的问题。YUI 2 和 Google Closure Library 都提出过基于 namespace 的方案，但治标不治本，仍然需要人肉确保脚本的加载、打包顺序。 CommonJS 提出后, 有人疑问 为什么 CommmonJS 只关注服务端，Kevin Dangoor 在它的 博客 中提到的特别加粗的几点内容并不只能是服务端专属，浏览器端的 JS 同样可以拥有。 In agreement on the desire to have some standardization around the areas that you've bold-ed in your post. One nit though: there's not really anything server-specific about this stuff. It applies to browser-based JS usage, and even other JS usage, like folks integrating with Gnome, Cocoa, etc. CommonJS 致力于 JavaScript 的服务端生态，模块同步加载，语法非常简洁，对服务端开发很友好。但这在浏览器端是无法接受的，从网络上读取一个模块比从磁盘上读取要花费更长的时间，只要加载模块的脚本正在运行，就会阻止浏览器运行其他，直到模块加载完成。 在 CommonJS 的论坛 中，Kevin Dangoor 发起过 关于异步加载 Commonjs 模块的讨论 以及 征集浏览器端的模块加载方案。论坛中也出现了很多关于如何在浏览器中异步加载 CommonJS 模块的帖子。 有提出 transport 方案的，在浏览器上运行前，先通过转换工具将模块转换为符合 Transport 规范的代码。 有提出 XHR 加载模块代码文本，再在浏览器中使用 eval 或者 new Function 执行的。 有提出应当直接改良 CommonJS，推出纯异步的模块加载方案的。 第三种方案的提出者 James Burke 认为：CommonJS 的模块格式不支持浏览器端的异步加载，需要通过 XHR 等其他方式加载 CommonJS 的模块，对 web 前端开发者很不友好。提出者认为浏览器端开发的最佳实践是：每个页面只加载一个模块。就像这样： &lt;!-- loader.js defines LOADER_ENTRY_FUNCTION --&gt; &lt;script src=&quot;loader.js&quot;&gt;&lt;/script&gt; &lt;script&gt;LOADER_ENTRY_FUNCTION([&quot;page1&quot;]);&lt;/script&gt; page1 模块可能长这样: LOADER_ENTRY_FUNCTION( &quot;page1&quot;, [&quot;b&quot;, &quot;c&quot;], function(b, c) { // document.addEventListener(&quot;DOMContentLoaded&quot;, function() { //Do page setup in here, use b and c }, false); } ); 在 dev 模式下，每个模块都可以单独加载，以提供最佳的调试体验。然后可以通过编译将所有 page1 模块的依赖项和嵌套依赖项并入其中，或者可以通过 loader.js 在运行时组合加载依赖。 RequireJS in AMD James Burke 于 09 年 12 月在 CommonJS in the browser 中写了很长的篇幅阐述了直接改良 CommonJS 的模块格式以适应浏览器端开发的诉求，但是 CommonJS 的发起者 Kevin Dangoor 并不同意此方案,这也就催生了 RequireJS，RequireJS 产生的过程可以翻看 James Burke 曾发起的讨论帖： New amd-implement list 11/5/25 Split off AMD? (was Re: [CommonJS] New amd-implement list) Harmony module execution AMD proposal change: define.amd AMD vs Wrappings Function.prototype.toString to discover function dependencies 10/9/16 Updated Transport proposals 10/3/31 James Burke 制定了 AMD 规范，并在 2010 年实现了遵循 AMD 规范的模块加载器 RequireJS。建议看下官网的这篇 WHY AMD? require.config({ path: { module: './module', } }); require(['module/module1.js','module/module2.js'],function(module1,module2){ module1.printModule1FileName(); module2.printModule2FileName(); }); SeaJS in CMD 玉伯 认为 RequireJS 不够完善： 执行时机有异议 ReqiureJS 模块加载完毕后是立即执行， SeaJS 在模块加载完毕后保存 factory 函数，在执行到 require 时再执行模块对应的 factory 函数返回模块的导出结果。 模块书写风格有争议 AMD 风格下，通过参数传入依赖模块的导出，破坏了 就近声明 原则。 他认为 AMD 的流行，很大程度上取决于 RequireJS 作者的推广，但火起来的东西未必好。他先后给 RequireJS 团队提的很多意见，但都未被采纳。在 CommonJS 的 Group 中提到： RequireJS is good but I found that its API is annoying me. I dived into reading topics in this group, and began to implement a module loader (SeaJS) from scratch some month ago. My fellow don't like RequireJS too after using it in two projects. 意思是：RequireJS 很好，但它的 API 让我很恼火。在两个项目中使用 RequireJS 之后，我的同事也不喜欢它。我阅读了这个 Group 中讨论的主题，并在几个月前开始从头开始实现模块加载程序（SeaJS）。 RequireJS 的作者 James Burke 看到后似乎有点恼火, 他们对于 RequireJS 和 SeaJS 的争论都在这个讨论中 Some thought on APIs for CommonJS module loader。James Burke 认为玉伯提出的 API RequireJS 都支持，对此玉伯详细列举了 RequireJS 的问题： 在 RequireJS 中， require 有多重用法，对于新手来说很容易出错，也不容易被用户很好的理解。 require('a') -- gets exports of module a require(['a']) -- fetch module a according to module name scheme require(['a.js']) -- fetch a.js directly relative to current page require({...}) -- set loader config 我不太在意是懒执行还是提前执行，但 RequireJS API 很容易在代码中留下垃圾，例如： define(['a', 'b'], function(a, b) { a.xx(); b.yy(); }); 虽然在这里说不在意是是懒执行还是提前执行， 但是在 前端模块化开发那点历史 中，他又提到了 执行时机 的问题， 囧 刚开始一切都很好，但随着时间的推移，一些模块代码将由不同的编码人员维护，代码将变得如下所示： define(['a', 'b', 'c', ..., 'z'], function(a, b, c, ..., z) { a.xx(); // b.yy(); z.bar(); }); 可以看到，模块 b 被注释了，不再使用了。但是依赖项数组仍然包含“b”。这太糟糕了！ 当然，我们可以强迫用户只使用另一种形式，例如： define(function(require, exports) { ... }); 但他们会知道这是 RequireJS，并且会花一些时间阅读 RequireJS 文档。一旦开始这样使用了，就很难禁止使用。 除了上述缺点之外，玉伯认为 RequireJS 还有以下缺点： 它压缩之后还有 16KB，它只是一个模块加载程序，应该更小，更快。 RequireJS 的源代码在我们的使用中包含许多不必要的函数。 我认为加载程序应该编译为： loader-for-browser.js loader-for-node.js loader-for-xx.js loader-for-browser.js 应该只包含客户端浏览器中模块加载器的基本功能。但是“require.js”包含 webworker、jQueryCheck、DOMContentLoaded、packages 等的代码，在我们的用例中这些是不必要的，但是我们无法删除这些不需要的代码。 基于以上这些原因， 玉伯开发了一个新的 Module Loader: SeaJS， 于 2011 年 11 月在 CommonJS Group 中宣布公开（Announcing SeaJS: A Module Loader for the Web），Sea.js 遵循 CMD 规范。 define(function(require, exports, module) { // exports 是 module.exports 的一个引用 console.log(module.exports === exports); // true // 重新给 module.exports 赋值 module.exports = new SomeClass(); // exports 不再等于 module.exports console.log(module.exports === exports); // false }); ES Module 本章节大部分摘自：Chen Yangjian 的博客：前端模块的现状 在 2015 年 6 月， ECMAScript6 标准正式发布，其中的 ES 模块化规范的提出目标是整合 CommonJS、AMD 等已有模块方案，在语言标准层面实现模块化，成为浏览器和服务器通用的模块解决方案。 模块功能由 export 和 import 两个命令完成。export 对外输出模块，import 用于引入模块。import 更多用法，export 更多用法。 // 导入单个接口 import {myExport} from '/modules/my-module.js'; // 导入多个接口 import {foo, bar} from '/modules/my-module.js'; // 导出早前定义的函数 export { myFunction }; // 导出常量 export const foo = Math.sqrt(2); ES Module 与 CommonJS 及 Loaders 等方案的区别主要在以下方面： 声明式而非命令式，或者说 import 是声明语句 Declaration 而非表达式 Statement，在 ES Module 中无法使用 import 声明带变量的依赖、或者动态引入依赖。 CommonJS 模块输出的是一个值的拷贝，ES6 模块输出的是值的引用。 import 是预先解析、预先加载的，不像 RequireJS 等是执行到点了再发一个请求。 对务实主义的 Node.js 开发者来说，这些区别都让 npm 所营造出来的海量社区代码陷入一种尴尬的境地，无论是升级还是兼容都需要大量的工作。对此，David Herman 撰文解释，ES Module 所带来的好处远大于不便： 静态 import 能确保被编译成变量引用，这些引用在当前执行环境运行时能被解析器（通过 JIT 编译 polymorphic inline cache）优化，执行更有效率。 静态 export 能让变量检测更准确，在 JSHint、ESLint 等代码检测工具中，变量是否定义是个非常受欢迎的功能，而静态 export 能让这一检测更具准确性。 更完备的循环依赖处理，在 Node.js 等已有的 CommonJS 实现中，循环依赖是通过传递未完成的 exports 对象解决的，对于直接引用 exports.foo 或者父模块覆盖 module.exports 的情况，传统方式无从解决，而因为 ES Module 传递的是引用，便不会有这些问题。 其他还有对未来可能新增的标准（宏、类型系统等）更兼容等。 ES Module in Browser 在 ES Module 标准出来之前，尽管社区实现的 Loader 一箩筐，但浏览器自身一直没有选定模块方案，支持 ES Module 对浏览器来说还是比较少顾虑的。 由于 ES Module 的执行环境和普通脚本不同，浏览器选择增加 &lt;script type=&quot;module&quot;&gt;，只有 &lt;script type=&quot;module&quot;&gt; 中的脚本（和 import 进来的脚本）才是 module 模式。也只有 module 模式执行的脚本，才可以声明 import。也就是说，下面这种代码是不行的： &lt;script&gt; import foo from &quot;./foo.js&quot; &lt;/script&gt; &lt;script type=&quot;javascript&quot;&gt; import bar from &quot;./bar.js&quot; &lt;/script&gt; 目前，几大常青浏览器都已支持 ES Module。最后一个支持的是 Firefox，2018 年 5 月 8 日发布的 Firefox 60 正式支持 ES Module。 此外，考虑到向后兼容，浏览器还增加 &lt;script nomodule&gt; 标签。开发者可以使用 &lt;script nomodule&gt; 标签兼容不支持 ES Module 的浏览器： &lt;!--在浏览器中，import 语句只能在声明了 type=&quot;module&quot; 的 script 的标签中使用。--&gt; &lt;script type=&quot;module&quot; src=&quot;./app.js&quot;&gt;&lt;/script&gt; &lt;!--在 script 标签中使用 nomodule 属性，可以确保向后兼容。--&gt; &lt;script nomodule src=&quot;./app.bundle.js&quot;&gt;&lt;/script&gt; ES Module in Node.js 但在 Node.js 这边，ES Module 遭遇的声音要大很多。前 Node.js 领导者 Isaacs Schlutuer 甚至认为 ES Module 太过阳春白雪且不考虑实际情况，毫无价值（adds nothing）。 首先纠结的是如何支持 module 执行模式，是自动检测，还是 'use module'，还是在 package.json 里增加 module 属性作为专门的入口，还是干脆增加一个新的扩展名？ 最终 Node.js 选择增加新的扩展名 .mjs： 在 .mjs 中可以自如使用 import 和 export； 在 .mjs 中不可以使用 require； 在 .js 中只能使用 require； 在 .js 中不可以使用 import 和 export。 也就是两套模块系统完全独立。此外，依赖查找方式也有变化，原本 require.extensions 是： { '.js': [Function], '.json': [Function], '.node': [Function] } 如今（需要开启 --experimental-modules 选项）则是： { '.js': [Function], '.json': [Function], '.node': [Function], '.mjs': [Function] } 但两套独立的模块系统也导致第二个纠结的方面，模块系统彼此之间如何互通？对浏览器来说这不是问题，但对 Node.js 来说，npm 中海量的 CommonJS 模块是它不得不考虑的。 最终确定的方案倒也简单，在 .mjs 里，开发者可以 import CommonJS（虽然只能 import default）： import 'fs' from 'fs' import { readFile } from 'fs' import foo from './foo' // etc. 在 .js 里，开发者自然不能 import ES Module，但他们可以 import()： import('./foo').then(foo =&gt; { // use foo }) async function() { const bar = await import('./bar') // use bar }() 注意，和浏览器以引入方式判断运行模式不同，Node.js 中脚本的运行模式是和扩展名绑定的。也就是说，依赖的查找方式会有所不同： 在 .js 中 require('./foo') 找的是 ./foo.js 或者 ./foo/index.js； 在 .mjs 中 import './bar' 找的是 ./bar.mjs 或者 ./bar/index.mjs。 善用这些特性，我们现在就可以将已有的 npm 模块升级成 ES Module，并且仍然支持 CommonJS 方式。 Dynamic Import 静态型的 import 是初始化加载依赖项的最优选择，使用静态 import 更容易从代码静态分析工具和 tree shaking 中受益。但当希望按照一定的条件或者按需加载模块的时候，需要动态引入依赖，例如： if (process.env.NODE_ENV !== 'production') { require('./cjs/react.development.js') } else { require('./cjs/react.production.js') } if (process.env.BROWSER) { require('./browser.js') } 为此，Domenic Denicola 起草 import() 标准提案。 //这是一个处于第三阶段的提案。 var promise = import(&quot;module-name&quot;); 除了可以用来处理动态依赖，HTML 中的 script 标签不需要声明 type=&quot;module&quot;。 &lt;script&gt; import('./foo.js').then(foo =&gt; { // use foo }) &lt;/script&gt; 在 Node.js 中（.js 文件）还可以使用 import() 引入使用 import 的 ES Module ： import('./foo.mjs').then(foo =&gt; { // use foo }) 使用 ES Module 编写浏览器、Node.js 通用的 JavaScript 模块化代码已经完全可行，我们还需要编译或者打包工具吗？ Module Bundler 本章节大部分摘自：Chen Yangjian 的博客：前端模块的历史沿革 在浏览器端使用模块加载器也存在很多弊端。例如 RequireJS 编码方式不友好、加载其他规范的模块比较麻烦、提前执行等，SeaJS 规则一直变化导致升级出现各种问题等，而 CommonJS 在服务端的使用就很方便稳定，引用第三方库只需简单三步： 在 package.json 里面配置模块名和版本号； npm install 安装模块； 直接使用 require 引入。 那能否在浏览器中也使用 CommonJS 规范的方式引入模块并可以很方便调用其他规范的模块呢？ 一种解决办法就是预编译，我们用 CommonJS 规范的方式书写代码定义和引入模块，然后将模块和依赖编译成一个 js 文件，我们都叫它 bundlejs。 Browserify 和 webpack 都是这种预编译的模块化方案， 最终都是 build 生成一个 bundle 文件，在这个 build 的过程里进行依赖关系的解析。 Browserify Node.js 社区早期活跃成员 substack 开发 Browserify 的初衷非常简单： Browsers don't have the require method defined, but Node.js does. With Browserify you can write code that uses require in the same way that you would use it in Node. Browserify[Github] 可以让你使用类似于 node 的 require() 的方式来组织浏览器端的 JavaScript 代码，通过预编译让前端 JavaScript 可以直接使用 Node NPM 安装的一些库, 也可以引入非 CommonJS 模块，但需要使用 transform（browserify.transform 配置转换插件）。 Browserify 的 require 与 Node.js 保持一致，不支持异步加载。社区希望Browserify支持异步加载的呼声一直很高 ，可见：Support for asynchronous loading (and not packing everything in one file)，但作者坚持认为 Browserify 的 require 应当和 Node.js 保持一致： 1: wrapping a whole file in a function block is ugly 2: node modules use synchronous requires 3: browserify's goal is to let code written for node run in the browser Webpack 晚于 Browserify 一年发布的 Webpack 结合了 CommonJS 和 AMD 的优缺点，开发时可按照 CommonJS 的编写方式，支持编译后按需加载和异步加载所有资源。 Webpack 最出色的特性一是它的模块解析粒度以及因此带来的强大打包能力，二是它的可扩展性，相关转换工具（Babel、PostCSS、CSS Modules）可以变成插件快速接入，还能自定义 Loader。这些特性加在一起，无往而不利。 而且它还支持 ES Module： import defaultExport from &quot;module-name&quot;; import * as name from &quot;module-name&quot;; import { export } from &quot;module-name&quot;; 这便是构建工具带来的好处了，发挥空间远比传统浏览器 Loader 来得大，可以轻松加入像 Babel、Traceur 等 transpiler 支持。 更多推荐阅读： webpack for browserify users how-is-webpack-different webpack 与 竞品的对比 Afterword 正如玉伯在 前端模块化开发那点历史 中所说： 随着 W3C 等规范、以及浏览器的飞速发展，前端的模块化开发会逐步成为基础设施。一切终究都会成为历史。 我们现在开发中不必再纠结使用哪种模块化方案， ES6 在语言标准层面为我们解决了这个问题。 Time Line 2009 年，美国程序员 Ryan Dahl 创造了node.js 项目，node.js 的模块系统就是参照CommonJS的模块规范写的。 但是 CommonJS 规范中的 require 是同步的，这在浏览器端是不能接受的。所以后来就有了 AMD 规范，2010 年，RequireJS 实现了是 AMD 规范。 2012 年来玉伯觉得 RequireJS 不够完善，给 RequireJS 团队提的很多意见都不被采纳，就自己写了 Sea.js，并制定了CMD 规范，Sea.js 遵循 CMD 规范。 2015 年 6 月正式发布了 ECMAScript6 标准，在语言标准层面实现了模块功能，完全可以取代 CommonJS 和 AMD 规范，成为浏览器和服务器通用的模块解决方案。(这是未来) 2015 年 10 月，UMD 出现，整合了 CommonJS 和 AMD 两个模块定义规范的方法。这时候 ES6 模块标准才刚出来，很多浏览器还不支持 ES6 模块化规范。 2016 年 browserify 发布 2017 年 webpack 发布 Current Situation CommonJS Node.js Start 67.3k 已成为服务端 JavaScript 标准 Module Loader（模块加载器已成过去式） RequireJS [GitHub] Start: 12.4k，已经不维护了 seajs[Github] Start: 8k, 已经不维护了。作者 2015 年就发布微博: 应该给 Sea.js 树一块墓碑了。 ES6 Module 语法在主流浏览器和 Node.js 8.5 版本以上都已支持，查看 浏览器兼容性。 Module Bundler webpack [GitHub] Star: 52.5k， 现在最火的打包工具 browserify [GitHub] Star: 13k，2019年11月有更新 最后，本文部分参考或摘录自以下文章： 前端模块的历史沿革 前端模块的现状 前端模块化开发那点历史 JavaScript Modules: A Beginner's Guide JavaScript Modules Part 2: Module Bundling ","link":"https://faded.auspicious.space/post/the-evolution-history-of-javascript-modularity/"},{"title":"深入 Node.js 的进程与子进程","content":" 深入Node.js的进程与子进程 进程：process模块 process 模块是 Node.js 提供给开发者用来和当前进程交互的工具，它的提供了很多实用的 API。从文档出发，管中窥豹，进一步认识和学习 process 模块： 如何处理命令参数 命令行参数指的是 2 个方面： 传给 node 的参数。例如 node --harmony script.js --version 中，--harmony 就是传给 node 的参数； 传给进程的参数。例如 node script.js --version --help 中，--version --help 就是传给进程的参数。 它们分别通过 process.argv 和 process.execArgv 来获得。 如何处理工作目录 通过 process.cwd() 可以获取当前的工作目录。 通过 process.chdir(directory) 可以切换当前的工作目录，失败后会抛出异常。实践如下： function safeChdir(dir) { try { process.chdir(dir); return true; } catch (error) { return false; } } 如何处理异常 uncaughtException 事件 Node.js 可以通过 try-catch 来捕获异常。如果异常未捕获，则会一直从底向事件循环冒泡。如是冒泡到事件循环的异常没被处理，那么就会导致当前进程异常退出。 根据文档，可以通过监听 process 的 uncaughtException 事件，来处理未捕获的异常： process.on(&quot;uncaughtException&quot;, (err, origin) =&gt; { console.log(err.message); }); const a = 1 / b; console.log(&quot;abc&quot;); // 不会执行 上面的代码，控制台的输出是：b is not defined。捕获了错误信息，并且进程以 0 退出。开发者可以在 uncaughtException 事件中，清除一些已经分配的资源（文件描述符、句柄等），不推荐在其中重启进程。 unhandledRejection 事件 如果一个 Promise 回调的异常没有被 .catch() 捕获，那么就会触发 process 的 unhandledRejection 事件： process.on(&quot;unhandledRejection&quot;, (err, promise) =&gt; { console.log(err.message); }); Promise.reject(new Error(&quot;错误信息&quot;)); // 未被catch捕获的异常，交由unhandledRejection事件处理 warning 事件 告警不是 Node.js 和 JavaScript 错误处理流程的正式组成部分。 一旦探测到可能导致应用性能问题，缺陷或安全隐患相关的代码实践，Node.js 就可发出告警。 比如前一段代码中，如果出现未被捕获的 Promise 回调的异常，那么就会触发 warning 事件。 如何处理进程退出 process.exit() VS process.exitCode 一个 Node.js 进程，可以通过 process.exit() 来指定退出代码，直接退出。不推荐直接使用 process.exit()，这会导致事件循环中的任务直接不被处理，以及可能导致数据的截断和丢失（例如 stdout 的写入）。 setTimeout(() =&gt; { console.log(&quot;我不会执行&quot;); }); process.exit(0); 正确安全的处理是，设置 process.exitCode，并允许进程自然退出。 setTimeout(() =&gt; { console.log(&quot;我不会执行&quot;); }); process.exitCode = 1; beforeExit 事件 用于处理进程退出的事件有：beforeExit 事件 和 exit 事件。 当 Node.js 清空其事件循环并且没有其他工作要安排时，会触发 beforeExit 事件。例如在退出前需要一些异步操作，那么可以写在 beforeExit 事件中： let hasSend = false; process.on(&quot;beforeExit&quot;, () =&gt; { if (hasSend) return; // 避免死循环 setTimeout(() =&gt; { console.log(&quot;mock send data to serve&quot;); hasSend = true; }, 500); }); console.log(&quot;.......&quot;); // 输出： // ....... // mock send data to serve 注意：在 beforeExit 事件中如果是异步任务，那么又会被添加到任务队列。此时，任务队列完成所有任务后，又回触发 beforeExit 事件。因此，不处理的话，可能出现死循环的情况。如果是显式调用 exit()，那么不会触发此事件。 exit 事件 在 exit 事件中，只能执行同步操作。在调用 exit 事件监听器之后，Node.js 进程将立即退出，从而导致在事件循环中仍排队的任何其他工作被放弃。 process 的标准流对象 process 提供了 3 个标准流。需要注意的是，它们有些在某些时候是同步阻塞的（请见文档）。 process.stderr：WriteStream 类型，console.error 的底层实现，默认对应屏幕 process.stdout：WriteStream 类型，console.log 的底层实现，默认对应屏幕 process.stdin：ReadStream 类型，默认对应键盘输入 下面是基于“生产者-消费者模型”的读取控制台输入并且及时输出的代码： process.stdin.setEncoding(&quot;utf8&quot;); process.stdin.on(&quot;readable&quot;, () =&gt; { let chunk; while ((chunk = process.stdin.read()) !== null) { process.stdout.write(`&gt;&gt;&gt; ${chunk}`); } }); process.stdin.on(&quot;end&quot;, () =&gt; { process.stdout.write(&quot;结束&quot;); }); 关于事件的含义，还是请看 stream 的文档。 深入理解 process.nextTick 我第一次看到 process.nextTick 的时候是比较懵的，看文档可以知道，它的用途是：把回调函数作为微任务，放入事件循环的任务队列中。但这么做的意义是什么呢？ 因为 Node.js 并不适合计算密集型的应用，一个进程就一个线程，在当下时间点上，就一个事件在执行。那么，如果我们的事件占用了很多 CPU 时间，那么之后的事件就要等待非常久。所以，Node.js 的一个编程原则是尽量缩短每一个事件的执行事件。process.nextTick 的作用就在这，将一个大的任务分解成多个小的任务。示例代码如下： // 被拆分成2个函数执行 function BigThing() { doPartThing(); process.nextTick(() =&gt; finishThing()); } 在事件循环中，何时执行 nextTick 注册的任务呢？请看下面的代码： setTimeout(function() { console.log(&quot;第一个1秒&quot;); process.nextTick(function() { console.log(&quot;第一个1秒：nextTick&quot;); }); }, 1000); setTimeout(function() { console.log(&quot;第2个1秒&quot;); }, 1000); console.log(&quot;我要输出1&quot;); process.nextTick(function() { console.log(&quot;nextTick&quot;); }); console.log(&quot;我要输出2&quot;); 输出的结果如下，nextTick 是早于 setTimeout： 我要输出1 我要输出2 nextTick 第一个1秒 第一个1秒：nextTick 第2个1秒 在浏览器端，nextTick 会退化成 setTimeout(callback, 0)。但在 Node.js 中请使用 nextTick 而不是 setTimeout，前者效率更高，并且严格来说，两者创建的事件在任务队列中顺序并不一样（请看前面的代码）。 子进程：child_process 模块 掌握 Nodejs 的 child_process 模块能够极大提高 Node.js 的开发能力，例如主从进程来优化 CPU 计算的问题，多进程开发等等。本文从以下几个方面介绍 child_process 模块的使用： 创建子进程 Node.js 的 child_process 模块创建子进程的方法：spawn，fork，exec，execFile。它们的关系如下： fork，exec，execFile 都是通过 spawn 来实现的。 exec 默认会创建 shell。execFile 默认不会创建 shell，意味着不能使用 I/O 重定向、file glob，但效率更高。 spawn、exec、execFile 都有同步版本，可能会造成进程阻塞。 child_process.spawn() 的使用： const { spawn } = require(&quot;child_process&quot;); // 返回ChildProcess对象，默认情况下其上的stdio不为null const ls = spawn(&quot;ls&quot;, [&quot;-lh&quot;]); ls.stdout.on(&quot;data&quot;, data =&gt; { console.log(`stdout: ${data}`); }); ls.stderr.on(&quot;data&quot;, data =&gt; { console.error(`stderr: ${data}`); }); ls.on(&quot;close&quot;, code =&gt; { console.log(`子进程退出，退出码 ${code}`); }); child_process.exec() 的使用： const { exec } = require(&quot;child_process&quot;); // 通过回调函数来操作stdio exec(&quot;ls -lh&quot;, (err, stdout, stderr) =&gt; { if (err) { console.error(`执行的错误: ${err}`); return; } console.log(`stdout: ${stdout}`); console.error(`stderr: ${stderr}`); }); 父子进程通信 fork() 返回的 ChildProcess 对象，监听其上的 message 事件，来接受子进程消息；调用 send 方法，来实现 IPC。 parent.js 代码如下： const { fork } = require(&quot;child_process&quot;); const cp = fork(&quot;./sub.js&quot;); cp.on(&quot;message&quot;, msg =&gt; { console.log(&quot;父进程收到消息：&quot;, msg); }); cp.send(&quot;我是父进程&quot;); sub.js 代码如下： process.on(&quot;message&quot;, m =&gt; { console.log(&quot;子进程收到消息：&quot;, m); }); process.send(&quot;我是子进程&quot;); 运行后结果： 父进程收到消息： 我是子进程 子进程收到消息： 我是父进程 独立子进程 在正常情况下，父进程一定会等待子进程退出后，才退出。如果想让父进程先退出，不受到子进程的影响，那么应该： 调用 ChildProcess 对象上的 unref() options.detached 设置为 true 子进程的 stdio 不能是连接到父进程 main.js 代码如下： const { spawn } = require(&quot;child_process&quot;); const subprocess = spawn(process.argv0, [&quot;sub.js&quot;], { detached: true, stdio: &quot;ignore&quot; }); subprocess.unref(); sub.js 代码如下： setInterval(() =&gt; {}, 1000); 进程管道 options.stdio 选项用于配置在父进程和子进程之间建立的管道。 默认情况下，子进程的 stdin、stdout 和 stderr 会被重定向到 ChildProcess 对象上相应的 subprocess.stdin、subprocess.stdout 和 subprocess.stderr 流。 这意味着可以通过监听其上的 data 事件，在父进程中获取子进程的 I/O 。 可以用来实现“重定向”： const fs = require(&quot;fs&quot;); const child_process = require(&quot;child_process&quot;); const subprocess = child_process.spawn(&quot;ls&quot;, { stdio: [ 0, // 使用父进程的 stdin 用于子进程。 &quot;pipe&quot;, // 把子进程的 stdout 通过管道传到父进程 。 fs.openSync(&quot;err.out&quot;, &quot;w&quot;) // 把子进程的 stderr 定向到一个文件。 ] }); 也可以用来实现“管道运算符”： const { spawn } = require(&quot;child_process&quot;); const ps = spawn(&quot;ps&quot;, [&quot;ax&quot;]); const grep = spawn(&quot;grep&quot;, [&quot;ssh&quot;]); ps.stdout.on(&quot;data&quot;, data =&gt; { grep.stdin.write(data); }); ps.stderr.on(&quot;data&quot;, err =&gt; { console.error(`ps stderr: ${err}`); }); ps.on(&quot;close&quot;, code =&gt; { if (code !== 0) { console.log(`ps 进程退出，退出码 ${code}`); } grep.stdin.end(); }); grep.stdout.on(&quot;data&quot;, data =&gt; { console.log(data.toString()); }); grep.stderr.on(&quot;data&quot;, data =&gt; { console.error(`grep stderr: ${data}`); }); grep.on(&quot;close&quot;, code =&gt; { if (code !== 0) { console.log(`grep 进程退出，退出码 ${code}`); } }); 参考链接 Nodejs v12 Stream 文档 Nodejs v12 process 文档 nodejs 学习笔记 一篇文章构建你的 NodeJS 知识体系 Node.js - 进程学习笔记 glob Nodejs 进阶：如何玩转子进程（child_process） ","link":"https://faded.auspicious.space/post/nodejs-process-and-child-process-in-deep/"},{"title":"详解消息队列","content":" 消息队列助你成为高薪的 Node.js 工程师 1 什么是消息队列 “消息队列”是在消息的传输过程中保存消息的容器。 个人理解：我把它分成两个词消息和队列。当一大批客户端同时产生大量的网络请求（消息）时候，服务器的承受能力肯定是有一个限制的。这时候要是有个容器，先让这些消息排队就好了，还好有个叫队列的数据结构，通过有队列属性的容器排队(先进先出)，把消息再传到我们的服务器，压力减小了好多，这个很棒的容器就是消息队列。 这段理解中还包含这个两个概念: 客户端-&gt;生产者 服务器-&gt;消费者 当有消息队列出现，生产者和消费者是必不可少的两个概念，上面的理解是多个生产者对应一个消费者，当然现实开发中还有许多消费者的情况哦。接下来的文章也会多次提到生产-消费模型。 1.1 消息队列优势 应用解耦 消息队列可以使消费者和生产者直接互不干涉，互不影响，只需要把消息发送到队列即可，而且可独立的扩展或修改两边的处理过程，只要能确保它们遵守同样的接口约定，可以生产者用 Node.js 实现，消费者用python 实现。 灵活性和峰值处理能力 当客户端访问量突然剧增，对服务器的访问已经超过服务所能处理的最大峰值，甚至导致服务器超时负载崩溃，使用消息队列可以解决这个问题，可以通过控制消费者的处理速度和生产者可进入消息队列的数量等来避免峰值问题。 排序保证 消息队列可以控制数据处理的顺序，因为消息队列本身使用的是队列这个数据结构，FIFO（先进选出），在一些场景数据处理的顺序很重要，比如商品下单顺序等。 异步通信 消息队列中的有些消息，并不需要立即处理，消息队列提供了异步处理机制，可以把消息放在队列中并不立即处理，需要的时候处理，或者异步慢慢处理，一些不重要的发送短信和邮箱功能可以使用。 可扩展性 前面提到了消息队列可以做到解耦，如果我们想增强消息入队和出队的处理频率，很简单，并不需要改变代码中任何内容，可以直接对消息队列修改一些配置即可，比如我们想限制每次发送给消费者的消息条数等。 1.2 消息队列的类型介绍 介绍几款目前市场上主流的消息队列（课外知识，可忽略）： Kafka 是由 Apache 软件基金会开发的一个开源流处理平台，由 Scala 和 Java 编写，是一种高吞吐量的分布式发布订阅消息系统，支持单机每秒百万并发。另外，Kafka 的定位主要在日志等方面， 因为Kafka 设计的初衷就是处理日志的，可以看做是一个日志（消息）系统一个重要组件，针对性很强。0.8 版本开始支持复制，不支持事物，因此对消息的重复、丢失、错误没有严格的要求。 RocketMQ 阿里开源的消息中间件，是一款低延迟、高可靠、可伸缩、易于使用的消息中间件，思路起源于 Kafka。最大的问题商业版收费，有些功能不开放。 RabbitMQ 由 Erlang（有着和原生 Socket 一样低的延迟）语言开发基于 AMQP 协议的开源消息队列系统。能保证消息的可靠性、稳定性、安全性。高并发 的特性，毋庸置疑，RabbitMQ 最高，原因是它的实现语言是天生具备高并发高可用的 Erlang 语言，天生的分布式优势。 2 初识消息队列 2.1 Rabbitmq基本安装 2.1.1 Mac版安装 直接通过 HomeBrew 安装，执行以下命令： brew install rabbitmq 2.1.2 启动 rabbitmq 进入安装目录 $ /usr/local/Cellar/rabbitmq/3.7.8 启动 $ sbin/rabbitmq-server 浏览器输入 http://localhost:15672/#/ 默认用户名密码 guest 2.1.3 安装后的基本示意图 2.1.4 几个端口区别说明 5672：通信默认端口号 15672：管理控制台默认端口号 25672：集群通信端口号 2.1.5 Rabbitmq 安装后的基本命令 以下列举一些在终端常用的操作命令： whereis rabbitmq：查看 rabbitmq 安装位置 rabbitmqctl start_app：启动应用 whereis erlang：查看 erlang 安装位置 rabbitmqctl start_app：启动应用 rabbitmqctl stop_app：关闭应用 rabbitmqctl status：节点状态 rabbitmqctl add_user username password：添加用户 rabbitmqctl list_users：列出所有用户 rabbitmqctl delete_user username：删除用户 rabbitmqctl add_vhost vhostpath：创建虚拟主机 rabbitmqctl list_vhosts：列出所有虚拟主机 rabbitmqctl list_queues：查看所有队列 rabbitmqctl -p vhostpath purge_queue blue：清除队列里消息 2.2 Node.js 实现一个简单的 HelloWorld 消息队列 画一张基本的图，HelloWorld 消息队列的图片，把下面几个概念都画进去。 看这段代码前先说几个概念: 生产者 ：生产消息的 消费者 ：接收消息的 通道 channel：建立连接后，会获取一个 channel 通道 exchange ：交换机，消息需要先发送到 exchange 交换机，也可以说是第一步存储消息的地方(交换机会有很多类型，后面会详细说)。 消息队列 : 到达消费者前一刻存储消息的地方，exchange 交换机会把消息传递到此 ack 回执：收到消息后确认消息已经消费的应答 2.2.1 amqplib 模块 推荐一个 npm 模块 amqplib。 Github: https://github.com/squaremo/amqp.node $ npm install amqplib 2.2.2 生产者代码 product.js const amqp = require('amqplib'); async function product(params) { // 1.创建链接对象 const connect = await amqp.connect('amqp://localhost:5672'); // 1. 创建链接对象 const connection = await amqp.connect('amqp://localhost:5672'); // 2. 获取通道 const channel = await connection.createChannel(); // 3. 声明参数 const routingKey = 'helloKoalaQueue'; const msg = 'hello koala'; for (let i = 0; i &lt; 10000; i++) { // 4. 发送消息 await channel.publish('', routingKey, Buffer.from(`${msg} 第${i}条消息`)); } // 5. 关闭通道 await channel.close(); // 6. 关闭连接 await connect.close(); } product(); 2.2.3 生产者代码解释与运行结果 代码注释中已经把基本的流程讲解了，我想很多小伙伴也会有疑问，说明下: 疑问1：前面提到过交换机这个名词，生产者发消息的时候必须要指定一个 exchange，若不指定 exchange（为空）会默认指向 AMQP default 交换机，AMQP default 路由规则是根据 routingKey 和 mq 上有没有相同名字的队列进行匹配路由。上面这段代码就是默认指定的交换机。不同类型交换机详细讲解请往下看。 疑问2：生产者发送消息后，消息是发送到交换机 exchange，但是这时候会创建队列吗？ 答案：代码中我们声明的是路由是 routingKey，但是它并没有创建 helloKoalaQueue 消息队列，消息只会发送到交 exchange 交换机。 运行代码后看队列截图可以证明这一点： 说明1：生产者发送消息后，注意关闭通道和连接，只要消息发送成功后，连接就可以关闭了，消费者用任何语言去获取消息都可以，这也证明了消息队列优秀解耦的特性。 说明2：可以多次执行 node product.js 生产者代码，消息会堆积到交换机 exchange 中，并不会覆盖，如果已执行过消费者并且确认了对应的消息队列，消息会从 exchange 交换机发送到消息队列，并存入到消息队列，等待消费者消费。 2.2.4 消费者代码 consumer.js // 构建消费者 const amqp = require('amqplib'); async function consumer() { // 1. 创建链接对象 const connection = await amqp.connect('amqp://localhost:5672'); // 2. 获取通道 const channel = await connection.createChannel(); // 3. 声明参数 const queueName = 'helloKoalaQueue'; // 4. 声明队列，交换机默认为 AMQP default await channel.assertQueue(queueName); // 5. 消费 await channel.consume(queueName, msg =&gt; { console.log('Consumer：', msg.content.toString()); channel.ack(msg); }); } consumer(); 2.2.5 生产者代码解释与运行结果 运行后的执行结果： 说明1：这时候我改变代码中的队列名称为 helloKoalaQueueHaHa，这时候去看 Rabbitmq 可视化界面中，队列模块，创建了这个队列： 看到这里再次证明了消息队列优秀的解耦特性，消费者和生产者模型之间没有任何联系，再次创建这个 helloKoalaQueueHaHa 路由名称的生产者，消费者也会正常消费，并且会打印消息，大家可以实际操作试一下。 2.3 如何释放掉消息队列 2.3.1 可视化界面中直接删除掉消息队列 访问 http://{rabbitmq安装IP}:15672，登录。 点击 queues，这里可以看到你创建的所有的 Queue， 选中某一个 Queue，然后会进入一个列表界面，下方有个 Delete 按钮，确认 Queue 删除队列/Purge Message 清除消息即可。 弊端： 这样只能一个队列一个队列的删除，如果队列中的消息过多就会特别慢。 3 消息队列交换机讲解 先记住一句话：生产者发消息的时候必须指定一个 exchange，否则消息无法直接到达消息队列，Exchange 将消息路由到一个或多个 Queue 中（或者丢弃） 然后开始本章节交换机的讲解 若不指定 exchange（为空）会默认指向 AMQP default 交换机，AMQP default 路由规则是根据 routingKey 和 mq 上有没有相同名字的队列进行匹配路由。 3.1 交换机的种类 常用的四种类型： fanout direct topic headers 不管是哪一种类型的交换机，都有一个绑定 binding 的操作，只不过根据不同的交换机类型有不同的路由绑定策略。不同类型做的下图红色框框中的事。 3.1.1 fanout（广播） fanout 类型的 Exchange 路由规则非常简单，它会把所有发送到该 Exchange 的消息路由到所有与它绑定的 Queue 中，不需要设置路由键。 上图中，生产者（Producter）发送到Exchange(X) 的所有消息都会路由到图中的两个 Queue，并最终被两个消费者（consumer1 与 consumer2）消费。 说明：所有消息都会路由到两个 Queue 中，是两个消费者都可以收到全部的完全相同的消息吗？答案是的，两个消费者收到的队列消息正常应该是完全相同的。这种类型常用于广播类型的需求，或者也可以消费者 1 记录日志 ，消费者 2 打印日志。 对应代码实现： 生产者： const amqp = require('amqplib'); async function producer() { // 创建链接对象 const connection = await amqp.connect('amqp://localhost:5672'); // 获取通道 const channel = await connection.createChannel(); // 声明参数 const exchangeName = 'fanout_koala_exchange'; const routingKey = ''; const msg = 'hello koala'; // 交换机 await channel.assertExchange(exchangeName, 'fanout', { durable: true, }); // 发送消息 await channel.publish(exchangeName, routingKey, Buffer.from(msg)); // 关闭链接 await channel.close(); await connection.close(); } producer(); 消费者： const amqp = require('amqplib'); async function consumer() { // 创建链接对象 const connection = await amqp.connect('amqp://localhost:5672'); // 获取通道 const channel = await connection.createChannel(); // 声明参数 const exchangeName = 'fanout_koala_exchange'; const queueName = 'fanout_kaola_queue'; const routingKey = ''; // 声明一个交换机 await channel.assertExchange(exchangeName, 'fanout', { durable: true }); // 声明一个队列 await channel.assertQueue(queueName); // 绑定关系（队列、交换机、路由键） await channel.bindQueue(queueName, exchangeName, routingKey); // 消费 await channel.consume(queueName, msg =&gt; { console.log('Consumer：', msg.content.toString()); channel.ack(msg); }); console.log('消费端启动成功！'); } consumer(); 3.1.2 direct direct 把消息路由到那些 binding key 与 routing key 完全匹配的 Queue 中。 以上图的配置为例，我们以 routingKey=&quot;error&quot; 发送消息到 Exchange，则消息会路由到 amq1 和 amq2；如果我们以 routingKey=&quot;info&quot; 或 routingKey= &quot;warning&quot; 来发送消息，则消息只会路由到 Queue2。如果我们以其他 routingKey 发送消息，则消息不会路由到这两个 Queue 中。 3.1.3 topic 生产者指定 RoutingKey 消息根据消费端指定的队列通过模糊匹配的方式进行相应转发，两种通配符模式： #：可匹配一个或多个关键字 *：只能匹配一个关键字 3.1.4 headers header exchange（头交换机）和主题交换机有点相似，但是不同于主题交换机的路由是基于路由键，头交换机的路由值基于消息的 header 数据。主题交换机路由键只有是字符串，而头交换机可以是整型和哈希值 header Exchange 类型用的比较少，可以自行 google 了解。 4 消息队列的思考与深入探索 4.1 消息队列实现 RPC RPC 远程调用服务端的方法，使用 MQ 可以实现 RPC 的异步调用，基于 Direct 交换机实现 客户端即是生产者又是消费者，向 RPC 请求队列发送 RPC 调用消息，同时监听 RPC 响应队列 服务端监听 RPC 请求队列，收到消息后执行服务端的方法 服务端将方法执行后的结果发送到 RPC 响应队列 4.2 是否有消息持久化的必要？ 消息队列是存在内存中的，如果出现问题挂掉,消息队列中的消息会丢失。所以对于一些需求非常有持久化的必要！RabbitMQ 可以开启持久化。不同开发语言都可以设置持久化参数。 这里以 Node.js 为例子，其他语言可以自行搜索 await channel.assertExchange(exchangeName, 'direct', { durable: true }); // 注意其中的{ durable: true }，这事对交换机持久化，还有其他的几种持久化方式 同时推荐一篇不错的写持久化的文章：RabbitMQ 持久化。 4.3 消费者完成后是否有消息应答的必要？ 消息应答简单的解释就是消费者完成了消费后，通知一下消息队列。 我觉得这个配置是有必要打开的，消费者完成消息队列中的任务，消费者可能中途失败或者挂掉，一旦 RabbitMQ 发送一个消息给消费者然后便迅速将该消息从消息队列内存中移除,这种情况下，消费者对应工作进程失败或者挂掉后，那该进程正在处理的消息也将丢失。而且，也将丢失所有发送给该进程的未被处理的消息。 为了确保消息永不丢失，RabbitMQ 支持消息应答机制。当消息被接受，处理之后一条应答便会从消费者回传至发送方，然后 RabbitMQ 将其删除。 如果某个消费者挂掉（信道、链接关闭或者 TCP 链接丢失）且没有发送 ack 应答，RabbitMQ 会认为该消息没有被处理完全然后会将其重新放置到队列中。通过这种方式你就可以确保消息永不丢失，甚至某个工作进程偶然挂掉的情况。 默认情况下消息应答是关闭的。是时候使用 false（auto-ack 配置项）参数将其开启了 这里以 Node.js 为例子，其他语言可以自行搜索 // 消费者消费时候的代码 await channel.consume(queueName, msg =&gt; { console.log('koala：', msg.content.toString()); //... 这里可以放业务逻辑处理的代码，消费者完成后发送回执应答 channel.ack(msg);// 消息应答 }, { noAck: false }); 4.4 如何实现公平调度？ 可以将 prefetch count 项的值配置为 1，这将会指示 RabbitMQ 在同一时间不要发送超过一条消息给每个消费者。换句话说，直到消息被处理和应答之前都不会发送给该消费者任何消息。取而代之的是，它将会发送消息至下一个比较闲的消费者或工作进程。 这里以 Node.js 为例子，amqplib 库对于限流实现提供的接口方法 prefetch。 prefetch 参数说明： count：每次推送给消费端 N 条消息数目，如果这 N 条消息没有被 ack，生产端将不会再次推送直到这 N 条消息被消费。 global：在哪个级别上做限制，ture 为 channel 上做限制，false 为消费端上做限制，默认为 false。 // 创建消费者的时候 限流参数设置 await channel.prefetch(1, false); 4.5 如何实现一个交换机给多个消费者依次发送消息，选择那种交换机？ 如果一个生产者，两个消费者，发放消息，我想要的队列先给消费者 1 发，发完消费者 1 发消费者 2，这样有顺序的交互发送，应该现在哪一种交换机呢？注意是交互，看完之后想一下？还有消费者完成后有没有手动回调消息队列完成的必要？消息持久化有必要没，持久化有什么好处？ (看完消息队列的消息传递，你会有疑问管道中的消息(生产者)是怎么被消费者消费的 放入队列，然后从队列被取出) 5 消息队列应用场景 5.1 双十一商品秒杀/抢票功能实现 我们在双 11 的时候，当我们凌晨大量的秒杀和抢购商品，然后去结算的时候，就会发现，界面会提醒我们，让我们稍等，以及一些友好的图片文字提醒。而不是像前几年的时代，动不动就页面卡死，报错等来呈现给用户。 用一张图来解释消息队列在秒杀抢票等场景的使用：（说明：往下看之前，如果你做过电商类秒杀，可以想想你是怎么实现的，我们可以一起讨论哦。这里只是想说下消息队列的作用，并不是最终优化的结果，比如用 redis 控制总缓存等）。 这里在生成订单时候，不需要直接操作数据库 IO ，预扣库存。先扣除了库存，保证不超卖，然后异步生成用户订单，这里用到一次即时消费队列，这样响应给用户的速度就会快很多；而且还要保证不少卖，用户拿到了订单，不支付怎么办？我们都知道现在订单都有有效期，再使用一个消息队列，用于判断订单支付超时，比如说用户五分钟内不支付，订单就失效了，订单一旦失效，就会加入新的库存。这也是现在很多网上零售企业保证商品不少卖采用的方案。订单量比较少的情况下，生成订单非常快，用户几乎不用排队。 5.2 积分兑换(积分可用于多平台) 积分兑换模块，有一个公司多个部门都要用到这个模块，这时候就可以通过消息队列解耦这个特性来实现。 各部门系统做各部门的事，但是他们都可以用这个积分系统进行商品的兑换等。其他模块与积分模块完全解耦。 5.3 发送邮件，用户大数据分析等 同步变异步功能实现 这个功能要说的比较多，从一个平台的用户注册开始。 用户注册 用户注册选择几个兴趣标签，这时候需要根据用户的属性，用户分析，计算出推荐内容 注册后可能需要发送邮件给用户 发送给用户一个包含操作指南的系统通知 等等 正常情况注册，不出现高并发： 对于用户来说，他就是想注册用一下这个软件，只要服务端将他的账户信息存到数据库中他便可以登录上去做他想做的事情了。用户并不 care 这些事，服务端就可以把其他的操作放入对应的消息队列中然后马上返回用户结果，由消息队列异步的进行这些操作。 假如有大量的用户注册，发生了高并发： 邮件接口承受不住，或是分析信息时的大量计算使 CPU 满载，这将会出现虽然用户数据记录很快的添加到数据库中了，但是却卡在发邮件或分析信息时的情况，导致请求的响应时间大幅增长，甚至出现超时，这就有点不划算了。面对这种情况一般也是将这些操作放入消息队列（生产者消费者模型），消息队列慢慢的进行处理，同时可以很快的完成注册请求，不会影响用户使用其他功能。 5.4 基于 RabbitMQ 的 Node.js 与 Python 或其他语言实现通信 这里也是利用了 RabbitMQ 的解耦特性，不仅仅可以与 Phython，还可以与其他很多语言通信，就不具体说了。 参考文章 消息队列 (2) java实现简单的RabbtMQ 消息队列的使用场景是怎样的？ 我为什么要选择RabbitMQ ，RabbitMQ简介，各种MQ选型对比 消息中间件 RabbitMQ 入门篇 消息队列在电商中的应用的几个问题，比如在下单，秒杀场景中使用的疑惑 如何选择消息队列？ ","link":"https://faded.auspicious.space/post/message-queue-in-deep/"},{"title":"消息队列技术点梳理","content":" 消息队列技术点梳理（思维导图版） 消息队列作为服务/应用之间的通信中间件，可以起到业务耦合、广播消息、保证最终一致性以及错峰流控（克服短板瓶颈）等作用。本文不打算详细深入讲解消息队列，而是体系化的梳理消息队列可能涉及的技术点，起到提纲挈领的作用，构造一个宏观的概念，使用思维导图梳理。 再介绍之前，先简短比较下 RPC 和消息队列。RPC 大多属于请求-应答模式，也包括越来越多响应式范式，对于需要点对点交互、强事务保证和延迟敏感的服务/应用之间的通信，RPC 是优于消息队列的。那么消息队列（下文也简称 MQ，即 Message Queue）可以看做是一种异步 RPC，把一次 RPC 变为两次，进行内容转存，再在合适的时机投递出去。消息队列中间件往往是一个分布式系统，内部组件间的通信仍然会用到 RPC。 目前开源界用的比较多的选型包括，ActiveMQ、RabbitMQ、Kafka、阿里巴巴的 Notify、MetaQ、RocketMQ。下文的技术点梳理也是学习借鉴了这些开源组件，然后萃取出一些通用技术点。 关于消息队列的体系化认知，见下方的思维导图。 整体架构 一般分为 producer，broker，consumer 三者。 RPC 通信 详细参考《体系化认识RPC》。 高性能保证 主要考虑 MQ 的延迟和吞吐。 高性能投递方面，分为 producer 和 broker 考虑。producer 可以同步变异步、单条变批量保证发送端高性能，批量发送的触发条件可以分为 buffer 满或者时间窗口到了。broker 可以进行多 topic 划分，再多分区/queue 来进行分治（Divide and Conquer）策略，加大并行度，分散投递压力。另外 broker 对于需要持久化的消息，可以使用顺序 IO，page cache，异步刷盘等技术提高性能，但是异步刷盘在掉电的情况下，可能会丢失数据，可以结合下面的高可用方案，在数据严格不丢和高性能吞吐之间做折中。 高性能消费，即 consumer 和 broker 通信，进行推、拉消息。使用 consumer group 水平扩展消费能力，需要按照业务场景使用分区有序或者无序消费。零拷贝技术节省 broker 端用户态到内核态的数据拷贝，直接从 page cache 发送到网络，从而最大化发送性能。consumer 批量 pull，broker 批量 push。broker 端还可以做消息过滤，可通过 tag 或者插件实现。 高可用保证 主要针对 broker 而言。 集群高可用，producer 通过 broker 投递消息，所以必然有且仅有一个 broker 主负责“写”，选主策略分为自动选主和非主动选择，自动选主使用分布一致性组件完成，例如 Kafka 使用 zookeeper，非自动选主，例如 RocketMQ 依赖多个无状态的 name server。 数据高可用，针对 broker 持久化积压消息场景。可借助分布式存储完成，但是往往性能上是个短板，所以大多数主流产品都进行本地 IO 顺序写，进行主从备份，多副本拷贝保证可用性，例如 RocketMQ 分为同步双写和异步复制，前者像 HDFS 一样，写完多个副本再返回 producer 成功，有一定性能损失，但不大，后者最大化性能，但是当主挂的时候，数据有丢失风险。 同样，MQ 集群也需要考虑跨机房高可用（非“异地多活”），broker 的写高可用，要考虑最小化 MTTR，同时不阻塞 consumer 消费。 扩展性保证 采用分治（Divide and Conquer）策略，加大投递和消费的并行度，多个 topic、多个分区/queue、多个副本、多个 slave 或者镜像。 协议 producer、consumer 和 broker 通信的协议，包括 AMQP、STOMP、MQTT、HTTP、OpenWire（ActiveMQ）、XMPP、自定义等等。 AMQP 是比较全面和复杂的一个协议，包括协议本身以及模型（broker、exchange、routing key 等概念），目前 RabbitMQ 是 AMQP 消息队列最有名的开源实现，有非常多语言已经支持基于 AMQP 协议与消息队列通信，同时还可以通过插件支持 STOMP、MQTT 等协议接入。Kafka、RocketMQ 均使用自定义的协议。 消费关系 包括三种： 点对点，也就是 P2P，FIFO 的队列，可以看做单播。 Topic 模式，Pub/Sub 发布订阅。 fanout 广播模式。 消息堆积能力 持久化消息，如果存储在本地磁盘，可以使用同步刷盘和异步刷盘两种策略。磁盘不能无限堆积，会有清理策略，例如 Kafka、RocketMQ 都按照时间、数据量进行 retention。 非持久化，仅放在内存，消费者处理完可选择删除掉。 可靠投递 对于 producer，从 API 和 I/O 层面可使用同步、异步，对于吞吐层面可使用单条、批量。fire-and-forget 模式，类似 UDP，尽管发送即可。针对可能发生的错误，例如连接 broker 失败，RPC 超时、发布消息失败、发布后无响应，可选择忽略或者重发，所以往往重复投递的情况不可避免。 对于 broker，如果要保证数据 100% 不丢，是可能的，但是需要牺牲下性能和吞吐，使用同步多写、多副本策略+同步刷盘持久化消息，可以严格保证不丢。另外，broker 对于写入消息的 payload，也会做完整性校验，例如 CRC 等。 可靠消费 消费次数，包括 at most once、at least once、exactly once，其中前两个比较好做到，最后的exactly once 需要 streaming consumer 系统和 broker 端协作完成，例如 storm 的 trident 和 flink。 推拉模式，push or pull。推模式最小化投递延迟，但是没有考虑 consumer 的承载能力，拉一般是轮询接收 broker 的数据，按照 consumer 自己的能力消费。 消费记录点，一般每个消息都有一个 offset、ID 或者时间戳，consumer 可以按照这个 offset 来进行定点消费以及消息重放。 消息确认，consumer 消费完成 ACK 回调 broker 或者集群高可用中间件（zk）通知消费进度。 错误处理，对于消费失败的情况，可以回复 NACK，要求重发/requeue 消息，当错误超多一定阈值时候，放到死信队列中。 消息重复消费，这和消费次数有关系，consumer 在某些时候需要做到幂等性，保证重复消费不会引起业务异常。 消息类型 顺序消息，有序的话，分为分区有序或者全局有序，前者可以按照某个业务 ID 取模，在发送端发到不同的分区/queue 即可，后者往往需要单个队列才可以满足。无序消费则可最大化吞吐。 定时消息，事务消息，例如 RocketMQ 均支持。 消息查询 目前 RocketMQ 支持消息根据 msgId 查询。 生态融合 客户端语言的丰富性，与其他系统的集成度，例如Kafka和大数据技术栈融合很紧密，Spark、Storm、Flink、Kylin 都有对应的 connector。 管理工具 分布式系统的管理是提高生产效率的必备保障，一个好的系统，如果周边工具不完善，对于使用者会很不友好，推广也会有困难。 对于消息队列，可以从 topic 管理、broker 管理、集群管理、权限/配额管理、多租户、客户端工具、监控、报警、控制台 Console UI 来全方位进行治理。 ","link":"https://faded.auspicious.space/post/message-queue-technical-point-combing/"},{"title":"Nginx access_log 日志简介","content":" nginx access_log日志简介 nginx 日志主要有两条指令： log_format：用来设置日志格式； access_log：用来指定日志文件的存放路径、格式。 log_format 日志格式 语法 log_format name（格式名字） 格式样式（即想要得到什么样的日志内容） 示例： log_format main '$remote_addr - $remote_user [$time_local] &quot;$request&quot; ' '$status $body_bytes_s ent &quot;$http_referer&quot; ' '&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;' 具体参数格式 参数 说明 示例 $remote_addr 客户端地址 211.28.65.253 $remote_user 客户端用户名称 -- $time_local 访问时间和时区 18/Jul/2012:17:00:01 +0800 $time_iso8601 ISO8601 标准格式下的本地时间 $request 请求的 URI 和 HTTP 协议 &quot;GET /article-10000.html HTTP/1.1&quot; $http_host 请求地址，即浏览器中你输入的地址（IP 或域名） www.it300.com192.168.100.100 $status HTTP 请求状态 200 $upstream_status upstream 状态 200 $body_bytes_sent 发送给客户端文件内容大小 1547 $http_referer URL 跳转来源 https://www.baidu.com $http_user_agent 用户终端浏览器等信息 &quot;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; SV1; GTB7.0; .NET4.0C;)&quot; $ssl_protocol SSL 协议版本 TLSv1 $ssl_cipher 交换数据中的算法 RC4-SHA $upstream_addr 后台 upstream 的地址，即真正提供服务的主机地址 10.10.10.100:80 $request_time 整个请求的总时间 0.205 $upstream_response_time 请求过程中，upstream 响应时间 0.002 $connection_requests 当前连接发生请求数 $connection 所用连接序号 $msec 日志写入时间。单位为秒，精度是毫秒 $pipe 如果请求是通过 HTTP 流水线（pipelined）发送，pipe 值为“p”，否则为“.” x_forwarded_for 通常 Web 服务器放在反向代理的后面，这样就不能获取到客户的 IP 地址了，通过 $remote_addr 拿到的 IP 地址是反向代理服务器的 IP 地址。反向代理服务器在转发请求的 HTTP 头信息中，可以增加 x_forwarded_for 信息，用以记录原有客户端的 IP 地址和原来客户端的请求的服务器地址。 注：在 server 中设置 x_forwarded_for proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; access_log 用了 log_format 指令设置了日志格式之后，需要用 access_log 指令指定日志文件的存放路径； 语法 access_log path (存放路径) format (自定义日志名称) 示例： access_log logs/access.log main; 设置刷盘策略 access_log /data/logs/nginx-access.log buffer=32k flush=5s; buffer 满 32k 才刷盘；假如 buffer 不满 5s 强制刷盘。 注：一般 log_format 在全局设置，可以设置多个。access_log 可以在全局设置，但往往是定义在虚拟主机（server）中的 location 中。 例如： http { log_format main '$remote_addr - $remote_user [$time_local] &quot;$request&quot; ' '&quot;$status&quot; $body_bytes_sent &quot;$http_referer&quot; ' '&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot; ' '&quot;$gzip_ratio&quot; $request_time $bytes_sent $request_length'; log_format srcache_log '$remote_addr - $remote_user [$time_local] &quot;$request&quot; ' '&quot;$status&quot; $body_bytes_sent $request_time $bytes_sent $request_length ' '[$upstream_response_time] [$srcache_fetch_status] [$srcache_store_status] [$srcache_expire]'; open_log_file_cache max=1000 inactive=60s; server { server_name ~^(www\\.)?(.+)$; access_log logs/$2-access.log main; error_log logs/$2-error.log; location /srcache { access_log logs/access-srcache.log srcache_log; } } } 其他 error_log 配置错误日志，例如上例。 open_log_file_cache 对于每一条日志记录，都将是先打开文件，再写入日志，然后关闭。可以使用 open_log_file_cache 来设置日志文件缓存（默认是 off）。 语法： open_log_file_cache max=N [inactive=time] [min_uses=N] [valid=time]; 参数注释如下： max：设置缓存中的最大文件描述符数量，如果缓存被占满，采用 LRU 算法将描述符关闭。 inactive：设置存活时间，默认是 10s min_uses：设置在 inactive 时间段内，日志文件最少使用多少次后，该日志文件描述符记入缓存中，默认是 1 次 valid：设置检查频率，默认 60s open_log_file_cache max=1000 inactive=20s valid=1m min_uses=2; 日志分析： 通过对日志格式的定义，就可以使用常见的 Linux 命令行工具进行分析了： 查找访问频率最高的 URL 和次数： cat access.log | awk -F ‘^A’ ‘{print $10}’ | sort | uniq -c 查找当前日志文件 500 错误的访问： cat access.log | awk -F ‘^A’ ‘{if($5 == 500) print $0}’ 查找当前日志文件 500 错误的数量： cat access.log | awk -F ‘^A’ ‘{if($5 == 500) print $0}’ | wc -l 查找某一分钟内 500 错误访问的数量: cat access.log | awk -F ‘^A’ ‘{if($5 == 500) print $0}’ | grep ’09:00’ | wc-l 查找耗时超过 1s 的慢请求： tail -f access.log | awk -F ‘^A’ ‘{if($6&gt;1) print $0}’ 假如只想查看某些位： tail -f access.log | awk -F ‘^A’ ‘{if($6&gt;1) print $3″|”$4}’ 查找 502 错误最多的 URL： cat access.log | awk -F ‘^A’ ‘{if($5==502) print $11}’ | sort | uniq -c 查找 200 空白页 cat access.log | awk -F ‘^A’ ‘{if($5==200 &amp;&amp; $8 &lt; 100) print $3″|”$4″|”$11″|”$6}’ 切割日志 Nginx 的日志都是写在一个文件当中的，不会自动地进行切割，如果访问量很大的话，将导致日志文件容量非常大，不便于管理和造成 Nginx 日志写入效率低下等问题。所以，往往需要要对 access_log、error_log 日志进行切割。 切割日志一般利用 USR1 信号让 Nginx 产生新的日志。实例： #!/bin/bashlogdir=&quot;/data/logs/nginx&quot; pid=`cat $logdir/nginx.pid` DATE=`date -d &quot;1 hours ago&quot; +%Y%m%d%H` DATE_OLD=`date -d &quot;7 days ago&quot; +%Y%m%d` for i in `ls $logdir/*access.log`; do mv $i $i.$DATE done for i in `ls $logdir/*error.log`; do mv $i $i.$DATE done kill -s USR1 $pid rm -v $logdir&quot;/access.log.&quot;$DATE_OLD*rm -v $logdir&quot;/error.log.&quot;$DATE_OLD* 分析 将上面的脚本放到 crontab 中，每小时执行一次（0），这样每小时会把当前日志重命名成一个新文件；然后发送 USR1 这个信号让 Nginx 重新生成一个新的日志。（相当于备份日志）将前 7 天的日志删除； 说明 在没有执行 kill -USR1 $pid 之前，即便已经对文件执行了 mv 命令而改变了文件名称，Nginx 还是会向新命名的文件 *access.log.2016032623 照常写入日志数据的。原因在于 Linux系统中，内核是根据文件描述符来找文件的。 logrotates 使用系统自带的 logrotates，也可以实现 Nginx 的日志分割，查看其 bash 源码，发现也是发送 USR1 这个信号。 ","link":"https://faded.auspicious.space/post/an-introduction-to-nginx-access-log/"},{"title":"grep","content":" 浅显易懂的grep命令详解 简介 grep（global search regular expression(RE) and print out the line，全面搜索正则表达式并把行打印出来）是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。 Unix 的 grep 家族包括 grep、egrep 和 fgrep。egrep 和 fgrep 的命令只跟 grep 有很小不同。egrep 是 grep 的扩展，支持更多的 re 元字符，fgrep 就是 fixed grep 或 fast grep，它们把所有的字母都看作单词，也就是说，正则表达式中的元字符表示回其自身的字面意义，不再特殊。Linux 使用 GNU 版本的 grep。它功能更强，可以通过 -G、-E、-F 命令行选项来使用 egrep 和 fgrep 的功能。 grep 常用用法 $ grep [-acinv] [--color=auto] '搜寻字符串' filename 选项与参数： -a ：将 binary 文件以 text 文件的方式搜寻数据 -c ：计算找到 '搜寻字符串' 的次数 -i ：忽略大小写的不同，所以大小写视为相同 -n ：顺便输出行号 -v ：反向选择，亦即显示出没有 '搜寻字符串' 内容的那一行！ --color=auto ：可以将找到的关键词部分加上颜色的显示喔！ 将 /etc/passwd，有出现 root 的行取出来。 $ grep root /etc/passwd root:x:0:0:root:/root:/bin/bash operator:x:11:0:operator:/root:/sbin/nologin 或 $ cat /etc/passwd | grep root root:x:0:0:root:/root:/bin/bash operator:x:11:0:operator:/root:/sbin/nologin 将 /etc/passwd，有出现 root 的行取出来,同时显示这些行在 /etc/passwd 的行号。 $ grep -n root /etc/passwd 1:root:x:0:0:root:/root:/bin/bash30:operator:x:11:0:operator:/root:/sbin/nologin 在关键字的显示方面，grep 可以使用 --color=auto 来将关键字部分使用颜色显示。 这可是个很不错的功能啊！但是如果每次使用 grep 都得要自行加上 --color=auto 又显的很麻烦～ 此时那个好用的 alias 就得来处理一下啦！你可以在 ~/.bashrc 内加上这行：alias grep='grep --color=auto' 再以 source ~/.bashrc 来立即生效即可喔！ 这样每次运行 grep 他都会自动帮你加上颜色显示啦。 将 /etc/passwd，将没有出现 root 的行取出来。 $ grep -v root /etc/passwdroot:x:0:0:root:/root:/bin/bash operator:x:11:0:operator:/root:/sbin/nologin 将 /etc/passwd，将没有出现 root 和 nologin 的行取出来。 $ grep -v root /etc/passwd | grep -v nologin root:x:0:0:root:/root:/bin/bash operator:x:11:0:operator:/root:/sbin/nologin 用 dmesg 列出核心信息，再以 grep 找出内含 eth 那行,要将捉到的关键字显色，且加上行号来表示： $ dmesg | grep -n --color=auto 'eth' 247:eth0: RealTek RTL8139 at 0xee846000, 00:90:cc:a6:34:84, IRQ 10 248:eth0: Identified 8139 chip type 'RTL-8139C' 294:eth0: link up, 100Mbps, full-duplex, lpa 0xC5E1305:eth0: no IPv6 routers present # 你会发现除了 eth 会有特殊颜色来表示之外，最前面还有行号喔！ 在关键字的显示方面，grep 可以使用 --color=auto 来将关键字部分使用颜色显示。 这可是个很不错的功能啊！但是如果每次使用 grep 都得要自行加上 --color=auto 又显的很麻烦～ 此时那个好用的 alias 就得来处理一下啦！你可以在 ~/.bashrc 内加上这行：alias grep='grep --color=auto' 再以 source ~/.bashrc 来立即生效即可喔！ 这样每次运行 grep 他都会自动帮你加上颜色显示啦。 用 dmesg 列出核心信息，再以 grep 找出内含 eth 那行,在关键字所在行的前两行与后三行也一起捉出来显示。 $ dmesg | grep -n -A3 -B2 --color=auto 'eth' 245-PCI: setting IRQ 10 as level-triggered 246-ACPI: PCI Interrupt 0000:00:0e.0[A] -&gt; Link [LNKB] ... 247:eth0: RealTek RTL8139 at 0xee846000, 00:90:cc:a6:34:84, IRQ 10 248:eth0: Identified 8139 chip type 'RTL-8139C' 249-input: PC Speaker as /class/input/input2 250-ACPI: PCI Interrupt 0000:00:01.4[B] -&gt; Link [LNKB] ... 251-hdb: ATAPI 48X DVD-ROM DVD-R-RAM CD-R/RW drive, 2048kB Cache, UDMA(66) # 如上所示，你会发现关键字 247 所在的前两行及 248 后三行也都被显示出来！ # 这样可以让你将关键字前后数据捉出来进行分析啦！ 根据文件内容递归查找目录： $ grep 'energywise' * #在当前目录搜索带'energywise'行的文件 $ grep -r 'energywise' * #在当前目录及其子目录下搜索'energywise'行的文件 $ grep -l -r 'energywise' * #在当前目录及其子目录下搜索'energywise'行的文件，但是不显示匹配的行，只显示匹配的文件 这几个命令很实用，是查找文件的利器。 grep 与正则表达式 字符类 字符类的搜索：如果我想要搜寻 test 或 taste 这两个单词时，可以发现到，其实她们有共通的 't?st' 存在～这个时候，我可以这样来搜寻： $ grep -n 't[ae]st' regular_express.txt 8:I can't finish the test.9:Oh! The soup taste good. 其实 [] 里面不论有几个字节，他都谨代表某一个字节， 所以，上面的例子说明了，我需要的字串是『tast』或『test』两个字串而已！ 字符类的反向选择 [^]：如果想要搜索到有 oo 的行，但不想要 oo 前面有 g，如下 $ grep -n '[^g]oo' regular_express.txt2:apple is my favorite food.3:Football game is not use feet only.18:google is the best tools for search keyword.19:goooooogle yes! 第 2,3 行没有疑问，因为 foo 与 Foo 均可被接受！ 但是第 18 行明明有 google 的 goo 啊～别忘记了，因为该行后面出现了 tool 的 too 啊！所以该行也被列出来～ 也就是说， 18 行里面虽然出现了我们所不要的项目 (goo) 但是由於有需要的项目 (too) ， 因此，是符合字串搜寻的喔！ 至於第 19 行，同样的，因为 goooooogle 里面的 oo 前面可能是 o ，例如： go(ooo)oogle ，所以，这一行也是符合需求的！ 字符类的连续：再来，假设我 oo 前面不想要有小写字节，所以，我可以这样写 [^abcd….z]oo， 但是这样似乎不怎么方便，由於小写字节的 ASCII 上编码的顺序是连续的， 因此，我们可以将之简化为底下这样： $ grep -n '[^a-z]oo' regular_express.txt 3:Football game is not use feet only. 也就是说，当我们在一组集合字节中，如果该字节组是连续的，例如大写英文/小写英文/数字等等， 就可以使用 [a-z]，[A-Z]，[0-9] 等方式来书写，那么如果我们的要求字串是数字与英文呢？ 呵呵！就将他全部写在一起，变成：[a-zA-Z0-9]。 我们要取得有数字的那一行，就这样： $ grep -n '[0-9]' regular_express.txt 5:However, this dress is about $ 3183 dollars.15:You are the best is mean you are the no. 1. 行首与行尾字节 ^ $ 行首字符：如果我想要让 the 只在行首列出呢？ 这个时候就得要使用定位字节了！我们可以这样做： $ grep -n '^the' regular_express.txt 12:the symbol '*' is represented as start. 此时，就只剩下第 12 行，因为只有第 12 行的行首是 the 开头啊～此外， 如果我想要开头是小写字节的那一行就列出呢？可以这样： $ grep -n '^[a-z]' regular_express.txt 2:apple is my favorite food. 4:this dress doesn't fit me. 10:motorcycle is cheap than car. 12:the symbol '*' is represented as start. 18:google is the best tools for search keyword. 19:goooooogle yes! 20:go! go! Let's go. 如果我不想要开头是英文字母，则可以是这样： $ grep -n '^[^a-zA-Z]' regular_express.txt 1:&quot;Open Source&quot; is a good mechanism to develop programs. 21:# I am VBird ^ 符号，在字符类符号(括号[])之内与之外是不同的！ 在 [] 内代表『反向选择』，在 [] 之外则代表定位在行首的意义！ 那如果我想要找出来，行尾结束为小数点 (.) 的那一行： $ grep -n '\\.$' regular_express.txt 1:&quot;Open Source&quot; is a good mechanism to develop programs. 2:apple is my favorite food. 3:Football game is not use feet only. 4:this dress doesn't fit me. 10:motorcycle is cheap than car.11:This window is clear. 12:the symbol '*' is represented as start. 15:You are the best is mean you are the no. 16:The world &lt;Happy&gt; is the same with &quot;glad&quot;. 17:I like dog. 18:google is the best tools for search keyword. 20:go! go! Let's go. 特别注意到，因为小数点具有其他意义（底下会介绍），所以必须要使用转义字符 () 来加以解除其特殊意义！ 找出空白行： $ grep -n '^$' regular_express.txt22: 因为只有行首跟行尾 (^$)，所以，这样就可以找出空白行啦！ 任意一个字节 . 与重复字节 * 这两个符号在正则表达式的意义如下： . (小数点)：代表『一定有一个任意字节』的意思；* (星号)：代表『重复前一个字符， 0 到无穷多次』的意思，为组合形态。 假设我需要找出 g??d 的字串，亦即共有四个字节， 起头是 g 而结束是 d ，我可以这样做： $ grep -n 'g..d' regular_express.txt 1:&quot;Open Source&quot; is a good mechanism to develop programs. 9:Oh! The soup taste good. 16:The world &lt;Happy&gt; is the same with &quot;glad&quot;. 因为强调 g 与 d 之间一定要存在两个字节，因此，第 13 行的 god 与第 14 行的 gd 就不会被列出来啦！ 如果我想要列出有 oo, ooo, oooo 等等的数据， 也就是说，至少要有两个(含) o 以上，该如何是好？ 因为 * 代表的是『重复 0 个或多个前面的 RE 字符』的意义， 因此，『o』代表的是：『拥有空字节或一个 o 以上的字节』，因此，grep -n 'o' regular_express.txt 将会把所有的数据都列印出来终端上！ 当我们需要『至少两个 o 以上的字串』时，就需要 ooo*，亦即是： $ grep -n 'ooo*' regular_express.txt 1:&quot;Open Source&quot; is a good mechanism to develop programs. 2:apple is my favorite food. 3:Football game is not use feet only. 9:Oh! The soup taste good. 18:google is the best tools for search keyword. 19:goooooogle yes! 如果我想要字串开头与结尾都是 g，但是两个 g 之间仅能存在至少一个 o ，亦即是 gog, goog, gooog…. 等等，那该如何？ $ grep -n 'goo*g' regular_express.txt 18:google is the best tools for search keyword. 19:goooooogle yes! 如果我想要找出 g 开头与 g 结尾的行，当中的字符可有可无 $ grep -n 'g.*g' regular_express.txt 1:&quot;Open Source&quot; is a good mechanism to develop programs. 14:The gd software is a library for drafting programs. 18:google is the best tools for search keyword. 19:goooooogle yes! 20:go! go! Let's go. 因为是代表 g 开头与 g 结尾，中间任意字节均可接受，所以，第 1, 14, 20 行是可接受的喔！ 这个 .* 的 RE 表示任意字符是很常见的. 如果我想要找出『任意数字』的行？因为仅有数字，所以就成为： $ grep -n '[0-9][0-9]*' regular_express.txt 5:However, this dress is about $ 3183 dollars. 15:You are the best is mean you are the no. 1. 限定连续 RE 字符范围 {} 我们可以利用 . 与 RE 字符及 * 来配置 0 个到无限多个重复字节， 那如果我想要限制一个范围区间内的重复字节数呢？ 举例来说，我想要找出两个到五个 o 的连续字串，该如何作？这时候就得要使用到限定范围的字符 {} 了。 但因为 { 与 } 的符号在 shell 是有特殊意义的，因此， 我们必须要使用字符 \\ 来让他失去特殊意义才行。 至於 {} 的语法是这样的，假设我要找到两个 o 的字串，可以是： $ grep -n 'o\\{2\\}' regular_express.txt 1:&quot;Open Source&quot; is a good mechanism to develop programs. 2:apple is my favorite food. 3:Football game is not use feet only. 9:Oh! The soup taste good.18:google is the best tools for search ke 19:goooooogle yes! 假设我们要找出 g 后面接 2 到 5 个 o ，然后再接一个 g 的字串，他会是这样： $ grep -n 'go\\{2,5\\}g' regular_express.txt 18:google is the best tools for search keyword. 如果我想要的是 2 个 o 以上的 goooo….g 呢？除了可以是 gooo*g，也可以是： $ grep -n 'go\\{2,\\}g' regular_express.txt18:google is the best tools for search keyword. 19:goooooogle yes! 扩展 grep(grep -E 或者 egrep) 使用扩展 grep 的主要好处是增加了额外的正则表达式元字符集。 打印所有包含 NW 或 EA 的行。如果不是使用 egrep，而是 grep，将不会有结果查出。 $ egrep 'NW|EA' testfile northwest NW Charles Main 3.0 .98 3 34 eastern EA TB Savage 4.4 .84 5 20 对于标准 grep，如果在扩展元字符前面加 \\，grep 会自动启用扩展选项 -E。 $ grep 'NW\\|EA' testfile northwest NW Charles Main 3.0 .98 3 34eastern EA TB Savage 4.4 .84 5 20 搜索所有包含一个或多个 3 的行。 $ egrep '3+' testfile $ grep -E '3+' testfile $ grep '3\\+' testfile 这3条命令将会 northwest NW Charles Main 3.0 .98 3 34western WE Sharon Gray 5.3 .97 5 23northeast NE AM Main Jr. 5.1 .94 3 13central CT Ann Stephens 5.7 .94 5 13 搜索所有包含 0 个或 1 个小数点字符的行。 $ egrep '2\\.?[0-9]' testfile $ grep -E '2\\.?[0-9]' testfile $ grep '2\\.\\?[0-9]' testfile 首先含有 2 字符，其后紧跟着 0 个或 1 个点，后面再是 0 和 9 之间的数字。 western WE Sharon Gray 5.3 .97 5 23southwest SW Lewis Dalsass 2.7 .8 2 18eastern EA TB Savage 4.4 .84 5 20 搜索一个或者多个连续的 no 的行。 $ egrep '(no)+' testfile $ grep -E '(no)+' testfile $ grep '\\(no\\)\\+' testfile 3个命令返回相同结果， northwest NW Charles Main 3.0 .98 3 34northeast NE AM Main Jr. 5.1 .94 3 13north NO Margot Weber 4.5 .89 5 9 不使用正则表达式 fgrep 查询速度比 grep 命令快，但是不够灵活：它只能找固定的文本，而不是规则表达式。 如果你想在一个文件或者输出中找到包含星号字符的行 $ fgrep '*' /etc/profile for i in /etc/profile.d/*.sh ; do 或 $ grep -F '*' /etc/profile for i in /etc/profile.d/*.sh ; do ","link":"https://faded.auspicious.space/post/grep/"},{"title":"LESS 学习笔记","content":" less学习笔记1-语言特性（概览） less学习笔记2-语言特性（变量） less学习笔记3-语言特性（extend） less学习笔记4-语言特性（mixins） less学习笔记5-语言特性（import） less学习笔记6-语言特性（Guards） less学习笔记7-语言特性（Loop&amp;Merge&amp;Parents Selectors） 1 概况 1.1 变量 变量名称定义需要 @ 前缀。 @blue: #5b83ad @light-blue: @blue + #111; .test{ color: @light-blue; } 变量实际上是“常量”，因为它们只被定义声明一次。 1.2 mixin（混合/混入） mixin 是将一个定义的样式规则包含进另外一个规则中。 .background{ background-image: url('test.png'); background-repeat: no-repeat; background-size: 80% 80%; } 如果我们需要在其他的样式规则中使用到上述的样式的话，可以直接在规则中引入。 #banner .img{ width: 50px; height: 50px; .background; } 1.3 嵌套规则 #header{ color: red; .nav{ display: inline-block; } .logo{ width: 30px; height: 30px; } } 编译为： #header{ color: red; } #header .nav{ display: inline-block; } #header .logo{ width: 30px; height: 30px; } 在嵌套规则中可以使用伪选择器（&amp; 代表当前选择器的父选择器）。 .list{ color: blue; &amp;:after{ content: &quot; &quot;; display: block; height: 10px; width: 10px; background-color: #000; } } 1.4 嵌套准则和冒泡 像 media 和 keyframe 的指令可以嵌套在和选择器一样的地方。冒泡就是将指令放置在顶部，并且保持同一个规则集中的其他元素的相对顺序不变。 条件指令，像 @media，@supports，@document 可以将选择器复制进它们的规则里面。 .screen-color{ @meida screen{ color: green; @media (min-width: 768px){ color: red; } } @media tv{ color: black; } } 编译为： @media screen{ color: green; } @media screen and (min-width: 768px){ color: red; } @media tv{ color: black; } 其余的非条件指令，如 font-face 或者 keyframes 也会冒泡，它们的身体不变。 #a{ color: blue; @font-face{ src: some-url; } padding: 2px; } 编译为： #a{ color: blue; } @font-face{ src: some-url; } #a{ padding: 2px; } 1.5 运算: 算术运算操作符 +，-，*，/ 可以对任意数字，颜色或者变量进行运算。 如果可能，运算操作符会在加，减或者比较之前代入单位并且进行转换，结果的单位会等同于运算式中最左边的第一个显式单位。 如果不能进行转换或者无意义时，会忽略单位。可能的转换：px-&gt;cm，rad-&gt;%。 @cvs: 5cm+10mm;//6cm @cvs: 2-3cm-5mm;//1.5cm 待定 @cvs: 2+5px-3cm;//4px @base: 5%; @filler: @base 2;//10% @other: @base + @filler;//15% @base-color: #000; .test{ color: #fff / 4; background-color: @base-color + #654; width: 50% / 2 +@filler } 乘法和除法不会对数字进行转换，在大部分的情况下都是无意义的，比如一个长度乘上另外一个长度得到面积，而 CSS 并不支持面积。LESS 会计算数字本身，然后分配一个显式的单位给计算结果。 @base: 2cm 3mm;// 6cm 色值会分割成 red，green，blue 和 α 维度，计算会分别作用在 rgb 三个量纲上。如果用户让两个色值相加，green 量纲的计算结果会等于两个色值的 green 量纲的值的相加结果，red 量纲和 blue 量纲也是同样计算方式。如果用户将一个色值和一个数字进行相乘，每个量纲都会乘上该数字。 注：算术运算在 α 上是没有定义的，因为色值的数学计算上是没有约定的标准含义的，不要依赖于当前方式，因为有可能在下一个版本就发生改变了。 色值上的计算总是得到有效的色值，如果某一个颜色量纲的计算结果超出了 FF 或者小于 00，最后得到的结果会等于 FF 或者 00，如果 α 的计算结果大于 1.0 或者小于 0.0，最后结果会等于 1.0 或者0.0。 @color: #224488 / 2； //#112244 @background-color: #112244 + #111; //#223355 LESS 可以理解色值和单位的之间区别： @var: 1px + 5; 这个变量的最终输出为 6 px。 1.7 转义（escaping） escaping 允许使用任意字符串来作为属性或者变量的值，任何在 ~'anything' 或者 ~&quot;anything&quot; 将会被直接使用除了插值（interpolation）。 .test{ color: ~&quot;green&quot;; } 编译为： .test{ color: green; } 1.8 函数 LESS 内置了多种函数用于转换颜色，处理字符串，算术运算等。函数在函数手册中有详细介绍。用法如下： @base: #f500f5; @width: 0.5; .test{ width: percentage(@width);//50% color: saturate(@base, 5%); } 1.9 命名空间和访问器（namespaces &amp; accessors） 有些时候，你想要组合你的变量或者 mixin，无论是出于组织目的还是只是想实现一些封装，你可以很直观的在 LESS 中实现。 @color: #ff4e56 #bundle{ .button{ display: block; background-color: @color; width: 100px; height: 30px; border-radius: 5px; &amp;:hover{ background-color: @color - #222; } } .tab{ ... } .input{ ... } } 如果你将 .button 这个类加入 #header button 中，可以这样做： #header{ color: green; #bundle &gt; .button } 注：变量在某命名空间内定义声明，该变量的作用范围仅限于该空间中，外部的空间中该变量将无法不可用。因此无法像上面的语法 (#namespace &gt; .mixin-name) 那样进行，(#namespace &gt; @var) 是不行的。 1.10 作用范围（scope） scope 在 LESS 中与编程语言中的非常相似，变量和 mixin 会先在当前规则中寻找，如果无发现，编译器会当前规则的上一级寻找，以此类推。 @var: red; #page{ @var: blue; #nav{ color: @var;//blue } } 变量和 mixin 不一定要在其使用之前定义声明，下面例子同上面例子： @var: red; #page{ #nav{ color: @var;//blue } @var: blue; } 1.11 注释 可以使用行注释或者块注释： / 这里是注释 / @var: red; //这里是注释 @var: blue; 1.12 导入（importing） 可以导入 .less 的文件，所有在该 .less 的文件中的变量都是可用的。 @import &quot;test.less&quot;; @import &quot;main.css&quot;; 2 变量 2.1 概述 在你的样式表中一个值被重复使用多次的情况很常见： .link{ color: #ff4e56; } .widget{ color: #fff; background-color: #ff4e56; } 变量可以使你的代码更容易维护，通过一个值来控制所有相同的值。 @color: #ff4e56; .link{ color: @color; } .widget{ color: #fff; background-color: @color; } 2.2 变量插入 上面的例子实现了在 CSS 规则中使用变量来控制值，但是变量也可以使用在其他的地方，比如选择器的名字，属性的名字，URLs 和 @import 声明中： 2.2.1 选择器（v1.4.0） @selector: banner; .@{selector}{ font-size: 16px; color: #fff; background-color: #ff4e56; } 编译为： .banner{ font-size: 16px; color: #fff; background-color: #ff4e56; } 2.2.2 URLs @imgUrl: &quot;.../image&quot;; .listImg{ background: url(&quot;@{imgUrl}/banner.png&quot;); } 2.2.3 import 声明（v1.4.0） 语法：@import &quot;@{themes}/test.less&quot;; 在 v2.0.0 之前，只有定义声明在根（root）或者当前作用范围内的变量才会被认定为变量，查找变量的时候只有会在当前文件和导入文件中查找。 @url: &quot;../less&quot;; @import &quot;@{url}/test.less&quot;; 2.2.4 属性（v1.6.0） @property: color; .test{ @{property}: blue; background-@{property}: #333; } 编译为： .test{ color: blue; background-color: #333; } 2.2.5 变量名 可以使用一个变量名去定义另外一个变量： @name: &quot;hello world&quot;; @var: &quot;name&quot;; content: @@var; 编译为： content: &quot;hello world&quot;; 2.3 延迟加载（lazy loading） 变量会延迟加载，不一定要在使用之前定义声明。 有效的less代码： .lazy-load{ width: @var; } @var: @a; @a: 50%; 下面也是有效的代码： .lazy-load{ width: @var; @a: 50%; } @var: @a; @a: 100%; 上面两部分代码均会编译成： .lazy-load{ width: 50%; } 当一个变量被定义声明了两次，最后一次定义声明的变量将会被使用，查找变量的时候从当前作用范围向上级搜索。这跟 CSS 本身最后一次被定义的属性将会被使用很相似。 @var: 0; .class{ @var: 1; .brass{ @var: 2; three: @var; @var: 3; } one: @var; } 编译为： .class{ one: 1; } .class .brass{ three: 3; } 2.3.1 默认变量 有时候我们需要默认变量——是一种在变量还没定义好之前，定义一个变量。该功能不是必须的，因为可以很轻易的复写一个变量通过在所需的地方进行定义声明。 @base-color: green; @import &quot;test.less&quot;; @base-color: red; 3 extend 3.1 继承（extend） extend 可以理解成一种 LESS 的伪类，形式为 A:extend(B)，其作用：使用这个伪类选择器(A)可以将符合其条件的选择器(B)的样式规则引用给自身用，即 A 会拥有 B 的样式规则。 （A:extend(B)）下面会用 A 泛指使用 extend 伪类的选择器，用 B 泛指 extend 伪类所去匹配的选择器。 发布于 v1.4.0： nav ul{ &amp;:extend(.inline); background: blue; } 上面设置的规则中，:extend 这个伪类会把“使用 extend 的选择器（nav ul）”应用 .inline 的规则。如果 .inline 这个选择器不存在，那么 A 选择器的声明代码块将会保持原样，但是 extend 语句将无效（因为 extend 不是 CSS）。 nav ul{ &amp;:extend(.inline); background: blue; } .inline{ color: red; } 编译为： nav ul{ background: blue; } .inline,nav ul{ color: red; } 注意到 nav ul:extend(.inline) 选择器编译成 nav ul 的过程——在编译之前，选择器中的 extend 伪类的那个语句会被移除，其余的语句保持原样。如果 extend 中的 B 选择器没有样式，那么 extend 在编译过程中会被移除（但是 extend 有可能会影响其他的选择器）。 3.2 extend 语法 extend 要么接在选择器后面要么放置在规则中。看起来很像是伪类选择器，并且拥有一个可选的参数 all： .a:extend(.b){ ... } //上面的代码与下面的代码编译出来的一致 .a{ &amp;:extend(.b); } .c:extend(.d all){ //包含了&quot;.d&quot;的所有实例，例如：&quot; .x.d&quot;或者&quot;.d.x&quot; } .c:extend(.d){ //只包含一个实例，选择器只会被编译成 &quot;.d&quot;的实例 } extend 中可以包含一个或者多个类，用逗号分隔： .e:extend(.f){...} .e:extend(.g){...} //上面和下面代码的编译结果一样 .e:extend(.f,.g){...} 3.3 extend 使用方式 extend 的使用方式就像使用普通的伪类一样，一个选择器可以包含多个 extend，但是所有 extend 都要放置在这个选择器的末端。 extend 置于选择器后面：pre:hover:extend(div pre) 选择器和 extend 之间允许间隔空格：pre:hover :extend(div pre) 允许多个 extend：pre:hover:extend(div pre):extend(.bucket tr) = pre:hover:extend(div pre, .bucket tr); 以下的用法是错误的：pre:hover:extend(div pre).nth-child(odd)，extend 必须放在最末端。 当多个选择器拥有同一个样式规则，每个选择器都可以拥有独自的 extend 关键词。下面为多个选择器拥有一个规则的情况： .big-division, .big-bag:extend(bag), .big-bucket:extend(bucket){ ... } 3.4 在规则中的 extend extend 可以通过使用 &amp;:extend(selector) 这个语法将 extend 放置在规则内部。将 extend 放在规则内部是将 extend 单独作用在每个选择器的简化形式。 // 放在内部： pre:hover,.some-class{ &amp;:extend(div pre); ... } // 等同于在每个选择器中单独添加Extend： pre:hover:extend(div pre), .some-class:extend(div pre){...} 3.5 extend 中使用嵌套选择器 extend 允许使用嵌套选择器（即 B 选择器为嵌套形选择器）： .bucket{ tr{ color: blue; } } .some-class:extend(.bucket tr){ ... } 编译为： .bucket tr, .some-class{ color: blue; } 3.6 extend 准确匹配 extend 默认会在所有选择器中寻找与 B 选择器匹配的选择器，无论 B 选择器是否使用 *（星号）。如果两个后代表达式（详细看下一段）对应同一个元素，但是匹配的时候也是要根据后代表达式中的格式进行匹配。一个唯一的例外就是属性选择器的，LESS 知道它们意思相同并且能进行匹配。 .a.class,.class.a,.class&gt;.a{ color: blue; } .test:extend(.class){ ... }//匹配不到上面的任何选择器 * 可以认为是任意选择器。*.class 和 .class 是对应同一个元素数组，但是 extend 进行匹配的时候不会将二者认为是同一个。 *.class{ color: blue; } .test:extend(.class){ ... }//不会匹配到*.class这个选择器 编译为： *.class{ color: blue; } 匹配时会根据伪类的顺序。选择器 link:hover:visited 和 link:visited:hover 匹配到的是同一个元素，但是 extend 会区分它们。 link:visited:hover{ color: blue; } .selector:extend(link:hover:visited){ ... } 编译为： link:visited:hover{ color: blue; } 3.7 后代表达式 匹配会根据后代表达式的形式进行匹配。后代表达式 1n+3 和 n+3 是相等的，但是 extend 不会匹配二者。 :nth-child(1n+3){ color: blue; } .child:extend(:nth-child(n+3)){ ... } 编译为： :nth-child(1n+3){ color: blue; } 属性选择器的引用类型不会进行匹配。下面的情况都相等。 [title=identifier]{ color: blue; } // or [title='identifier']{ color: blue; } // or [title=&quot;identifier&quot;]{ color: blue; } .noQuote:extend([title=identifier]){} .singleQuote:extend([title='identifier']){} .doubleQuote:extend([title=&quot;identifier&quot;]){} 编译为： [title=indentifier], .noQuote, .singleQuote, .doubleQuote{ color: blue; } /* or */ [title='identifier'], .noQuote, .singleQuote, .doubleQuote { color: blue; } /* or */ [title=&quot;identifier&quot;], .noQuote, .singleQuote, .doubleQuote { color: blue; } 3.8 Extend&quot;all&quot; 当你指定了extend 的最后一个参数为 all 时，LESS 将会匹配一些名字包含 B 选择器的名字的 C 选择器，类似模糊匹配那样。当匹配到选择器后，C 选择器将会被复制，并且将 C 选择器名字中匹配到的那部分替换 B 选择器的名字，变成新的选择器。 .a.b.test, .test.c{ color: green; } .test{ &amp;:hover{ color: red; } } .replace:extend(.test all){} 编译成： .a.b.test, .test.c, .a.b.replace, .replace.c{ color: green; } .test:hover, .replace:hover{ color: red; } 你可以将这个操作理解成做一个非破坏性的搜索和匹配。 3.9 在 extend 中使用选择器插入（Selector Interpolation with Extend） extend 不能够匹配有变量的选择器，如果选择器包含变量，extend 将会将其忽略。 这是一个未定的特性，但是不会轻易改变。但是 extend 可以连接插入性选择器（包含变量的选择器 A）。 包含变量的选择器将不会被匹配： @var: .bucket; @{var}{ //插入性选择器 color: blue; } .some-class:extend(.bucket){}//不做任何事 在目标选择器进行extend中使用变量将不会被匹配： .bucket{ color: blue; } .some-class:extend(@{var}){}//匹配不到选择器 @var: .bucket; 上面两个都会编译成： .bucket{ color: blue; } 然而，:extend 可以连接到插入性选择器： .bucket{ color: blue; } @{var}:extend(.bucket){} @var: .test; 编译为： .bucket,.test{ color: blue; } 总结就是：A 选择器可以包含变量，但是 B 选择器不能包含变量。 3.10 作用范围/@media 里面的 extend 写在一个 media 声明中的 extend 只能匹配在同一个 media 里面的选择器： @media print{ .screenClass:extend(.selector){} .selector{ color: black; } } .selector{ color: red; } @media screen{ .selector{ color: blue; } } 编译为： @meida print{ .screenClass,.selector{ color: black; } } .selector{ color: red; } @media screen{ .selector{ color: blue; } } 写在一个 media 声明中的 extend 不会匹配到嵌套在里面的 media 声明里面的选择器： @media screen{ .selectorClass:extend(.selector){} @media (min-width: 768px){ .selector{ color: blue; } } } 编译为： @media screen and (min-width: 768px){ .selector{ color: blue; } } 外部 extend 可以匹配范围无限制，包括内部的嵌套 media 里面的选择器： @media screen{ .selector{ color: blue; } @media (min-width: 768px){ .selector{ color: red; } } } .topLevel:extend(.selector){} 编译为： @media screen{ .selector, .topLevel{ color: blue; } } @media screen and (min-width: 768px){ .selector, .topLevel{ color: red; } } 3.10.1 重复检测 现在是没有重复检测的功能。 .alert,.widget{ ... } .test:extend(.alert, .widget){} 编译为： .alert,.widget,.test,.test{ ... } 所以在使用 extend 的时候需要注意不要 extend 具有相同规则的选择器。 3.11 extend 的使用情况 3.11.1 典型使用情况 典型的使用情况是为了避免增加一个类： 如果你有一个类 animal .animal{ background-color: black; color: white; } 然后你想要通过覆写 background-color 这个属性来得到一个 animal 的子类，那么 HTML 就会变成这样： &lt;a class=&quot;animal bear&quot;&gt;bear&lt;/a&gt; CSS 样式就会变成这样： .animal{ background-color: black; color: white; } .bear{ background-color: grey; } 不然你可以在 LESS 中使用 extend，那样你的 HTML 就会变得很简单： &lt;a class=&quot;bear&quot;&gt;bear&lt;/a&gt; LESS 就会为： .animal{ background-color: black; color: white; } .bear{ &amp;:extend(.animal); background-color: grey; } 3.11.2 减少 CSS 大小 mixin 将所有的属性都复制到新的选择器中，从而导致了不必要的重复。因此你可以使用 extend 去代替 mixin 将你想要用的属性添加到新的选择器中，从而减少 CSS 代码。 使用mixin： .mine{ color: #fff; font-size: 16px; } .test1{ .mine; } .test2{ .mine; } 编译为： .mine{ color: #fff; font-size: 16px; } .test1{ color: #fff; font-size: 16px; } .test2{ color: #fff; font-size: 16px; } 使用 extend： .mine{ color: #fff; font-size: 16px; } .test1{ &amp;:extend(.mine); } .test2{ &amp;:extend(.mine); } 编译为： .mine, .test1, .test2{ color: #fff; font-size: 16px; } 3.11.3 组合样式/更先进的 mixin 另一种使用情况是作为 mixin 的一种替代方法，因为 mixin 只能用在建档的选择器，如果在 HTML 中有两个不同的块（block），但是需要应用同一个样式，那么你就可以使用 extend 去链接两个块元素。 li.list &gt;a{ ... } button.list-style{ &amp;:extend(li.list&gt;a); } 4 mixins 4.1 混入（mixins） 从已有的样式中 mix-in 属性。 你可以混入 class 选择器和 id 选择器： .a, #b{ color: red; } .mixin-class{ .a(); } .mixin-id{ #b(); } 编译为： .a, #b{ color: red; } .mixin-class{ color: red; } .mixin-id{ color: red; } 注意当你使用 mixin 的时候，括号是可选的。 .a(); .a; //上面两个语句编译后是相同的 4.1.1 无输出的 mixin（not outputting the mixin） 如果你想创建一个 mixin 但是不想这个 mixin 输出到样式表中，那么你可以在 mixin 后面加上括号 ()。 .my-mixin{ color: black; } .my-other-mixin(){ background: white; } .class{ .my-mixin; .my-other-mixin; } 编译为： .my-mixin{ color: black; } .class{ color: black; background: white; } 4.1.2 选择器 in mixins mixins 不仅能包含样式属性，还能包含选择器。 .my-hover-mixin(){ &amp;:hover{ //伪类选择器 border: 1px solid red; } } button{ .my-hover-mixin(); } 编译为： button:hover{ border: 1px solid red; } 4.1.3 命名空间 如果你想要 mixin 一个结构复杂的选择器中的属性，你可以累积多个 id 或者 class 的选择器。 #outer{ .inner{ color: red; } } .c{ #outer &gt; .inner; } &gt; 和空格都是可选的： #outer &gt; .inner; #outer &gt; .inner(); #outer .inner; #outer .inner(); #outer.inner; #outer.inner(); 你可以将你的 mixins 放在 id 选择器的下级，这样可以保证不会和其他库冲突。 #my-library{ .my-mixin(){ color: black; } } .class{ #my-library &gt; .my-mixin(); } 4.1.4 命名空间监控（guarded namespaces） 如果命名空间设置了监控，只有监控的条件判断为真，mixins 才会被定义声明。命名空间的监控和 mixin 的监控是相同的，所以下面两个 mixins 编译结果一样。 #namespaces when (@mode=huge){ .mixin(){ ... } } #namespace{ .mixin() when (@mode=huge){ ... } } default 函数是对于所有的嵌套命名空间和 mixin 都会假定有一个相同的值。下面的 mixin 将不会被编译，因为其中一个监控的条件判断必定为假： #sp_1 when(default()){ #sp_2 when(default()){ .mixin() when not(default()){ ... } } } 4.1.5 关键词 &quot;! important&quot; 在 mixin 后面使用关键词 &quot;! important&quot;，将会让 mixin 里面的所有属性都继承 &quot;! important&quot;： .foo(@bg: #f5f5f5, @color: #999){ background: @bg; color: @color; } .unimportant{ .foo(); } .important{ .foo() !important; } 编译为： .unimportant{ background: #f5f5f5; color: #999; } .important{ background: #f5f5f5 !important; color: #999 !important; } 4.2 参数混入（parametric mixins） mixins 可以带参数。mixins 会将传入选择器的变量进行参数混入。 .border-radius(@radius){ -webkit-border-radius: @radius; -moz-border-radius: @radius; border-radius: @radius; } 下面我们就可以将其混入规则中： #header{ .border-radius(4px); } .button{ .border-radius(6px); } 参数混入所带的参数可以设置默认值： .border-radius(@radius: 5px){ -webkit-border-radius: @radius; -moz-border-radius: @radius; border-radius: @radius; } 我们就可以直接使用： #header{ .border-radius;//将会设置5px的圆角 } 你也可以使用不带参数的参数混入。如果你不想把该 mixin 编译显示在样式表中，但是又想要将 mixin 里面的属性添加到其他的样式规则中时，这个将会非常有用。很容易注意到，其实这个就是我们上面讲到的“无输出的 mixin（not outputting the mixin）”。 .wrap(){ text-wrap: wrap; word-wrap: break-word; } pre{ .wrap } 编译为： pre{ text-wrap: wrap; word-wrap: break-word; } 4.2.1 多参数的 mixins 参数可以使用分号 ; 或者逗号 , 进行分隔，不过推荐使用分号 ;。逗号 , 有两种含义，它可以被解释成 mixin 的参数分隔符或者是样式表的分隔符。在 mixin 分隔符使用逗号 , 会导致无法使用逗号 , 作为 CSS 的参数分隔符。另一方面，如果编译器只要在 mixin 调用或者声明处发现到一个分号 ;，就会认定参数是使用分号作为分隔符，所有的逗号都是属于样式表的，不会当成 mixin 的分隔符： 具有两个参数，并且参数中含有逗号的情况：.name(1,2,3 ; some,else) 具有三个参数，每个参数包含一个数字：.name(1,2,3) 使用假的分号去创建 mixin，该 mixin 具有一个用逗号去分隔样式的参数：.name(1,2,3 ;) 逗号分隔默认值：.name(@param1: red , blue) 定义多参数 mixin 的时候，参数的名字重复和用数字做参数是不符合要求的。LESS 会使用所有可以使用的属性，如果你使用带有一个参数的 mixin: .mixin(green);，那么所有符合条件的 mixin 将会被使用。 注：符合的条件为：该 mixin 所需参数为一个的就会被使用，注意不是参数为一个的 mixin，而是所需参数为一个的 mixin。因为参数可以设置默认值，带有默认值的参数不是必需的，不带默认值的参数是必需的。 .mixin(@color){ color-1: @color; } .mixin(@color; @padding: 2){ color-2: @color; padding-2: @padding; } .mixin(@color; @padding; @margin: 2){ color-3: @color; padding-3: @padding; margin: @margin @margin @margin @margin; } .test{ .mixin(#999); } 编译为： .test{ color-1: #999; color-2: #999; padding-2: 2; } 4.2.2 参数命名 使用 mixin 的时候会通过参数名称进行参数匹配而不是参数定义的位置。 .mixin(@color: black; @margin: 10px; @padding: 20px){ color: @color; margin: @margin; padding: @padding; } .class1{ .mixin(@margin: 20px; @color: #336699); } .class2{ .mixin(#336699; @padding: 40px); } 编译为： .class1{ color: #336699; margin: 20px; padding: 20px; } .class2{ color: #336699; margin: 10px; padding: 40px; } 4.2.3 变量 @arguments 在 mixin 中，@arguments 具有特殊的意思，当 mixin 被调用的时候，它包含了所有传入的参数，如果你不想单独处理参数的话，这个将会很好用。 .box-shadow(@x: 0; @y: 0; @blur: 1px; @color: #000){ -webkit-box-shadow: @arguements; -moz-box-shadow: @arguements; box-shadow: @arguements; } .test{ .box-shadow(2px; 5px); } 编译为： .test{ -webkit-box-shadow: 2px 5px 1px #000; -moz-box-shadow: 2px 5px 1px #000; box-shadow: 2px 5px 1px #000; } 4.2.4 可变参数和变量 @rest 如果你想使用数量可变的参数的 mixin 的时候，你可以使用 ... 这个参数。在一个变量名后面使用这个参数，将会给变量分配参数。 .mixin(...){}//匹配0-N个参数 .mixin(){}//只匹配0个参数 .mixin(@a: 1){}//匹配0-1个参数 .mixin(@a: 1; ...){}//匹配0-N个参数 .mixin(@a;...){}//匹配1-N个参数 此外： .mixin(@a; @rest...){ //@rest 代表@a参数后面的所有参数 //@arguement 代表所有参数 } 4.2.5 模式匹配 有时候，你可能想要通过传入的参数来改变 mixin 的行为。 下面先让我们从基础情况开始： .mixin(@s; @color){ ... } .class{ .mixin(@switch; #888); } 现在我们想要 .mixin 基于 @switch 的值表现的不一样，那么我们可以这样定义 .mixin： .mixin(dark; @color){ color: darken(@color, 10%); } .mixin(light; @color){ color: lighten(@color, 10%); } .mixin(@_; @color){ display: block; } 如果我们这个编写代码： @switch: light; .class{ .mixin(@switch; #888) } 编译为： .class{ color: #a2a2a2; display: block; } 传入 .mixin 的颜色是亮色。如果 @switch 的值是 dark，结果将会是暗色。 上面运行的过程可以这么分析： 第一个 mixin 不匹配是因为它的第一个参数值是匹配 dark； 第二个 mixin 匹配是因为它的参数匹配 light； 第三个 mixin 匹配是因为它的参数匹配任何值。 我们也可以匹配参数数量： .mixin(@a){ color: @a; } .mixin(@a; @b){ color: fade(@a; @b); } 现在如果我们调用 .mixin 并且传入一个参数，我们将会得到第一个 mixin 的编译结果，但是如果我们调用然后传入两个参数，我们将会得到第二个 mixin 的编译结果。 4.3 函数化 Mixins（Mixins as Function） 从 mixins 中返回变量或者 mixin。 在一个 mixin 中定义的变量和 mixins 在其使用范围内是可见并且可用的。只有一个例外，如果调用者（caller）包含了一个变量，而这个变量（可能是定义在另外的 mixin 的调用处）与被调用的 mixin 中的变量重名的话，变量不会被覆盖。在调用者（caller）本身的变量是受保护的，继承自父作用范围（caller parent）的变量是会被覆盖的。 .mixin(){ @width: 100%; @height: 200px; } .caller{ .mixin(); width: @width; height: @height; } 编译为： .caller{ width: 100%; height: 200px; } 然而，定义在 mixin 中的变量可以作为一个返回值，因此可以像使用函数那样创建 mixin。 .average(@x, @y){ @average: ((@x + @y) / 2); } div{ .average(16px, 50px); padding: @average; } 编译为： div{ padding: 33px; } 定义在调用者（caller）自身下的变量是不能被覆盖的，因此，定义在调用者的父代（caller parent）的范围内的变量是不受保护可以被覆盖的。 .mixin(){ @size: in-mixin; @defineOnlyInMixin: in-mixin; } .class{ margin: @size @defineOnlyInMixin; .mixin(); } @size: global-define-value; 编译为： .class{ margin: in-mixin in-mixin; } 最后，定义在 mixin 中的 mixin 也可以作为返回值。 .unlock(@value){ .dosomething(){ declaration: @value; } } #namespace{ .unlock(5); .dosomething(); } 编译为： #namespace{ declaration: 5; } 4.3.1 Passing Rulesets to Mixins 发布于 v1.7.0。 ruleset 可以理解成 CSS 属性，嵌套的 ruleset，media 声明或者存储任何东西的变量的组合。可以将一个 ruleset 包含进另一个 ruleset 或者其他的区域块，然后这个 ruleset 的所有属性都会被复制到此。你也可以将其当成 mixin 的参数，像变量一样传入。 @detached-ruleset: { background: red }; .top{ @detached-ruleset(); } 编译为： .top{ background: red; } 使用 detached ruleset 的时候后面一定要带上括号，如果就使用 @detached-ruleset 是不起作用的。 如果你想将媒体查询的代码或者浏览器不支持的类名在 mixin 中进行定义，这个特性会很好用。ruleset 可以被传入到 mixin，然后 mixin 会包装传入的内容。 .desktop-and-old-ie(@rules){ @meida screen and (min-width: 1200){ @rules(); } html.lt-ie9 &amp; { @rules(); } } header{ background-color: blue; .desktop-and-ie({ background-color: red; }) } 编译为： header { background-color: blue; } @media screen and (min-width: 1200) { header { background-color: red; } } html.lt-ie9 header { background-color: red; } 5 import 5.1 import 准则 将其他样式表中的样式导入进当前样式表。 在标准 CSS 中，@import 声明必须放在其他类型的声明之上，也就是放在顶部。但是在 LESS 中，@import 声明可以放在任何地方。 .foo{ background: #900; } @import &quot;this-is-valid.less&quot;; 5.2 文件后缀名 在 LESS 中，@import 声明会根据引入的文件的后缀进行相应的解析。 如果文件是 .css 的后缀，该文件将会被解析成 CSS 样式，然后 @import 语句会保持原样。 如果文件是其他类型的后缀的话，文件会被解析成 LESS 文件，然后进行导入。 如果文件没有任何后缀，将会为文件添加 .less 的后缀，并且当成 LESS 文件进行导入。 @import &quot;foo&quot;;//会被当成foo.less导入 @import &quot;foo.less&quot;;//导入foo.less @import &quot;foo.php&quot;;//会被当成foo.less导入 @import &quot;foo.css&quot;;//导入foo.css 使用下面所讲的参数可以用来覆写这个行为。 5.3 import options less 提供了数种后缀给 CSS 的 @import 语句，可以更加灵活的去使用外部文件。 语法：@import（keyword）&quot;filename&quot;; 下面是已经实现了的 import 准则： reference：使用 LESS 文件到时不将其输出。 inline：将源文件包含进来但是不进行处理。 less：无论文件后缀是什么类型，都当成less格式的文件。 css：无论文件后缀是什么类型，都当成css格式的文件。 once：只引入文件一次（为默认行为）。 multiple：引入文件数次。 optional：当文件没找到时会继续编译。 @import 语句允许多个 keyword，你需要用逗号将这些 keyword 进行分隔： @import (optional, reference) &quot;foo.less&quot;; 5.3.1 reference 使用 @import (reference) 导入外部文件，导入的样式不会添加到编译输出，除非该样式被引用。 发布于 v1.5.0 @import (reference) &quot;foo.less&quot;; 可以想象一下在导入的文件中，reference 会使所有的指令和选择器都会被标记上一个引用标记，导入时是正常的，但是生成 CSS 的时候，被标记的选择器不会输出到 CSS 上，被标记的样式不会在你生成的 CSS 中出现，直到被标记的样式被当成 mixins 或者 extend 使用的时候。 此外，reference 会生成不同的结果，取决于哪个方法被调用（mixin 或者 extend）： extend：当一个选择器被继承的时候，只有新的选择器会被被标记成“非 reference”，它会被放置在引用 @import 语句的位置。 mixins：当一个被标记的样式被当成一个隐式的 mixin 时，样式会被混入并且标记为“非 reference”，然后被放置在引用这个样式的地方。 你可以通过做一些像下面一样的事情来将某些指定的样式从一个样式库（像 bootstrap）中加进来： .navbar:extend(.navbar all){ } 你只会将 .navbar 相关的样式从 bootstrap 中引用进来。 5.3.2 inline 使用 @import(inline) 引用外部文件，但是不处理它们。 发布于 v1.5.0。 @import (inline) &quot;not-less-compatible.css&quot;; 当一个 CSS 文件可能无法被 LESS 所兼容时，你可以使用这个特性。尽管 LESS 支持大部分的标准 CSS，但是它不支持在某些地方的 comments 和所有没有修改 CSS 的 css hacks。 所以你可以使用这个特性将文件包含进输出中，因此所有的 CSS 都会在一个文件中。 5.3.3 less 使用 @import (less) 将会把导入文件解析成 LESS 格式，无论文件是什么格式。 发布于 v1.4.0。 @import (less) &quot;foo.css&quot;; 5.3.4 css 使用 @import (css) 将会把文件解析成常规的 CSS 文件，无论文件是什么格式。这意味着 import 语句会保持原样。 发布于 v1.4.0 @import (css) &quot;foo.less&quot;; 编译为： @import &quot;foo.less&quot;; 5.3.5 once @import 语句的默认行为，意味着文件只会被导入一次，后续的导入相同文件的语句会被忽略。 发布于 v1.4.0 @import (once) &quot;foo.less&quot;; @import (once) &quot;foo.less&quot;;//该语句会被忽略 5.3.6 multiple 使用 @import (multiple) 会允许导入多个相同名字的文件，这行为跟 once 截然相反。 发布于 v1.4.0。 //在文件foo.less中 .a{ color: green; } //在文件main.less @import (multiple) &quot;foo.less&quot;; @import (multiple) &quot;foo.less&quot;; 编译为： .a{ color: green; } .a{ color: green; } 5.3.7 optional 使用 @import (optional) 只会导入存在的文件。当导入一个找不到的文件时，如果没有 optional 的参数，LESS 将会抛出一个 fileerror 的错误然后停止编译。 发布于 v2.3.0。 6 guards 6.1 Mixin Guards（mixin 监控） 带条件的 mixins。 当你想要匹配一个表达式的时候，guards 是非常有用的。如果你熟悉函数式编程的话，那你很可能已经见过了。 为了尽可能的接近 CSS 的声明性性质，LESS 选择实现条件化执行的方式，如：在 media 查询特性规范的结构中，使用 guards mixins 代替 if/else 语句。 .mixin (@a) when (lightness(@a) &gt;= 50%) { background-color: black; } .mixin (@a) when (lightness(@a) &lt; 50%) { background-color: white; } .mixin (@a) { color: @a; } 关键在 when 这个关键词上，when 这个关键词定义了监控原则（这里只有一个监控）。如果我们运行以下代码： .class1 { .mixin(#ddd) } .class2 { .mixin(#555) } 编译为： .class1 { background-color: black; color: #ddd; } .class2 { background-color: white; color: #555; } 6.1.1 guard 中比较运算符 在 guards 中可用的运算符为：&gt;，&gt;=，=，=&lt;，&lt; 。此外，关键词 true 是唯一的真值，下面两个 mixins 相等： .truth (@a) when (@a) { ... } .truth (@a) when (@a = true) { ... } 除了关键词 true 以外，任何值都是假的： .class { .truth(40); //不会匹配上面声明的语句 } 你也可以进行参数之间的比较，或者参数和非参数之间进行比较： @media: mobile; .mixin (@a) when (@media = mobile) { ... } .mixin (@a) when (@media = desktop) { ... } .max (@a; @b) when (@a &gt; @b) { width: @a } .max (@a; @b) when (@a &lt; @b) { width: @b } 6.1.2 guard 中的逻辑运算符 你可以在 guards 中使用逻辑运算符，语法是基于 CSS 的媒体查询。 使用关键词 and 去连接 guards： .mixin (@a) when (isnumber(@a)) and (@a &gt; 0) { ... } 你可以模仿 or 运算符通过使用逗号将 guards 进行分隔。如果任何 guards 被判断为真，会被进行匹配： .mixin (@a) when (@a &gt; 10), (@a &lt; -10) { ... } 使用关键词 not 与条件取反 .mixin (@b) when not (@b &gt; 0) { ... } 6.1.3 类型检查函数（Type Checking Functions） 最后，如果你想要基于类型来匹配 mixins 的话，你可以使用 is 函数： .mixin (@a; @b: 0) when (isnumber(@b)) { ... } .mixin (@a; @b: black) when (iscolor(@b)) { ... } 下面是基本的类型检查函数： iscolor isnumber isstring iskeyword isurl 如果你想要检查一个值是否在一个特定的单元中（不是当做数字），你可以使用： ispixel ispercentage isem isunit 6.1.4 有条件的 mixins 另外，default 函数可以根据其他 mixin 匹配来进行 mixin 匹配，您可以使用它创建类似于 else 或默认语句（分别是 if 和 case 结构）的“条件 mixin”： .mixin (@a) when (@a &gt; 0) { ... } .mixin (@a) when (default()) { ... } // 只有第一个mixin不匹配时才会匹配这个 6.2 CSS Guards（css 监控） 发布于 v1.5.0。 guards 也可以用在 CSS 选择器上，这是一种语法糖，用于声明 mixin，然后立即调用它。 在 1.5.0 之前你可能需要这样做： .my-optional-style() when (@my-option = true) { button { color: white; } } .my-optional-style(); 现在你可以直接在样式中使用 guard。 button when (@my-option = true) { color: white; } 你也可以通过结合这个和 &amp; 特性，从而实现一个 if 类型语句，允许组合多个 guards： &amp; when (@my-option = true) { button { color: white; } a { color: blue; } } 7 Loop&amp;Merge&amp;Parents Selectors 7.1 循环（Loop） 在 LESS 中一个 mixin 可以调用其本身。当结合了 guards 表达式和模式匹配后，这样一个递归的 mixins 可以用来创建各种各样的迭代/循环的结构。 .loop(@counter) when (@counter &gt; 0) { .loop((@counter - 1)); // 下一次迭代 width: (10px * @counter); // 每次迭代的代码 } div { .loop(5); //启动循环 } 编译为： div { width: 10px; width: 20px; width: 30px; width: 40px; width: 50px; } 使用递归循环的一般的例子是生成 CSS 的格网类（grid classes）： .generate-columns(4); .generate-columns(@n, @i: 1) when (@i =&lt; @n) { .column-@{i} { width: (@i * 100% / @n); } .generate-columns(@n, (@i + 1)); } 编译为： .column-1 { width: 25%; } .column-2 { width: 50%; } .column-3 { width: 75%; } .column-4 { width: 100%; } 7.2 合并（Merge） 合并属性。 合并特性允许将多个属性的值聚合到一个使用逗号或者空格分隔的单属性列表中。merge 对于像 background 和 transform 属性来说是非常有用的。 7.2.1 逗号（comma） 发布于 v1.5.0。 .mixin() { box-shadow+: inset 0 0 10px #555; } .myclass { .mixin(); box-shadow+: 0 0 20px black; } 编译为： .myclass { box-shadow: inset 0 0 10px #555, 0 0 20px black; } 7.2.2 空格（space） 发布于 v1.7.0。 .mixin() { transform+_: scale(2); } .myclass { .mixin(); transform+_: rotate(15deg); } 编译为： .myclass { transform: scale(2) rotate(15deg); } 为了避免各种非故意的连接，merge 要求一个明确的 + 或者 +_ 作为标记在每个需要连接的声明中。 7.3 父代选择器（parent selectors） &amp; 运算符代表了嵌套中的父代选择器，在将一个修改过的类或者伪类加到一个已存在的选择器中的情况下是很常用的。 a { color: blue; &amp;:hover { color: green; } } 编译为： a { color: blue; } a:hover { color: green; } 注意到如果没有 &amp;，上面的例子将不会产生 a:hover 的样式（会变成一个在 &lt;a&gt; 标签里面的可以匹配任意悬停元素的后代选择器），那就不是我们想要的嵌套的 :hover 的伪类选择器。 “父代选择器”运算符具有各种用途，你可以将嵌套规则的选择器以非默认方式的其他方式组合在一起。另外一种典型用法就是产生重复的类名： .button { &amp;-ok { background-image: url(&quot;ok.png&quot;); } &amp;-cancel { background-image: url(&quot;cancel.png&quot;); } &amp;-custom { background-image: url(&quot;custom.png&quot;); } } 编译为： .button-ok { background-image: url(&quot;ok.png&quot;); } .button-cancel { background-image: url(&quot;cancel.png&quot;); } .button-custom { background-image: url(&quot;custom.png&quot;); } 7.3.1 多个 &amp; &amp; 可以在一个选择器中出现多次，这使得可以重复地引用父代选择器而不重复它的名字。 .link { &amp; + &amp; { color: red; } &amp; &amp; { color: green; } &amp;&amp; { color: blue; } &amp;, &amp;ish { color: cyan; } } 编译为： .link + .link { color: red; } .link .link { color: green; } .link.link { color: blue; } .link, .linkish { color: cyan; } 注意到 &amp; 代表了所有的父代选择器（不是最近的父代选择器）： .grand { .parent { &amp; &gt; &amp; { color: red; } &amp; &amp; { color: green; } &amp;&amp; { color: blue; } &amp;, &amp;ish { color: cyan; } } } 编译为： .grand .parent &gt; .grand .parent { color: red; } .grand .parent .grand .parent { color: green; } .grand .parent.grand .parent { color: blue; } .grand .parent, .grand .parentish { color: cyan; } 7.3.2 改变选择器的顺序 可以使当前选择器比其父代选择器优先，这个可以通过将 &amp; 放在当前选择器的后面来实现。当使用 Modernize 时，您可能希望根据支持的特性指定不同的规则： .header { .menu { border-radius: 5px; .no-borderradius &amp; { background-image: url('images/button-background.png'); } } } .no-borderradius &amp; 选择器 将会使 .no-borderradius 选择器比其父代选择器 .header .menu 优先。从而会编译成 .no-borderradius .header .menu .header .menu { border-radius: 5px; } .no-borderradius .header .menu { background-image: url('images/button-background.png'); } 7.3.3 组合遍历（Combinatorial Explosion） &amp; 可以生产所有可能的选择器排列，这些选择器在一个由逗号分隔的列表中。 p, a, ul, li { border-top: 2px dotted #366; &amp; + &amp; { border-top: 0; } } 这产生了指定元素所有可能的组合： p, a, ul, li { border-top: 2px dotted #366; } p + p, p + a, p + ul, p + li, a + p, a + a, a + ul, a + li, ul + p, ul + a, ul + ul, ul + li, li + p, li + a, li + ul, li + li { border-top: 0; } ","link":"https://faded.auspicious.space/post/less-study-notes/"},{"title":"爱的五种能力","content":" 爱的五种能力 俗话说的好“相爱容易相处难”，“家家有本难念的经”，说的大都是婚后生活夫妻之间的相处问题。本书作者认为为什么夫妻相处有问题是因为缺少“爱的五种能力”： 情绪管理 述情 共情 允许 影响 爱的能力之一 “情绪管理”： 首选是“情绪管理”，每个人要管理好自己的情绪，才有能力去爱别人。不能管理好自己情绪的人，常常让与自己相爱的人痛苦，容易错失爱的机会，甚至会伤害人。 想要爱情幸福，家庭和睦，两人就要都经常强调“爱”和“情”。一个人在强调“爱”，另一个人在强调“理”——“应该”，两人显然不在一个频道沟通，是爱人之间产生矛盾或吵架的主要原因。我们首选要理清两个概念： 控制情绪是当有了情绪后把它强压在心里，不去表达，也不释放，也就是我们常说的“忍”。但“忍”字头上一把刀，忍久了，要么伤害自己，身体生病，或心理出问题；要么，积累到一定的程度受不了，爆发，对关系的伤害更大。 情绪管理是使自己根本就少起或不起情绪，整个人更加平和，更多的时候处于一种平静的状态，不情绪化，这样人就会能更多感受幸福，也能更好与人相处。 那我们如何情绪管理？ 情绪管理方法一：保持客观 ，我们必须学会管理因假设而起的情绪，让自己可以更加平静地去享受生活，而不是经常编个故事气自己。对于这种假设情绪我们可以无为或者验证假设是否成立。 情绪管理方法二：避免条件反射情绪，我们必须有效避免对方说了一句或者一个动作而导致你情绪失控。 情绪管理方法三：放下因自己对和错标准而起的情绪，用心去感受对方的心，感受一下对方做这个事情时内在的感受是什么？爱人的开心快乐，有时比自己的正确可能还要重要，因为人们找爱人，不是为了正确的，而是为了开心快乐、幸福。 爱的能力之二 “述情”： 是指用不伤害关系的方式表达自己的需求、想法和感受。人们在表达和沟通上常犯的错误是要么有了情绪或需求不说，闷在心里，隐忍，等到忍不住了就爆发了，要么常常用指责和抱怨的方式表达和沟通。隐忍伤自己，指责和抱怨伤害对方。而述情是情感关系里最合适的，不伤害任何人的沟通方式。 作者提供几种述情方法： 基本功一：分清是不是事实 ，这就是述情的基本功，能够在分清事实是什么，然后在述情的时候，说出的话里用事实，而不是自己的主观想象和假设。 能保持客观，清楚地知道事实和假设之间的区别既然是述情的基本功，也就需要经常练习的，练习的方法倒也简单，就是当你尝试对一件事情做出描述时，就问自己，我说的是事实吗？ 基本功二：准确描述感受 ，所以，述情时要遵循一个原则：关系越远，用词“颗粒度”越粗；关系越近，用词“颗粒度”越细。 述情的几个注意事项： 越述情问题越多 述情后对方没有变化 失语期 六句话练成述情高手 述情是用不伤害关系的方式表达出自己真实的需求、感受和想法，情感关系里只要使用这样的方式去跟对方沟通，就不会轻易伤害到两人的关系，却可以达到有效沟通的目的。 句型一：当你心情不好的时候直接告诉对方。 句型二：当对方令你感受不好（或好）时，告诉对方你的感受！ 句型三：告诉对方你的需求，而不是解决方案。 句型四：说出你喜欢的，而不是你不喜欢的。 句型五：使用“可以”“能帮我”替代命令 句型六：说出“我希望”，不说“你应该”。 爱的能力之三 “共情”： 理解并支持对方、善解人意。这几乎是所有人都希望爱人能具备的能力，可惜很多人都没有。大家基本都是习惯了讲道理、教育对方、给建议，而不知道很多时候对方需要的其实是共情。 善解人意的能力其实就可以理解为共情的能力，就是经常可以理解到对方的内心感受。特别是当对方内心有不好的感受时，善解人意的人会让爱人感觉到自己被理解，这等于在内心给了对方接纳和认可，使其感受到来自爱人的支持 我将在下面的共情的四个步骤里详细地描述这个内容。 第一步：接受对方的负面情绪 ，并开始关注这个人的情绪。 第二步：引导对方分享他内在的感受和外在发生的事情。 第三步：肯定对方情绪的逻辑。 第四步：启发 ， 启发的第一步：启发对方理解他人，或从另外的角度看问题。 启发的第二步：是引导对方去关注未来、关注解决方案。 启发就是这样做的，整个过程中都是提问，并不给答案，而是促使对方自己去思考问题，进而使用他的智慧去解决问题。 爱的能力之四 “允许”： 尊重差异、允许成长。爱人之间吵架，发生分歧，很多时候都是因为不允许所导致的，不允许对方跟自己不一样，不允许对方有些特点，不接纳真实的对方，想要控制对方或改变对方。这会让双方都痛苦，有了允许的能力，才能给对方做真实自己的机会和空间。 允许的意思是说，我不需要去判断这些事情是对的还是错的，也不需要把这些事情装到自己的心里去，我只是允许这些事情以它本来的面目存在，不去做抗争，臣服于宇宙和自然的规律中。在这个大的规律中，她是大的，我是小的，我允许她的存在。 越是强大的人越能允许别人贬低自己、否定自己。相反，越是弱小的人越做不到允许别人贬低自己、否定自己。 可以经常做这个练习，刻意回想过去的一些事情，发现内心还有一些不允许时对自己说：“我允许！” 爱的能力之五“影响”： 每个人都会变，在爱情关系里的人们更是会因为对方而变，可以说一个人找了不同的爱人就会变成不同的人。有可能越变越完整，也有可能越变问题越严重，那么，自己怎么做，对方就会变得越来越完整呢？对方就能成长得更好呢？这就是影响的能力，让对方成长的能力。 鼓励好的方面，不责备坏得方面，让对方勇于尝试！ 驯兽师训练动物的过程使用的就是影响的方法，当动物们做得不好时不惩罚，是本书讲的前一种影响的方法：允许，给动物们一个成长的机会；而当动物们做得好时奖励就是本书要讲的影响的第二种方法：强化。这些方法用在动物身上和用在人的身上同样有效果，因为人也是动物，只不过是高级动物。 其他 婆媳关系为什么就那么难处理呢？ 媳妇和婆婆两人本来就没有血缘关系，又没有深厚的感情，有的只是因为婚姻关系和亲子关系的对接而应该遵守的道德约束，还有因为争夺来自同一个男人的爱而产生的矛盾，所以，婆婆和媳妇两人关系不好也是正常的。 情绪词 颗粒度粗的情绪词：心情好、舒服、不舒服、难过、不开心等。 以下情绪词汇，都是相对精确的情绪词汇，属于颗粒度细的情绪词： 高兴、自豪、开心、自信、感激、快乐、愉悦、温暖、喜悦、愉快、幸福、满足、欣慰、惬意、爱、喜欢、感动、兴奋、充实、平静、放松、温暖、踏实、祥和； 伤心、尴尬、担心、焦虑、害怕、紧张、沮丧、迷茫、惊恐、内疚、失落、无助、无奈、失望、绝望、伤感、凄凉、苦闷、疲惫、悲伤、愤怒、生气、恨、厌恶、厌烦、惊讶、困惑、孤独、寂寞、郁闷、羞愧、遗憾、嫉妒、后悔，等等。 ","link":"https://faded.auspicious.space/post/five-abilities-of-love/"},{"title":"视频基本概念","content":" 简单聊聊视频基本概念以及在信息流内容中心的处理 随着 4G 网络的普及以及 5G 网络的展开，单位流量越来越便宜，人们从而能在碎片时间内通过移动网络毫无压力的观看视频内容。抖音在 2018 年春节突然火爆，全民都在刷抖音，据悉，抖音（包括海外版的 TikTok）用户日产生短视频近百亿。一般来说，用户上传的短视频会自动加上对应短视频 App 的水印 Logo，我们今天就来聊一下视频处理，首先介绍一下视频基本概念和视频处理所用的 API、工具。 视频基本概念 什么是视频？ 一组图片以给定的速率（例如 30 张图片/秒）在人眼前翻过，人的视觉就会产生一种图片在动的感觉，小时候动画片的制作也是基于这个原理，从这个角度讲，视频可以理解为以一定速率运动的一组图片的集合。 帧率 每秒播放的频数，fps(frames per second),拍摄的帧率和播放的帧率一般保持一致。比如，电影或者电视剧以 30 fps 拍摄，然后以 30 fps 播放。对于高速运动的赛车比赛，为了让观众看清赛车到达终点的场面或者两辆实力很接近的选手到底谁赢了，会以高帧率拍摄（比如 60 fps），然后转成 30 fps 来回放播放，这样就会产生慢镜头，从而让观众看清赛车越过终点那一刹那的完整情形。 码率 每秒传输的 bit 位，bit/s，对应到带宽。有几点需要注意一下： **并不是画面越清晰要求的带宽就一定越大。**对于在线课程类视频（比如，吴恩达的机器学习课程），高速运动的画面很少，画面之间的像素很多也都是一样的（白板背景），即使是 1080p 视频，带宽也不会占用太多。1080p 在线课程类视频会比 1080p 汽车赛事类视频占用的带宽少。进一步的，1080p 在线课程类视频可能还比 720p 汽车赛事类视频占用的带宽还少。 **动态的码率要好于固定的码率。**视频存在部分画面间变化不大、部分画面间变化较大的情形，若用同一码率，则会使得画面间变化不大的浪费了带宽，画面间变化较大的损失了画质。 **码率的设置需同时考虑移动手机端环境和画质情况。**首先，对于移动手机端，很少需要 1080p 的视频，720p、480p 已经足够。其次，移动手机端对流量仍然比较敏感，为了看 1080p 电影视频（假设平均需要 4 Mb/s 的码率），30 分钟就会消耗 1 GB 的流量。若出现网络环境较差出现抖动，低码率视频能够顺畅的观看，高码率视频则会出现卡顿，严重影响观看体验。 视频流的编码方式 一个视频包括视频流、音频流、字幕脚本三大部分，字幕可以包括多语言字幕。视频流的编码方式有如下两种： ITU/ISO 国际标准组织发起，包括 H.264、H.265(也可写作 HEVC)编码等，软件实现是 libx264、libx265，是收费的。 Google 发起，包括 VP8、VP9（几乎和 H.265 一样好）、AV1（声称比 H.265 还好）编码等，软件实现是 libvpx、libaom，是免费的。 音频流的编码方式 ISO 国际标准组织发起，包括 MP3、AAC 编码等，软件实现是 libmp3lame、libfdk-aac，是收费的。 Xiph.Org 基金会发起，包括 Vorbis、Opus 编码等，软件实现是 libvorbis、libopus，是免费的。 视频的制式 视频的制式可以理解成定义一种多媒体存储格式，用于存放视频流、音频流、字幕脚本，以及后续可能的扩展， 像一个容器一样，目前主要有如下几个： 视频制式 简单描述 MPEG4 应用最广泛的视频制式，用于存放 H.264，H.265 视频，ACC 音频，各种字幕等。 Matroska 一种多媒体封装格式，这个封装格式可把多种不同编码的影像及 16 条或以上不同格式的音频和语言不同的字幕封装到一个 Matroska Media 档内。 WebP 以 Matroska 格式为基础，该视频文件格式应能提供高质量的视频压缩以配合 HTML5 使用，用于存放 VP8、VP9、AV1 视频，Vorbis、Opus 音频，字幕等。 AVI 微软推出的多媒体封装格式，可以存放多种视频、音频格式，包括 H.264、ACC 等。因为索引放在文档底部，所以对互联网串流媒体的场景显得力不从心。 视频的尺寸 视频的尺寸是指视频每一帧横竖方向的像素数目，比如 1920 * 1080，像素越多画质越好。电影或者电视剧在拍摄的时候，会用蓝光甚至 4K 摄像机进行拍摄，这么优质的画面若放到手机上面用移动网络播放，对带宽和流量的要求都很高，所以信息流内容中心一般都会事先进行视频尺寸的转换。 720P、1080P 等中 P 代表什么意思，720，1080又分别代表什么？ 视频有两种展现模式：横视频（Landscape），宽比高长，电影类视频适合此种模式；竖视频（Portrait），宽比高短，人物类视频适合此种模式，例如，抖音视频里面的小姐姐。 长宽比分辨率（aspect ratio resolution）：16:9（目前主流的长宽比），4:3（早期的长宽比）。 P 代表 pixels，1080P 代表视频垂直方向的像素的个数，主流视频尺寸比是 16:9，通过简单换算得到水平方向的像素个数是 1080 * 16 / 9 = 1920 。 视频信息举例 以 4K 网站上面的一个 4K 视频（地址是：https://4ksamples.com/puppies-bath-in- 4k/）为例： 视频总体信息 视频总体维度（包括视频流和音频流） 维度取值 Format（制式） MPEG-4 Codec ID（制式对应的 ID） mp42 Overall bit rate mode（总体码率模式） Variable（可变码率） Overall bit rate（总体码率） 21.2 Mbps（平均码率） 视频流信息 视频流维度 维度取值 编码率 H.264(AVC, Advanced Video Coding) Codec ID(编解码对应的 ID) avc1 Bit rate(码率) 21.0 Mbps(平均码率) Maximum bit rate(最大码率) 39.7 Mbps Width 4.096 pixels Heigth 2.304 pixels Display aspect ratio(长宽比) 16:9(主流尺寸) Frame rate mode(帧率模式) Variable(可变，即每秒的帧率不是固定的) Frame rate 23.976 fps Minimum frame rate 16.000 fps Maximum frame rate 24.000 fps Color space YUV(一种颜色编码方式，比 RGB 节省带宽，传输容错性也更加高效)，也写作 YCbCr，其中 Y 代表 Iuminance(亮度)，Cb 代表 Blue，Cr 代表 Red。 Chroma subsampling 4:2:0 Bit depth 8 bits Scan type Progressive(像素条顺序展现，还有一种是 Interlaced，这种是奇数和偶数像素条交替展现，老式 CRT 显示器使用的较多) 音频流信息 音频流维度 维度取值 编解码 ACC Codec ID(编解码对应的 ID) 40 Bit rate mode Variable Bit rate(码率) 192 Kbps(平均码率) Maximum bit rate(最大码率) 203 Kbps Sampling rate 44.1 KHz Compression mode Lossy(有损压缩) 视频处理库 讲到视频处理，不得不提及 ffmpeg，一个开源的视频处理库（工具），有两种使用方式： 二进制可执行程序 ffmepg 提供了很多参数和设置，可以在命令行执行或者通过系统函数调用来执行。ffmpeg {1} {2} -i {3} {4} {5}， 其中，{1} 是全局选项，{2} 输入选项，{3} 输入文件名或者 URL，{4} 输出选项，{5} 输出文件名或者 URL。下面是一个样例， $ ffmpeg -i PUPPIES_BATH_IN_4K_Original_H.264_AAC.mp4 \\ #输入文件名 -f image2 -vf fps=fps=1/20 \\ #输出选项， puppies_%d.png #输出文件名 API 库 可以在程序里面通过调用 API 来实现视频处理。 Lib 库名 作用 libavutil 提供实用函数库 API libavformat 提供用于处理视频的制式 API，例如 MPEG4，AVI，VP9 等。 libavdevice 提供用于从输入设备数据抓取和输出数据到输出设备 API。 libavcodec 提供编解码库的 API，例如处理 H.264，H.265 等。 libavfilter 提供视频和音频的各种过滤处理 API，例如，针对视频原始帧的尺寸调整、像素类型转换等。 libswscale 提供处理视频原始帧尺寸的伸缩和像素转换的 API libswresample 提供处理音频重采样等功能的 API libpostproc 提供视频后期处理的 API ffmepg 处理视频的流程大致如下： 信息流内容中心业务的处理 业务处理维度 视频处理维度 作用 抽帧 视频处理的第一步，抽取视频的画面帧（关键帧等），将针对视频的处理转换成对图片的处理。 重复识别 根据抽取的帧，进行视频内容的重复识别。 色情检测 根据抽取的帧，进行视频内容色情的检测。 封面图优化 从抽取的帧和原始封面图进行比较，选择一张更加清晰的图片作为封面。 黑边检测 根据抽取的帧，进行视频是否包含黑边的检测。 水印处理 包括水印 Logo 检测，加水印，去水印。 制式处理 例如，从 WebM 转为 MPEG4，从 AVI 转为 MPEG4 等。 编解码处理 例如，从 H.264 转为 H.265，从 VP8 转为 AVI 等，实现更高的压缩率。 码率转换 例如，从高码率转为低码率，适应移动场景下的要求，在带宽和视频质量之间做权衡。 尺寸处理 l传输，将尺寸宽高调整为 480*270 等，以便适合手机终端观看。 视频处理特点 视频处理一般都比较耗时，视频文件越大处理越耗时，仍然以上面的 4K 视频为例，列举 2 个常用操作的耗时： 抽帧，操作耗时在 34.87 秒左右。fmpeg -i PUPPIES_BATH_IN_4K_Original_H.264_AAC.mp4 -f image2 -vf fps=fps=1/20 puppies_%d.png 调整码率和尺寸，将 4K 的视频转成 720P 的视频（文件大小从 400 MB 左右减少为 15 MB 左右），以便适合手机等移动网络观看，操作耗时在 77.43 秒左右。ffmpeg -i PUPPIES_BATH_IN_4K_Original_H.264_AAC.mp4 -crf 32 -b 0.5M -minrate 0.5M -maxrate 1M -bufsize 1M -vf scale=1280:720 PUPPIES_BATH_IN_1280_720p.mp4 不同耗时 API 的处理方式： 耗时短的 API(&lt;= 1s)，这类 API 通常无须考虑中断接续，一般采用无状态的设计方式，对外的 proxy 只用考虑鉴权、流量限制等常规措施，只要请求足够多（例如，QPS 1万/s），并遵循一定的随机或者后端服务选择算法，后端服务的负载通常是均衡的。调用方发请求，然后再接受处理结果，若本次请求若处理失败，调用方进行重试的代价也不高。 耗时长的 API(&gt;= 30s，不是绝对的)，这类 API 执行一次耗时长，QPS 不高，对外的 proxy 除了要考虑鉴权、流量限制等常规措施，还需要考虑实际处理服务器的负载均衡问题，否则很容易出现请求全部打到部分机器上面去的情况。假设，一个 3 分钟的短视频处理耗时 30 秒，1 个 4 核 8 GB 内存的云服务器每天处理 11520 个视频，某款短视频 App 的日生成视频量为 1000 万，则需要近 900 台这样云服务器，整体的 QPS 却只有 120/s，容易出现机器繁忙程度不一。 QQ 看点内容中心的两种做法： 异步回调，调用方提供回调 API，视频处理结束后通过回调 API 得到处理结果。 同步等待，通过 TCP 长连接，在回包中将处理结果告知调用方，视频处理的 QPS 不高，这种方式也是可以的，同时简化了调用方的逻辑。 总结 本文简单介绍了一下视频的基本概念以及实际工程中服务模块的搭建和分工，后面再针对具体使用场景（例如水印处理）单独进行讲解。 ","link":"https://faded.auspicious.space/post/basic-video-concepts/"},{"title":"HTTP 状态码超详解","content":" HTTP状态码详解（上）(建议收藏) HTTP状态码详解（下）(建议收藏) 导读 当浏览者访问一个网页时，浏览者的浏览器会向网页所在服务器发出请求。当浏览器接收并显示网页前，此网页所在的服务器会返回一个包含 HTTP 状态码的信息头（server header）用以响应浏览器的请求。 HTTP 状态码的英文为 HTTP Status Code。下面是常见的 HTTP 状态码： 200 - 请求成功 301 - 资源（网页等）被永久转移到其它URL 404 - 请求的资源（网页等）不存在 500 - 内部服务器错误 0、HTTP 状态码的分类 HTTP 状态码由三个十进制数字组成，第一个十进制数字定义了状态码的类型，后两个数字没有分类的作用。HTTP 状态码共分为 5 种类型： 分类 分类描述 1** 信息，服务器收到请求，需要请求者继续执行操作 2** 成功，操作被成功接收并处理 3** 重定向，需要进一步的操作以完成请求 4** 客户端错误，请求包含语法错误或无法完成请求 5** 服务器错误，服务器在处理请求的过程中发生了错误 1、详细的描述状态码之（1**） 100：客户端应当继续发送请求。这个临时响应是用来通知客户端它的部分请求已经被服务器接收，且仍未被拒绝。客户端应当继续发送请求的剩余部分，或者如果请求已经完成，忽略这个响应。服务器必须在请求完成后向客户端发送一个最终响应。 101：服务器已经理解了客户端的请求,并将通过 Upgrade 消息头通知客户端采用不同的协议来完成这个请求。在发送完这个响应最后的空行后，服务器将会切换到在 Upgrade 消息头中定义的那些协议。只有在切换新的协议更有好处的时候才应该采取类似措施。例如，切换到新的 HTTP 版本比旧版本更有优势，或者切换到一个实时且同步的协议以传送利用此类特性的资源。 102：由 WebDAV（RFC 2518）扩展的状态码，代表处理将被继续执行。 2、详细的描述状态码之（2**） 200：请求已成功，请求所希望的响应头或数据体将随此响应返回。 201：请求已经被实现，而且有一个新的资源已经依据请求的需要而建立，且其 URI 已经随 Location 头信息返回。假如需要的资源无法及时建立的话，应当返回 202 Accepted。 202：服务器已接受请求，但尚未处理。正如它可能被拒绝一样，最终该请求可能会也可能不会被执行。在异步操作的场合下，没有比发送这个状态码更方便的做法了。返回 202 状态码的响应的目的是允许服务器接受其他过程的请求（例如某个每天只执行一次的基于批处理的操作），而不必让客户端一直保持与服务器的连接直到批处理操作全部完成。在接受请求处理并返回 202 状态码的响应应当在返回的实体中包含一些指示处理当前状态的信息，以及指向处理状态监视器或状态预测的指针，以便用户能够估计操作是否已经完成。 203：服务器已成功处理了请求，但返回的实体头部元信息不是在原始服务器上有效的确定集合，而是来自本地或者第三方的拷贝。当前的信息可能是原始版本的子集或者超集。 204：服务器成功处理了请求，但不需要返回任何实体内容，并且希望返回更新了的元信息。响应可能通过实体头部的形式，返回新的或更新后的元信息。如果存在这些头部信息，则应当与所请求的变量相呼应。如果客户端是浏览器的话，那么用户浏览器应保留发送了该请求的页面，而不产生任何文档视图上的变化，即使按照规范新的或更新后的元信息应当被应用到用户浏览器活动视图中的文档。由于 204 响应被禁止包含任何消息体，因此它始终以消息头后的第一个空行结尾。 205：服务器成功处理了请求，且没有返回任何内容。但是与 204 响应不同，返回此状态码的响应要求请求者重置文档视图。该响应主要是被用于接受用户输入后，立即重置表单，以便用户能够轻松地开始另一次输入。 206：服务器已经成功处理了部分 GET 请求。类似于 FlashGet 或者迅雷这类的 HTTP 下载工具都是使用此类响应实现断点续传或者将一个大文档分解为多个下载段同时下载。该请求必须包含 Range 头信息来指示客户端希望得到的内容范围，并且可能包含 If-Range 来作为请求条件。响应必须包含如下的头部域：Content-Range 用以指示本次响应中返回的内容的范围；如果是 Content-Type 为 multipart/byteranges 的多段下载，则每一 multipart 段中都应包含 Content-Range 域用以指示本段的内容范围。假如响应中包含 Content-Length，那么它的数值必须匹配它返回的内容范围的真实字节数。Date ETag 和/或 Content-Location，假如同样的请求本应该返回 200 响应。Expires，Cache-Control，和/或 Vary，假如其值可能与之前相同变量的其他响应对应的值不同的话。假如本响应请求使用了 If-Range 强缓存验证，那么本次响应不应该包含其他实体头；假如本响应的请求使用了 If-Range 弱缓存验证，那么本次响应禁止包含其他实体头；这避免了缓存的实体内容和更新了的实体头信息之间的不一致。否则，本响应就应当包含所有本应该返回 200 响应中应当返回的所有实体头部域。假如 ETag 或 Last-Modified 头部不能精确匹配的话，则客户端缓存应禁止将 206 响应返回的内容与之前任何缓存过的内容组合在一起。任何不支持 Range 以及 Content-Range 头的缓存都禁止缓存 206 响应返回的内容。 207：由 WebDAV(RFC 2518) 扩展的状态码，代表之后的消息体将是一个 XML 消息，并且可能依照之前子请求数量的不同，包含一系列独立的响应代码。 3、详细的描述状态码之（3**） 300：被请求的资源有一系列可供选择的回馈信息，每个都有自己特定的地址和浏览器驱动的商议信息。用户或浏览器能够自行选择一个首选的地址进行重定向。 301：被请求的资源已永久移动到新位置，并且将来任何对此资源的引用都应该使用本响应返回的若干个 URI 之一。 302：请求的资源现在临时从不同的URI响应请求。由于这样的重定向是临时的，客户端应当继续向原有地址发送以后的请求。 303：对应当前请求的响应可以在另一个 URI 上被找到，而且客户端应当采用 GET 的方式访问那个资源。这个方法的存在主要是为了允许由脚本激活的 POST 请求输出重定向到一个新的资源。这个新的 URI 不是原始资源的替代引用。同时，303 响应禁止被缓存。 304：如果客户端发送了一个带条件的 GET 请求且该请求已被允许，而文档的内容（自上次访问以来或者根据请求的条件）并没有改变，则服务器应当返回这个状态码。 305：被请求的资源必须通过指定的代理才能被访问。Location 域中将给出指定的代理所在的 URI 信息，接收者需要重复发送一个单独的请求，通过这个代理才能访问相应资源。只有原始服务器才能建立 305 响应。 306：在最新版的规范中，306 状态码已经不再被使用。 307：请求的资源现在临时从不同的 URI 响应请求。由于这样的重定向是临时的，客户端应当继续向原有地址发送以后的请求。 4、详细的描述状态码之（4**） 相对于其它状态码 4 的就比较多了，天地万物，且听在下娓娓道来。 400： 语义有误，当前请求无法被服务器理解。除非进行修改，否则客户端不应该重复提交这个请求。 请求参数有误。 401：当前请求需要用户验证。该响应必须包含一个适用于被请求资源的 WWW-Authenticate 信息头用以询问用户信息。客户端可以重复提交一个包含恰当的 Authorization 头信息的请求。如果当前请求已经包含了 Authorization 证书，那么 401 响应代表着服务器验证已经拒绝了那些证书。如果 401 响应包含了与前一个响应相同的身份验证询问，且浏览器已经至少尝试了一次验证，那么浏览器应当向用户展示响应中包含的实体信息，因为这个实体信息中可能包含了相关诊断信息。 402：该状态码是为了将来可能的需求而预留的。 403：服务器已经理解请求，但是拒绝执行它。与 401 响应不同的是，身份验证并不能提供任何帮助，而且这个请求也不应该被重复提交。 404：请求失败，请求所希望得到的资源未被在服务器上发现。没有信息能够告诉用户这个状况到底是暂时的还是永久的。 405：请求行中指定的请求方法不能被用于请求相应的资源。该响应必须返回一个 Allow 头信息用以表示出当前资源能够接受的请求方法的列表。 406：请求的资源的内容特性无法满足请求头中的条件，因而无法生成响应实体。 407：与 401 响应类似，只不过客户端必须在代理服务器上进行身份验证。 408：请求超时。客户端没有在服务器预备等待的时间内完成一个请求的发送。客户端可以随时再次提交这一请求而无需进行任何更改。 409：由于和被请求的资源的当前状态之间存在冲突，请求无法完成。 410：被请求的资源在服务器上已经不再可用，而且没有任何已知的转发地址。 411：服务器拒绝在没有定义 Content-Length 头的情况下接受请求。在添加了表明请求消息体长度的有效 Content-Length 头之后，客户端可以再次提交该请求。 412：服务器在验证在请求的头字段中给出先决条件时，没能满足其中的一个或多个。 413：服务器拒绝处理当前请求，因为该请求提交的实体数据大小超过了服务器愿意或者能够处理的范围。 414：请求的 URI 长度超过了服务器能够解释的长度，因此服务器拒绝对该请求提供服务。 415：对于当前请求的方法和所请求的资源，请求中提交的实体并不是服务器中所支持的格式，因此请求被拒绝。 416：如果请求中包含了 Range 请求头，并且 Range 中指定的任何数据范围都与当前资源的可用范围不重合，同时请求中又没有定义 If-Range 请求头，那么服务器就应当返回 416 状态码。 417：在请求头 Expect 中指定的预期内容无法被服务器满足，或者这个服务器是一个代理服务器，它有明显的证据证明在当前路由的下一个节点上，Expect 的内容无法被满足。 421：从当前客户端所在的 IP 地址到服务器的连接数超过了服务器许可的最大范围。 423：请求格式正确，但是由于含有语义错误，无法响应。 424：由于之前的某个请求发生的错误，导致当前请求失败，例如 PROPPATCH。 425：在 WebDav Advanced Collections 草案中定义，但是未出现在《WebDAV 顺序集协议》（RFC 3658）中。 426：客户端应当切换到 TLS/1.0。 449：由微软扩展，代表请求应当在执行完适当的操作后进行重试。 5、详细的描述状态码之（5**） 500：服务器遇到了一个未曾预料的状况，导致了它无法完成对请求的处理。一般来说，这个问题都会在服务器的程序码出错时出现。 501：服务器不支持当前请求所需要的某个功能。当服务器无法识别请求的方法，并且无法支持其对任何资源的请求。 502：作为网关或者代理工作的服务器尝试执行请求时，从上游服务器接收到无效的响应。 503：由于临时的服务器维护或者过载，服务器当前无法处理请求。这个状况是临时的，并且将在一段时间以后恢复。注意：503 状态码的存在并不意味着服务器在过载的时候必须使用它。某些服务器只不过是希望拒绝客户端的连接。 504：作为网关或者代理工作的服务器尝试执行请求时，未能及时从上游服务器（URI 标识出的服务器，例如 HTTP、FTP、LDAP）或者辅助服务器（例如 DNS）收到响应。 505：服务器不支持，或者拒绝支持在请求中使用的 HTTP 版本。这暗示着服务器不能或不愿使用与客户端相同的版本。响应中应当包含一个描述了为何版本不被支持以及服务器支持哪些协议的实体。 506：由《透明内容协商协议》（RFC 2295）扩展，代表服务器存在内部配置错误：被请求的协商变元资源被配置为在透明内容协商中使用自己，因此在一个协商处理中不是一个合适的重点。 507：服务器无法存储完成请求所必须的内容。这个状况被认为是临时的。WebDAV (RFC 4918) 509：服务器达到带宽限制。这不是一个官方的状态码，但是仍被广泛使用。 510：获取资源所需要的策略并没有没满足。（RFC 2774） ","link":"https://faded.auspicious.space/post/http-status-code-in-deep/"},{"title":"HTML 空白汉字占位符","content":" html 空白汉字占位符 空白汉字占位符 前端开发中，大家可能会遇到这样的问题：标题存在字数不一样的情况，但是产品大哥，让你要对齐。还必须对齐。他说他有强迫症 全角： 是一种电脑字符，是指一个全角字符占用两个标准字符（或两个半角字符）的位置。全角占两个字节。 半角： 是指一个字符占用一个标准的字符位置。半角占一个字节。 不管是半角还是全角，汉字都要占两个字节。 &amp;#12288; == 一个中文宽度 &amp;#32; == 普通的英文半角空格 &amp;#160; == &amp;nbsp; == &amp;#xA0; == no-break space （普通的英文半角空格但不换行） &amp;#8194; == &amp;ensp; == en 空格 （半个中文宽度） &amp;#8195; == &amp;emsp; == em 空格 （一个中文宽度） &amp;#8197; == 四分之一em空格 （四分之一中文宽度） 相比平时的空格（&amp;#32;），&amp;nbsp 拥有不间断（non-breaking）特性。即连续的 &amp;nbsp 会在同一行内显示。 即使有 100 个连续的 &amp;nbsp，浏览器也不会把它们拆成两行。 实践效果 我我我 我 我——（普通的英文半角空格） 我我我 我 我——（普通的英文半角空格但不换行） 我我我 我 我——（一个中文宽度，但可以看作一个空白汉字） 我我我 我 我——（半个中文宽度） 我我我 我 我——（四分之一中文宽度） 我我我 我 我——（半角的不断行的空白格） 我我我 我 我——（半角的空格） 我我我 我 我——（全角的空格） ","link":"https://faded.auspicious.space/post/html-blank-chinese-character-placeholder/"},{"title":"COPYING OBJECTS IN JAVASCRIPT","content":" COPYING OBJECTS IN JAVASCRIPT In this article we will look at the various ways an object can be copied in Javascript. We will take a look at both shallow and deep copying. Before we begin, it is worth mentioning a few basics: objects in Javascript are simply references to a location in memory. These references are mutable, i.e. they can be reassigned. Thus, simply making a copy of a reference only results in 2 references pointing to the same location in memory: var foo = { a : &quot;abc&quot; } console.log(foo.a); // abc var bar = foo; console.log(bar.a); // abc foo.a = &quot;yo foo&quot;; console.log(foo.a); // yo foo console.log(bar.a); // yo foo bar.a = &quot;whatup bar?&quot;; console.log(foo.a); // whatup bar? console.log(bar.a); // whatup bar? As you see in the above example, both foo and bar are reflecting the change done in either object. Thus, making a copy of an object in Javascript requires some care depending upon your use case. SHALLOW COPY If your object only has properties which are value types, you can use the spread syntax or Object.assign(...) var obj = { foo: &quot;foo&quot;, bar: &quot;bar&quot; }; var copy = { ...obj }; // Object { foo: &quot;foo&quot;, bar: &quot;bar&quot; } var obj = { foo: &quot;foo&quot;, bar: &quot;bar&quot; }; var copy = Object.assign({}, obj); // Object { foo: &quot;foo&quot;, bar: &quot;bar&quot; } Note that both of the above methods can be used to copy property values from multiple source objects to a target object: var obj1 = { foo: &quot;foo&quot; }; var obj2 = { bar: &quot;bar&quot; }; var copySpread = { ...obj1, ...obj2 }; // Object { foo: &quot;foo&quot;, bar: &quot;bar&quot; } var copyAssign = Object.assign({}, obj1, obj2); // Object { foo: &quot;foo&quot;, bar: &quot;bar&quot; } The problem with the above methods lies in the fact that for objects with properties which are themselves objects, only the references are copied over, i.e. it is the equivalent of doing var bar = foo; as in the first code example: var foo = { a: 0 , b: { c: 0 } }; var copy = { ...foo }; copy.a = 1; copy.b.c = 2; console.dir(foo); // { a: 0, b: { c: 2 } } console.dir(copy); // { a: 1, b: { c: 2 } } DEEP COPY (WITH CAVEATS) In order to deep copy objects, a potential solution can be to serialize the object to a string and then deserialize it back: var obj = { a: 0, b: { c: 0 } }; var copy = JSON.parse(JSON.stringify(obj)); Unfortunately, this method only works when the source object contains serializable value types and does not have any circular references. An example of a non-serializable value type is the Date object - even though it is printed in ISO format on stringification, JSON.parse only interprets it as a string and not as a Date object. DEEP COPY (WITH FEWER CAVEATS) For more complex cases, one could make use of a newer HTML5 cloning algorithm called &quot;structured clone&quot;. Unfortunately, at the time of writing it is still limited to certain built-in types but it supports many more types than what JSON.parse does: Date, RegExp, Map, Set, Blob, FileList, ImageData, sparse and typed Array. It also preserves references within the cloned data, allowing it to support cyclical and recursive structures that don't work with the above mentioned serialization method. Currently, there is no direct way of calling the structured clone algorithm but there are some newer browser features that use this algorithm under the hood. Thus, there are a couple of workarounds that could potentially be used to deep copy objects. Via MessageChannels The idea behind this is to leverage the serialization algorithm used by a communication feature. Since this feature is event based, the resultant clone is also an asynchronous operation. class StructuredCloner { constructor() { this.pendingClones_ = new Map(); this.nextKey_ = 0; const channel = new MessageChannel(); this.inPort_ = channel.port1; this.outPort_ = channel.port2; this.outPort_.onmessage = ({data: {key, value}}) =&gt; { const resolve = this.pendingClones_.get(key); resolve(value); this.pendingClones_.delete(key); }; this.outPort_.start(); } cloneAsync(value) { return new Promise(resolve =&gt; { const key = this.nextKey_++; this.pendingClones_.set(key, resolve); this.inPort_.postMessage({key, value}); }); } } const structuredCloneAsync = window.structuredCloneAsync = StructuredCloner.prototype.cloneAsync.bind(new StructuredCloner); const main = async () =&gt; { const original = { date: new Date(), number: Math.random() }; original.self = original; const clone = await structuredCloneAsync(original); // different objects: console.assert(original !== clone); console.assert(original.date !== clone.date); // cyclical: console.assert(original.self === original); console.assert(clone.self === clone); // equivalent values: console.assert(original.number === clone.number); console.assert(Number(original.date) === Number(clone.date)); console.log(&quot;Assertions complete.&quot;); }; main(); Via the history API Both history.pushState() and history.replaceState() create a structured clone of their first argument! Note that while this method is synchronous, manipulating browser history is not a fast operation and calling this method repeatedly can lead to browser unresponsiveness. const structuredClone = obj =&gt; { const oldState = history.state; history.replaceState(obj, null); const clonedObj = history.state; history.replaceState(oldState, null); return clonedObj; }; Via the notification API When creating a new notification, the constructor creates a structured clone of its associated data. Note that it also attempts to display a browser notification to the user, but this will silently fail unless the application has requested permissions to display notifications. In the case that permission was granted, the notification is immediately closed. const structuredClone = obj =&gt; { const n = new Notification(&quot;&quot;, {data: obj, silent: true}); n.onshow = n.close.bind(n); return n.data; }; DEEP COPY IN NODE.JS As of version 8.0.0, Node.js provides a serialization api which is compatible with structured clone. Note that this API is marked as experimental at the time of writing: const v8 = require('v8'); const buf = v8.serialize({a: 'foo', b: new Date()}); const cloned = v8.deserialize(buf); cloned.b.getMonth(); For versions below 8.0.0 or for a more stable implementation, one can use lodash's cloneDeep method, which is also loosely based on the structured clone algorithm. CONCLUSION To sum up, the best algorithm for copying objects in Javascript is heavily dependent on the context and type of objects that you are looking to copy. While lodash is the safest bet for a generic deep copy function, you might get a more efficient implementation if you roll your own, the following is an example of a simple deep clone that works for dates as well: function deepClone(obj) { var copy; // Handle the 3 simple types, and null or undefined if (null == obj || &quot;object&quot; != typeof obj) return obj; // Handle Date if (obj instanceof Date) { copy = new Date(); copy.setTime(obj.getTime()); return copy; } // Handle Array if (obj instanceof Array) { copy = []; for (var i = 0, len = obj.length; i &lt; len; i++) { copy[i] = deepClone(obj[i]); } return copy; } // Handle Function if (obj instanceof Function) { copy = function() { return obj.apply(this, arguments); } return copy; } // Handle Object if (obj instanceof Object) { copy = {}; for (var attr in obj) { if (obj.hasOwnProperty(attr)) copy[attr] = deepClone(obj[attr]); } return copy; } throw new Error(&quot;Unable to copy obj as type isn't supported &quot; + obj.constructor.name); } Personally, I'm looking forward to be able to use structured clone everywhere and finally put this issue to rest, happy cloning 😃 HackerNews submission / discussion ","link":"https://faded.auspicious.space/post/copying-objects-in-javascript/"},{"title":"让一个元素看不见的 N 种方法","content":" 让一个元素看不见的N种方法 1、最常用之——display: none; 给元素设置 display: none; 后，元素会从页面中彻底消失，它原本占据的空间会被其他元素占有，会造成浏览器的回流与重绘。 2、最常用之——visibility: hidden; 给元素设置 visibility: hidden; 后，元素会从页面中消失，它原本占据的空间会被保留，会造成浏览器的重绘，适用于希望元素隐藏又不影响页面布局的场景。 3、隐身大法——opacity: 0; 给元素设置 opacity: 0; 后，元素变成透明的我们肉眼就看不到了，所以原本占据的空间还在。 4、设置盒模型属性为 0 将 height、width、padding、border、margin 等盒模型属性的值全设为 0，如果元素內还有子元素或内容，还应 overflow: hidden; 来隐藏子元素。 .box1 { width: 0; height: 0; padding: 0; border: 0; margin: 0; overflow: hidden; } 5、最鸡贼——设置元素绝对定位与 top、right、bottom、left 等将元素移出屏幕 如： .box1 { position: absolute; left: 100%; } 或： .box1 { position: absolute; top: 9999px; } 6、设置元素的绝对定位与 z-index，将 z-index 设置成尽量小的负数。 但 z-index 是相对而言的 ，用 z-index 就要设置其他元素的 z-index 值，且如果元素本身占据空间很大就不一定会被 z-index 值比它大的元素完全覆盖，所以不推荐这种方法。 如： .box1 { position: absolute; z-index: -9999; } .box2 { position: absolute; z-index: 1; } ","link":"https://faded.auspicious.space/post/n-ways-to-make-an-element-invisible/"},{"title":"黑客与画家","content":" 读后感+笔记.黑客与画家 读后感 创作者利用一切条件来磨练技艺， 工作者则是由重复工作积累经验。 笔记 黑客 自由软件基金会创始人理查德·斯托尔曼：出于兴趣而解决某个难题，不管它有没有用，这就是黑客。 《黑客：计算机革命的英雄》中的黑客伦理： 使用计算机以及所有有助于了解这个世界本质的事物，都不应受到任何限制，任何事情都应该亲手尝试。 信息应该全部免费。 不信任权威，提倡去中心化。 判断一名黑客的水平应该看他的技术能力，而不是看他的学历、年龄或地位等其他标准。 你可以用计算机创造美和艺术。 计算机使生活更美好。 黑客价值观的核心原则概括：分享，开放，民主，计算机的自由使用，进步。 优秀的黑客养成了一种质疑一切的习惯。 编程语言 你选择什么语言，决定了你能说什么话。 编程语言就是程序员的思维方式。 在学校里书呆子为什么会被歧视和欺负？ 青少年在心理上还没有摆脱儿童状态，许多人都会残忍地对待他人。在一个人产生良知之前，折磨就是一种娱乐。 在任何社会等级制度中，那些对自己没自信的人就会通过虐待他们眼中的下等人来凸显自己的身份。 追求“受欢迎”的心理。不停地设法使自己与其他受欢迎的人变的关系更密切。没有什么比一个共同的敌人更能使得人们团结起来了。 黑客与画家的共同之处 他们都是创作者。 如何凭空创造出问题？ 没有什么比一个错误的前提更容易产生大量待解决的问题了。 公正的评价 塞缪尔·约翰逊（英国词典学家）：人们对一个作家的评价需要 100 年才能达成一致。你必须先等他的那些有影响力的朋友都死了，然后再等他的追随者都死了，才能对他有一个公正的评价。 想清楚 你把整个程序想清楚的时间点，应该是在编写代码的同时，而不是在编写代码之前，这与作家、画家和建筑师的做法完全一样。 善用语言，发挥优势 对于互联网软件，没人规定只能使用某些语言开发，因为所有的硬件都控制在你手里，你想要用什么语言，就能用什么语言。 不同的语言适合不同的任务，你应该根据不同场合，挑选最合适的工具。 如果你不利用语言的优势，那就会听任对手超过你。 互联网软件的优势 开发互联网软件不需要得到任何人的许可，没有人能够阻止你。 你不需要去申请许可证，不需要在零售店的货架上谋得一席之地，也不需要卑躬屈膝地求人家，将你的软件与操作系统捆绑在一起。 你能够通过浏览器发布软件，没有人能在你和浏览网站的用户之间插上一脚。 职位的可测量性和可放大性 你的职位产生的业绩应该是可测量的，从而可以计算报酬。 同时，该职位还必须具有可放大性，即做出的决定能够产生巨大的效应。 小团队=可测量性 高科技=可放大性 收购 潜在的买家会尽可能地拖延收购，收购这件事最难的地方就是让买方真正拿出钱。 大多数时候，促成买方掏钱的最好办法，不是让买方看到有获利的可能，而是让他们感到失去机会的恐惧（比如竞争对手收购你，或是你以后成长为他们的竞争对手）。 用户数量 用户数量也许不是最好的测量指标，但应该也相差不远了。 买家关心它，收入依赖它，竞争对手恐惧它，记者和潜在用户则是被它打动。 无论你的技术水平有多高，用户数量都比你自己的判断更能准确反映哪些问题应该优先解决。 创业的基本原则 你必须时刻牢记的最基本原则是：创造人们需要的东西，也就是创造财富。 你必须知道人们需要什么。 一个社会需要有富人，这主要是因为他们在致富过程中做出的事情。如果你让福特致富，他就会造出一台拖拉机，使你不再需要使用马匹耕田。 好设计 好设计是简单的设计。 好设计是永不过时的设计。 好设计是解决主要问题的设计。 好设计是启发性的设计。 好设计通常是有点趣味性的设计。 好设计是艰苦的设计。 好设计是看似容易的设计。 好设计是对称的设计。 对称有两种：重复性对称和递归性对称。 对称的危险在于它可以用来取代思考。 好设计是模仿大自然的设计 大自然在长期的演化中已经解决了很多问题。 好设计是一种再设计。 好设计是能够复制的设计。 最伟大的大师最终会达到一种超脱自我的境界。 他们一心想找到正确答案，如果别人已经回答出了一部分，那就没理由不拿来用。 他们足够自信地使用他人成果，完全不担心因此丧失个人特点。 好设计常常是奇特的设计。 好设计是成批出现的 特定的环境提供了好设计的沃土。 好设计常常是大胆的设计。 选择 选择使用哪一种技术的时候，你不能考虑别人的想法，只能考虑什么样的技术能最好地完成工作。 大公司可以互相模仿，但是创业公司就不行。如果你掌管创业公司最好做一些独特的事情，否则就会有麻烦。 重复 你只需要不停地重复同一句话，最终人们将会开始倾听。 人们真正注意到你的时候，不是第一眼看到你站在那里，而是发现过了这么久你居然还在那里。 程序是给人看的 《计算机程序的结构与解释》:程序写出来是给人看的，附带能在机器上运行。 思考 如果你想清晰地思考，就必须远离人群。 如果自己就是潮水的一部分，怎么能看见潮流的方向呢？ 很少有人鼓励你继续成长，变成一个怀疑和抵制社会错误潮流的人。 深入 只有深入了解当前技术，才能构想下一代技术。 ","link":"https://faded.auspicious.space/post/hacker-and-painter/"},{"title":"如何做好 APP 的概要设计","content":" 关于如何做好APP概要设计的一些想法 但凡做过实际商业项目的开发人员都知道，开发前期通过脑力劳动做好程序设计，会大大减少开发后期修 bug、维护的体力劳动。 程序设计分概要设计和详细设计。作为技术负责人应重点关注概要设计，把握整体架构、识别出开发过程中的重难点和潜在风险。至于详细设计应给实际业务的开发人员足够的发挥空间。 因为详细设计涉及到功能的技术细节，会和实际业务强相关。但对于概要设计来讲，主要在于明确怎样做？肯定是可以形成一套通用的范式的。 结合个人的实际工作经验，要做到通过设计提升程序稳定性和质量的目的，一份合格的概要设计应包含如下内容： 一、需求分析 1. 产品需求分析 用于描述对产品提出的需求的理解和分析：产品定位、新增功能、业务目标等，避免对需求的理解和产品不一致，导致做无用功。 2. 技术需求分析 用于描述对技术团队提出的技术需求的理解和分析：技术债务清理、市场反馈问题改善、性能体验优化、技术指标达成、技术方案更新等。此处应包含技术改善项目和大致的实现策略。 3. 关键技术指标 性能指标： 为提升产品性能体验提出的性能指标，此处应包含性能指标项、性能指标、指标的大致实现策略等。 技术指标： 为提升产品竞争力提出的业务指标，此处应包含业务指标项、业务指标、指标的大致实现策略等。 二、框架设计 1. 整体框架图 展示应用整体的框架图，包括整体结构、应用分层、模块解耦、包括的关键技术点等。 2. 核心功能流程图 展示新增核心功能的流程图，帮助梳理整个功能的逻辑，减少编码过程中由于前期考虑不周导致的不稳定性和随机 bug。 三、资源需求 1. 商务资源 列出和其它公司或组织的商务合作，此处应写明关联功能、合作方等。 2. 内容资源 列出产品对内容资源的需求，此处应写明关联功能、内容资源需求等。 3. 技术选型 列出对新增核心功能的技术选型，此处应包含关联功能、技术选型结论、技术选型依据（详细的测试数据对比）等。 四、接口设计 1. 数据结构定义 输出需要新增的数据结构设计或数据库设计。 2. 后台接口定义 列出后台网络请求的接口定义，此处可简单列出所有接口及对应的关联功能，详细接口参数可在开发中与后台人员做详细设计与评审。 3. 对外接口定义 列出需要向其它模块提供的功能接口，应包括关联功能、接口接口及参数和大致的实现策略等。 4. 依赖接口约定 列出需要依赖其它模块提供的接口，应包括关联功能、接口及参数和大致的实现策略等。 五、风险及应对方案 列出开发相关的风险项：实现风险、资源风险、第三方合作风险、指标达成风险、进度风险等，以及针对各项风险的应对方案，有素质的开发人员都是问题的终结者，不能只抛出问题不想解决方案。 六、测试指导 列出项目的测试重点，以及对应的测试方案，以便为测试往正确的方向进行提供指导。对于有明确测试需求的地方，应重点提出来，比如针对指标的专项测试。详细应包含：测试分类、测试重点、测试指导、测试需求等。 七、时间安排 列出各领域的关键事项及时间节点，以便各领域协同、项目透明、进度可预知。 八、其它 列出概要设计模板中没有覆盖到，但对项目又非常重要的事项 整个 APP 概要设计的“范式”包括如上八大部分，为了保证概要设计的质量，应在概要设计的末尾提供如下的 CheckList，让设计人员签署每一项，确保都有考虑到。 序号 检查项 检查结果 1 文档中是否写明了对产品需求的理解和分析 2 文档中是否写明了对技术需求的理解和分析 3 文档中是否写明了版本的关键技术指标（性能指标、业务指标等） 4 文档中是否写明了对项目整体的框架设计 5 文档中是否输出了新增核心功能的流程图 6 文档中是否写明了对外部资源（商务合作、内容资源、开源项目等）的需求 7 文档中是否写明了对数据结构的设计 8 文档中是否写明了对接口的定义（后台接口、对外提供的接口、依赖其他模块的接口等） 9 文档中是否写明了风险项及应对方案 10 文档中是否写明了项目关键时间节点的安排 11 文档中是否明确了测试重点，并提供了测试指导 工作越久，经历的项目越多，就越发重视程序设计，特别是概要设计，他为正确高效工作指明了方向。上述的概要设计“范式”只是本人当下的理解，它会随着工作经验的积累不断更新，但终极目标不会变：将编程变成一项脑力劳动，写出高质量的程序、让自己成为靠谱的开发人员。 ","link":"https://faded.auspicious.space/post/how-to-do-a-good-general-design-of-app/"},{"title":"APP 开发流程规范","content":" app开发流程规范 一、主要流程 二、产品立项 工作概述 产品立项阶段亦称为准备阶段，该阶段主要基于需求大纲通过针对性的市场调研、用户访谈及竞品分析，尽可能的评估产品的核心功能，方向定位、目标用户群、成本投入和市场前景。在决策层评估通过的条件下，组建虚拟开发小组，协调资源，明确项目负责人及产品计划上线时间等事项。若为甲方需求的项目，可省略市场调研及商业价值评估的相关内容。 描绘远景，设定目标 产品的远景是什么？计划需要做什么实现这个远景？明确各个阶段的产品目标，为什么设定这样的目标？ 市场调研，竞品分析 通过针对性的市场调研和充分的竞品分析，测算产品市场前景和风险成本。 收集需求，排优先级 收集各业务市场部门反馈的需求意见，做典型用户的深度访谈，组相开发设计运营人员头脑风暴，明确产品核心功能和开发需求优先级。 组建团队，定负责人 依据产品定位和投入资源，组建合适的虚拟开发小组，指定项目负责人，团队相互熟悉各个岗位人员。 定期碰头，制定计划 商定项目相关人员定期碰头会，保持团队所有人最新需求信息同步，初步制定产品各个阶段完成时间节点。 成果 《竞品分析报告》、《产品立项说明书》、《产品BRD文档》 三、需求分析评审 工作概述 基于产品定位和运营策略，与产品各需求方进行深度的需求沟通，将抽象繁杂的需求整理分析成可落地执行的方案，召开需求评审，排定各功能点的开发优先级，规划产品各个版本迭代的功能计划表，设计产品原型，撰写产品需求说明书，与设计开发团队沟通确定各阶段的完成时间节点，明确产品实际上线时间，与市场运营团队沟通上线运营计划方案等。 需求分析，原型设计 与市场业务运营同事深度沟通，形成初步的需求大纲，功能列表，组织团队全员头脑风暴，分析需求的真伪及紧迫性，确定需求开发优先级，制定产品功能迭代计划表，设计产品原型初稿及页面结构图； 需求评审，确定方案 由产品经理牵头召开需求评审会议，向开发团队详细讲解产品逻辑流程和交互细节，评估技术实现的可行性。对不明确的需求做二次需求更新； 需求文档，开发周期 依据需求评审结果，修改设计最终版原型及交互，标注原型及撰写产品需求说明书，管理后台数据相关数据统计等需求，技术根据需求文档反馈每个阶段的完成时间节点。 成果 《产品PRD文档》、《产品交互原型稿》（低/高保真）、《产品开发进度计划表》 四、UI 界面设计 工作概述 基于原型交互稿及产品 PRD 文档设计产品页面效果图，与产品沟通确定详细的交互细节及效果。与需求业务方确定完善效果图设计最终版，依据开发需求进行效果图细节标注，设计产品 ICON 及应用市场审核宣传材料，配合市场运营部门设计产品运营活动页面等。 用户分析，设计梳理 收集相关资料分析目标用户的使用特征、情感、习惯、心理、需求等，基于 3W 法明确使用者，使用环境及使用方式； 素材收集，确定风格 在深度熟悉产品整体业务流程和商业需求的基础上，确定页面主辅色，制定交互方式，操作与跳转流程、结构、布局、信息和其他元素； 界面设计，规范输出 设计产品页面、图标、ICON，皮肤及一些界面交互的表现。与前端开发沟通，明确切图命名及标注规范，输出最终设计稿。 UE 测试，整体复盘 产品测试阶段包含 UE 测试，负责测试页面的还原度及交互的易用性，针对设计稿和需求文档提出测试反馈优化意见。产品上线发布后，全面复盘本次设计架构和细节，总结设计经验和优化迭代建议，并撰写相关的分析优化报告。 成果 《PSD源文件》、《切图源文件》、《交互描述及标注细节规范说明》 五、程序开发 工作概述 分为用户端、服务端两类开发。其中用户端开发，主流有 iOS 和 Android，依据需求文档和设计稿，实现前端页面的交互效果，与服务端确定数据交换接口协议。服务端开发依据需求文档，设计数据库表结构，评估核心复杂功能的实现方案，撰写开发设计概要文档及反馈重要功能的完成时间节点。 成果 《开发设计概要》、《接口协议文档》、《自测通过的产品1.0版》 六、测试验收 工作概述 参考产品需求文档和开发设计概要，撰写产品测试用例，召开用例讲解会，对产品全方位的进行测试，将测试不通过的内容反馈给开发，判定 bug 严重程度和跟进修复进度，评估产品上线发布的可行性，协助产品和业务人员撰写产品验收报告。 测试类型 功能性测试、容错性测试、性能效率测试、易用性测试、兼容性测试、压力测试 成果 《测试用例》、《测试bug反馈记录表》、《测试验收报告》 七、项目总结大会 项目完成之后，需要发项目参与的所有人员组织起来，总结项目过程中的问题，避免以后再次发生，个人觉得这点很重要。 ","link":"https://faded.auspicious.space/post/app-development-process-specification/"},{"title":"物联网 MQTT 服务质量级别","content":" 物联网 MQTT 服务质量级别 消息队列遥测传输（MQTT）是一种客户端服务器发布 / 订阅消息传输协议。它轻量，开放，简单，其设计也易于实施。这些特性使其非常适合用于很多情况，包括在网络连接受限的，需要代码长度较小且 / 或网络带宽非常重要的环境里面，例如在机器对机器（M2M）和物联网（IoT）环境中的通信。该协议通过 TCP / IP 或其他能提供有序，无损，双向的连接的网络协议运行。 MQTT 支持三种服务质量级别，如上图所示： 最多发送一次（发完就忘），也就是不确认； 至少发送一次，需要进行确认； 正好发送一次，要进行 4 步握手。 QoS（服务质量）定义了服务端（Broker） / 客户端（Client）确保能收到消息的工作或尝试的方式。消息可以以任何 QoS 级别发送，客户端也可以选择以任意 QoS 级别来订阅主题，后者选择的是他们能收到的最高 QoS 级别。例如，如有消息以 QoS 2 级别发布并且有一客户端以 Qos 0 级别订阅了相应主题，则那一客户端就会以 QoS 0 级别收到该消息。如果有第二个客户端也订阅了相同的主题，但用的是 QoS 2，则它将以 QoS 2 级别收到这一消息。举另外一个例子，如有一客户端以 QoS 2 订阅了一个主题，并且有一消息以 QoS 0 在相应主题上发布，则客户端将会基于 QoS 0 级别接收这一消息。高级的 QoS 会更可靠，但也会带来更高的延迟，并占用更多的带宽。每个 QoS 级别的一些细节就如下所示。MQTT 控制数据包内容的表格位于本文的最后部分，用于描述来自每个 QoS 流的控制数据包。 服务质量级别 0 该消息最多只发送一次，或者在通过网络的传送受阻的时候根本不发送。发送的消息不会被保存。如果客户端断开了连接，或者服务端出现了故障，该消息可能就会因此丢失。这也是最快的传输模式。MQTT 协议并没有要求服务器端将 QoS = 0 的发布消息转发给客户端。如果客户端在服务器收到发布的消息时断开了连接，则发布的消息可能会被丢弃，具体取决于服务器。遥测（MQXR）服务不会丢弃以 QoS = 0 发送的消息。它们会被作为非持久消息而保存，且只有在队列管理器停止运作时才会被丢弃。 在 QoS 0 传送协议中： 发送者：必须发送 QoS = 0，DUP = 0 的 PUBLISH 包； 接收者：在接收到 PUBLISH 包的同时也接受消息的所有权。 服务质量级别 1 该消息至少发送一次。如果发送方没有收到确认包，则会再次发送加上 DUP 标志的该消息，直到收到确认包为止。因此，接收者可能会把相同的消息发送好几次，并且也可能把它处理了好几遍。消息必须保存在发送者以及接收者的本地环境里面，直到这一消息被妥善处理为止。接收者在处理完消息后会把消息删掉。如果接收者是个服务端，则它会将把该消息发布给其订阅者作为对消息的处理。如果接收者是客户端，则会将把消息传递给作为订阅者的应用程序作为处理。在消息被删除之后，接收方会向发送方发送确认包。发送方在收到接收方的确认后会删掉保存在发送方的消息。 这个级别可以用于传送例如环境传感器这样的数据。在这种情况下，单个读数的传送失败了也没多大关系，因为传感器很快就会再把读数发送一遍。 在 QoS 1 传送协议中： 发送方： 必须在每次有新的应用消息发布时为其分配一个没被占用的包标识符。 必须发送一个 PUBLISH 包，其中包含 QoS = 1，DUP = 0 的包标识符。 必须将 PUBLISH 数据包视为 “未经确认” 的，直到它收到了接收方发来的，相应的 PUBACK 数据包为止。 一旦发送者收到 PUBACK 包，对应的消息的包标识符就能收回并重用。请注意，当发送方正在等待接收确认时，它可以用不同的包标识符发送更多的 PUBLISH 包。 接收方： 在接受了应用消息的所有权后，必须用包含传入 PUBLISH 包的包标识符的 PUBACK 包来进行响应。 在发送 PUBACK 包后，接收方必须把每一个传入的包含相同包标识符 PUBLISH 包视为一个全新的发布消息，而不管这些发布消息有没有加上 DUP 标志。 服务质量级别 2 该消息始终只发送一次。消息必须存储在发送方和接收方的本地环境中，直到它被妥善处理为止。QoS = 2 是最安全但也是最慢的传输模式。从发送方删掉消息之前，发送方和接收方之间至少需要两次相互的传输。在第一次传输后，接收方就可以开始处理这一消息。在第一次互传中，发送方会发送消息并从接收方拿到对这一消息的确认。如果发送方没有收到确认，则会再次发送加上了 DUP 标志的该消息，直至收到确认。在第二次互传中，发送方通过给接收方发送 PUBREL 消息来告知后者它可以完成对发布的消息的处理了。如果发送方没有收到接收方对 PUBREL 消息的确认，则会把 PUBREL 消息再发一遍，直到收到确认为止。当发送者收到对 PUBREL 消息的确认时，发送者就会删掉它保存的消息。接收者可以在第一或第二次互传的时候处理消息，只要它不把消息又重新处理一遍就可以了。如果接收者是服务端，它会将消息发布给订阅者。如果接收方是客户端，它会将消息传递给作为订阅者的应用程序。最后接收方会向发送方发送处理完成的消息，来表明它已完成了消息的处理。 举例来说，计费系统可以使用这个级别，因为消息的重复或丢失会导致这样的应用产生错误的计费。 在 QoS 2 传送协议中： 发送方： 必须在每次有新的应用消息发布时为其分配一个没被占用的包标识符。 必须发送一个包含 QoS = 2，DUP = 0 的包标识符的 PUBLISH 包。 必须将 PUBLISH 数据包视为 “未经确认” 的，直到它收到了接收方发来的，相应的 PUBREC 数据包为止。 当它从接收器收到一个 PUBREC 包时，必须发送一个 PUBREL 包。这个 PUBREL 包应该包含与原始 PUBLISH 分组相同的包标识符。 必须将 PUBREL 数据包视为 “未经确认” 的，直到它从接收方收到了相应的 PUBCOMP 包为止。 一旦发送了相应的 PUBREL 包，发送方就不能再发送原始的 PUBLISH 包。一旦发送者收到 PUBCOMP 包，包标识符就可以收回并重用。注意，当发送方正在等待接收确认时，它可以使用不同的包标识符发送更多的 PUBLISH 包。 接收方： 在接受了 PUBLISH 包的应用消息的所有权后，必须用包含传入 PUBLISH 包的包标识符的 PUBREC 包来进行响应。 在接收方收到相应的 PUBREL 包之前，它必须对每一个具有和传入 PUBLISH 包相同包标识符的后续 PUBLISH 包发送一个 PUBREC 包来进行确认。它绝不能容许把一个内容重复的消息传给任何位于下游的接收方。 在收到发送方发来的 PUBREL 包之后，它必须通过发送包含与 PUBREL 相同的包标识符的 PUBCOMP 包来响应。 发送 PUBCOMP 包后，接收方必须把任何后续的具有与传入 PUBLISH 包相同包标识符的后续 PUBLISH 包视为全新的发布消息。 MQTT 控制数据包的详述 控制包 发送方向 描述 CONNECT 客户端 -&gt; 服务端 客户端请求与服务端建立连接 CONNACK 服务端 -&gt; 客户端 连接成功建立 PUBLISH 客户端 -&gt; 服务端 / 服务端 -&gt; 客户端 发布消息 PUBACK 客户端 -&gt; 服务端 / 服务端 -&gt; 客户端 收到发布消息的确认 PUBREC 客户端 -&gt; 服务端 / 服务端 -&gt; 客户端 收到发布消息（Qos 2 的第二次握手） PUBREL 客户端 -&gt; 服务端 / 服务端 -&gt; 客户端 不再发布消息（Qos 2 的第三次握手） PUBCOMP 客户端 -&gt; 服务端 / 服务端 -&gt; 客户端 消息发布的完结（Qos 2 的第四次握手） SUBSCRIBE 客户端 -&gt; 服务端 客户端请求订阅某主题 SUBACK 服务端 -&gt; 客户端 订阅操作成功 UNSCBSCRIBE 客户端 -&gt; 服务端 客户端请求取消订阅某主题 UNSCBACK 服务端 -&gt; 客户端 取消订阅操作成功 PINGREQ 客户端 -&gt; 服务端 PING 请求 PINGRESP 服务端 -&gt; 客户端 PING 响应 DISCONNECT 客户端 -&gt; 服务端 客户端断开了与服务端的连接 ","link":"https://faded.auspicious.space/post/iot-mqtt-service-quality-level/"},{"title":"消息中间件选型分析","content":" 消息中间件选型分析 一、前言 消息队列中间件（简称消息中间件）是指利用高效可靠的消息传递机制进行与平台无关的数据交流，并基于数据通信来进行分布式系统的集成。通过提供消息传递和消息排队模型，它可以在分布式环境下提供应用解耦、弹性伸缩、冗余存储、流量削峰、异步通信、数据同步等等功能，其作为分布式系统架构中的一个重要组件，有着举足轻重的地位。 目前开源的消息中间件可谓是琳琅满目，能让大家耳熟能详的就有很多，比如 ActiveMQ、RabbitMQ、Kafka、RocketMQ、ZeroMQ 等。不管选择其中的哪一款，都会有用的不趁手的地方，毕竟不是为你量身定制的。有些大厂在长期的使用过程中积累了一定的经验，其消息队列的使用场景也相对稳定固化，或者目前市面上的消息中间件无法满足自身需求，并且也具备足够的精力和人力而选择自研来为自己量身打造一款消息中间件。但是绝大多数公司还是不会选择重复造轮子，那么选择一款合适自己的消息中间件显得尤为重要。就算是前者，那么在自研出稳定且可靠的相关产品之前还是会经历这样一个选型过程。 在整体架构中引入消息中间件，势必要考虑很多因素，比如成本及收益问题，怎么样才能达到最优的性价比？虽然消息中间件种类繁多，但是各自都有各自的侧重点，选择合适自己、扬长避短无疑是最好的方式。如果你对此感到无所适从，本文或许可以参考一二。 二、各类消息队列简述 ActiveMQ ActiveMQ 是 Apache 出品的、采用 Java 语言编写的完全基于 JMS1.1 规范的面向消息的中间件，为应用程序提供高效的、可扩展的、稳定的和安全的企业级消息通信。不过由于历史原因包袱太重，目前市场份额没有后面三种消息中间件多，其最新架构被命名为 Apollo，号称下一代 ActiveMQ，有兴趣的同学可行了解。 RabbitMQ RabbitMQ 是采用 Erlang 语言实现的 AMQP 协议的消息中间件，最初起源于金融系统，用于在分布式系统中存储转发消息。RabbitMQ 发展到今天，被越来越多的人认可，这和它在可靠性、可用性、扩展性、功能丰富等方面的卓越表现是分不开的。 Kafka Kafka 起初是由 LinkedIn 公司采用 Scala 语言开发的一个分布式、多分区、多副本且基于 Zookeeper 协调的分布式消息系统，现已捐献给 Apache 基金会。它是一种高吞吐量的分布式发布订阅消息系统，以可水平扩展和高吞吐率而被广泛使用。目前越来越多的开源分布式处理系统如 Cloudera、Apache Storm、Spark、Flink 等都支持与 Kafka 集成。 RocketMQ RocketMQ 是阿里开源的消息中间件，目前已经捐献个 Apache 基金会，它是由 Java 语言开发的，具备高吞吐量、高可用性、适合大规模分布式系统应用等特点，经历过双 11 的洗礼，实力不容小觑。 ZeroMQ ZeroMQ 号称史上最快的消息队列，基于 C 语言开发。ZeroMQ 是一个消息处理队列库，可在多线程、多内核和主机之间弹性伸缩，虽然大多数时候我们习惯将其归入消息队列家族之中，但是其和前面的几款有着本质的区别，ZeroMQ 本身就不是一个消息队列服务器，更像是一组底层网络通讯库，对原有的 Socket API 上加上一层封装而已。 目前市面上的消息中间件还有很多，比如腾讯系的 PhxQueue、CMQ、CKafka，又比如基于 Go 语言的 NSQ，有时人们也把类似 Redis 的产品也看做消息中间件的一种，当然它们都很优秀，但是本文篇幅限制无法穷极所有，下面会针对性的挑选 RabbitMQ 和 Kafka 两款典型的消息中间件来做分析，力求站在一个公平公正的立场来阐述消息中间件选型中的各个要点。 三、选型要点概述 衡量一款消息中间件是否符合需求需要从多个维度进行考察，首要的就是功能维度，这个直接决定了你能否最大程度上的实现开箱即用，进而缩短项目周期、降低成本等。如果一款消息中间件的功能达不到想要的功能，那么就需要进行二次开发，这样会增加项目的技术难度、复杂度以及增大项目周期等。 1. 功能维度 功能维度又可以划分个多个子维度，大致可以分为以下这些： 优先级队列 优先级队列不同于先进先出队列，优先级高的消息具备优先被消费的特权，这样可以为下游提供不同消息级别的保证。不过这个优先级也是需要有一个前提的：如果消费者的消费速度大于生产者的速度，并且消息中间件服务器（一般简单的称之为 Broker）中没有消息堆积，那么对于发送的消息设置优先级也就没有什么实质性的意义了，因为生产者刚发送完一条消息就被消费者消费了，那么就相当于 Broker 中至多只有一条消息，对于单条消息来说优先级是没有什么意义的。 延迟队列 当你在网上购物的时候是否会遇到这样的提示：“三十分钟之内未付款，订单自动取消”？这个是延迟队列的一种典型应用场景。延迟队列存储的是对应的延迟消息，所谓“延迟消息”是指当消息被发送以后，并不想让消费者立刻拿到消息，而是等待特定时间后，消费者才能拿到这个消息进行消费。延迟队列一般分为两种：基于消息的延迟和基于队列的延迟。基于消息的延迟是指为每条消息设置不同的延迟时间，那么每当队列中有新消息进入的时候就会重新根据延迟时间排序，当然这也会对性能造成极大的影响。实际应用中大多采用基于队列的延迟，设置不同延迟级别的队列，比如 5s、10s、30s、1min、5mins、10mins 等，每个队列中消息的延迟时间都是相同的，这样免去了延迟排序所要承受的性能之苦，通过一定的扫描策略（比如定时）即可投递超时的消息。 死信队列 由于某些原因消息无法被正确的投递，为了确保消息不会被无故的丢弃，一般将其置于一个特殊角色的队列，这个队列一般称之为死信队列。与此对应的还有一个“回退队列”的概念，试想如果消费者在消费时发生了异常，那么就不会对这一次消费进行确认（Ack）,进而发生回滚消息的操作之后消息始终会放在队列的顶部，然后不断被处理和回滚，导致队列陷入死循环。为了解决这个问题，可以为每个队列设置一个回退队列，它和死信队列都是为异常的处理提供的一种机制保障。实际情况下，回退队列的角色可以由死信队列和重试队列来扮演。 重试队列 重试队列其实可以看成是一种回退队列，具体指消费端消费消息失败时，为防止消息无故丢失而重新将消息回滚到 Broker 中。与回退队列不同的是重试队列一般分成多个重试等级，每个重试等级一般也会设置重新投递延时，重试次数越多投递延时就越大。举个例子：消息第一次消费失败入重试队列 Q1，Q1 的重新投递延迟为 5s，在 5s 过后重新投递该消息；如果消息再次消费失败则入重试队列 Q2，Q2 的重新投递延迟为 10s，在 10s 过后再次投递该消息。以此类推，重试越多次重新投递的时间就越久，为此需要设置一个上限，超过投递次数就入死信队列。重试队列与延迟队列有相同的地方，都是需要设置延迟级别，它们彼此的区别是：延迟队列动作由内部触发，重试队列动作由外部消费端触发；延迟队列作用一次，而重试队列的作用范围会向后传递。 消费模式 消费模式分为推（push）模式和拉（pull）模式。推模式是指由Broker主动推送消息至消费端，实时性较好，不过需要一定的流制机制来确保服务端推送过来的消息不会压垮消费端。而拉模式是指消费端主动向 Broker 端请求拉取（一般是定时或者定量）消息，实时性较推模式差，但是可以根据自身的处理能力而控制拉取的消息量。 广播消费 消息一般有两种传递模式：点对点（P2P，Point-to-Point）模式和发布/订阅（Pub/Sub）模式。对于点对点的模式而言，消息被消费以后，队列中不会再存储，所以消息消费者不可能消费到已经被消费的消息。虽然队列可以支持多个消费者，但是一条消息只会被一个消费者消费。发布订阅模式定义了如何向一个内容节点发布和订阅消息，这个内容节点称为主题（topic），主题可以认为是消息传递的中介，消息发布者将消息发布到某个主题，而消息订阅者则从主题中订阅消息。主题使得消息的订阅者与消息的发布者互相保持独立，不需要进行接触即可保证消息的传递，发布/订阅模式在消息的一对多广播时采用。RabbitMQ 是一种典型的点对点模式，而 Kafka 是一种典型的发布订阅模式。但是 RabbitMQ 中可以通过设置交换器类型来实现发布订阅模式而达到广播消费的效果，Kafka 中也能以点对点的形式消费，你完全可以把其消费组（consumer group）的概念看成是队列的概念。不过对比来说，Kafka 中因为有了消息回溯功能的存在，对于广播消费的力度支持比 RabbitMQ 的要强。 消息回溯 一般消息在消费完成之后就被处理了，之后再也不能消费到该条消息。消息回溯正好相反，是指消息在消费完成之后，还能消费到之前被消费掉的消息。对于消息而言，经常面临的问题是“消息丢失”，至于是真正由于消息中间件的缺陷丢失还是由于使用方的误用而丢失一般很难追查，如果消息中间件本身具备消息回溯功能的话，可以通过回溯消费复现“丢失的”消息进而查出问题的源头之所在。消息回溯的作用远不止与此，比如还有索引恢复、本地缓存重建，有些业务补偿方案也可以采用回溯的方式来实现。 消息堆积+持久化 流量削峰是消息中间件的一个非常重要的功能，而这个功能其实得益于其消息堆积能力。从某种意义上来讲，如果一个消息中间件不具备消息堆积的能力，那么就不能把它看做是一个合格的消息中间件。消息堆积分内存式堆积和磁盘式堆积。RabbitMQ 是典型的内存式堆积，但这并非绝对，在某些条件触发后会有换页动作来将内存中的消息换页到磁盘（换页动作会影响吞吐），或者直接使用惰性队列来将消息直接持久化至磁盘中。Kafka是一种典型的磁盘式堆积，所有的消息都存储在磁盘中。一般来说，磁盘的容量会比内存的容量要大得多，对于磁盘式的堆积其堆积能力就是整个磁盘的大小。从另外一个角度讲，消息堆积也为消息中间件提供了冗余存储的功能。援引纽约时报的案例（https://www.confluent.io/blog/publishing-apache-kafka-new-york-times/），其直接将 Kafka 用作存储系统。 消息追踪 对于分布式架构系统中的链路追踪（trace）而言，大家一定不会陌生。对于消息中间件而言，消息的链路追踪（以下简称消息追踪）同样重要。对于消息追踪最通俗的理解就是要知道消息从哪来，存在哪里以及发往哪里去。基于此功能下，我们可以对发送或者消费完的消息进行链路追踪服务，进而可以进行问题的快速定位与排查。 消息过滤 消息过滤是指按照既定的过滤规则为下游用户提供指定类别的消息。就以 Kafka 而言，完全可以将不同类别的消息发送至不同的 topic 中，由此可以实现某种意义的消息过滤，或者 Kafka 还可以根据分区对同一个 topic 中的消息进行分类。不过更加严格意义上的消息过滤应该是对既定的消息采取一定的方式按照一定的过滤规则进行过滤。同样以 Kafka 为例，可以通过客户端提供的 ConsumerInterceptor 接口或者 Kafka Stream 的 filter 功能进行消息过滤。 多租户 也可以称为多重租赁技术，是一种软件架构技术，主要用来实现多用户的环境下公用相同的系统或程序组件，并且仍可以确保各用户间数据的隔离性。RabbitMQ 就能够支持多租户技术，每一个租户表示为一个 vhost，其本质上是一个独立的小型 RabbitMQ 服务器，又有自己独立的队列、交换器及绑定关系等，并且它拥有自己独立的权限。vhost 就像是物理机中的虚拟机一样，它们在各个实例间提供逻辑上的分离，为不同程序安全保密地允许数据，它既能将同一个 RabbitMQ 中的众多客户区分开，又可以避免队列和交换器等命名冲突。 多协议支持 消息是信息的载体，为了让生产者和消费者都能理解所承载的信息（生产者需要知道如何构造消息，消费者需要知道如何解析消息），它们就需要按照一种统一的格式描述消息，这种统一的格式称之为消息协议。有效的消息一定具有某种格式，而没有格式的消息是没有意义的。一般消息层面的协议有 AMQP、MQTT、STOMP、XMPP 等（消息领域中的 JMS 更多的是一个规范而不是一个协议），支持的协议越多其应用范围就会越广，通用性越强，比如 RabbitMQ 能够支持 MQTT 协议就让其在物联网应用中获得一席之地。还有的消息中间件是基于其本身的私有协议运转的，典型的如 Kafka。 跨语言支持 对很多公司而言，其技术栈体系中会有多种编程语言，如 C/C++、JAVA、Go、PHP 等，消息中间件本身具备应用解耦的特性，如果能够进一步的支持多客户端语言，那么就可以将此特性的效能扩大。跨语言的支持力度也可以从侧面反映出一个消息中间件的流行程度。 流量控制 流量控制（flow control）针对的是发送方和接收方速度不匹配的问题，提供一种速度匹配服务抑制发送速率使接收方应用程序的读取速率与之相适应。通常的流控方法有 Stop-and-wait、滑动窗口以及令牌桶等。 消息顺序性 顾名思义，消息顺序性是指保证消息有序。这个功能有个很常见的应用场景就是 CDC（Change Data Chapture），以 MySQL 为例，如果其传输的 binlog 的顺序出错，比如原本是先对一条数据加 1，然后再乘以 2，发送错序之后就变成了先乘以 2 后加 1 了，造成了数据不一致。 安全机制 在 Kafka 0.9 版本之后就开始增加了身份认证和权限控制两种安全机制。身份认证是指客户端与服务端连接进行身份认证，包括客户端与 Broker 之间、Broker 与 Broker 之间、Broker 与 ZooKeeper 之间的连接认证，目前支持 SSL、SASL 等认证机制。权限控制是指对客户端的读写操作进行权限控制，包括对消息或 Kafka 集群操作权限控制。权限控制是可插拔的，并支持与外部的授权服务进行集成。对于 RabbitMQ 而言，其同样提供身份认证（TLS/SSL、SASL）和权限控制（读写操作）的安全机制。 消息幂等性 对于确保消息在生产者和消费者之间进行传输而言一般有三种传输保障（delivery guarantee）： At most once：至多一次，消息可能丢失，但绝不会重复传输； At least once：至少一次，消息绝不会丢，但是可能会重复； Exactly once：精确一次，每条消息肯定会被传输一次且仅一次。 对于大多数消息中间件而言，一般只提供 At most once 和 At least once 两种传输保障，对于第三种一般很难做到，由此消息幂等性也很难保证。 Kafka 自 0.11 版本开始引入了幂等性和事务，Kafka 的幂等性是指单个生产者对于单分区单会话的幂等，而事务可以保证原子性地写入到多个分区，即写入到多个分区的消息要么全部成功，要么全部回滚，这两个功能加起来可以让 Kafka 具备 EOS（Exactly Once Semantic）的能力。 不过如果要考虑全局的幂等，还需要与从上下游方面综合考虑，即关联业务层面，幂等处理本身也是业务层面所需要考虑的重要议题。以下游消费者层面为例，有可能消费者消费完一条消息之后没有来得及确认消息就发生异常，等到恢复之后又得重新消费原来消费过的那条消息，那么这种类型的消息幂等是无法有消息中间件层面来保证的。如果要保证全局的幂等，需要引入更多的外部资源来保证，比如以订单号作为唯一性标识，并且在下游设置一个去重表。 事务性消息 事务本身是一个并不陌生的词汇，事务是由事务开始（Begin Transaction）和事务结束（End Transaction）之间执行的全体操作组成。支持事务的消息中间件并不在少数，Kafka 和 RabbitMQ 都支持，不过此两者的事务是指生产者发生消息的事务，要么发送成功，要么发送失败。消息中间件可以作为用来实现分布式事务的一种手段，但其本身并不提供全局分布式事务的功能。 下表是对 Kafka 与 RabbitMQ 功能的总结性对比及补充说明。 功能项 Kafka（1.1.0版本） RabbitMQ（3.6.10版本） 优先级队列 不支持 支持。建议优先级大小设置在 0-10 之间。 延迟队列 不支持 支持 死信队列 不支持 支持 重试队列 不支持 不支持。RabbitMQ 中可以参考延迟队列实现一个重试队列，二次封装比较简单。如果要在 Kafka 中实现重试队列，首先得实现延迟队列的功能，相对比较复杂。 消费模式 推模式 推模式+拉模式 广播消费 支持。Kafka 对于广播消费的支持相对而言更加正统。 支持，但力度较 Kafka 弱。 消息回溯 支持。Kafka 支持按照 offset 和 timestamp 两种维度进行消息回溯。 不支持。RabbitMQ 中消息一旦被确认消费就会被标记删除。 消息堆积 支持 支持。一般情况下，内存堆积达到特定阈值时会影响其性能，但这不是绝对的。如果考虑到吞吐这因素，Kafka 的堆积效率比 RabbitMQ 总体上要高很多。 持久化 支持 支持 消息追踪 不支持。消息追踪可以通过外部系统来支持，但是支持粒度没有内置的细腻。 支持。RabbitMQ 中可以采用 Firehose 或者 rabbitmq_tracing 插件实现。不过开启 rabbitmq_tracing 插件件会大幅影响性能，不建议生产环境开启，反倒是可以使用 Firehose 与外部链路系统结合提供高细腻度的消息追踪支持。 消息过滤 客户端级别的支持 不支持。但是二次封装一下也非常简单。 多租户 不支持 支持 多协议支持 只支持定义协议，目前几个主流版本间存在兼容性问题。 RabbitMQ 本身就是 AMQP 协议的实现，同时支持 MQTT、STOMP 等协议。 跨语言支持 采用 Scala 和 Java 编写，支持多种语言的客户端。 采用 Erlang 编写，支持多种语言的客户端。 流量控制 支持 client 和 use r级别，通过主动设置可将流控作用于生产者或消费者。 RabbitMQ 的流控基于 Credit-Based 算法，是内部被动触发的保护机制，作用于生产者层面。 消息顺序性 支持单分区（partition）级别的顺序性。 顺序性的条件比较苛刻，需要单线程发送、单线程消费并且不采用延迟队列、优先级队列等一些高级功能，从某种意义上来说不算支持顺序性。 安全机制 （TLS/SSL、SASL）身份认证和（读写）权限控制 与Kafka相似 幂等性 支持单个生产者单分区单会话的幂等性。 不支持 事务性消息 支持 支持 2. 性能 功能维度是消息中间件选型中的一个重要的参考维度，但这并不是唯一的维度。有时候性能比功能还要重要，况且性能和功能很多时候是相悖的，鱼和熊掌不可兼得，Kafka 在开启幂等、事务功能的时候会使其性能降低，RabbitMQ 在开启 rabbitmq_tracing 插件的时候也会极大的影响其性能。消息中间件的性能一般是指其吞吐量，虽然从功能维度上来说，RabbitMQ 的优势要大于 Kafka，但是 Kafka 的吞吐量要比 RabbitMQ 高出 1 至 2 个数量级，一般 RabbitMQ 的单机 QPS 在万级别之内，而 Kafka 的单机 QPS 可以维持在十万级别，甚至可以达到百万级。 消息中间件的吞吐量始终会受到硬件层面的限制。就以网卡带宽为例，如果单机单网卡的带宽为 1 Gbps，如果要达到百万级的吞吐，那么消息体大小不得超过(1Gb/8)/100W，即约等于 134 B，换句话说如果消息体大小超过 134 B，那么就不可能达到百万级别的吞吐。这种计算方式同样可以适用于内存和磁盘。 时延作为性能维度的一个重要指标，却往往在消息中间件领域所被忽视，因为一般使用消息中间件的场景对时效性的要求并不是很高，如果要求时效性完全可以采用 RPC 的方式实现。消息中间件具备消息堆积的能力，消息堆积越大也就意味着端到端的时延也就越长，与此同时延时队列也是某些消息中间件的一大特色。那么为什么还要关注消息中间件的时延问题呢？消息中间件能够解耦系统，对于一个时延较低的消息中间件而言，它可以让上游生产者发送消息之后可以迅速的返回，也可以让消费者更加快速的获取到消息，在没有堆积的情况下可以让整体上下游的应用之间的级联动作更加高效，虽然不建议在时效性很高的场景下使用消息中间件，但是如果所使用的消息中间件的时延方面比较优秀，那么对于整体系统的性能将会是一个不小的提升。 3. 可靠性+可用性 消息丢失是使用消息中间件时所不得不面对的一个同点，其背后消息可靠性也是衡量消息中间件好坏的一个关键因素。尤其是在金融支付领域，消息可靠性尤为重要。然而说到可靠性必然要说到可用性，注意这两者之间的区别，消息中间件的可靠性是指对消息不丢失的保障程度；而消息中间件的可用性是指无故障运行的时间百分比，通常用几个 9 来衡量。 从狭义的角度来说，分布式系统架构是一致性协议理论的应用实现，对于消息可靠性和可用性而言也可以追溯到消息中间件背后的一致性协议。对于 Kafka 而言，其采用的是类似 PacificA 的一致性协议，通过 ISR（In-Sync-Replica）来保证多副本之间的同步，并且支持强一致性语义（通过 acks 实现）。对应的 RabbitMQ 是通过镜像环形队列实现多副本及强一致性语义的。多副本可以保证在 master 节点宕机异常之后可以提升 slave 作为新的 master 而继续提供服务来保障可用性。Kafka 设计之初是为日志处理而生，给人们留下了数据可靠性要求不要的不良印象，但是随着版本的升级优化，其可靠性得到极大的增强，详细可以参考 KIP101。就目前而言，在金融支付领域使用 RabbitMQ 居多，而在日志处理、大数据等方面 Kafka 使用居多，随着 RabbitMQ 性能的不断提升和 Kafka 可靠性的进一步增强，相信彼此都能在以前不擅长的领域分得一杯羹。 同步刷盘是增强一个组件可靠性的有效方式，消息中间件也不例外，Kafka 和 RabbitMQ 都可以支持同步刷盘，但是笔者对同步刷盘有一定的疑问：绝大多数情景下，一个组件的可靠性不应该由同步刷盘这种极其损耗性能的操作来保障，而是采用多副本的机制来保证。 这里还要提及的一个方面是扩展能力，这里我狭隘地将此归纳到可用性这一维度，消息中间件的扩展能力能够增强其用可用能力及范围，比如前面提到的 RabbitMQ 支持多种消息协议，这个就是基于其插件化的扩展实现。还有从集群部署上来讲，归功于 Kafka 的水平扩展能力，其基本上可以达到线性容量提升的水平，在 LinkedIn 实践介绍中就提及了有部署超过千台设备的 Kafka 集群。 5. 运维管理 在消息中间件的使用过程中难免会出现各式各样的异常情况，有客户端的，也有服务端的，那么怎样及时有效的进行监测及修复。业务线流量有峰值又低谷，尤其是电商领域，那么怎样前进行有效的容量评估，尤其是大促期间？脚踢电源、网线被挖等事件层出不穷，如何有效的做好异地多活？这些都离不开消息中间件的衍生产品——运维管理。 运维管理也可以进行进一步的细分，比如：申请、审核、监控、告警、管理、容灾、部署等。 申请、审核很好理解，在源头对资源进行管控，既可以进行有效校正应用方的使用规范，配和监控也可以做好流量统计与流量评估工作，一般申请、审核与公司内部系统交融性较大，不适合使用开源类的产品。 监控、告警也比较好理解，对消息中间件的使用进行全方位的监控，即可以为系统提供基准数据，也可以在检测到异常的情况配合告警，以便运维、开发人员的迅速介入。除了一般的监控项（比如硬件、GC等）之外，对于消息中间件还需要关注端到端时延、消息审计、消息堆积等方面。对于 RabbitMQ 而言，最正统的监控管理工具莫过于 rabbitmq_management 插件了，但是社区内还有 AppDynamics，Collectd，DataDog，Ganglia，Munin，Nagios，New Relic，Prometheus，Zenoss 等多种优秀的产品。Kafka 在此方面也毫不逊色，比如：Kafka Manager，Kafka Monitor，Kafka Offset Monitor，Burrow，Chaperone，Confluent Control Center 等产品，尤其是 Cruise 还可以提供自动化运维的功能。 不管是扩容、降级、版本升级、集群节点部署、还是故障处理都离不开管理工具的应用，一个配套完备的管理工具集可以在遇到变更时做到事半功倍。故障可大可小，一般是一些应用异常，也可以是机器掉电、网络异常、磁盘损坏等单机故障，这些故障单机房内的多副本足以应付。如果是机房故障就要涉及异地容灾了，关键点在于如何有效的进行数据复制，对于 Kafka 而言，可以参考 MirrorMarker、uReplicator 等产品，而 RabbitMQ 可以参考 Federation 和 Shovel。 6. 社区力度及生态发展 对于目前流行的编程语言而言，如 Java、Python，如果你在使用过程中遇到了一些异常，基本上可以通过搜索引擎的帮助来得到解决，因为一个产品用的人越多，踩过的坑也就越多，对应的解决方案也就越多。对于消息中间件也同样适用，如果你选择了一种“生僻”的消息中间件，可能在某些方面运用的得心应手，但是版本更新缓慢、遇到棘手问题也难以得到社区的支持而越陷越深；相反如果你选择了一种“流行”的消息中间件，其更新力度大，不仅可以迅速的弥补之前的不足，而且也能顺应技术的快速发展来变更一些新的功能，这样可以让你以“站在巨人的肩膀上”。在运维管理维度我们提及了 Kafka 和 RabbitMQ 都有一系列开源的监控管理产品，这些正是得益于其社区及生态的迅猛发展。 四、消息中间件选型误区探讨 在进行消息中间件选型之前可以先问自己一个问题：是否真的需要一个消息中间件？在搞清楚这个问题之后，还可以继续问自己一个问题：是否需要自己维护一套消息中间件？很多初创型公司为了节省成本会选择直接购买消息中间件有关的云服务，自己只需要关注收发消息即可，其余的都可以外包出去。 很多人面对消息中间件时会有一种自研的冲动，你完全可以对 Java 中的 ArrayBlockingQueue 做一个简单的封装，你也可以基于文件、数据库、Redis 等底层存储封装而形成一个消息中间件。消息中间件做为一个基础组件并没有想象中的那么简单，其背后还需要配套的管理运维整个生态的产品集。自研还有会交接问题，如果文档不齐全、运作不规范将会带给新人噩梦般的体验。是否真的有自研的必要？如果不是 KPI 的压迫可以先考虑下这 2 个问题：1. 目前市面上的消息中间件是否都真的无法满足目前业务需求？ 2. 团队是否有足够的能力、人力、财力、精力来支持自研？ 很多人在做消息中间件选型时会参考网络上的很多对比类的文章，但是其专业性、严谨性、以及其政治立场问题都有待考证，需要带着怀疑的态度去审视这些文章。比如有些文章会在没有任何限定条件及场景的情况下直接定义某款消息中间件最好，还有些文章没有指明消息中间件版本及测试环境就来做功能和性能对比分析，诸如此类的文章都可以唾弃之。 消息中间件犹如小马过河，选择合适的才最重要，这需要贴合自身的业务需求，技术服务于业务，大体上可以根据上一节所提及的功能、性能等 6 个维度来一一进行筛选。更深层次的抉择在于你能否掌握其魂，笔者鄙见：RabbitMQ 在于 routing，而 Kafka 在于 streaming，了解其根本对于自己能够对症下药选择到合适的消息中间件尤为重要。 消息中间件选型切忌一味的追求性能或者功能，性能可以优化，功能可以二次开发。如果要在功能和性能方面做一个抉择的话，那么首选性能，因为总体上来说性能优化的空间没有功能扩展的空间大。然而对于长期发展而言，生态又比性能以及功能都要重要。 很多时候，对于可靠性方面也容易存在一个误区：想要找到一个产品来保证消息的绝对可靠，很不幸的是这世界上没有绝对的东西，只能说尽量趋于完美。想要尽可能的保障消息的可靠性也并非单单只靠消息中间件本身，还要依赖于上下游，需要从生产端、服务端和消费端这 3 个维度去努力保证，《RabbitMQ 消息可靠性分析》这篇文章就从这 3 个维度去分析了 RabbitMQ 的可靠性。 消息中间件选型还有一个考量标准就是尽量贴合团队自身的技术栈体系，虽然说没有蹩脚的消息中间件只有蹩脚的程序员，但是让一个C栈的团队去深挖 PhxQueue 总比去深挖 Scala 编写的 Kafka 要容易的多。 五、总结 消息中间件大道至简：一发一存一消费，没有最好的消息中间件，只有最合适的消息中间件。 ","link":"https://faded.auspicious.space/post/analysis-of-message-middleware-selection/"},{"title":"26 部外国名著的经典开头","content":" 26部外国名著的经典开头 名著之所以是名著，是因为它无论经历怎样的风霜打磨，依然散发思想和艺术的魅力。不少文学名著中的第一句话、第一段话，往往定下了整部作品的基调，是整部著作的神韵所在。 《海的女儿》 ［丹麦］安徒生 在很远很远的海上，那里水像最美丽的矢车菊那么蓝，像水晶那么清澈，非常非常深，说实在的，深得没法用锚链来测量它的深度。 《彼得 • 潘》 ［英］詹姆斯 • 巴里 所有的孩子都会长大，只有一个例外。 《汤姆叔叔的小屋》 ［美］斯托夫人 2 月的某一天，天气依然比较寒冷。黄昏时分，在 P 城一间布置典雅兼作餐厅的接待室里，两位绅士相对而坐，喝着酒。他们没有要仆人在旁边侍候。他们紧挨着坐着，好像在商量什么很重要的事情。 《百年孤独》 ［哥伦比亚］ 加西亚 • 马尔克斯 很多年以后，奥雷连诺上校站在行刑队面前，准会想起父亲带他去参观冰块的那个遥远的下午。当时，马孔多是个二十户人家的村庄，一座座土房都盖在河岸上，河水清澈，沿着遍布石头的河床流去，河里的石头光滑、洁白，活像史前的巨蛋。 《哈克贝里 • 费恩历险记》 ［美］马克• 吐温 你要是没读过一本叫做《汤姆 • 索亚历险记》的书，你肯定不知道我是谁，不过这没关系， 这本书的作者是一个叫马克 • 吐温的人，他大多讲真话。 《呼啸山庄》 ［英］艾米莉 • 勃朗特 1801 年。我刚刚拜访过我的房东回来——就是那个将要给我惹麻烦的孤独的邻居。 《飘》 ［美］玛格丽特 • 米切尔 斯佳丽 • 奥哈拉长得并不漂亮， 但是男人们像塔尔顿家那对孪生兄弟为她的魅力所迷住时，就不会这样想了。她脸上有着两种特征。一种是她母亲的娇柔，来自法兰西血统的海滨贵族：一种是她父亲的粗犷，来自浮华俗气的爱尔兰人，这两种特征混在一起显得不太协调，但这张脸上尖尖的下巴和四方的牙床骨，是很引人注意的，她那双淡绿色的眼睛纯净得没有一丝褐色， 配上乌黑的睫毛和翘起的眼角，显得韵味十足，上面是两条墨黑的浓眉斜在那里，给她木兰花般白晳的肌肤划上十分分明的斜线，这样白皙的皮肤对南方妇女是极其珍贵的。她们常常用帽子、面纱和手套把皮肤保护起来，以防受到佐治亚炎热太阳的暴晒。 《堂 • 吉诃德》 ［西班牙］塞万提斯 曼查有个地方，地名就不用提了，不久前住着一位贵族。他那类贵族，矛架上有一支长矛，还有一面皮盾、一匹瘦马和一只猎兔狗。锅里牛肉比羊肉多，晚餐常吃凉拌肉丁，星期六吃脂油煎鸡蛋，星期五吃扁豆，星期日加一只野雏鸽，这就用去了他四分之三的收入，其余的钱买了节日穿的黑呢外套、长毛绒袜子和平底鞋，而平时，他总是得意洋洋地穿着上好的棕色粗呢衣。家里有一个四十多岁的女管家，一个不到二十岁的外甥女，还有一个能种地、能采购的小伙子，为他备马、修剪树枝。 《复活》 ［俄］列夫 • 托尔斯泰 尽管好几十万人聚居在一小块地方，竭力把土地糟蹋得面目全非，尽管他们肆意把石头砸进地里， 不让花草树木生长，尽管他们除尽刚出土的小草，把煤炭和石油烧得烟雾腾腾，尽管他们滥伐树木，驱逐鸟兽，在城市里，春天毕竟还是春天。 《安娜• 卡列尼娜》 ［俄］列夫 • 托尔斯泰 幸福的家庭皆有相似，不幸的家庭各个不同。 《追风筝的人》 ［美］卡勒德 • 胡塞尼 我成为今天的我，是在 1975 年某个阴云密布的寒冷冬日，那年我十二岁。我清楚地记得当时自己趴在一堵坍塌的泥墙后面，窥视着那条小巷，旁边是结冰的小溪。许多年过去了，人们说陈年旧事可以被埋葬，然而我终于明白这是错的，因为往事会自行爬上来。回首前尘，我意识到在过去二十六年里，自己始终在窥视着那荒芜的小径。 《瓦尔登湖》 ［美］亨利 • 戴维 • 梭罗 当我写后面那些篇页，或者后面那一大堆文字的时候， 我是在孤独地生活着，在森林中，在马萨诸塞州的康科德城，瓦尔登湖的湖岸上，在我亲手建筑的木屋里，距离任何邻居一英里，只靠着我双手劳动，养活我自己。在那里，我住了两年又两个月。目前，我又是文明生活中的过客了。 《变形记》 ［奥地利］卡夫卡 一天早晨， 格里高尔 • 萨姆沙从不安的睡梦中醒来，发现自己躺在床上变成了一只巨大的甲虫。 《包法利夫人》 ［法］福楼拜 我们在自修室上课，校长进来了，后面跟着个没穿制服的新生，还有个校工端着张大课桌。打瞌睡的同学惊醒过来，全班起立，仿佛刚才大家都只顾用功似的。 《罪与罚》 ［俄］陀思妥耶夫斯基 七月初的一个酷热异常的傍晚，有个青年从自己的斗室里走了出来，这间斗室是他在S 胡同里向二房东租来的。他走到街上，便慢悠悠地、仿佛踌躇不决地向 K 桥走去。 《红与黑》 ［法］司汤达 维里业称得上是弗兰什- 孔泰地区风光旖旎的一座小城。白色的房子，尖顶红瓦，撒落在一个小山坡上。茂密遒劲的栗子树，郁郁苍苍，随地形而逶迤起伏。杜河在城下数百尺外流过。城墙昔时为西班牙人所建，如今只剩下断壁残垣。 《了不起的盖茨比》 ［美］弗 • 司各特 • 菲茨杰拉德 我年纪还轻、阅历不深的时候，我父亲教导过我一句话， 我至今还念念不忘。“每逢你想要批评任何人的时候，”他对我说，“你就记住， 这个世界上所有的人，并不是个个都有过你拥有的那些优越条件。” 《老人与海》 ［美］海明威 他是个独自在湾流中一条小船上钓鱼的老人，至今已去了八十四天，一条鱼也没逮住。 《刀锋》 ［英］毛姆 我以前写小说从没有像写这一本更感到惶惑过。我叫它做小说，只是因为除了小说以外，想不出能叫它做什么。故事是几乎没有可述的，结局既不是死，也不是结婚。死是一切的了结，所以是一个故事的总收场，但是，用结婚来结束也很合适；那些世俗的所谓大团圆，自命风雅的人也犯不着加以鄙弃。普通人有一种本能，总相信这么一来， 一切该交代的都交代了。男的女的，不论经过怎样的悲欢离合，终于被撮合在一起，两性的生物功能已经完成，兴趣也就转移到未来的一代上去。可是， 我写到末尾， 还是使读者摸不着边际…… 《乞力马扎罗的雪》 ［美］海明威 乞力马扎罗是一座海拔一万九千七百一十英尺的长年积雪的高山， 据说它是非洲最高的一座山。 西高峰叫马塞人的“鄂阿奇—鄂阿伊”， 即上帝的庙殿。在西高峰的近旁， 有一具已经冻僵风干的豹子的尸体。豹子到这样高寒的地方来寻找什么，没有人作过解释。 《荆棘鸟》 ［澳］考琳 • 麦卡洛 有一个传说，说的是有那么一只鸟儿，它一生只唱一次，那歌声比世上所有一切生灵的歌声都更加优美动听。从离开巢窝的那一刻起，它就在寻找着荆棘树，直到如愿以偿，才歇息下来。然后，它把自己的身体扎进最长、最尖的荆棘上，便在那荒蛮的枝条之间放开了歌喉。在奄奄一息的时刻，它超脱了自身的痛苦，而那歌声竟然使云雀和夜莺都黯然失色。这是一曲无比美好的歌，曲终而命竭。然而，整个世界都在静静地谛听着，上帝也在苍穹中微笑。因为最美好的东西只能用最深痛的巨创来换取……这就是荆棘鸟的传说。 《双城记》 ［英］查尔斯 • 狄更斯 那是最美好的时代，那是最糟糕的时代； 那是智慧的年头， 那是愚昧的年头；那是信仰的时期，那是怀疑的时期；那是光明的季节，那是黑暗的季节；那是希望的春天，那是失望的冬天；我们全都在直奔天堂，我们全都在直奔相反的方向——简而言之，那时跟现在非常相像，某些最喧嚣的权威坚持要用形容词的最高级来形容它。说它好， 是最高级的； 说它不好，也是最高级的。 《简·爱》 ［英］夏洛蒂 · 勃朗特 那天，出去散步是不可能了。其实，早上我们还在光秃秃的灌木林中溜达了一个小时，但从午饭时起（无客造访时，里德太太很早就用午饭）便刮起了冬日凛冽的寒风，随后阴云密布，大雨滂沱，室外的活动也就只能作罢了。 《蝴蝶梦》 ［英］达夫妮 · 杜穆里埃 昨晚，我梦见自己又回到了曼陀丽庄园。 《茶花女》 ［法］小仲马 我认为只有深刻地研究过人，才能创造出人物，如同只有认真地学习了一种语言才能讲它一样。 《局外人》 ［法］阿尔贝 · 加缪 今天，妈妈死了。也许是昨天，我不知道。我收到养老院的一封电报，说：“母死。明日葬。专此通知。”这说明不了什么。可能是昨天死的。 ","link":"https://faded.auspicious.space/post/26-classic-openings-of-famous-foreign-books/"},{"title":"红楼梦人物归类","content":" 红楼梦人物归类 金陵十二金钗 林黛玉、薛宝钗、贾元春、贾迎春、贾探春、贾惜春、李 纨、妙 玉、史湘云、王熙凤、 贾巧姐、秦可卿。 十二丫环 晴雯、麝月、袭人、鸳鸯、雪雁、紫鹃、碧痕、平儿、香菱、金钏、司棋、抱琴。 十二家人 赖大、焦大、王善保、周 瑞、林之孝、乌进孝、包勇、吴贵、吴新登、邓好时、王柱儿、余 信。 十二儿 庆儿、昭儿、兴儿、隆儿、坠儿、喜儿、寿儿、丰儿、住儿、小舍儿、李十儿、玉柱儿。 十二贾氏 贾敬、贾赦、贾政、贾宝玉、贾琏、贾珍、贾环、贾蓉、贾兰、贾芸、贾蔷、贾芹。 十二官 琪官、芳官、藕官、蕊官、药官、玉官、宝官、龄官、茄官、艾官、豆官、葵官。 七尼 妙玉、智能、智通、智善、圆信、大色空、净虚。 七彩 彩屏、彩儿、彩凤、彩霞、彩鸾、彩明、彩云。 四春 贾元春、贾迎春、贾探春、贾惜春。 四宝 贾宝玉、甄宝玉、薛宝钗、薛宝琴。 四薛 薛蟠、薛蝌、薛宝钗、薛宝琴。 四王 王夫人、王熙凤、王子腾、王仁。 四 尤 尤老娘、尤氏、尤二姐、尤三姐。 四草辈 贾蓉、贾兰、贾芸、贾芹。 四玉辈 贾珍、贾琏、贾环、贾瑞。 四文辈 贾敬、贾赦、贾政、贾敏。 四代辈 贾代儒、贾代化、贾代修、贾代善。 四烈婢 晴雯、金钏、鸳鸯、司棋。 四清客 詹光、单聘仁、程日兴、王作梅。 四无辜 石呆子、张华、冯渊、张金哥。 四小厮 茗烟、扫红、锄药、伴鹤。 四小 小鹊、小红、小蝉、小舍儿。 四婆子 刘姥姥、马道婆、宋嬷嬷、张妈妈。 四情友 秦锺、蒋玉菡、柳湘莲、东平王。 四壮客 乌进孝、冷子兴、山子野、方椿。 四宦官 载权、夏秉忠、周太监、裘世安。 文房四宝 抱琴、司棋、侍画、入画。 四珍宝 珍珠、琥珀、玻璃、翡翠。 一主三仆 史湘云 翠缕、笑儿、篆儿。 贾探春 侍画、翠墨、小蝉。 贾宝玉 茗烟、袭人、晴雯。 林黛玉 紫鹃、雪雁、春纤。 贾惜春 入画、彩屏、彩儿。 贾迎春 彩凤、彩云、彩霞。 ","link":"https://faded.auspicious.space/post/hong-lou-meng-character-classification/"},{"title":"设计模式——代码如若初相见","content":" 设计模式——代码如若初相见 一、何为设计模式 设计模式不是一种方法和技术，而是一种思想。 设计模式（Design pattern）是一套被反复使用、多数人知晓的、经过分类编目的、代码设计经验的总结。 设计模式和具体的语言无关，学习设计模式就是要建立面向对象的思想，尽可能的面向接口编程，低耦合，高内聚，使设计的程序可复用。学习设计模式能够促进对面向对象思想的理解，反之亦然。它们相辅相成。 二、设计模式的用途 使用设计模式的目的： 为了代码可重用性、让代码更容易被他人理解、保证代码可靠性。 设计模式使代码编写真正工程化；设计模式是软件工程的基石脉络，如同大厦的结构一样。 三、六大设计原则 在实际的开发中，我们要想更深入的了解面向对象思想，就必须熟悉前人总结过的面向对象的思想的设计原则，以下六大设计原则也是学习23种设计模式的前提。 1、单一职责原则 其实就是开发人员经常说的“高内聚，低耦合”。也就是说，每个类应该只有一个职责，对外只能提供一种功能，而引起类变化的原因应该只有一个。在设计模式中，所有的设计模式都遵循这一原则。 2、开闭原则 核心思想是：一个对象对扩展开放，对修改关闭。 其实开闭原则的意思就是：对类的改动是通过增加代码进行的，而不是修改现有代码。 也就是说软件开发人员一旦写出了可以运行的代码，就不应该去改动它，而是要保证它能一直运行下去，如何能够做到这一点呢?这就需要借助于抽象和多态，即把可能变化的内容抽象出来，从而使抽象的部分是相对稳定的，而具体的实现则是可以改变和扩展的。 3、里氏替换原则 核心思想：在任何父类出现的地方都可以用它的子类来替代。 其实就是说：同一个继承体系中的对象应该有共同的行为特征。 4、依赖注入原则 核心思想：要依赖于抽象，不要依赖于具体实现。 其实就是说：在应用程序中，所有的类如果使用或依赖于其他的类，则应该依赖这些其他类的抽象类，而不是这些其他类的具体类。为了实现这一原则，就要求我们在编程的时候针对抽象类或者接口编程，而不是针对具体实现编程。 5、接口分离原则 核心思想：不应该强迫程序依赖它们不需要使用的方法。 其实就是说：一个接口不需要提供太多的行为，一个接口应该只提供一种对外的功能，不应该把所有的操作都封装到一个接口中。 6、迪米特原则 核心思想：一个对象应当对其他对象尽可能少的了解。 其实就是说：降低各个对象之间的耦合，提高系统的可维护性。在模块之间应该只通过接口编程，而不理会模块的内部工作原理，它可以使各个模块耦合度降到最低，促进软件的复用。 四、三大设计模式类型 设计模式分三大类型： 创建型模式 （对象的创建）5种： 工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。 结构型模式 （对象的组成(结构)）7种： 适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。 行为型模式 （对象的行为）11种： 策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。 ","link":"https://faded.auspicious.space/post/design-pattern-as-the-first-time-we-meet/"},{"title":"协程简介","content":" 说一说协程 首先，我们了解一下进程，线程和协程三个概念之间的区别。 进程，线程，协程区别 进程： 拥有自己独立的堆和栈，既不共享堆，亦不共享栈，进程由操作系统调度。 线程： 拥有自己独立的栈和共享的堆，共享堆，不共享栈，线程亦由操作系统调度。 协程： 和线程一样共享堆，不共享栈，协程由程序员在协程的代码里显示调度。 协程的优势 实现协程的语言，主要以 Python 和 Go 语言为主，当然 Java 也有协程的第三方库，但生产环境使用的不多，典型的协程实现还是以 Go 语言为代表的，下面我们以 Go 语言来说明协程的优势。 内存消耗： 每个 goroutine（协程） 默认占用内存远比 Java 、C 的线程少。 我们知道，线程是有固定的栈的，基本都是 2 MB，当然，不同系统可能大小不太一样，但是的确都是固定分配的。这个栈用于保存局部变量，用于在函数切换时使用。但是对于 goroutine 这种轻量级的协程来说，一个大小固定的栈可能会导致资源浪费：比如一个协程里面只 print 了一个语句，那么栈基本没怎么用；当然，也有可能嵌套调用很深，那么可能也不够用。 所以 Go 采用了动态扩张收缩的策略：初始化为 2 KB，最大可扩张到 1 GB。 切换调度开销方面，goroutine 远比线程小。 协程和线程的区别在于：线程切换需要陷入内核，然后进行上下文切换，而协程在用户态由协程调度器完成，不需要陷入内核，这代价就小了；另外，协程的切换时间点是由调度器决定的，而不是系统内核决定的。 多线程编程的槽点 线程，是操作系统的内核对象，多线程编程时，如果线程数过多，就会导致频繁的上下文切换，这些 CPU 时间是一个额外的耗费。所以在一些高并发的网络服务器编程中，使用一个线程服务一个 Socket 连接是很不明智的。于是操作系统提供了基于事件模式的异步编程模型。用少量的线程来服务大量的网络连接和 I/O 操作。但是采用异步和基于事件的编程模型，复杂化了程序代码的编写，非常容易出错。因为线程穿插，也提高排查错误的难度。 协程，是在应用层模拟的线程，他避免了上下文切换的额外耗费，兼顾了多线程的优点。简化了高并发程序的复杂度。举个例子，一个高并发的网络服务器，每一个 Socket 连接进来，服务器用一个协程来对他进行服务。代码非常清晰。而且兼顾了性能。 协程底层实现原理 协程（Coroutine）是在 1963 年由 Melvin E. Conway USAF，Bedford，MA 等人提出的一个概念，而且协程的概念是早于线程（Thread）提出的，它是一种非抢占式的线程调度。【参考 线程的调度 · 协同式调度】 协程和线程的原理是一样的，当 a 线程 切换到 b 线程 的时候，需要将 a 线程 的相关执行进度压入栈，然后将 b 线程 的执行进度出栈，进入 b 线程 的执行序列。协程只不过是在应用层实现这一点。但是，协程并不是由操作系统调度的，而且应用程序也没有能力和权限执行 CPU 调度。怎么解决这个问题？ 答案是，协程是基于线程的。内部实现上，维护了一组数据结构和 n 个线程，真正的执行还是线程，协程执行的代码被扔进一个待执行队列中，由这 n 个线程从队列中拉出来执行。这就解决了协程的执行问题。那么协程是又是怎么切换的呢？ Golang 对各种 IO函数 进行了封装，这些封装的函数提供给应用程序使用，而其内部调用了操作系统的异步 IO函数，当这些异步函数返回 busy 或 bloking 时，Golang 利用这个时机将现有的执行序列压栈，让线程去拉另外一个协程的代码来执行，基本原理就是这样，利用并封装了操作系统的异步函数。包括 Linux 的 epoll、select 和 Windows 的 iocp、event 等。 尽管，在任务调度上，协程是弱于线程的。但是在资源消耗上，协程则是极低的。一个线程的内存在 MB 级别，而协程只需要 KB 级别。而且线程的调度需要内核态与用户的频繁切入切出，资源消耗也不小。 至此，我们把协程的基本特点归纳为： 协程调度机制无法实现公平调度。 协程的资源开销是非常低的，一台普通的服务器就可以支持百万协程。 Golang 协程的应用 我们知道，协程（coroutine）是 Go 语言中的轻量级线程实现，由 Go 运行时（runtime）管理。 go 关键字 go 关键字用来创建 goroutine（协程），是实现并发的关键。go 关键字的用法如下： // go 关键字放在方法调用前新建一个 goroutine 并让他执行方法体 go GetThingDone(param1, param2); //上例的变种，新建一个匿名方法并执行 go func(param1, param2) { }(val1, val2) //直接新建一个 goroutine 并在 goroutine 中执行代码块 go { //do someting... } 在一个函数调用前加上 go 关键字，这次调用就会在一个新的 goroutine 中并发执行。当被调用的函数返回时，这个 goroutine 也自动结束。需要注意的是，如果这个函数有返回值，那么这个返回值会被丢弃。 先看一下下面的程序代码： func Add(x, y int) { z := x + y fmt.Println(z) } func main() { for i:=0; i&lt;10; i++ { go Add(i, i) } } 执行上面的代码，会发现屏幕什么也没打印出来，程序就退出了。 对于上面的例子，main() 函数启动了 10 个 goroutine，然后返回，这时程序就退出了，而被启动的执行 Add() 的 goroutine 没来得及执行。我们想要让 main() 函数等待所有 goroutine 退出后再返回，但如何知道 goroutine 都退出了呢？这就引出了多个 goroutine 之间通信的问题。 在工程上，有两种最常见的并发通信模型：共享内存和消息传递【参考：并发编程模型的分类】；Go 语言主要使用消息机制 channel 来作为通信模型。 channel 消息机制认为每个并发单元是自包含的、独立的个体，并且都有自己的变量，但在不同并发单元间这些变量不共享。每个并发单元的输入和输出只有一种，那就是消息。 channel 是 Go 语言在语言级别提供的 goroutine 间的通信方式，我们可以使用 channel 在多个 goroutine 之间传递消息。channel 是进程内的通信方式，因此通过 channel 传递对象的过程和调用函数时的参数传递行为比较一致，比如也可以传递指针等。channel 是类型相关的，一个 channel 只能传递一种类型的值，这个类型需要在声明 channel 时指定。 CSP模型 要想理解 channel 要先知道 CSP 模型。CSP 是 Communicating Sequential Process 的简称，中文可以叫做通信顺序进程，是一种并发编程模型，由 Tony Hoare 于 1977 年提出。简单来说，CSP 模型由并发执行的实体（线程或者进程）所组成，实体之间通过发送消息进行通信，这里发送消息时使用的就是通道，或者叫 channel。CSP 模型的关键是关注 channel，而不关注发送消息的实体。Go 语言实现了 CSP 部分理论，goroutine 对应 CSP 中并发执行的实体，channel 也就对应着 CSP 中的 channel。 channel 典型用法 channel 的声明形式为：var chanName chan ElementType 声明一个传递 int 类型的 channel：var ch chan int 使用内置函数 make() 定义一个 channel：ch := make(chan int) 在 channel 的用法中，最常见的包括写入和读出： // 将一个数据value写入至channel，这会导致阻塞，直到有其他goroutine从这个channel中读取数据 ch &lt;- value // 从channel中读取数据，如果channel之前没有写入数据，也会导致阻塞，直到channel中被写入数据为止 value := &lt;-ch 默认情况下，channel 的接收和发送都是阻塞的，除非另一端已准备好。 我们还可以创建一个带缓冲的 channel： ch := make(chan int, 1024) // 从带缓冲的channel中读数据 for i:=range ch { ... } 此时，创建一个大小为 1024 的 int 类型的 channel，即使没有读取方，写入方也可以一直往 channel 里写入，在缓冲区被填完之前都不会阻塞。 可以关闭不再使用的 channel：close(ch) 应该在生产者的地方关闭 channel，如果在消费者的地方关闭，容易引起 panic； 一个非阻塞简单示例 阻塞的意思是调用方在被调用的代码返回之前必须一直等待，不能处理别的事情。而非阻塞调用则不用等待，调用之后立刻返回。那么返回值如何获取呢？ Node.js 使用的是回调的方式，Golang 使用的是 channel。 /** * 每次调用方法会新建一个 channel : resultChan， * 同时新建一个 goroutine 来发起 http 请求并获取结果。 * 获取到结果之后 goroutine 会将结果写入到 resultChan。 */ func UnblockGet(requestUrl string) chan string { resultChan := make(chan string) go func() { request := httplib.Get(requestUrl) content, err := request.String() if err != nil { content = &quot;&quot; + err.Error() } resultChan &lt;- content } () return resultChan } fmt.Println(time.Now()) resultChan1 := UnblockGet(&quot;http://127.0.0.1/test.php?i=1&quot;) resultChan2 := UnblockGet(&quot;http://127.0.0.1/test.php?i=2&quot;) fmt.Println(&lt;-resultChan1) fmt.Println(&lt;-resultChan1) fmt.Println(time.Now()) 上面两个 HTTP 请求是在两个 goroutine 中并行的。总的执行时间小于两个请求时间和。 ","link":"https://faded.auspicious.space/post/a-introduction-to-coroutine/"},{"title":"curl 的用法","content":" curl 模拟请求 一般情况下我们会在网页上请求后台接口，但是对于需要进行多次测试的人来说，每一次都要在网页上模拟请求，是存在很大局限性的。因此，我们需要学会模拟请求，以达到跟实际请求一样的效果。 curl 的用法 curl [options] [params] {网页url} options = -v 显示详细的请求信息 curl -v www.baidu.com options = -X 指定请求方式 GET请求： curl -X GET https://www.baidu.com/ POST请求： curl -X POST -d &quot;data=123&amp;key=456&quot; http://localhost:8080/search 由于 -d 选项为使用 POST 方式向 server 发送数据，因此在使用 -d 的时候，可以省略 -X POST。使用 -d 时，将使用 Content-type:application/x-www-form-urlencoded 方式发送数据： curl -d &quot;q=三生三世&amp;type=note&amp;page=1&amp;order_by=default&quot; https://www.jianshu.com/search/do\\?q\\=%E4%B8%89%E7%94%9F%E4%B8%89%E4%B8%96\\&amp;type\\=note\\&amp;page\\=1\\&amp;order_by\\=default 如果想使用 JSON 形式 post 数据，可以使用 -H 指定头部类型： curl -H &quot;Content-Type:application/json&quot; -d '{&quot;data&quot;:&quot;123&quot;,&quot;key&quot;:&quot;456&quot;}' http://localhost:8080/search 如果想在请求的时候带上 Cookie，可以这样： curl -H &quot;Cookie:username=XXX&quot; {URL} options = -H 增加头部信息 curl -H &quot;Cookie:username=XXX&quot; {URL} options = -c 存储 cookie 到文件 curl -d &quot;name=zhangsan&amp;password=123&quot; http://localhost:8080/login -c ./cookie 使用用户名和密码登录系统，并将 cookie 信息存储在当前目录的 cookie 文件中。 options = -b 携带 cookie 文件 curl http://localhost:8080/login -b ./cookie options = --cookie 直接指定 cookie curl --cookie &quot;name=zhangsan&quot; http://localhost:8080/login options = -F/--form 表单提交操作 curl 可以通过 -F 命令来以 Content-Type:multipart/form-data 的形式向 server post 数据，该命令允许提交二进制文件等。可以使用 @ 前缀来制定提交的内容为一个文件，也可以使用 &lt; 符号来提交文件中的内容： curl -F profile=@portrait.jpg https://example.com/upload.cgi 向服务器上传一个图片，图片的表单域名为 profile，内容为 protrait.jpg 的二进制。 ","link":"https://faded.auspicious.space/post/how-to-use-curl/"},{"title":"网页渲染原理与优化","content":" 还在为网页渲染性能优化而苦恼吗？ 1 渲染原理 在讨论性能优化之前，我们有必要了解一些浏览器的渲染原理。不同的浏览器进行渲染有着不同的实现方式，但是大体流程都是差不多的，我们通过 Chrome 浏览器来大致了解一下这个渲染流程。 1.1 关键渲染路径 关键渲染路径是指浏览器将 HTML、CSS 和 JavaScript 转换成实际运作的网站必须采取的一系列步骤，通过渲染流程图我们可以大致概括如下： 处理 HTML 并构建 DOM Tree。 处理 CSS 并构建 CSSOM Tree。 将 DOM Tree 和 CSSOM Tree 合并成 Render Object Tree。 根据 Render Object Tree 计算节点的几何信息并以此进行布局。 绘制页面需要先构建 Render Layer Tree 以便用正确的顺序展示页面，这棵树的生成与 Render Object Tree 的构建同步进行。然后还要构建 Graphics Layer Tree 来避免不必要的绘制和使用硬件加速渲染，最终才能在屏幕上展示页面。 1.2 DOM Tree DOM（Document Object Model——文档对象模型）是用来呈现以及与任意 HTML 或 XML 交互的 API 文档。DOM 是载入到浏览器中的文档模型，它用节点树的形式来表现文档，每个节点代表文档的构成部分。 需要说明的是 DOM 只是构建了文档标记的属性和关系，并没有说明元素需要呈现的样式，这需要 CSSOM 来处理。 1.2.1 构建流程 获取到 HTML 字节数据后，会通过以下流程构建 DOM Tree： 编码：HTML 原始字节数据转换为文件指定编码的字符串。 词法分析（标记化）：对输入字符串进行逐字扫描，根据 构词规则 识别单词和符号，分割成一个个我们可以理解的词汇（学名叫 Token）的过程。 语法分析（解析器）：对 Tokens 应用 HTML 的语法规则，进行配对标记、确立节点关系和绑定属性等操作，从而构建 DOM Tree 的过程。 词法分析和语法分析在每次处理 HTML 字符串时都会执行这个过程，比如使用 document.write 方法。 1.2.2 词法分析（标记化） HTML 结构不算太复杂，大部分情况下识别的标记会有开始标记、内容标记和结束标记，对应一个 HTML 元素。除此之外还有 DOCTYPE、Comment、EndOfFile 等标记。 标记化是通过状态机来实现的，状态机模型在 W3C 中已经定义好了。 想要得到一个标记，必须要经历一些状态，才能完成解析。我们通过一个简单的例子来了解一下流程。 &lt;a href=&quot;www.w3c.org&quot;&gt;W3C&lt;/a&gt; 开始标记：&lt;a href=&quot;www.w3c.org&quot;&gt; Data state：碰到 &lt;，进入 Tag open state Tag open state：碰到 a，进入 Tag name state 状态 Tag name state：碰到 空格，进入 Before attribute name state Before attribute name state：碰到 h，进入 Attribute name state Attribute name state：碰到 =，进入 Before attribute value state Before attribute value state：碰到 &quot;，进入 Attribute value (double-quoted) state Attribute value (double-quoted) state：碰到 w，保持当前状态 Attribute value (double-quoted) state：碰到 &quot;，进入 After attribute value (quoted) state After attribute value (quoted) state：碰到 &gt;，进入 Data state，完成解析 内容标记：W3C Data state：碰到 W，保持当前状态，提取内容 Data state：碰到 &lt;，进入 Tag open state，完成解析 结束标记：&lt;/a&gt; Tag open state：碰到 /，进入 End tag open state End tag open state：碰到 a，进入 Tag name state Tag name state：碰到 &gt;，进入 Data state，完成解析 通过上面这个例子，可以发现属性是开始标记的一部分。 1.2.3 语法分析（解析器） 在创建解析器后，会关联一个 Document 对象作为根节点。 我会简单介绍一下流程，具体的实现过程可以在 Tree construction 查看。 解析器在运行过程中，会对 Tokens 进行迭代；并根据当前 Token 的类型转换到对应的模式，再在当前模式下处理 Token；此时，如果 Token 是一个开始标记，就会创建对应的元素，添加到 DOM Tree 中，并压入还未遇到结束标记的开始标记栈中；此栈的主要目的是实现浏览器的容错机制，纠正嵌套错误，具体的策略在 W3C 中定义。更多标记的处理可以在 状态机算法 中查看。 1.2.4 参考资料 浏览器的工作原理：新式网络浏览器幕后揭秘 —— 解析器和词法分析器的组合 浏览器渲染过程与性能优化 —— 构建DOM树与CSSOM树 在浏览器的背后（一） —— HTML语言的词法解析 在浏览器的背后（二） —— HTML语言的语法解析 50 行代码的 HTML 编译器 AST解析基础: 如何写一个简单的html语法分析库 WebKit中的HTML词法分析 HTML文档解析和DOM树的构建 从Chrome源码看浏览器如何构建DOM树 构建对象模型 —— 文档对象模型 (DOM) 1.3 CSSOM Tree 1.3.1 加载 在构建 DOM Tree 的过程中，如果遇到 link 标记，浏览器就会立即发送请求获取样式文件。当然我们也可以直接使用内联样式或嵌入样式，来减少请求；但是会失去模块化和可维护性，并且像缓存和其他一些优化措施也无效了，利大于弊，性价比实在太低了；除非是为了极致优化首页加载等操作，否则不推荐这样做。 1.3.2 阻塞 CSS 的加载和解析并不会阻塞 DOM Tree 的构建，因为 DOM Tree 和 CSSOM Tree 是两棵相互独立的树结构。但是这个过程会阻塞页面渲染，也就是说在没有处理完 CSS 之前，文档是不会在页面上显示出来的，这个策略的好处在于页面不会重复渲染；如果 DOM Tree 构建完毕直接渲染，这时显示的是一个原始的样式，等待 CSSOM Tree 构建完毕，再重新渲染又会突然变成另外一个模样，除了开销变大之外，用户体验也是相当差劲的。另外 link 标记会阻塞 JavaScript 运行，在这种情况下，DOM Tree 是不会继续构建的，因为 JavaScript 也会阻塞 DOM Tree 的构建，这就会造成很长时间的白屏。 通过一个例子来更加详细的说明： &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;ie=edge&quot;&gt; &lt;script&gt; var startDate = new Date(); &lt;/script&gt; &lt;link href=&quot;https://cdn.bootcss.com/bootstrap/4.0.0-alpha.6/css/bootstrap.css&quot; rel=&quot;stylesheet&quot;&gt; &lt;script&gt; console.log(&quot;link after script&quot;, document.querySelector(&quot;h2&quot;)); console.log(&quot;经过 &quot; + (new Date() - startDate) + &quot; ms&quot;); &lt;/script&gt; &lt;title&gt;性能&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;标题&lt;/h1&gt; &lt;h2&gt;标题2&lt;/h2&gt; &lt;/body&gt; &lt;/html&gt; 首先需要在 Chrome 控制台的 Network 面板设置网络节流，让网络速度变慢，以便更好进行调试。 下图说明 JavaScript 的确需要在 CSS 加载并解析完毕之后才会执行。 为什么需要阻塞 JavaScript 的运行呢？ 因为 JavaScript 可以操作 DOM 和 CSSOM，如果 link 标记不阻塞 JavaScript 运行，这时 JavaScript 操作 CSSOM，就会发生冲突。更详细的说明可以在 使用 JavaScript 添加交互 这篇文章中查阅。 1.3.3 解析 CSS 解析的步骤与 HTML 的解析是非常类似的。 词法分析 CSS 会被拆分成如下一些标记： CSS 的色值使用十六进制优于函数形式的表示？ 函数形式是需要再次计算的，在进行词法分析时会将它变成一个函数标记，由此看来使用十六进制的确有所优化。 语法分析 每个 CSS 文件或嵌入样式都会对应一个 CSSStyleSheet 对象（authorStyleSheet），这个对象由一系列的 Rule（规则） 组成；每一条 Rule 都会包含 Selectors（选择器） 和若干 Declearation（声明），Declearation 又由 Property（属性）和 Value（值）组成。另外，浏览器默认样式表（defaultStyleSheet）和用户样式表（UserStyleSheet）也会有对应的 CSSStyleSheet 对象，因为它们都是单独的 CSS 文件。至于内联样式，在构建 DOM Tree 的时候会直接解析成 Declearation 集合。 内联样式和 authorStyleSheet 的区别 所有的 authorStyleSheet 都挂载在 document 节点上，我们可以在浏览器中通过 document.styleSheets 获取到这个集合。内联样式可以直接通过节点的 style 属性查看。 通过一个例子，来了解下内联样式和 authorStyleSheet 的区别： &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;ie=edge&quot;&gt; &lt;style&gt; body .div1 { line-height: 1em; } &lt;/style&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;./style.css&quot;&gt; &lt;style&gt; .div1 { background-color: #f0f; height: 20px; } &lt;/style&gt; &lt;title&gt;Document&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;div1&quot; style=&quot;background-color: #f00;font-size: 20px;&quot;&gt;test&lt;/div&gt; &lt;/body&gt; &lt;/html&gt; 可以看到一共有三个 CSSStyleSheet 对象，每个 CSSStyleSheet 对象的 rules 里面会有一个 CSSStyleDeclaration，而内联样式获取到的直接就是 CSSStyleDeclaration。 需要属性合并吗？ 在解析 Declearation 时遇到属性合并，会把单条声明转变成对应的多条声明，比如： .box { margin: 20px; } margin: 20px 就会被转变成四条声明；这说明 CSS 虽然提倡属性合并，但是最终还是会进行拆分的；所以属性合并的作用应该在于减少 CSS 的代码量。 1.3.4 计算 为什么需要计算？ 因为一个节点可能会有多个 Selector 命中它，这就需要把所有匹配的 Rule 组合起来，再设置最后的样式。 准备工作 为了便于计算，在生成 CSSStyleSheet 对象后，会把 CSSStyleSheet 对象最右边 Selector 类型相同的 Rules 存放到对应的 Hash Map 中，比如说所有最右边 Selector 类型是 id 的 Rules 就会存放到 ID Rule Map 中；使用最右边 Selector 的原因是为了更快的匹配当前元素的所有 Rule，然后每条 Rule 再检查自己的下一个 Selector 是否匹配当前元素。 idRules classRules tagRules ... * 选择器命中 一个节点想要获取到所有匹配的 Rule，需要依次判断 Hash Map 中的 Selector 类型（id、class、tagName 等）是否匹配当前节点，如果匹配就会筛选当前 Selector 类型的所有 Rule，找到符合的 Rule 就会放入结果集合中；需要注意的是通配符总会在最后进行筛选。 从右向左匹配规则 上文说过 Hash Map 存放的是最右边 Selector 类型的 Rule，所以在查找符合的 Rule 最开始，检验的是当前 Rule 最右边的 Selector；如果这一步通过，下面就要判断当前的 Selector 是不是最左边的 Selector；如果是，匹配成功，放入结果集合；否则，说明左边还有 Selector，递归检查左边的 Selector 是否匹配，如果不匹配，继续检查下一个 Rule。 为什么需要从右向左匹配呢？ 先思考一下正向匹配是什么流程，我们用 div p .yellow 来举例，先查找所有 div 节点，再向下查找后代是否是 p 节点，如果是，再向下查找是否存在包含 class=&quot;yellow&quot; 的节点，如果存在则匹配；但是不存在呢？就浪费一次查询，如果一个页面有上千个 div 节点，而只有一个节点符合 Rule，就会造成大量无效查询，并且如果大多数无效查询都在最后发现，那损失的性能就实在太大了。 这时再思考从右向左匹配的好处，如果一个节点想要找到匹配的 Rule，会先查询最右边 Selector 是当前节点的 Rule，再向左依次检验 Selector；在这种匹配规则下，开始就能避免大多无效的查询，当然性能就更好，速度更快了。 设置样式 设置样式的顺序是先继承父节点，然后使用用户代理的样式，最后使用开发者（authorStyleSheet）的样式。 authorStyleSheet 优先级 放入结果集合的同时会计算这条 Rule 的优先级；来看看 blink 内核对优先级权重的定义： switch (m_match) { case Id: return 0x010000; case PseudoClass: return 0x000100; case Class: case PseudoElement: case AttributeExact: case AttributeSet: case AttributeList: case AttributeHyphen: case AttributeContain: case AttributeBegin: case AttributeEnd: return 0x000100; case Tag: return 0x000001; case Unknown: return 0; } return 0; 因为解析 Rule 的顺序是从右向左进行的，所以计算优先级也会按照这个顺序取得对应 Selector 的权重后相加。来看几个例子： /* * 65793 = 65536 + 1 + 256 */ #container p .text { font-size: 16px; } /* * 2 = 1 + 1 */ div p { font-size: 14px; } 当前节点所有匹配的 Rule 都放入结果集合之后，先根据优先级从小到大排序，如果有优先级相同的 Rule，则比较它们的位置。 内联样式优先级 authorStyleSheet 的 Rule 处理完毕，才会设置内联样式；内联样式在构建 DOM Tree 的时候就已经处理完成并存放到节点的 style 属性上了。 内联样式会放到已经排序的结果集合最后，所以如果不设置 !important，内联样式的优先级是最大的。 !important 优先级 在设置 !important 的声明前，会先设置不包含 !important 的所有声明，之后再添加到结果集合的尾部；因为这个集合是按照优先级从小到大排序好的，所以 !important 的优先级就变成最大的了。 书写 CSS 的规则 结果集合最后会生成 ComputedStyle 对象，可以通过 window.getComputedStyle 方法来查看所有声明。 可以发现图中的声明是没有顺序的，说明书写规则的最大作用是为了良好的阅读体验，利于团队协作。 调整 Style 这一步会调整相关的声明；例如声明了 position: absolute;，当前节点的 display 就会设置成 block。 1.3.5 参考资料 从Chrome源码看浏览器如何计算CSS 探究 CSS 解析原理 Webkit内核探究【2】——Webkit CSS实现 Webkit CSS引擎分析 css加载会造成阻塞吗？ 原来 CSS 与 JS 是这样阻塞 DOM 解析和渲染的 外链 CSS 延迟 DOM 解析和 DOMContentLoaded CSS/JS 阻塞 DOM 解析和渲染 构建对象模型 —— CSS 对象模型 (CSSOM) 阻塞渲染的 CSS 1.4 Render Object Tree 在 DOM Tree 和 CSSOM Tree 构建完毕之后，才会开始生成 Render Object Tree（Document 节点是特例）。 1.4.1 创建 Render Object 在创建 Document 节点的时候，会同时创建一个 Render Object 作为树根。Render Object 是一个描述节点位置、大小等样式的可视化对象。 每个非 display: none | contents 的节点都会创建一个 Render Object，流程大致如下：生成 ComputedStyle（在 CSSOM Tree 计算这一节中有讲），之后比较新旧 ComputedStyle（开始时旧的 ComputedStyle 默认是空）；不同则创建一个新的 Render Object，并与当前处理的节点关联，再建立父子兄弟关系，从而形成一棵完整的 Render Object Tree。 1.4.2 布局（重排） Render Object 在添加到树之后，还需要重新计算位置和大小；ComputedStyle 里面已经包含了这些信息，为什么还需要重新计算呢？因为像 margin: 0 auto; 这样的声明是不能直接使用的，需要转化成实际的大小，才能通过绘图引擎绘制节点；这也是 DOM Tree 和 CSSOM Tree 需要组合成 Render Object Tree 的原因之一。 布局是从 Root Render Object 开始递归的，每一个 Render Object 都有对自身进行布局的方法。为什么需要递归（也就是先计算子节点再回头计算父节点）计算位置和大小呢？因为有些布局信息需要子节点先计算，之后才能通过子节点的布局信息计算出父节点的位置和大小；例如父节点的高度需要子节点撑起。如果子节点的宽度是父节点高度的 50%，要怎么办呢？这就需要在计算子节点之前，先计算自身的布局信息，再传递给子节点，子节点根据这些信息计算好之后就会告诉父节点是否需要重新计算。 数值类型 所有相对的测量值（rem、em、百分比...）都必须转换成屏幕上的绝对像素。如果是 em 或 rem，则需要根据父节点或根节点计算出像素。如果是百分比，则需要乘以父节点宽或高的最大值。如果是 auto，需要用 (父节点的宽或高 - 当前节点的宽或高) / 2 计算出两侧的值。 盒模型 众所周知，文档的每个元素都被表示为一个矩形的盒子（盒模型），通过它可以清晰的描述 Render Object 的布局结构；在 blink 的源码注释中，已经生动的描述了盒模型，与原先耳熟能详的不同，滚动条也包含在了盒模型中，但是滚动条的大小并不是所有的浏览器都能修改的。 // ***** THE BOX MODEL ***** // The CSS box model is based on a series of nested boxes: // http://www.w3.org/TR/CSS21/box.html // top // |----------------------------------------------------| // | | // | margin-top | // | | // | |-----------------------------------------| | // | | | | // | | border-top | | // | | | | // | | |--------------------------|----| | | // | | | | | | | // | | | padding-top |####| | | // | | | |####| | | // | | | |----------------| |####| | | // | | | | | | | | | // left | ML | BL | PL | content box | PR | SW | BR | MR | // | | | | | | | | | // | | | |----------------| | | | | // | | | | | | | // | | | padding-bottom | | | | // | | |--------------------------|----| | | // | | | ####| | | | // | | | scrollbar height ####| SC | | | // | | | ####| | | | // | | |-------------------------------| | | // | | | | // | | border-bottom | | // | | | | // | |-----------------------------------------| | // | | // | margin-bottom | // | | // |----------------------------------------------------| // // BL = border-left // BR = border-right // ML = margin-left // MR = margin-right // PL = padding-left // PR = padding-right // SC = scroll corner (contains UI for resizing (see the 'resize' property) // SW = scrollbar width box-sizing box-sizing: content-box | border-box，content-box 遵循标准的 W3C 盒子模型，border-box 遵守 IE 盒子模型。 它们的区别在于 content-box 只包含 content area，而 border-box 则一直包含到 border。通过一个例子说明： // width // content-box: 40 // border-box: 40 + (2 * 2) + (1 * 2) div { width: 40px; height: 40px; padding: 2px; border: 1px solid #ccc; } 1.4.3 参考资料 从Chrome源码看浏览器如何layout布局 Chromium网页Render Object Tree创建过程分析 浏览器的工作原理：新式网络浏览器幕后揭秘 —— 呈现树和 DOM 树的关系 谈谈我对盒模型的理解 渲染树构建、布局及绘制 1.5 Render Layer Tree Render Layer 是在 Render Object 创建的同时生成的，具有相同坐标空间的 Render Object 属于同一个 Render Layer。这棵树主要用来实现层叠上下文，以保证用正确的顺序合成页面。 1.5.1 创建 Render Layer 满足层叠上下文条件的 Render Object 一定会为其创建新的 Render Layer，不过一些特殊的 Render Object 也会创建一个新的 Render Layer。 创建 Render Layer 的原因如下： NormalLayer position 属性为 relative、fixed、sticky、absolute 透明的（opacity 小于 1）、滤镜（filter）、遮罩（mask）、混合模式（mix-blend-mode 不为 normal） 剪切路径（clip-path） 2D 或 3D 转换（transform 不为 none） 隐藏背面（backface-visibility: hidden） 倒影（box-reflect） column-count（不为 auto）或者 column-widthZ（不为 auto） 对不透明度（opacity）、变换（transform）、滤镜（filter）应用动画 OverflowClipLayer 剪切溢出内容（overflow: hidden） 另外以下 DOM 元素对应的 Render Object 也会创建单独的 Render Layer： Document HTML Canvas Video 如果是 NoLayer 类型，那它并不会创建 Render Layer，而是与其第一个拥有 Render Layer 的父节点共用一个。 1.5.2 参考资料 无线性能优化：Composite —— 从 LayoutObjects 到 PaintLayers Chromium网页Render Layer Tree创建过程分析 WEBKIT 渲染不可不知的这四棵树 1.6 Graphics Layer Tree 1.6.1 软件渲染 软件渲染是浏览器最早采用的渲染方式。在这种方式中，渲染是从后向前（递归）绘制 Render Layer 的；在绘制一个 Render Layer 的过程中，它的 Render Objects 不断向一个共享的 Graphics Context 发送绘制请求来将自己绘制到一张共享的位图中。 1.6.2 硬件渲染 有些特殊的 Render Layer 会绘制到自己的后端存储（当前 Render Layer 会有自己的位图），而不是整个网页共享的位图中，这些 Layer 被称为 Composited Layer（Graphics Layer）。最后，当所有的 Composited Layer 都绘制完成之后，会将它们合成到一张最终的位图中，这一过程被称为 Compositing；这意味着如果网页某个 Render Layer 成为 Composited Layer，那整个网页只能通过合成来渲染。除此之外，Compositing 还包括 transform、scale、opacity 等操作，所以这就是硬件加速性能好的原因，上面的动画操作不需要重绘，只需要重新合成就好。 上文提到软件渲染只会有一个 Graphics Context，并且所有的 Render Layer 都会使用同一个 Graphics Context 绘制。而硬件渲染需要多张位图合成才能得到一张完整的图像，这就需要引入 Graphics Layer Tree。 Graphics Layer Tree 是根据 Render Layer Tree 创建的，但并不是每一个 Render Layer 都会有对应的 Composited Layer；这是因为创建大量的 Composited Layer 会消耗非常多的系统内存，所以 Render Layer 想要成为 Composited Layer，必须要给出创建的理由，这些理由实际上就是在描述 Render Layer 具备的特征。如果一个 Render Layer 不是 Compositing Layer，那就和它的祖先共用一个。 每一个 Graphics Layer 都会有对应的 Graphics Context。Graphics Context 负责输出当前 Render Layer 的位图，位图存储在系统内存中，作为纹理（可以理解为 GPU 中的位图）上传到 GPU 中，最后 GPU 将多张位图合成，然后绘制到屏幕上。因为 Graphics Layer 会有单独的位图，所以在一般情况下更新网页的时候硬件渲染不像软件渲染那样重新绘制相关的 Render Layer；而是重新绘制发生更新的 Graphics Layer。 提升原因 Render Layer 提升为 Composited Layer 的理由大致概括如下，更为详细的说明可以查看 无线性能优化：Composite —— 从 PaintLayers 到 GraphicsLayers。 iframe 元素具有 Composited Layer。 video 元素及它的控制栏。 使用 WebGL 的 canvas 元素。 硬件加速插件，例如 flash。 3D 或透视变换（perspective transform） CSS 属性。 backface-visibility 为 hidden。 对 opacity、transform、fliter、backdropfilter 应用了 animation 或者 transition（需要是 active 的 animation 或者 transition，当 animation 或者 transition 效果未开始或结束后，提升的 Composited Layer 会恢复成普通图层）。 will-change 设置为 opacity、transform、top、left、bottom、right（其中 top、left 等需要设置明确的定位属性，如 relative 等）。 有 Composited Layer 后代并本身具有某些属性。 元素有一个 z-index 较低且为 Composited Layer 的兄弟元素。 为什么需要 Composited Layer？ 避免不必要的重绘。例如网页中有两个 Layer a 和 b，如果 a Layer 的元素发生改变，b Layer 没有发生改变；那只需要重新绘制 a Layer，然后再与 b Layer 进行 Compositing，就可以得到整个网页。 利用硬件加速高效实现某些 UI 特性。例如滚动、3D 变换、透明度或者滤镜效果，可以通过 GPU（硬件渲染）高效实现。 层压缩 由于重叠的原因，可能会产生大量的 Composited Layer，就会浪费很多资源，严重影响性能，这个问题被称为层爆炸。浏览器通过 Layer Squashing（层压缩）处理这个问题，当有多个 Render Layer 与 Composited Layer 重叠，这些 Render Layer 会被压缩到同一个 Composited Layer。来看一个例子： &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;ie=edge&quot;&gt; &lt;style&gt; div { position: absolute; width: 100px; height: 100px; } .div1 { z-index: 1; top: 10px; left: 10px; will-change: transform; background-color: #f00; } .div2 { z-index: 2; top: 80px; left: 80px; background-color: #f0f; } .div3 { z-index: 2; top: 100px; left: 100px; background-color: #ff0; } &lt;/style&gt; &lt;title&gt;Document&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;div1&quot;&gt;&lt;/div&gt; &lt;div class=&quot;div2&quot;&gt;&lt;/div&gt; &lt;div class=&quot;div3&quot;&gt;&lt;/div&gt; &lt;/body&gt; &lt;/html&gt; 可以看到后面两个节点重叠而压缩到了同一个 Composited Layer。 有一些不能被压缩的情况，可以在 无线性能优化：Composite —— 层压缩 中查看。 1.6.3 参考资料 无线性能优化：Composite —— 从-PaintLayers-到-GraphicsLayers Webkit 渲染基础与硬件加速 Chromium网页Graphics Layer Tree创建过程分析 Chrome中的硬件加速合成 浏览器渲染流程 详细分析 WebKit 渲染流程基础及分层加速 2 性能优化 上文简单介绍了浏览器渲染流程上的各个组成部分，下面我们通过像素管道来研究如何优化视觉变化效果所引发的更新。 2.1 像素管道 JavaScript。一般来说，我们会使用 JavaScript 来实现一些视觉变化的效果。比如用 jQuery 的 animate 函数做一个动画、对一个数据集进行排序或者往页面里添加一些 DOM 元素等。当然，除了 JavaScript，还有其他一些常用方法也可以实现视觉变化效果，比如：CSS Animations、Transitions 和 Web Animation API。 样式计算。此过程是根据匹配选择器（例如 .headline 或 .nav &gt; .nav__item）计算出哪些元素应用哪些 CSS 规则的过程。从中知道规则之后，将应用规则并计算每个元素的最终样式。 布局。在知道对一个元素应用哪些规则之后，浏览器即可开始计算它要占据的空间大小及其在屏幕的位置。网页的布局模式意味着一个元素可能影响其他元素，例如 元素的宽度一般会影响其子元素的宽度以及树中各处的节点，因此对于浏览器来说，布局过程是经常发生的。 绘制。绘制是填充像素的过程。它涉及绘出文本、颜色、图像、边框和阴影，基本上包括元素的每个可视部分。绘制一般是在多个表面（通常称为层）上完成的。 合成。由于页面的各部分可能被绘制到多层，由此它们需要按正确顺序绘制到屏幕上，以便正确渲染页面。对于与另一元素重叠的元素来说，这点特别重要，因为一个错误可能使一个元素错误地出现在另一个元素的上层。 渲染时的每一帧都会经过管道的各部分进行处理，但并不意味着所有的部分都会执行。实际上，在实现视觉变化效果时，管道针对指定帧通常有三种方式： JS / CSS &gt; 样式 &gt; 布局 &gt; 绘制 &gt; 合成 如果你修改一个 DOM 元素的 Layout 属性，也就是改变了元素的样式（比如 width、height 或者 position 等），那么浏览器会检查哪些元素需要重新布局，然后对页面激发一个 reflow（重排）过程完成重新布局。被 reflow（重排）的元素，接下来也会激发绘制过程，最后激发渲染层合并过程，生成最后的画面。 JS / CSS &gt; 样式 &gt; 绘制 &gt; 合成 如果你修改一个 DOM 元素的 Paint Only 属性，比如背景图片、文字颜色或阴影等，这些属性不会影响页面的布局，因此浏览器会在完成样式计算之后，跳过布局过程，只会绘制和渲染层合并过程。 JS / CSS &gt; 样式 &gt; 合成 如果你修改一个非样式且非绘制的 CSS 属性，那么浏览器会在完成样式计算之后，跳过布局和绘制的过程，直接做渲染层合并。这种方式在性能上是最理想的，对于动画和滚动这种负荷很重的渲染，我们要争取使用第三种渲染过程。 影响 Layout、Paint 和 Composite 的属性都可以通过 CSS Triggers 网站查阅。 2.2 刷新率 上面提到每一帧都要经过像素管道处理，也就是说每一帧都是一次重新渲染。我们需要引出另外一个概念：刷新率。 刷新率是一秒钟能够重新渲染多少次数的指标。目前大多数设备的屏幕刷新率为 60 次/秒；因此如果在页面中有动画、渐变、滚动效果，那么浏览器每一次重新渲染的时间间隔必须跟设备的每一次刷新保持一致，才能比较流畅。需要注意的是，大多数浏览器也会对重新渲染的时间间隔进行限制，因为即使超过屏幕刷新率，用户体验也不会提升。 刷新率（Hz）取决与显示器的硬件水平。 帧率（FPS）取决于显卡或者软件制约。 每次重新渲染的时间不能超过 16.66 ms（1 秒 / 60 次）。但实际上，浏览器还有很多整理工作，因此我们的所有工作最好在 10 毫秒之内完成。如果超过时间，刷新率下降，就会导致页面抖动，感觉卡顿。 2.3 优化 JavaScript 执行 JavaScript 是触发视觉变化的主要因素，时机不当或长时间运行的 JavaScript 可能是导致性能下降的常见原因。针对 JavaScript 的执行，下面有一些常用的优化措施。 2.3.1 window.requestAnimationFrame 在没有 requestAnimationFrame 方法的时候，执行动画，我们可能使用 setTimeout 或 setInterval 来触发视觉变化；但是这种做法的问题是：回调函数执行的时间是不固定的，可能刚好就在末尾，或者直接就不执行了，经常会引起丢帧而导致页面卡顿。 归根到底发生上面这个问题的原因在于时机，也就是浏览器要知道何时对回调函数进行响应。setTimeout 或 setInterval 是使用定时器来触发回调函数的，而定时器并无法保证能够准确无误的执行，有许多因素会影响它的运行时机，比如说：当有同步代码执行时，会先等同步代码执行完毕，异步队列中没有其他任务，才会轮到自己执行。并且，我们知道每一次重新渲染的最佳时间大约是 16.6 ms，如果定时器的时间间隔过短，就会造成 过度渲染，增加开销；过长又会延迟渲染，使动画不流畅。 requestAnimationFrame 方法不同与 setTimeout 或 setInterval，它是由系统来决定回调函数的执行时机的，会请求浏览器在下一次重新渲染之前执行回调函数。无论设备的刷新率是多少，requestAnimationFrame 的时间间隔都会紧跟屏幕刷新一次所需要的时间；例如某一设备的刷新率是 75 Hz，那这时的时间间隔就是 13.3 ms（1 秒 / 75 次）。需要注意的是这个方法虽然能够保证回调函数在每一帧内只渲染一次，但是如果这一帧有太多任务执行，还是会造成卡顿的；因此它只能保证重新渲染的时间间隔最短是屏幕的刷新时间。 requestAnimationFrame 方法的具体说明可以看 MDN 的相关文档，下面通过一个网页动画的示例来了解一下如何使用。 let offsetTop = 0; const div = document.querySelector(&quot;.div&quot;); const run = () =&gt; { div.style.transform = `translate3d(0, ${offsetTop += 10}px, 0)`; window.requestAnimationFrame(run); }; run(); 如果想要实现动画效果，每一次执行回调函数，必须要再次调用 requestAnimationFrame 方法；与 setTimeout 实现动画效果的方式是一样的，只不过不需要设置时间间隔。 参考资料 被誉为神器的requestAnimationFrame requestAnimationFrame 知多少？ 浅析 requestAnimationFrame 告别定时器，走向 window.requestAni0mationFrame() requestAnimationFrame 性能更好 谈谈requestAnimationFrame的动画循环 2.3.2 window.requestIdleCallback requestIdleCallback 方法只在一帧末尾有空闲的时候，才会执行回调函数；它很适合处理一些需要在浏览器空闲的时候进行处理的任务，比如：统计上传、数据预加载、模板渲染等。 以前如果需要处理复杂的逻辑，不进行分片，用户界面很可能就会出现假死状态，任何的交互操作都将无效；这时使用 setTimeout 就可以把任务拆分成多个模块，每次只处理一个模块，这样能很大程度上缓解这个问题。但是这种方式具有很强的不确定性，我们不知道这一帧是否空闲，如果已经塞满了一大堆任务，这时在处理模块就不太合适了。因此，在这种情况下，我们也可以使用 requestIdleCallback 方法来尽可能高效地利用空闲来处理分片任务。 如果一直没有空闲，requestIdleCallback 就只能永远在等待状态吗？当然不是，它的参数除了回调函数之外，还有一个可选的配置对象，可以使用 timeout 属性设置超时时间；当到达这个时间，requestIdleCallback 的回调就会立即推入事件队列。来看下如何使用： // 任务队列 const tasks = [ () =&gt; { console.log(&quot;第一个任务&quot;); }, () =&gt; { console.log(&quot;第二个任务&quot;); }, () =&gt; { console.log(&quot;第三个任务&quot;); }, ]; // 设置超时时间 const rIC = () =&gt; window.requestIdleCallback(runTask, {timeout: 3000}) function work() { tasks.shift()(); } function runTask(deadline) { if ( ( deadline.timeRemaining() &gt; 0 || deadline.didTimeout ) &amp;&amp; tasks.length &gt; 0 ) { work(); } if (tasks.length &gt; 0) { rIC(); } } rIC(); 回调函数参数的详细说明可以查看 MDN 的文档。 改变 DOM 不应该在 requestIdleCallback 方法的回调函数中改变 DOM。我们来看下在某一帧的末尾，回调函数被触发，它在一帧中的位置： 回调函数安排在帧提交之后，也就是说这时渲染已经完成了，布局已经重新计算过；如果我们在回调中改变样式，并且在下一帧中读取布局信息，那之前所作的所有布局计算全都浪费掉了，浏览器会强制重新进行布局计算，这也被称为 强制同步布局。 如果真的想要修改 DOM，那么最佳实践是：在 requestIdleCallback 的回调中构建 Document Fragment，然后在下一帧的 requestAnimationFrame 回调进行真实的 DOM 变动。 Fiber React 16 推出了新的协调器，Fiber Reconciler（纤维协调器）。它和原先 Stack Reconciler（栈协调器）不同的是：整个渲染过程不是连续不中断完成的；而是进行了分片，分段处理任务，这就需要用到 requestIdleCallback 和 requestAnimationFrame 方法来实现。requestIdleCallback 负责低优先级的任务，requestAnimationFrame 负责动画相关的高优先级任务。 参考资料 requestIdleCallback-后台任务调度 你应该知道的requestIdleCallback 使用requestIdleCallback React Fiber初探 —— 调和（Reconciliation） 2.3.3 Web Worker JavaScript 采用的是单线程模型，也就是说，所有任务都要在一个线程上完成，一次只能执行一个任务。有时，我们需要处理大量的计算逻辑，这是比较耗费时间的，用户界面很有可能会出现假死状态，非常影响用户体验。这时，我们就可以使用 Web Worker 来处理这些计算。 Web Worker 是 HTML5 中定义的规范，它允许 JavaScript 脚本运行在主线程之外的后台线程中。这就为 JavaScript 创造了 多线程 的环境，在主线程，我们可以创建 Worker 线程，并将一些任务分配给它。Worker 线程与主线程同时运行，两者互不干扰。等到 Worker 线程完成任务，就把结果发送给主线程。 Web Worker 与其说创造了多线程环境，不如说是一种回调机制。毕竟 Worker 线程只能用于计算，不能执行更改 DOM 这些操作；它也不能共享内存，没有 线程同步 的概念。 Web Worker 的优点是显而易见的，它可以使主线程能够腾出手来，更好的响应用户的交互操作，而不必被一些计算密集或者高延迟的任务所阻塞。但是，Worker 线程也是比较耗费资源的，因为它一旦创建，就一直运行，不会被用户的操作所中断；所以当任务执行完毕，Worker 线程就应该关闭。 Web Workers API 一个 Worker 线程是由 new 命令调用 Worker() 构造函数创建的；构造函数的参数是：包含执行任务代码的脚本文件，引入脚本文件的 URI 必须遵守同源策略。 Worker 线程与主线程不在同一个全局上下文中，因此会有一些需要注意的地方： 两者不能直接通信，必须通过消息机制来传递数据；并且，数据在这一过程中会被复制，而不是通过 Worker 创建的实例共享。详细介绍可以查阅 worker中数据的接收与发送：详细介绍。 不能使用 DOM、window 和 parent 这些对象，但是可以使用与主线程全局上下文无关的东西，例如 WebScoket、indexedDB 和 navigator 这些对象，更多能够使用的对象可以查看 Web Workers可以使用的函数和类。 下面我来简单介绍一下使用方式，更多的 API 可以查看 使用 Web Workers。 使用方式 Web Worker 规范中定义了两种不同类型的线程；一个是 Dedicated Worker（专用线程），它的全局上下文是 DedicatedWorkerGlobalScope 对象；另一个是 Shared Worker（共享线程），它的全局上下文是 SharedWorkerGlobalScope 对象。其中，Dedicated Worker 只能在一个页面使用，而 Shared Worker 则可以被多个页面共享。 下面我来简单介绍一下使用方式，更多的 API 可以查看 使用 Web Workers。 专用线程 下面代码最重要的部分在于两个线程之间怎么发送和接收消息，它们都是使用 postMessage 方法发送消息，使用 onmessage 事件进行监听。区别是：在主线程中，onmessage 事件和 postMessage 方法必须挂载在 Worker 的实例上；而在 Worker 线程，Worker 的实例方法本身就是挂载在全局上下文上的。 &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;ie=edge&quot;&gt; &lt;title&gt;Web Worker 专用线程&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;input type=&quot;text&quot; name=&quot;&quot; id=&quot;number1&quot;&gt; &lt;span&gt;+&lt;/span&gt; &lt;input type=&quot;text&quot; name=&quot;&quot; id=&quot;number2&quot;&gt; &lt;button id=&quot;button&quot;&gt;确定&lt;/button&gt; &lt;p id=&quot;result&quot;&gt;&lt;/p&gt; &lt;script src=&quot;./main.js&quot;&gt;&lt;/script&gt; &lt;/body&gt; &lt;/html&gt; // main.js const number1 = document.querySelector(&quot;#number1&quot;); const number2 = document.querySelector(&quot;#number2&quot;); const button = document.querySelector(&quot;#button&quot;); const result = document.querySelector(&quot;#result&quot;); // 1. 指定脚本文件，创建 Worker 的实例 const worker = new Worker(&quot;./worker.js&quot;); button.addEventListener(&quot;click&quot;, () =&gt; { // 2. 点击按钮，把两个数字发送给 Worker 线程 worker.postMessage([number1.value, number2.value]); }); // 5. 监听 Worker 线程返回的消息 // 我们知道事件有两种绑定方式，使用 addEventListener 方法和直接挂载到相应的实例 worker.addEventListener(&quot;message&quot;, e =&gt; { result.textContent = e.data; console.log(&quot;执行完毕&quot;); }) // worker.js // 3. 监听主线程发送过来的消息 onmessage = e =&gt; { console.log(&quot;开始后台任务&quot;); const result= +e.data[0]+ +e.data[1]; console.log(&quot;计算结束&quot;); // 4. 返回计算结果到主线程 postMessage(result); } 共享线程 共享线程虽然可以在多个页面共享，但是必须遵守同源策略，也就是说只能在相同协议、主机和端口号的网页使用。 示例基本上与专用线程的类似，区别是： 创建实例的构造器不同。 主线程与共享线程通信，必须通过一个确切打开的端口对象；在传递消息之前，两者都需要通过 onmessage 事件或者显式调用 start 方法打开端口连接。而在专用线程中这一部分是自动执行的。 // main.js const number1 = document.querySelector(&quot;#number1&quot;); const number2 = document.querySelector(&quot;#number2&quot;); const button = document.querySelector(&quot;#button&quot;); const result = document.querySelector(&quot;#result&quot;); // 1. 创建共享实例 const worker = new SharedWorker(&quot;./worker.js&quot;); // 2. 通过端口对象的 start 方法显式打开端口连接，因为下文没有使用 onmessage 事件 worker.port.start(); button.addEventListener(&quot;click&quot;, () =&gt; { // 3. 通过端口对象发送消息 worker.port.postMessage([number1.value, number2.value]); }); // 8. 监听共享线程返回的结果 worker.port.addEventListener(&quot;message&quot;, e =&gt; { result.textContent = e.data; console.log(&quot;执行完毕&quot;); }); // worker.js // 4. 通过 onconnect 事件监听端口连接 onconnect = function (e) { // 5. 使用事件对象的 ports 属性，获取端口 const port = e.ports[0]; // 6. 通过端口对象的 onmessage 事件监听主线程发送过来的消息，并隐式打开端口连接 port.onmessage = function (e) { console.log(&quot;开始后台任务&quot;); const result= e.data[0] * e.data[1]; console.log(&quot;计算结束&quot;); console.log(this); // 7. 通过端口对象返回结果到主线程 port.postMessage(result); } } 参考资料 优化 JavaScript 执行 —— 降低复杂性或使用 Web Worker 使用 Web Workers 深入 HTML5 Web Worker 应用实践：多线程编程 JS与多线程 2.3.4 防抖和节流函数 在进行改变窗口大小、滚动网页、输入内容这些操作时，事件回调会十分频繁的被触发，严重增加了浏览器的负担，导致用户体验非常糟糕。此时，我们就可以考虑采用防抖和节流函数来处理这类调动频繁的事件回调，同时它们也不会影响实际的交互效果。 我们先来简单了解一下这两个函数： 防抖（debounce）函数。在持续触发事件时，并不执行事件回调；只有在一段时间之内，没有再触发事件的时候，事件回调才会执行一次。 节流（throttle）函数。在持续触发事件时，事件回调也会不断的间隔一段时间后执行一次。 这两个函数最大的区别在于执行的时机，防抖函数会在事件触发停止一段时间后执行事件回调；而节流函数会在事件触发时不断的间隔一段时间后执行事件回调。我们用定时器来简单实现一下这两个函数，详细版本可以参考 Underscore 和 Lodash —— debounce、Lodash —— throttle。节流函数其实在浏览器拥有 requestAnimationFrame 方法之后，使用这个方法调用事件回调会更好一些。 实现防抖函数 每次执行到 debounce 返回的函数，都先把上一个定时器清理掉，再重新运行一个定时器；等到最后一次执行这个返回的函数的时候，定时器不会被清理，就可以正常等待定时器结束，执行事件回调了。 function debounce(func, wait) { let timeout = null; return function run(...args) { clearTimeout(timeout); timeout = setTimeout(() =&gt; { func.apply(this, args); }, wait); } }; 实现节流函数 在定时器存在的时候，不在重新生成定时器；等到定时器结束，事件回调执行，就把定时器清空；在下一次执行 throttle 返回的函数的时候，再生成定时器，等待下一个事件回调执行。 function throttle(func, wait) { let timeout = null; return function run(...args) { if (!timeout) { timeout = setTimeout(() =&gt; { timeout = null; func.apply(this, args); }, wait); } } } 参考资料 JS的防抖与节流 使输入处理程序去除抖动 Underscore Lodash —— debounce Lodash —— throttle 2.4 降低 Style 的复杂性 我们知道 CSS 最重要的组成部分是选择器和声明，所以我会通过这两方面来讲解如何降低 Style 的复杂性。 2.4.1 避免选择器嵌套 我们在 CSSOM Tree 这一节中了解到：嵌套的选择器会从右向左匹配，这是一个递归的过程，而递归是一种比较耗时的操作。更不用说一些 CSS3 的选择器了，它们会需要更多的计算，例如： .text:nth-child(2n) .strong { /* styles */ } 为了确定哪些节点应用这个样式，浏览器必须先询问这是拥有 &quot;strong&quot; class 的节点吗？其父节点恰好是偶数的 &quot;text&quot; class 节点吗？如此多的计算过程，都可以通过一个简单的 class 来避免： .text-even-strong { /* styles */ } 这么简单的选择器，浏览器只要匹配一次就可以了。为了准确描述网页结构、可复用和代码共享等方面的考虑，我们可以使用 BEM 来协助开发。 BEM（块，元素，修饰符） BEM 简单来讲就是一种 class 的命名规范，它建议所有元素都有单个类，并且嵌套也能够很好的组织在类中： .nav {} .nav__item {} 如果节点需要与其他节点进行区分，就可以加入修饰符来协助开发： .nav__item--active {} 更为详细的描述和用法可以查看 Get BEM。 使用开销更小的样式 因为屏幕显示效果的不同，所以浏览器渲染每一个样式的开销也会不一样。例如，绘制阴影肯定要比绘制普通背景的时间要长。我们来对比下这两者之间的开销。 &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;ie=edge&quot;&gt; &lt;style&gt; .simple { background-color: #f00; } .complex { box-shadow: 0 4px 4px rgba(0, 0, 0, 0.5); } &lt;/style&gt; &lt;title&gt;性能优化&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;container&quot;&gt;&lt;/div&gt; &lt;script&gt; const div = document.querySelector(&quot;.container&quot;); let str = &quot;&quot;; for (let i = 0; i &lt; 1000; i++) { str += &quot;&lt;div class=\\&quot;simple\\&quot;&gt;background-color: #f00;&lt;/div&gt;&quot;; // str += &quot;&lt;div class=\\&quot;complex\\&quot;&gt;box-shadow: 0, 4px, 4px, rgba(0,0,0,0.5);&lt;/div&gt;&quot;; } div.innerHTML = str; &lt;/script&gt; &lt;/body&gt; &lt;/html&gt; 可以看到阴影的 Layout 是 31.35 ms，paint 是 6.43 ms；背景的 Layout 是 10.81 ms，paint 是 4.30 ms。Layout 的差异还是相当明显的。 因此，如果可能，还是应该使用开销更小的样式替代当前样式实现最终效果。 参考资料 缩小样式计算的范围并降低其复杂性 CSS BEM 书写规范 2.5 最小化重排（Reflow）和重绘（Repaint） 首先我们先来了解一下什么是重排和重绘。 重排是指因为修改 style 或调整 DOM 结构重新构建部分或全部 Render Object Tree 从而计算布局的过程。这一过程至少会触发一次，既页面初始化。 重绘是指重新绘制受影响的部分到屏幕。 观察像素通道会发现重绘不一定会触发重排，比如改变某个节点的背景色，只会重新绘制这个节点，而不会发生重排，这是因为布局信息没有发生变化；但是重排是一定会触发重绘的。 下面的情况会导致重排或者重绘： 调整 DOM 结构 修改 CSS 样式 用户事件，如页面滚动，改变窗口大小等 2.5.1 浏览器优化策略 重排和重绘会不断触发，这是不可避免的。但是，它们非常消耗资源，是导致网页性能低下的根本原因。 提高网页性能，就是要降低重排和重绘的频率和成本，尽可能少的触发重新渲染。 浏览器面对集中的 DOM 操作时会有一个优化策略：创建一个变化的队列，然后一次执行，最终只渲染一次。 div2.style.height = &quot;100px&quot;; div2.style.width = &quot;100px&quot;; 上面的代码在浏览器优化后只会执行一次渲染。但是，如果代码写得不好变化的队列就会立即刷新，并进行渲染；这通常是在修改 DOM 之后，立即获取样式信息的时候。下面的样式信息会触发重新渲染： offsetTop/offsetLeft/offsetWidth/offsetHeight scrollTop/scrollLeft/scrollWidth/scrollHeight clientTop/clientLeft/clientWidth/clientHeight getComputedStyle() 2.5.2 提高性能的技巧 多利用浏览器优化策略。相同的 DOM 操作（读或写），应该放在一起。不要在读操作中间插入写操作。 不要频繁计算样式。如果某个样式是通过重排得到的，那么最好缓存结果。避免下一次使用的时候，再进行重排。// Bad const div1 = document.querySelector(&quot;.div1&quot;); div1.style.height = div1.clientHeight + 200 + &quot;px&quot;; div1.style.width = div1.clientHeight * 2 + &quot;px&quot;; // Good const div2 = document.querySelector(&quot;.div2&quot;); const div2Height = div1.clientHeight + 200; div2.style.height = div2Height + &quot;px&quot;; div2.style.width = div2Height * 2 + &quot;px&quot;; 不要逐条改变样式。通过改变 className 或 cssText 属性，一次性改变样式。// Bad const top = 10; const left = 10; const div = document.querySelector(&quot;.div&quot;); div.style.top = top + &quot;px&quot;; div.style.left = left + &quot;px&quot;; // Good div.className += &quot;addClass&quot;; // Good div.style.cssText += &quot;top: 10px; left: 10px&quot;; 使用离线 DOM。离线意味着不对真实的节点进行操作，可以通过以下方式实现： 操纵 Document Fragment 对象，完成后再把这个对象加入 DOM Tree 使用 cloneNode 方法，在克隆的节点上进行操作，然后再用克隆的节点替换原始节点 将节点设为 display: none;（需要一次重排），然后对这个节点进行多次操作，最后恢复显示（需要一次重排）。这样一来，就用两次重排，避免了更多次的重新渲染。 将节点设为 visibility: hidden; 和设为 display: none; 是类似的，但是这个属性只对重绘有优化，对重排是没有效果的，因为它只是隐藏，但是节点还在文档流中的。 设置 position: absolute | fixed;。节点会脱离文档流，这时因为不用考虑这个节点对其他节点的影响，所以重排的开销会比较小。 使用虚拟 DOM，例如 Vue、React 等。 使用 flexbox 布局。flexbox 布局的性能要比传统的布局模型高得多，下面是对 1000 个 div 节点应用 float 或 flex 布局的开销对比。可以发现，对于相同数量的元素和相同视觉的外观，flex 布局的开销要小得多（float 37.92 ms | flex 13.16 ms）。 参考资料 网页性能管理详解 渲染优化：重排重绘与硬件加速 浏览器渲染流程 详细分析 CSS Animation性能优化 2.6 Composite 的优化 终于，我们到了像素管道的末尾。对于这一部分的优化策略，我们可以从为什么需要 Composited Layer（Graphics Layer）来入手。这个问题我们在构建 Graphics Layer Tree 的时候，已经说明过，现在简单回顾一下： 避免不必要的重绘。 利用硬件加速高效实现某些 UI 特性。 根据 Composited Layer 的这两个特点，可以总结出以下几点优化措施。 2.6.1 使用 transform 和 opacity 属性来实现动画 上文我们说过像素管道的 Layout 和 Paint 部分是可以略过，只进行 Composite 的。实现这种渲染方式的方法很简单，就是使用只会触发 Composite 的 CSS 属性；目前，满足这个条件的 CSS 属性，只有 transform 和 opacity。 使用 transform 和 opacity 需要注意的是：元素必须是 Composited Layer；如果不是，Paint 还是会照常触发（Layout 要看情况，一般 transform 会触发）。来看一个例子： &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;ie=edge&quot;&gt; &lt;style&gt; .div { width: 100px; height: 100px; background-color: #f00; /* will-change: transform; */ } &lt;/style&gt; &lt;title&gt;性能优化&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;div&quot;&gt;&lt;/div&gt; &lt;script&gt; const div = document.querySelector(&quot;.div&quot;); const run = () =&gt; { div.style.transform = &quot;translate(0, 100px)&quot;; }; setTimeout(run, 2000); &lt;/script&gt; &lt;/body&gt; &lt;/html&gt; 我们将使用 transform 来向下位移，开始我们先不把 div 节点提升为 Composited Layer；通过下图可以看到：还是会触发 Layout 和 Paint 的。 这时，把 div 节点提升为 Composited Layer，我们发现 Layout 和 Paint 已经被略过了，符合我们的预期。 2.6.2 减少绘制的区域 如果不能避免绘制，我们就应该尽可能减少需要重绘的区域。例如，页面顶部有一块固定区域，当页面某个其他区域需要重绘的时候，很可能整块屏幕都要重绘，这时，固定区域也会被波及到。像这种情况，我们就可以把需要重绘或者受到影响的区域提升为 Composited Layer，避免不必要的绘制。 提升成 Composited Layer 的最佳方式是使用 CSS 的 will-change 属性，它的详细说明可以查看 MDN 的文档。 .element { will-change: transform; } 对于不支持的浏览器，最简单的 hack 方法，莫过于使用 3D 变形来提升为 Composited Layer 了。 .element { transform: translateZ(0); } 根据上文所讲的例子，我们尝试使用 will-change 属性来让固定区域避免重绘。 &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;ie=edge&quot;&gt; &lt;style&gt; .div { width: 100px; height: 100px; background-color: #f00; } .header { position: fixed; z-index: 9999; width: 100%; height: 50px; background-color: #ff0; /* will-change: transform; */ } &lt;/style&gt; &lt;title&gt;性能优化&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;header class=&quot;header&quot;&gt;固定区域&lt;/header&gt; &lt;div class=&quot;div&quot;&gt;变动区域&lt;/div&gt; &lt;script&gt; const div = document.querySelector(&quot;.div&quot;); const run = () =&gt; { div.style.opacity = 0.5; }; setTimeout(run, 2000); &lt;/script&gt; &lt;/body&gt; &lt;/html&gt; 首先，我们来看下没有经过优化的情况；顺带说明查看浏览器一帧绘制详情的过程。 打开控制台的 Performance 界面。 点击设置（标记 1），开启绘制分析仪（标记 2）。 启动 Record（标记 3），获取到想要的信息后，点击 Stop（标记 4）， 停止 Record。 点击这一帧的 Paint（标记 5）查看绘制详情。 切换到 Paint Profiler 选项卡（标记 6），查看绘制的步骤。 通过上面的图片（标记 7 和标记 8）可以看到，固定区域的确被波及到，并且触发重绘了。我们再对比使用 will-change 属性优化过的情况，发现固定区域没有触发重绘。 并且，我们也可以通过一帧（标记 1）的布局详情（标记 2），查看固定区域（标记 3）是不是提升成 Composited Layer（标记 4），才避免的不必要绘制。 2.6.3 合理管理 Composited Layer 提升成 Composited Layer 的确会优化性能；但是，要知道创建一个新的 Composited Layer 必须要额外的内存和管理，这是非常昂贵的代价。所以，在内存资源有限的设备上，Composited Layer 带来的性能提升，很可能远远抵不上创建多个 Composited Layer 的代价。同时，由于每一个 Composited Layer 的位图都需要上传到 GPU；所以，不免需要考虑 CPU 和 GPU 之间的带宽以及用多大内存处理 GPU 纹理的问题。 我们通过 1000 个 div 节点，来对比普通图层与提升成 Composited Layer 之后的内存使用情况。可以发现差距还是比较明显的。 最小化提升 通过上文的说明，我们知道 Composited Layer 并不是越多越好。尤其是，千万不要通过下面的代码提升页面的所有元素，这样的资源消耗将是异常恐怖的。 * { /* or transform: translateZ(0) */ will-change: transform; } 最小化提升，就是要尽量降低页面 Composited Layer 的数量。为了做到这一点，我们可以不把像 will-change 这样能够提升节点为 Composited Layer 的属性写在默认状态中。至于这样做的原因，我会在下面讲解。 看这个例子，我们先把 will-change 属性写在默认状态里；然后，再对比去掉这个属性后渲染的情况。 .box { width: 100ox; height: 100px; background-color: #f00; will-change: transform; transition: transform 0.3s; } .box:hover { transform: scale(1.5); } 使用 will-change 属性提升的 Composited Layer： 普通图层： 我们发现区别仅在于，动画的开始和结束，会触发重绘；而动画运行的时候，删除或使用 will-change 是没有任何分别的。 我们在构建 Graphics Layer Tree 的时候讲到过这样一条理由： 对 opacity、transform、fliter、backdropfilter 应用了 animation 或者 transition（需要是 active 的 animation 或者 transition，当 animation 或者 transition 效果未开始或结束后，提升的 Composited Layer 会恢复成普通图层）。 这条理由赐予了我们动态提升 Composited Layer 的权利；因此我们应该多利用这一点，来减少不必要的 Composited Layer 的数量。 防止层爆炸 我们在 Graphics Layer Tree 中介绍过层爆炸，它指的是由于重叠而导致的大量额外 Composited Layer 的问题。浏览器的层压缩可以在很大程度上解决这个问题，但是，有很多特殊的情况，会导致 Composited Layer 无法被压缩；这就很可能产生一些不在我们预期中的 Composited Layer，也就是说还是会出现大量额外的 Composited Layer。 在层压缩这一节，我们已经给出了使用层压缩优化的例子，这里就不再重复了。下面再通过解决一个无法被层压缩的例子，来更为深入的了解如何防止层爆炸。 &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;ie=edge&quot;&gt; &lt;style&gt; .animating { width: 300px; height: 30px; line-height: 30px; background-color: #ff0; will-change: transform; transition: transform 3s; } .animating:hover { transform: translateX(100px); } ul { padding: 0; border: 1px solid #000; } .box { position: relative; display: block; width: auto; background-color: #00f; color: #fff; margin: 5px; overflow: hidden; } .inner { position: relative; margin: 5px; } &lt;/style&gt; &lt;title&gt;性能优化&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;animating&quot;&gt;动画&lt;/div&gt; &lt;ul&gt; &lt;li class=&quot;box&quot;&gt; &lt;p class=&quot;inner&quot;&gt;提升成合成层&lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;box&quot;&gt; &lt;p class=&quot;inner&quot;&gt;提升成合成层&lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;box&quot;&gt; &lt;p class=&quot;inner&quot;&gt;提升成合成层&lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;box&quot;&gt; &lt;p class=&quot;inner&quot;&gt;提升成合成层&lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;box&quot;&gt; &lt;p class=&quot;inner&quot;&gt;提升成合成层&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/body&gt; &lt;/html&gt; 当我们的鼠标移入 .animating 元素的时候，通过查看 Layers 面板，可以很清晰的看到出现的大量 Composited Layer。 这个例子虽然表面上看起来没有发生重叠；但是，因为在运行动画的时候，很可能与其他元素造成重叠，所以 .animating 元素会假设兄弟元素在一个 Composited Layer 之上。这时，又因为 .box 元素设置了 overflow: hidden; 导致自己与 .animating 元素有了不同的裁剪容器（Clipping Container），所以就出现了层爆炸的现象。 解决这个问题的办法也很简单，就是让 .animating 元素的 z-index 比其他兄弟元素高。因为 Composited Layer 在普通元素之上，所以也就没有必要提升普通元素，修正渲染顺序了。这里我在顺便多说一句，默认情况下 Composited Layer 渲染顺序的优先级是比普通元素高的；但是在普通元素设置 position: relative; 之后，因为层叠上下文，并且在文档流后面的原因，所以会比 Composited Layer 的优先级高。 .animating { position: relative; z-index: 1; ... } 当然，如果兄弟元素一定要覆盖在 Composited Layer 之上，那我们也可以把 overflow: hidden; 或者 position: relative; 去掉，来优化 Composited Layer 创建的数量或者直接就不创建 Composited Layer。 参考资料 无线性能优化：Composite 坚持仅合成器的属性和管理层计数 简化绘制的复杂度、减小绘制区域 CSS Animation性能优化 使用CSS3 will-change提高页面滚动、动画等渲染性能 CSS3硬件加速也有坑 深入理解CSS中的层叠上下文和层叠顺序 总结 本文首先讲了渲染需要构建的一些树，然后通过这些树与像管道各部分的紧密联系，整理了一些优化措施。例如，我们对合成所进行的优化措施，就是通过 Graphics Layer Tree 来入手的。 优化也不能盲目去做，例如，提升普通图层为 Composite Layer 来说，使用不当，反而会造成非常严重的内存消耗。应当善加利用 Google 浏览器的调试控制台，帮助我们更加详尽的了解网页各方面的情况；从而有针对性的优化网页。 文章参考了很多资料，这些资料都在每一节的末尾给出。它们具有非常大的价值，有一些细节，本文可能并没有整理，可以通过查看它们来更为深入的了解。 ","link":"https://faded.auspicious.space/post/webpage-rendering-principles-and-optimization/"},{"title":"深入浅出 OAuth 2.0 授权机制","content":" 深入浅出OAuth 2.0授权机制 前言 举个简单的例子。新浪微博是你的家，有时候你会想让一些人（第三方应用）去你的家里帮你办点事情，或者取点东西。你可以直接复制一把钥匙（用户名和密码）给他们，但这里存在几个问题： 别人拿了钥匙后可以去你家里的所有房间。 别人拿到你的钥匙后也许会不小心丢到，甚至故意送到它人手里。这样你都不知到谁有你家钥匙。 过一段时间你也许会想要回自己的钥匙，但别人不还怎么办？ 总结起来就是两个问题：其一，拿到钥匙的人权限太大，可以进入任一房间；其二，拿到钥匙的人可能对钥匙进行复制和更改。 OAuth 是高级钥匙，可以理解为指纹识别，它主要解决了以上的缺陷： 你可以配置不同权限的钥匙。有些只能进大厅（读取你的微博流）。有些钥匙可以进储藏柜（读取你的相片）。 钥匙上带着指纹验证程序（指纹 = appkey），只有收到钥匙的人自己能使用钥匙。 钥匙有一定的时效性，同时你也可以远程废除钥匙。 正文 OAuth 是一个关于授权（authorization）的开放网络标准，在全世界得到广泛应用，目前的版本是 2.0 版。 本文对 OAuth 2.0 的设计思路和运行流程，做一个简明通俗的解释。 应用场景 为了理解 OAuth 的适用场景，举一个通俗易懂的例子。 有一个“云冲印”的网站，可以将用户储存在 Google 的照片，冲印出来。用户为了使用该服务，必须让“云冲印”读取自己储存在 Google 上的照片。 问题是只有得到用户的授权，Google 才会同意“云冲印”读取这些照片。那么，“云冲印”怎样获得用户的授权呢？传统方法是，用户将自己的 Google 用户名和密码，告诉“云冲印”，后者就可以读取用户的照片了。这样的做法有以下几个严重的缺点。 “云冲印”为了后续的服务，会保存用户的密码，这样很不安全。 Google 不得不部署密码登录，而我们知道，单纯的密码登录并不安全。 “云冲印”拥有了获取用户储存在 Google 所有资料的权利，用户没法限制“云冲印”获得授权的范围和有效期。 用户只有修改密码，才能收回赋予“云冲印”的权力。但是这样做，会使得其他所有获得用户授权的第三方应用程序全部失效。 只要有一个第三方应用程序被破解，就会导致用户密码泄漏，以及所有被密码保护的数据泄漏。 OAuth就是为了解决上面这些问题而诞生的。 专业术语 在详细讲解 OAuth 2.0 之前，需要了解几个专用名词。它们对读懂后面的讲解，尤其是几张图，至关重要。 专业术语 中文含义 具体解释说明 Third-party application 第三方应用程序 本文中又称“客户端”(client)，即上一节例子中的“云冲印” Resource Owner 资源所有者 本文中又称“用户”(user) HTTP service HTTP服务提供商 本文中简称“服务提供商”，即上一节例子中的 Google User Agent 用户代理 本文中就是指浏览器 Authorization server 认证服务器 即服务提供商专门用来处理认证的服务器 Resource server 资源服务器 即服务提供商存放用户生成的资源的服务器。它与认证服务器，可以是同一台服务器，也可以是不同的服务器 OAuth的思路 OAuth 在“客户端”与“服务提供商”之间，设置了一个授权层（authorization layer）。“客户端”不能直接登录“服务提供商”，只能登录授权层，以此将用户与客户端区分开来。“客户端”登录授权层所用的令牌（token），与用户的密码不同。用户可以在登录的时候，指定授权层令牌的权限范围和有效期。“客户端”登录授权层以后，“服务提供商”根据令牌的权限范围和有效期，向“客户端”开放用户储存的资料。 具体流程 (A) 用户打开客户端以后，客户端要求用户给予授权。 (B) 用户同意给予客户端授权。 (C) 客户端拿到上一步获取到的授权，向认证服务器申请令牌。 (D) 认证服务器对客户端进行认证以后，确认无误，统一发放令牌。 (E) 客户端使用令牌，向资源服务器申请获取用户的资源。 (F) 资源服务器确认令牌无误，同意向客户端开放资源。 不难看出来，上面六个步骤之中，步骤 (B) 是关键，即用户怎样才能给于客户端授权。有了这个授权以后，客户端就可以获取令牌，进而凭令牌获取资源。 几种授权模式 客户端必须得到了用户的授权（authorization grant），才能获取令牌（access token）。OAuth 2.0 定义了四种授权方式。 授权码模式（authorization code） 简化模式（implicit） 密码模式（resource owner password credentials） 客户端模式（client credentials） 下面一一讲解客户端获取授权的四种模式。 (一). 授权码模式 授权码模式（authorization code）是功能最完整、流程最严密的授权模式。它的特点就是通过客户端的后台服务器，与“服务提供商”的认证服务器进行互动。 它的步骤如下： (A) 用户访问客户端，后者将前者导向认证服务器。 (B) 用户选择是否给予客户端授权。 (C) 假设用户给予授权，认证服务器将用户导向客户端事先指定的“重定向URI”（redirection URI），同时附上一个授权码。 (D) 客户端收到授权码，附上早先的“重定向URI”，向认证服务器申请令牌。这一步是在客户端的后台的服务器上完成的，对用户不可见。 (E) 认证服务器核对了授权码和重定向 URI，确认无误后，向客户端发送访问令牌（access token）和更新令牌（refresh token)。 对于每个步骤具体所需要参数如下： A 步骤中，客户端申请认证的 URI，包含以下参数： 参数 具体含义 是否必填 response_type 授权类型 必选项，此处的值固定为“code” client_id 客户端的 ID 必选项 redirect_uri 重定向 URI 可选项 scope 申请的权限范围 可选项 state 客户端的当前状态，认证服务器会原封不动地返回这个值 可选项，可以指定任意值 下面是一个例子： GET /authorize?response_type=code&amp;client_id=s6BhdRkqt3&amp;state=xyz &amp;redirect_uri=https%3A%2F%2Fclient%2Eexample%2Ecom%2Fcb HTTP/1.1 Host: server.example.com C 步骤中，服务器回应客户端的 URI，包含以下参数： 参数 具体含义 是否必填 code 授权码。该码的有效期应该很短，通常设为 10 分钟，客户端只能使用该码一次，否则会被授权服务器拒绝。该码与客户端 ID 和重定向 URI，是一一对应关系 必选项 state 如果客户端的请求中包含这个参数，认证服务器的回应也必须一模一样包含这个参数。 可选项 下面是一个例子： HTTP/1.1 302 Found Location: https://client.example.com/cb?code=SplxlOBeZQQYbYS6WxSbIA&amp;state=xyz D 步骤中，客户端向认证服务器申请令牌的 HTTP 请求，包含以下参数： 参数 具体含义 是否必填 grant_type 授权模式 必选项，此处的值固定为“authorization_code” code 上一步获得的授权码 必选项 redirect_uri 重定向 URI 必选项，且必须与 A 步骤中的该参数值保持一致 client_id 客户端 ID 必选项 下面是一个例子： POST /token HTTP/1.1 Host: server.example.com Authorization: Basic czZCaGRSa3F0MzpnWDFmQmF0M2JW Content-Type: application/x-www-form-urlencoded grant_type=authorization_code&amp;code=SplxlOBeZQQYbYS6WxSbIA &amp;redirect_uri=https%3A%2F%2Fclient%2Eexample%2Ecom%2Fcb E 步骤中，认证服务器发送的 HTTP 回复，包含以下参数： 参数 具体含义 是否必填 access_token 访问令牌 必选项 token_type 牌类型，该值大小写不敏感 必选项，可以是 bearer 类型或 mac 类型 expires_in 过期时间，单位为秒。如果省略该参数，必须其他方式设置过期时间 可选项 refresh_token 更新令牌，用来获取下一次的访问令牌 可选项 scope 申请的权限范围 可选项 下面是一个例子： HTTP/1.1 200 OK Content-Type: application/json;charset=UTF-8 Cache-Control: no-store Pragma: no-cache { &quot;access_token&quot;:&quot;2YotnFZFEjr1zCsicMWpAA&quot;, &quot;token_type&quot;:&quot;bearer&quot;, &quot;expires_in&quot;:3600, &quot;refresh_token&quot;:&quot;tGzv3JOkF0XG5Qx2TlKWIA&quot;, &quot;example_parameter&quot;:&quot;example_value&quot; } 从上面代码可以看到，相关参数使用 JSON 格式发送（Content-Type: application/json）。此外，HTTP 头信息中明确指定不得缓存。 (二). 简化模式 简化模式（implicit grant type）不通过第三方应用程序的服务器，直接在浏览器中向认证服务器申请令牌，跳过了“授权码”这个步骤，因此得名。所有步骤在浏览器中完成，令牌对访问者是可见的，且客户端不需要认证。 它的步骤如下： (A) 客户端将用户导向认证服务器。 (B) 用户决定是否给于客户端授权。 (C) 假设用户给予授权，认证服务器将用户导向客户端指定的“重定向URI”，并在 URI 的 Hash 部分包含了访问令牌。 (D) 浏览器向资源服务器发出请求，其中不包括上一步收到的 Hash 值。 (E) 资源服务器返回一个网页，其中包含的代码可以获取 Hash 值中的令牌。 (F) 浏览器执行上一步获得的脚本，提取出令牌。 (G) 浏览器将令牌发给客户端。 下面是上面这些步骤所需要的参数： A 步骤中，客户端发出的 HTTP 请求，包含以下参数： 参数 具体含义 是否必填 response_type 授权类型 必选项，此处的值固定为“token” client_id 客户端的 ID 必选项 redirect_uri 重定向 URI 可选项 scope 申请的权限范围 可选项 state 客户端的当前状态，认证服务器会原封不动地返回这个值 可选项，可以指定任意值 下面是一个例子： GET /authorize?response_type=token&amp;client_id=s6BhdRkqt3&amp;state=xyz &amp;redirect_uri=https%3A%2F%2Fclient%2Eexample%2Ecom%2Fcb HTTP/1.1 Host: server.example.com C 步骤中，认证服务器回应客户端的 URI，包含以下参数： 参数 具体含义 是否必填 access_token 访问令牌 必选项 token_type 牌类型，该值大小写不敏感 必选项 expires_in 过期时间，单位为秒。如果省略该参数，必须其他方式设置过期时间 可选项 scope 权限范围 如果与客户端申请的范围一致，此项可省略 state 如果客户端的请求中包含这个参数，认证服务器的回应也必须一模一样包含这个参数 可选项 下面是一个例子： HTTP/1.1 302 Found Location: http://example.com/cb#access_token=2YotnFZFEjr1zCsicMWpAA &amp;state=xyz&amp;token_type=example&amp;expires_in=3600 在上面的例子中，认证服务器用 HTTP 头信息的 Location 栏，指定浏览器重定向的网址。注意，在这个网址的 Hash 部分包含了令牌。 根据上面的 D 步骤，下一步浏览器会访问 Location 指定的网址，但是 Hash 部分不会发送。 接下来的 E 步骤，服务提供商的资源服务器发送过来的脚本代码，会提取出 Hash 中的令牌。 (三). 密码模式 密码模式（Resource Owner Password Credentials Grant）中，用户向客户端提供自己的用户名和密码。客户端使用这些信息，向“服务商提供商”索要授权。 在这种模式中，用户必须把自己的密码给客户端，但是客户端不得储存密码。这通常用在用户对客户端高度信任的情况下，比如客户端是操作系统的一部分，或者由一个著名公司出品。而认证服务器只有在其他授权模式无法执行的情况下，才能考虑使用这种模式。 它的步骤如下： (A) 用户向客户端提供用户名和密码。 (B) 客户端将用户名和密码发给认证服务器，向后者请求令牌。 (C) 认证服务器确认无误后，向客户端提供访问令牌。 B 步骤中，客户端发出的 HTTP 请求，包含以下参数： 参数 具体含义 是否必填 grant_type 授权类型 必选项，此处的值固定为“password” username 用户名 必选项 password 用户的密码 必选项 scope 申请的权限范围 可选项 下面是一个例子： POST /token HTTP/1.1 Host: server.example.com Authorization: Basic czZCaGRSa3F0MzpnWDFmQmF0M2JW Content-Type: application/x-www-form-urlencoded grant_type=password&amp;username=johndoe&amp;password=A3ddj3w C 步骤中，认证服务器向客户端发送访问令牌： 参数 具体含义 是否必填 access_token 访问令牌 必选项 token_type 牌类型，该值大小写不敏感 必选项 expires_in 过期时间，单位为秒。如果省略该参数，必须其他方式设置过期时间 可选项 scope 权限范围 如果与客户端申请的范围一致，此项可省略 example_parameter 如果客户端的请求中包含这个参数，认证服务器的回应也必须一模一样包含这个参数 可选项 下面是一个例子： HTTP/1.1 200 OK Content-Type: application/json;charset=UTF-8 Cache-Control: no-store Pragma: no-cache { &quot;access_token&quot;:&quot;2YotnFZFEjr1zCsicMWpAA&quot;, &quot;token_type&quot;:&quot;example&quot;, &quot;expires_in&quot;:3600, &quot;refresh_token&quot;:&quot;tGzv3JOkF0XG5Qx2TlKWIA&quot;, &quot;example_parameter&quot;:&quot;example_value&quot; } (四). 客户端模式 客户端模式（Client Credentials Grant）指客户端以自己的名义，而不是以用户的名义，向“服务提供商”进行认证。严格地说，客户端模式并不属于 OAuth 框架所要解决的问题。 在这种模式中，用户直接向客户端注册，客户端以自己的名义要求“服务提供商”提供服务，其实不存在授权问题。 它的步骤如下： (A) 客户端向认证服务器进行身份认证，并要求一个访问令牌。 (B) 认证服务器确认无误后，向客户端提供访问令牌。 A 步骤中，客户端发出的 HTTP 请求，包含以下参数： 参数 具体含义 是否必填 grant_type 授权类型 必选项，此处的值固定为“clientcredentials” scope 权限范围 可选项。 POST /token HTTP/1.1 Host: server.example.com Authorization: Basic czZCaGRSa3F0MzpnWDFmQmF0M2JW Content-Type: application/x-www-form-urlencoded grant_type=client_credentials 认证服务器必须以某种方式，验证客户端身份。 B 步骤中，认证服务器向客户端发送访问令牌，下面是一个例子： HTTP/1.1 200 OK Content-Type: application/json;charset=UTF-8 Cache-Control: no-store Pragma: no-cache { &quot;access_token&quot;:&quot;2YotnFZFEjr1zCsicMWpAA&quot;, &quot;token_type&quot;:&quot;example&quot;, &quot;expires_in&quot;:3600, &quot;example_parameter&quot;:&quot;example_value&quot; } 总结 本文介绍了 OAuth 2.0 的一些基本概念，以及四种授权模式：授权码模式、简单模式、密码模式和客户端模式。 ","link":"https://faded.auspicious.space/post/a-simple-explanation-of-oauth-2-0-authorization-mechanism/"},{"title":"Web 性能优化","content":" 嗨，送你一张Web性能优化地图 我们都知道对于Web应用来说性能很重要。然而性能优化相关的知识却非常的庞大并且杂乱。对于性能优化需要做些什么以及性能瓶颈是什么，通常我们并不清楚（不包括那些对性能优化有丰富经验的高手）。 事实上关于Web性能有很多可以优化的点，其中涉及到的知识大致可以划分为几类：度量标准、 编码优化、静态资源优化、交付优化、 构建优化、性能监控。 1 度量标准与设定目标 在进行性能优化之前，我们需要为应用选择一个正确的度量标准（性能指标）以及设定一个合理的优化目标。 并不是所有指标都同样重要，这取决于你的应用。最后根据度量标准设定一个现实的目标 1.1 度量标准 下面是一些值得考虑的指标： 首次有效绘制（First Meaningful Paint，简称 FMP，当主要内容呈现在页面上）。 英雄渲染时间（Hero Rendering Times，度量用户体验的新指标，当用户最关心的内容渲染完成） 可交互时间（Time to Interactive，简称 TTI，指页面布局已经稳定，关键的页面字体是可见的，并且主进程可用于处理用户输入，基本上用户可以点击UI并与其交互）。 输入响应（Input responsiveness，界面响应用户输入所需的时间）。 感知速度指数（Perceptual Speed Index，简称 PSI，测量页面在加载过程中视觉上的变化速度，分数越低越好）。 自定义指标，由业务需求和用户体验来决定。 FMP 与英雄渲染时间非常相似，但它们不一样的地方在于 FMP 不区分内容是否有用，不区分渲染出的内容是否是用户关心的。 1.2 设定目标 100 毫秒的界面响应时间与 60 fps。 速度指标（Speed Index）小于 1250 ms。 3G 网络环境下可交互时间小于 5s。 重要文件的大小预算小于 170 kb。 以上四种指标的设定都有据可循。详细信息请查看 RAIL 性能模型。 2 编码优化 编码优化涉及到应用的运行时性能，本小节介绍几个可以提升程序运行时性能的建议。 2.1 数据读取速度 事实上数据访问速度有快慢之分，下面列出几个影响数据访问速度的因素： 字面量与局部变量的访问速度最快，数组元素和对象成员相对较慢。 变量从局部作用域到全局作用域的搜索过程越长速度越慢。 对象嵌套的越深，读取速度就越慢。 对象在原型链中存在的位置越深，找到它的速度就越慢。 推荐的做法是缓存对象成员值。将对象成员值缓存到局部变量中会加快访问速度 2.2 DOM 应用在运行时，性能的瓶颈主要在于 DOM 操作的代价非常昂贵，下面列出一些关于 DOM 操作相关提升性能的建议： 在 JavaScript 中对 DOM 进行访问的代价非常高。请尽可能减少访问 DOM 的次数（建议缓存 DOM 属性和元素、把 DOM 集合的长度缓存到变量中并在迭代中使用。读变量比读 DOM 的速度要快很多。） 重排与重绘的代价非常昂贵。如果操作需要进行多次重排与重绘，建议先让元素脱离文档流，处理完毕后再让元素回归文档流，这样浏览器只会进行两次重排与重绘（脱离时和回归时）。 善于使用事件委托。 2.3 流程控制 下面列出一些流程控制相关的一些可以略微提升性能的细节，这些细节在大型开源项目中大量运用（例如 Vue）： 避免使用 for...in（它能枚举到原型，所以很慢）。 在 JavaScript 中倒序循环会略微提升性能。 减少迭代的次数。 基于循环的迭代比基于函数的迭代快 8 倍。 用 Map 表代替大量的 if-else 和 switch 会提升性能。 3 静态资源优化 Web应用的运行离不开静态资源，所以对静态资源的优化至关重要。 3.1 使用 Brotli 或 Zopfli 进行纯文本压缩 在最高级别的压缩下 Brotli 会非常慢（但较慢的压缩最终会得到更高的压缩率）以至于服务器在等待动态资源压缩的时间会抵消掉高压缩率带来的好处，但它非常适合静态文件压缩，因为它的解压速度很快。 使用 Zopfli 压缩可以比 Zlib 的最大压缩提升 3％ 至 8％。 3.2 图片优化 尽可能通过 srcset，sizes 和 &lt;picture&gt; 元素使用响应式图片。还可以通过 &lt;picture&gt; 元素使用 WebP 格式的图像。 响应式图片可能大家未必听说过，但响应式布局大家肯定都听说过。响应式图片与响应式布局类似，它可以在不同屏幕尺寸与分辨率的设备上都能良好工作（比如自动切换图片大小、自动裁切图片等）。 当然，如果您不满足这种尺度的优化，还可以对图片进行更深层次的优化。例如：模糊图片中不重要的部分以减小文件大小、使用自动播放与循环的 HTML5 视频替换 GIF 图，因为视频比 GIF 文件还小（好消息是未来可以通过 img 标签加载视频）。 4 交付优化 交付优化指的是对页面加载资源以及用户与网页之间的交付过程进行优化。 4.1 异步无阻塞加载 js JavaScript 文件的加载与执行会阻塞页面渲染，可以将 script 标签放到页面的最底部。但是更好的做法是异步无阻塞加载 js。有多种无阻塞加载 js 的方法：defer、async、动态创建 script 标签、使用 XHR 异步请求 js 代码并注入到页面。 但更推荐的做法是使用 defer 或 async。如果使用 defer 或 async 请将 script 标签放到 head 标签中，以便让浏览器更早地发现资源并在后台线程中解析并开始加载 js。 4.2 使用 Intersection Observer 实现懒加载 懒加载是一个比较常用的性能优化手段，下面列出了一些常用的做法： 可以通过 Intersection Observer 延迟加载图片、视频、广告脚本、或任何其他资源。 可以先加载低质量或模糊的图片，当图片加载完毕后再使用完整版图片替换它。 延迟加载所有体积较大的组件、字体、JS、视频或 iframe 是一个好主意 4.3 优先加载关键的 CSS CSS 资源的加载对浏览器渲染的影响很大，默认情况下浏览器只有在完成 &lt;head&gt; 标签中 CSS 的加载与解析之后才会渲染页面。如果 CSS 文件过大，用户就需要等待很长的时间才能看到渲染结果。针对这种情况可以将首屏渲染必须用到的CSS提取出来内嵌到 &lt;head&gt; 中，然后再将剩余部分的 CSS 用异步的方式加载。可以通过 Critical 做到这一点。 4.4 资源提示（Resource Hints） Resource Hints（资源提示）定义了 HTML 中的 link 元素与 dns-prefetch、preconnect、prefetch 与 prerender 之间的关系。它可以帮助浏览器决定应该连接到哪些源，以及应该获取与预处理哪些资源来提升页面性能。 4.4.1 dns-prefetch dns-prefetch 可以指定一个用于获取资源所需的源（origin），并提示浏览器应该尽可能早的解析。 &lt;link rel=&quot;dns-prefetch&quot; href=&quot;//example.com&quot;&gt; 4.4.2 preconnect preconnect 用于启动预链接，其中包含 DNS 查找，TCP 握手，以及可选的 TLS 协议，允许浏览器减少潜在的建立连接的开销。 &lt;link rel=&quot;preconnect&quot; href=&quot;//example.com&quot;&gt; &lt;link rel=&quot;preconnect&quot; href=&quot;//cdn.example.com&quot; crossorigin&gt; 4.4.3 prefetch prefetch 用于标识下一个导航可能需要的资源。浏览器会获取该资源，一旦将来请求该资源，浏览器可以提供更快的响应。 &lt;link rel=&quot;prefetch&quot; href=&quot;//example.com/next-page.html&quot; as=&quot;html&quot; crossorigin=&quot;use-credentials&quot;&gt; &lt;link rel=&quot;prefetch&quot; href=&quot;/library.js&quot; as=&quot;script&quot;&gt; 浏览器不会预处理、不会自动执行、不会将其应用于当前上下文。 as 与 crossorigin 选项都是可选的。 4.4.4 prerender prerender 用于标识下一个导航可能需要的资源。浏览器会获取并执行，一旦将来请求该资源，浏览器可以提供更快的响应。 &lt;link rel=&quot;prerender&quot; href=&quot;//example.com/next-page.html&quot;&gt; 浏览器将预加载目标页面相关的资源并执行来预处理 HTML 响应。 4.5 preload 通过一个现有元素（例如：img，script，link）声明资源会将获取与执行耦合在一起。然而应用可能只是想要先获取资源，当满足某些条件时再执行资源。 preload 提供了预获取资源的能力，可以将获取资源的行为从资源执行中分离出来。因此，preload 可以构建自定义的资源加载与执行。 例如，应用可以使用 preload 进行 CSS 资源的预加载、并且同时具备：高优先级、不阻塞渲染等特性。然后应用程序在合适的时间使用 CSS 资源： &lt;!-- 通过声明性标记预加载 CSS 资源 --&gt; &lt;link rel=&quot;preload&quot; href=&quot;/styles/other.css&quot; as=&quot;style&quot;&gt; &lt;!-- 或，通过JavaScript预加载 CSS 资源 --&gt; &lt;script&gt; var res = document.createElement(&quot;link&quot;); res.rel = &quot;preload&quot;; res.as = &quot;style&quot;; res.href = &quot;styles/other.css&quot;; document.head.appendChild(res); &lt;/script&gt; &lt;!-- 使用HTTP头预加载 --&gt; Link: &lt;https://example.com/other/styles.css&gt;; rel=preload; as=style 4.6 快速响应的用户界面 PSI（Perceptual Speed Index，感知速度指数）是提升用户体验的重要指标，让用户感觉到页面的反馈比没有反馈体验要好很多。 可以尝试使用骨架屏或添加一些 Loading 过渡动画提示用户体验。 输入响应（Input responsiveness）指标同样重要，甚至更重要。试想，用户点击了网页后缺毫无反应会是什么心情。JavaScript 的单线程大家已经不能再熟悉，这意味着当 JavaScript 在运行时用户界面处于“锁定”状态，所以 JavaScript 同步执行的时间越长，用户等待响应的时间也就越长。 据调查，JavaScript 执行 100 毫秒以上用户就会明显觉得网页变卡了。所以要严格限制每个 js 任务执行时间不能超过 100 毫秒。 解决方案是可以将一个大任务拆分成多个小任务分布在不同的 Macrotask 中执行（通俗的说是将大的 js 任务拆分成多个小任务异步执行）。或者使用 WebWorkers，它可以在 UI 线程外执行 js 代码运算，不会阻塞 UI 线程，所以不会影响用户体验。 应用越复杂，主动管理 UI 线程就越重要 5 构建优化 现代前端应用都需要有构建的过程，项目在构建过程中是否进行了合理的优化，会对 Web 应用的性能有着巨大的影响。例如：影响构建后文件的体积、代码执行效率、文件加载时间、首次有效绘制指标等。 5.1 使用预编译 拿 Vue 举例，如果您使用单文件组件开发项目，组件会在编译阶段将模板编译为渲染函数。最终代码被执行时可以直接执行渲染函数进行渲染。而如果您没有使用单文件组件预编译代码，而是在网页中引入 vue.min.js，那么应用在运行时需要先将模板编译成渲染函数，然后再执行渲染函数进行渲染。相比预编译，多了模板编译的步骤，所以会浪费很多性能。 5.2 使用 Tree-shaking、Scope hoisting、Code-splitting Tree-shaking 是一种在构建过程中清除无用代码的技术。使用 Tree-shaking 可以减少构建后文件的体积。 目前 Webpack 与 Rollup 都支持 Scope Hoisting。它们可以检查 import 链，并尽可能的将散乱的模块放到一个函数中，前提是不能造成代码冗余。所以只有被引用了一次的模块才会被合并。使用 Scope Hoisting 可以让代码体积更小并且可以降低代码在运行时的内存开销，同时它的运行速度更快。前面 2.1 节介绍了变量从局部作用域到全局作用域的搜索过程越长执行速度越慢， Scope Hoisting 可以减少搜索时间。 code-splitting 是 Webpack 中最引人注目的特性之一。此特性能够把代码分离到不同的 bundle 中，然后可以按需加载或并行加载这些文件。code-splitting 可以用于获取更小的 bundle，以及控制资源加载优先级，如果使用合理，会极大影响加载时间。 5.3 服务端渲染（SSR） 单页应用需要等 js 加载完毕后在前端渲染页面，也就是说在 js 加载完毕并开始执行渲染操作前的这段时间里浏览器会产生白屏。 服务端渲染（Server Side Render，简称 SSR）的意义在于弥补主要内容在前端渲染的成本，减少白屏时间，提升首次有效绘制的速度。可以使用服务端渲染来获得更快的首次有效绘制。 比较推荐的做法是：使用服务端渲染静态 HTML 来获得更快的首次有效绘制，一旦 JavaScript 加载完毕再将页面接管下来。 5.4 使用 import 函数动态导入模块 使用 import 函数可以在运行时动态地加载 ES2015 模块，从而实现按需加载的需求。 这种优化在单页应用中变得尤为重要，在切换路由的时候动态导入当前路由所需的模块，会避免加载冗余的模块（试想如果在首次加载页面时一次性把整个站点所需要的所有模块都同时加载下来会加载多少非必须的 js，应该尽可能的让加载的 js 更小，只在首屏加载需要的 js）。 使用静态 import 导入初始依赖模块。其他情况下使用动态 import 按需加载依赖 5.5 使用 HTTP 缓存头 正确设置 expires，cache-control 和其他 HTTP 缓存头。 推荐使用 Cache-control: immutable 避免重新验证。 6 其他 其他一些值得考虑的优化点： HTTP2。 使用最高级的 CDN（付费的比免费的强的多）。 优化字体。 其他垂直领域的性能优化。 ","link":"https://faded.auspicious.space/post/web-performance-optimization/"},{"title":"前端路由","content":" 你真的了解前端路由吗？ 前端路由方案 目前前端路由方案主要有以下几种： hash：可能是大多数人了解的模式，主要是基于锚点的原理实现。简单易用。 browser：即使用 html5 标准中的 History API 通过监听 popstate 事件来对 DOM 进行操作。每次路由变化都会引起重定向。 memory：这种实现是在内存中维护一个堆栈用于管理访问历史的方式，比较复杂。在早起移动端使用比较多。实现麻烦，问题也较多。现在很少有使用。RN 在使用这种路由模式。 static：主要用于 ssr。需要后端去管理路由。 前端路由解决的问题 根据路由变化显示不同的页面，完成页面切换。 通过 query 传参。 前端路由各种实现方案的对比 hash 路由优缺点 优点： 实现简单，兼容性好（兼容到 IE 8）。 绝大多数前端框架均提供了给予 hash 的路由实现。 不需要服务器端进行任何设置和开发。 除了资源加载和 Ajax 请求以外，不会发起其他请求。 缺点： 对于部分需要重定向的操作，后端无法获取 hash 部分内容，导致后台无法取得 URL 中的数据，典型的例子就是微信公众号的 OAuth 验证。 服务器端无法准确跟踪前端路由信息。 对于需要锚点功能的需求会与目前路由机制冲突。 browser 路由优缺点 优点： 对于重定向过程中不会丢失 URL 中的参数。后端可以拿到这部分数据。 绝大多数前段框架均提供了 browser 的路由实现。 后端可以准确跟踪路由信息。 可以使用 history.state 来获取当前 URL 对应的状态信息。 缺点： 兼容性不如 hash 路由（只兼容到 IE10）。 需要后端支持，每次返回 HTML 文档。 memory路由 优缺点 优点： 不存在兼容性问题，路由保存在内存中。 不需要服务器端提供支持。 缺点： 目前很少有前端路由模块提供对 memory 路由的实现（react-router 提供了 memory 实现）。 自己实现难度较大，且工作量也很大。 对于前进后退操作的路由管理非常麻烦，尤其是 Android 设备的 Backbutton。 static 路由优缺点（该路由方式主要用于 SSR。不做比较。） 如何选择合适的前端路由方案 以下建议作为参考： hash 模式适用场景 兼容 IE8。 没有重定向传参需求（第三方认证 OAuth）。 没有锚点跳跃需求。 后端不需要跟踪前端路由信息。 Hybrid App 需要将前端资源打包在应用内，因为 HTML 的域在 file:// 下，所以不能发生重定向。 history 模式适用场景 页面内锚点需求。 需要重定向传参。 同构直出。 后端跟踪路由信息。 附加路由信息（history.state）获取路由状态。 memory 模式适用场景： IE 8 以下兼容。 React Native。 ","link":"https://faded.auspicious.space/post/front-end-router/"},{"title":"二维码的生成细节和原理","content":" 二维码的生成细节和原理 二维码又称QR Code，QR 全称Quick Response，是一个近几年来移动设备上超流行的一种编码方式，它比传统的 Bar Code 条形码能存更多的信息，也能表示更多的数据类型：比如：字符，数字，日文，中文等等。这两天学习了一下二维码图片生成的相关细节，觉得这个玩意就是一个密码算法，在此写一这篇文章 ，揭露一下。供好学的人一同学习之。 关于QR Code Specification，可参看这个PDF：http://raidenii.net/files/datasheets/misc/qr_code.pdf。 1 基础知识 首先，我们先说一下二维码一共有 40 个尺寸。官方叫版本 Version。Version 1 是 21 x 21 的矩阵，Version 2 是 25 x 25 的矩阵，Version 3 是 29 的尺寸，每增加一个 version，就会增加 4 的尺寸，公式是：(V−1)×4+21(V-1)\\times 4 + 21(V−1)×4+21（VVV 是版本号） 最高 Version 40，(40−1)×4+21=177(40-1)\\times 4+21 = 177(40−1)×4+21=177，所以最高是 177 x 177 的正方形。 下面我们看看一个二维码的样例： 1.1 定位图案 Position Detection Pattern 是定位图案，用于标记二维码的矩形大小。这三个定位图案有白边叫 Separators for Postion Detection Patterns。之所以三个而不是四个意思就是三个就可以标识一个矩形了。 Timing Patterns 也是用于定位的。原因是二维码有 40 种尺寸，尺寸过大了后需要有根标准线，不然扫描的时候可能会扫歪了。 Alignment Patterns 只有 Version 2 以上（包括 Version2）的二维码需要这个东东，同样是为了定位用的。 1.2 功能性数据 Format Information 存在于所有的尺寸中，用于存放一些格式化数据的。 Version Information 在 &gt;= Version 7 以上，需要预留两块 3 x 6 的区域存放一些版本信息。 1.3 数据码和纠错码 除了上述的那些地方，剩下的地方存放 Data Code 数据码 和 Error Correction Code 纠错码。 2 数据编码 我们先来说说数据编码。QR码支持如下的编码： Numeric mode 数字编码，从 0 到 9。如果需要编码的数字的个数不是 3 的倍数，那么，最后剩下的 1 或 2 位数会被转成 4 或 7 bits，则其它的每 3 位数字会被编成 10，12，14 bits，编成多长还要看二维码的尺寸（下面有一个表 Table 3 说明了这点）。 Alphanumeric mode 字符编码。包括 0-9，大写的 A 到 Z（没有小写），以及符号 $ % * + – . / : 包括空格。这些字符会映射成一个字符索引表。如下所示：（其中的 SP 是空格，Char 是字符，Value 是其索引值） 编码的过程是把字符两两分组，然后转成下表的 45 进制，然后转成 11 bits 的二进制，如果最后有一个落单的，那就转成 6bits 的二进制。而编码模式和字符的个数需要根据不同的 version 尺寸编成 9，11 或 13 个二进制（如下表中 Table 3）。 Char Value Char Value Char Value Char Value Char Value Char Value Char Value Char Value 0 0 6 6 C 12 I 18 O 24 U 30 SP 36 . 42 1 1 7 7 D 13 J 19 P 25 V 31 $ 37 / 43 2 2 8 8 E 14 K 20 Q 26 W 32 % 38 : 44 3 3 9 9 F 15 L 21 R 27 X 33 * 39 4 4 A 10 G 16 M 22 S 28 Y 34 + 40 5 5 B 11 H 17 N 23 T 29 Z 35 - 41 Byte mode，字节编码，可以是 0-255 的 ISO-8859-1 字符。有些二维码的扫描器可以自动检测是否是 UTF-8 的编码。 Kanji mode 这是日文编码，也是双字节编码。同样，也可以用于中文编码。日文和汉字的编码会减去一个值。如：在 0x8140 to 0x9FFC 中的字符会减去 8140，在 0xE040 到 0xEBBF 中的字符要减去 0xC140，然后把结果前两个 16 进制位拿出来乘以 0xC0，然后再加上后两个 16 进制位，最后转成 13 bit 的编码。如下图示例： Input character “点” “茗” (Shift JIS value): 935F E4AA 1. Subtract 8140 or C140 935F - 8140 = 121F E4AA - C140 = 236A 2. Multiply m.s.b. by C0 12 × C0 = D80 23 × C0 = 1A40 3. Add I.s.b. D80 + 1F = D9F 1A40 + 6A = 1AAA 4. Convert to 13 bit binary 0D9F → 0 1101 1001 1111 1AAA → 1 1010 1010 1010 Extended Channel Interpretation (ECI) mode 主要用于特殊的字符集。并不是所有的扫描器都支持这种编码。 Structured Append mode 用于混合编码，也就是说，这个二维码中包含了多种编码格式。 FNC1 mode 这种编码方式主要是给一些特殊的工业或行业用的。比如 GS1 条形码之类的。 简单起见，后面三种不会在本文 中讨论。 下面两张表中， Table 2 是各个编码格式的“编号”，这个东西要写在 Format Information 中。注：中文是 1101。 Table 3 表示了，不同版本（尺寸）的二维码，对于，数字，字符，字节和 Kanji 模式下，对于单个编码的 2 进制的位数。（在二维码的规格说明书中，有各种各样的编码规范表，后面还会提到）。 Table 2 - Mode indicators Mode Indicator ECI 0111 Numeric 0001 Alphanumeric 0010 8-bit Byte 0100 Kanji 1000 Structured Append 0011 FNC1 0101(First position) 1001(Second posotion) Terminator(End of Message) 0000 Table 3 - Number of bits in Character Count Indicator Version Numeric Mode Alphanumeric Mode 8-bit Byte Mode Kanji Mode 1 to 9 10 9 8 8 10 to 26 12 11 16 10 27 to 40 14 13 16 12 下面我们看几个示例： 2.1 示例一：数字编码 在 Version 1 的尺寸下，纠错级别为 H 的情况下，编码：01234567： 把上述数字分成三组：012 345 67。 把他们转成二进制：012 转成 0000001100；345 转成 0101011001；67 转成 1000011。 把这三个二进制串起来：0000001100 0101011001 1000011。 把数字的个数转成二进制（Version 1-H 是10 bits）：8 个数字的二进制是 0000001000。 把数字编码的标志 0001 和第 4 步的编码加到前面：0001 0000001000 0000001100 0101011001 1000011。 2.2 示例二：字符编码 在 Version 1 的尺寸下，纠错级别为 H 的情况下，编码：AC-42： 从字符索引表中找到 AC-42 这五个字条的索引 (10,12,41,4,2)。 两两分组：(10,12) (41,4) (2)。 把每一组转成 11 bits 的二进制: (10,12) 10*45+12 等于 462 转成 00111001110； (41,4) 41*45+4 等于 1849 转成 11100111001； (2) 等于 2 转成 000010。 把这些二进制连接起来：00111001110 11100111001 000010。 把字符的个数转成二进制（Version 1-H 为 9 bits）：5 个字符，5 转成 000000101。 在头上加上编码标识 0010 和第 5 步的个数编码: 0010 000000101 00111001110 11100111001 000010。 3 结束符和补齐符 假如我们有个 HELLO WORLD 的字符串要编码，根据上面的示例二，我们可以得到下面的编码： 编码 字符数 HELLO WORLD 的编码 0010 000001011 01100001011 01111000110 10001011100 10110111000 10011010100 001101 我们还要加上结束符： 编码 字符数 HELLO WORLD 的编码 结束 0010 000001011 01100001011 01111000110 10001011100 10110111000 10011010100 001101 0000 3.1 按 8 bits 重排 如果所有的编码加起来不是 8 个倍数我们还要在后面加上足够的 0，比如上面一共有 78 个 bits，所以，我们还要加上 2 个 0，然后按 8 个 bits 分好组： 00100000 01011011 00001011 01111000 11010001 01110010 11011100 01001101 01000011 01000000 3.2 补齐码（Padding Bytes） 最后，如果如果还没有达到我们最大的 bits 数的限制，我们还要加一些补齐码（Padding Bytes），Padding Bytes 就是重复下面的两个 bytes：11101100 00010001（这两个二进制转成十进制是 236 和 17，我也不知道为什么，只知道 Spec 上是这么写的）关于每一个 version 的每一种纠错级别的最大 Bits 限制，可以参看 QR Code Spec 的第 28 页到 32 页的 Table-7 一表。 假设我们需要编码的是 Version 1 的 Q 纠错级，那么，其最大需要 104 个 bits，而我们上面只有 80 个 bits，所以，还需要补 24 个 bits，也就是需要 3 个 Padding Bytes，我们就添加三个，于是得到下面的编码： 00100000 01011011 00001011 01111000 11010001 01110010 11011100 01001101 01000011 01000000 11101100 00010001 11101100 上面的编码就是数据码了，叫 Data Codewords，每一个 8 bits 叫一个 codeword，我们还要对这些数据码加上纠错信息。 4 纠错码 上面我们说到了一些纠错级别，Error Correction Code Level，二维码中有四种级别的纠错，这就是为什么二维码有残缺还能扫出来，也就是为什么有人在二维码的中心位置加入图标。 错误 修正容量 L 水平 7% 的字码可被修正 M 水平 15% 的字码可被修正 Q 水平 25% 的字码可被修正 H 水平 30% 的字码可被修正 那么，QR 是怎么对数据码加上纠错码的？首先，我们需要对数据码进行分组，也就是分成不同的 Block，然后对各个 Block 进行纠错编码，对于如何分组，我们可以查看 QR Code Spec 的第 33 页到 44 页的 Table-13 到 Table-22 的定义表。注意最后两列： Number of Error Code Correction Blocks：需要分多少个块。 Error Correction Code Per Blocks：每一个块中的 code 个数，所谓的 code 的个数，也就是有多少个 8 bits 的字节。 5 134 L 26 1 (134, 108, 13) M 48 2 (67, 43, 12) Q 72 22 (33, 15, 9)(34, 16, 9) H 88 22 (33, 11, 11)(34, 12, 11) 6 172 L 36 2 (86, 68, 9) M 64 4 (43, 27, 8) Q 96 4 (43, 19, 12) H 112 4 (43, 15, 14) a (c, k, r): c = total number of codewords k = number of data codewords r = number of error correction capacity b Error correction capacity is less than half the number of error correction codewords to reduce the probability of misdecodes. 举个例子：上述的 Version 5 + Q 纠错级：需要 4 个 Blocks（2 个 Blocks 为一组，共两组），头一组的两个 Blocks 中各 15 个 bits 数据 + 各 9 个 bits 的纠错码（注：表中的 codewords 就是一个 8 bits 的 byte）（再注：最后一例中的（c, k, r）的公式为：c = k + 2 * r，因为后脚注解释了：纠错码的容量小于纠错码的一半）。 下图给一个 5-Q 的示例（因为二进制写起来会让表格太大，所以，我都用了十进制，我们可以看到每一块的纠错码有 18 个 codewords，也就是 18 个 8 bits 的二进制数）。 组 块 数据 对每个块的纠错码 1 1 67 85 70 134 87 38 85 194 119 50 6 18 6 103 38 213 199 11 45 115 247 241 223 229 248 154 117 154 111 86 161 111 39 2 246 246 66 7 118 134 242 7 38 86 22 198 199 146 6 87 204 96 60 202 182 124 157 200 134 27 129 209 17 163 163 120 133 2 1 182 230 247 119 50 7 118 134 87 38 82 6 134 151 50 7 148 116 177 212 76 133 75 242 238 76 195 230 189 10 108 240 192 141 2 70 247 118 86 194 6 151 50 16 236 17 236 17 236 17 236 235 159 5 173 24 147 59 33 106 40 255 172 82 2 131 32 178 236 注：二维码的纠错码主要是通过 Reed-Solomon error correction（里德-所罗门纠错算法）来实现的。对于这个算法，对于我来说是相当的复杂，里面有很多的数学计算，比如：多项式除法，把 1-255 的数映射成 2 的 nnn 次方（0≤n≤2550\\le n \\le 2550≤n≤255）的伽罗瓦域 Galois Field 之类的神一样的东西，以及基于这些基础的纠错数学公式，因为我的数据基础差，对于我来说太过复杂，所以我一时半会儿还有点没搞明白，还在学习中，所以，我在这里就不展开说这些东西了。还请大家见谅了。（当然，如果有朋友很明白，也繁请教教我） 5 最终编码 5.1 穿插放置 如果你以为我们可以开始画图，你就错了。二维码的混乱技术还没有玩完，它还要把数据码和纠错码的各个 codewords 交替放在一起。如何交替呢，规则如下： 对于数据码：把每个块的第一个 codewords 先拿出来按顺度排列好，然后再取第一块的第二个，如此类推。如：上述示例中的 Data Codewords 如下： 块 1 67 85 70 134 87 38 85 194 119 50 6 18 6 103 38 块 2 246 246 66 7 118 134 242 7 38 86 22 198 199 146 6 块 3 182 230 247 119 50 7 118 134 87 38 82 6 134 151 50 7 块 4 70 247 118 86 194 6 151 50 16 236 17 236 17 236 17 236 我们先取第一列的：67，246，182，70 然后再取第二列的：67，246，182，70，85，246，230，247 如此类推：67，246，182，70，85，246，230，247………，38，6，50，17，7，236 对于纠错码，也是一样： 块 1 213 199 11 45 115 247 241 223 229 248 154 117 154 111 86 161 111 39 块 2 87 204 96 60 202 182 124 157 200 134 27 129 209 17 163 163 120 133 块 3 148 116 177 212 76 133 75 242 238 76 195 230 189 10 108 240 192 141 块 4 235 159 5 173 24 147 59 33 106 40 255 172 82 2 131 32 178 236 和数据码取的一样，得到：213，87，148，235，199，204，116，159，…… …… 39，133，141，236 然后，再把这两组放在一起（纠错码放在数据码之后）得到： 67, 246, 182, 70, 85, 246, 230, 247, 70, 66, 247, 118, 134, 7, 119, 86, 87, 118, 50, 194, 38, 134, 7, 6, 85, 242, 118, 151, 194, 7, 134, 50, 119, 38, 87, 16, 50, 86, 38, 236, 6, 22, 82, 17, 18, 198, 6, 236, 6, 199, 134, 17, 103, 146, 151, 236, 38, 6, 50, 17, 7, 236, 213, 87, 148, 235, 199, 204, 116, 159, 11, 96, 177, 5, 45, 60, 212, 173, 115, 202, 76, 24, 247, 182, 133, 147, 241, 124, 75, 59, 223, 157, 242, 33, 229, 200, 238, 106, 248, 134, 76, 40, 154, 27, 195, 255, 117, 129, 230, 172, 154, 209, 189, 82, 111, 17, 10, 2, 86, 163, 108, 131, 161, 163, 240, 32, 111, 120, 192, 178, 39, 133, 141, 236 这就是我们的数据区。 5.2 Remainder Bits 最后再加上 Reminder Bits，对于某些 version 的 QR，上面的还不够长度，还要加上 Remainder Bits，比如：上述的 5Q 版的二维码，还要加上 7 个 bits，Remainder Bits 加零就好了。关于哪些 version 需要多少个 Remainder bit，可以参看 QR Code Spec 的第 15 页的 Table-1 的定义表。 6 画二维码图 6.1 Position Detection Pattern 首先，先把 Position Detection 图案画在三个角上。（无论 version 如何，这个图案的尺寸就是这么大）。 6.2 Alignment Pattern 然后，再把 Alignment 图案画上（无论 version 如何，这个图案的尺寸就是这么大）。 关于 Alignment 的位置，可以查看 QR Code Spec 的第 81 页的 Table-E.1 的定义表（下表是不完全表格）。 Table E.1 - Row/column coordinates of center module of Alignment Patterns Version Number of Alignment Patterns Row/Column coordinates of center module 10- 21618 31622 41626 51630 61634 7662238 8662442 9662646 10662850 下图是根据上述表格中的 Version 8 的一个例子（6，24，42） 6.3 Timing Pattern 接下来是 Timing Pattern 的线（这个不用多说了） 6.4 Format Information 再接下来是 Formation Information，下图中的蓝色部分。 Format Information 是一个 15 个 bits 的信息，每一个 bit 的位置如下图所示：（注意图中的 Dark Module，那是永远出现的） 这 15 个 bits 中包括： 5 个数据 bits：其中，2 个 bits 用于表示使用什么样的 Error Correction Level， 3 个 bits 表示使用什么样的 Mask。 10 个纠错 bits。主要通过 BCH Code 来计算。 然后 15 个 bits 还要与 101010000010010 做 XOR 操作。这样就保证不会因为我们选用了 00 的纠错级别和 000 的 Mask，从而造成全部为白色，这会增加我们的扫描器的图像识别的困难。 下面是一个示例： 关于 Error Correction Level 如下表所示： Error Correction Level Binary indicator L 01 M 00 Q 11 H 10 关于Mask图案如后面的Table 23所示。 6.5 Version Information 再接下来是 Version Information（版本 7 以后需要这个编码），下图中的蓝色部分。 Version Information 一共是 18 个 bits，其中包括 6 个 bits 的版本号以及 12 个 bits 的纠错码，下面是一个示例： 而其填充位置如下： 6.6 数据和数据纠错码 然后是填接我们的最终编码，最终编码的填充方式如下：从左下角开始沿着红线填我们的各个 bits，1 是黑色，0 是白色。如果遇到了上面的非数据区，则绕开或跳过。 6.7 掩码图案 这样下来，我们的图就填好了，但是，也许那些点并不均衡，如果出现大面积的空白或黑块，会告诉我们扫描识别的困难。所以，我们还要做 Masking 操作（靠，还嫌不复杂）QR 的 Spec 中说了，QR 有 8 个 Mask 你可以使用，如下所示：其中，各个 mask 的公式在各个图下面。所谓 mask，说白了，就是和上面生成的图做 XOR 操作。Mask 只会和数据区进行 XOR，不会影响功能区。（注：选择一个合适的 Mask 也是有算法的） 其 Mask 的标识码如下所示：（其中的 i, j 分别对应于上图的 x, y） Table 23 - Mask pattern generation conditions Mask Pattern Reference Condition 000 (i+j)mod 2=0(i+j)\\mod 2 = 0(i+j)mod2=0 001 imod 2=0i \\mod 2 = 0imod2=0 010 jmod 3=0j \\mod 3 = 0jmod3=0 011 (i+j)mod 3=0(i+j)\\mod 3 = 0(i+j)mod3=0 100 ((i div 2)+(j div 3))mod 2=0((i \\text{ div } 2)+(j \\text{ div } 3)) \\mod 2 = 0((i div 2)+(j div 3))mod2=0 101 (ij)mod 2+(ij)mod 3=0(ij)\\mod 2 + (ij)\\mod 3 = 0(ij)mod2+(ij)mod3=0 110 ((ij)mod 2+(ij)mod 3)mod 2=0((ij)\\mod 2 + (ij)\\mod 3) \\mod 2 = 0((ij)mod2+(ij)mod3)mod2=0 111 ((ij)mod 3+(i+j)mod 2)mod 2=0((ij) \\mod 3 + (i+j) \\mod 2) \\mod 2 = 0((ij)mod3+(i+j)mod2)mod2=0 下面是 Mask 后的一些样子，我们可以看到被某些 Mask XOR 了的数据变得比较零散了。 Mask 过后的二维码就成最终的图了。 好了，大家可以去尝试去写一下 QR 的编码程序，当然，你可以用网上找个 Reed Soloman 的纠错算法的库，或是看看别人的源代码是怎么实现这个繁锁的编码。 ","link":"https://faded.auspicious.space/post/qr-code-generation-details-and-principles/"},{"title":"curl 与 wget 高级用法","content":" curl与wget高级用法 curl（文件传输工具） 常用参数 -c，–cookie-jar：将cookie写入到文件 -b，–cookie：从文件中读取cookie -C，–continue-at：断点续传 -d，–data：http post方式传送数据 -D，–dump-header：把header信息写入到文件 -F，–from：模拟http表达提交数据 -s，–slient：减少输出信息 -o，–output：将信息输出到文件 -O，–remote-name：按照服务器上的文件名，存在本地 –l，–head：仅返回头部信息 -u，–user[user:pass]：设置http认证用户和密码 -T，–upload-file：上传文件 -e，–referer：指定引用地址 -x，–proxy：指定代理服务器地址和端口 -w，–write-out：输出指定格式内容 –retry：重试次数 –connect-timeout：指定尝试连接的最大时间/s 使用示例 例1：抓取页面到指定文件，如果有乱码可以使用 iconv 转码 # curl -o baidu.html www.baidu.com # curl –s –o baidu.html www.baidu.com |iconv -f utf-8 #减少输出信息 例2：模拟浏览器头（user-agent） # curl -A “Mozilla/4.0 (compatible;MSIE 6.0; &lt;a href=&quot;http://www.ttlsa.com/windows/&quot; title=&quot;windows&quot;target=&quot;_blank&quot;&gt;Windows&lt;/a&gt; NT 5.0)” www.baidu.com 例3：处理重定向页面 # curl –L http://192.168.1.100/301.&lt;a href=&quot;http://www.ttlsa.com/php/&quot; title=&quot;php&quot;target=&quot;_blank&quot;&gt;php&lt;/a&gt; #默认curl是不处理重定向 例4：模拟用户登陆，保存cookie信息到cookies.txt文件，再使用cookie登陆 # curl -c ./cookies.txt -F NAME=user -F PWD=***URL #NAME和PWD是表单属性不同，每个网站基本都不同 # curl -b ./cookies.txt –o URL 例5：获取HTTP响应头headers # curl -I http://www.baidu.com # curl -D ./header.txt http://www.baidu.com #将headers保存到文件中 例6：访问HTTP认证页面 ```bash # curl –u user:pass URL 例7：通过ftp上传和下载文件 # curl -T filename ftp://user:pass@ip/docs #上传 # curl -O ftp://user:pass@ip/filename #下载 wget（文件下载工具） 常用参数 启动参数 -V，–version：显示版本号 -h，–help：查看帮助 -b，–background：启动后转入后台执行 日志记录和输入文件参数 -o，–output-file=file：把记录写到file文件中 -a，–append-output=file：把记录追加到file文件中 -i，–input-file=file：从file读取url来下载 下载参数 -bind-address=address：指定本地使用地址 -t，-tries=number：设置最大尝试连接次数 -c，-continue：接着下载没有下载完的文件 -O，-output-document=file：将下载内容写入到file文件中 -spider：不下载文件 -T，-timeout=sec：设置响应超时时间 -w，-wait=sec：两次尝试之间间隔时间 –limit-rate=rate：限制下载速率 -progress=type：设置进度条 目录参数 -P，-directory-prefix=prefix：将文件保存到指定目录 HTTP参数 -http-user=user：设置http用户名 -http-passwd=pass：设置http密码 -U，–user-agent=agent：伪装代理 -no-http-keep-alive：关闭http活动链接，变成永久链接 -cookies=off：不使用cookies -load-cookies=file：在开始会话前从file文件加载cookies -save-cookies=file：在会话结束将cookies保存到file文件 FTP参数 -passive-ftp：默认值，使用被动模式 -active-ftp：使用主动模式 递归下载排除参数 -A，–accept=list：分号分割被下载扩展名的列表 -R，–reject=list：分号分割不被下载扩展名的列表 -D，–domains=list：分号分割被下载域的列表 –exclude-domains=list：分号分割不被下载域的列表 使用示例 例1：下载单个文件到当前目录下，也可以-P指定下载目录 # wget http://nginx.org/download/nginx-1.8.0.tar.gz 例2：对于网络不稳定的用户可以使用-c和–tries参数，保证下载完成 # wget –tries=20 -c http://nginx.org/download/nginx-1.8.0.tar.gz 例3：下载大的文件时，我们可以放到后台去下载，这时会生成wget-log文件来保存下载进度 # wget -b http://nginx.org/download/nginx-1.8.0.tar.gz 例4：可以利用—spider参数判断网址是否有效 # wget –spider http://nginx.org/download/nginx-1.8.0.tar.gz 例5：自动从多个链接下载文件 # cat url_list.txt #先创建一个URL文件 http://nginx.org/download/nginx-1.8.0.tar.gz http://nginx.org/download/nginx-1.6.3.tar.gz # wget -i url_list.txt 例6：限制下载速度 # wget –limit-rate=1m http://nginx.org/download/nginx-1.8.0.tar.gz 例7：登陆ftp下载文件 # wget –ftp-user=user –ftp-password=pass ftp://ip/filenam ","link":"https://faded.auspicious.space/post/curl-and-wget-in-deep/"},{"title":"追 MM 与设计模式","content":" 追MM与设计模式（23种设计模式巧妙解析，趣味理解） 在网络上流畅很广的一篇旧文，暂时没找到原作者，目前所看到的最早转载时间是 2005 年 2 月 28 日。作者用轻松的语言，形象解释了 23 种模式，有很好的启发作用。 创建型模式 FACTORY 追 MM 少不了请吃饭了，麦当劳的鸡翅和肯德基的鸡翅都是 MM 爱吃的东西，虽然口味有所不同，但不管你带 MM 去麦当劳或肯德基，只管向服务员说“来四个鸡翅”就行了。麦当劳和肯德基就是生产鸡翅的 Factory。 工厂模式 客户类和工厂类分开。消费者任何时候需要某种产品，只需向工厂请求即可。消费者无须修改就可以接纳新产品。缺点是当产品修改时，工厂类也要做相应的修改。如：如何创建及如何向客户端提供。 BUILDER MM 最爱听的就是“我爱你”这句话了，见到不同地方的 MM，要能够用她们的方言跟她说这句话哦，我有一个多种语言翻译机，上面每种语言都有一个按键，见到 MM 我只要按对应的键，它就能够用相应的语言说出“我爱你”这句话了，国外的 MM 也可以轻松搞掂，这就是我的“我爱你”builder。（这一定比美军在伊拉克用的翻译机好卖）。 建造模式 将产品的内部表象和产品的生成过程分割开来，从而使一个建造过程生成具有不同的内部表象的产品对象。建造模式使得产品内部表象可以独立的变化，客户不必知道产品内部组成的细节。建造模式可以强制实行一种分步骤进行的建造过程。 FACTORY METHOD 请 MM 去麦当劳吃汉堡，不同的MM有不同的口味，要每个都记住是一件烦人的事情，我一般采用 Factory Method 模式，带着MM到服务员那儿，说“要一个汉堡”，具体要什么样的汉堡呢，让 MM 直接跟服务员说就行了。 工厂方法模式 核心工厂类不再负责所有产品的创建，而是将具体创建的工作交给子类去做，成为一个抽象工厂角色，仅负责给出具体工厂类必须实现的接口，而不接触哪一个产品类应当被实例化这种细节。 PROTOTYPE 跟 MM 用 QQ 聊天，一定要说些深情的话语了，我搜集了好多肉麻的情话，需要时只要 copy 出来放到 QQ 里面就行了，这就是我的情话 prototype 了。（100 块钱一份，你要不要） 原始模型模式 通过给出一个原型对象来指明所要创建的对象的类型，然后用复制这个原型对象的方法创建出更多同类型的对象。原始模型模式允许动态的增加或减少产品类，产品类不需要非得有任何事先确定的等级结构，原始模型模式适用于任何的等级结构。缺点是每一个类都必须配备一个克隆方法。 SINGLETON 俺有 6 个漂亮的老婆，她们的老公都是我，我就是我们家里的老公 Sigleton，她们只要说道“老公”，都是指的同一个人，那就是我（刚才做了个梦啦，哪有这么好的事）。 单例模式 单例模式确保某一个类只有一个实例，而且自行实例化并向整个系统提供这个实例单例模式。单例模式只应在有真正的“单一实例”的需求时才可使用。 结构型模式 ADAPTER 在朋友聚会上碰到了一个美女 Sarah，从香港来的，可我不会说粤语，她不会说普通话，只好求助于我的朋友 Kent 了，他作为我和 Sarah 之间的 Adapter，让我和 Sarah 可以相互交谈了(也不知道他会不会耍我)。 适配器（变压器）模式 把一个类的接口变换成客户端所期待的另一种接口，从而使原本因接口原因不匹配而无法一起工作的两个类能够一起工作。适配类可以根据参数返还一个合适的实例给客户端。 BRIDGE 早上碰到 MM，要说早上好，晚上碰到 MM，要说晚上好；碰到 MM 穿了件新衣服，要说你的衣服好漂亮哦，碰到 MM 新做的发型，要说你的头发好漂亮哦。不要问我“早上碰到 MM 新做了个发型怎么说”这种问题，自己用 BRIDGE 组合一下不就行了。 桥梁模式 将抽象化与实现化脱耦，使得二者可以独立的变化，也就是说将他们之间的强关联变成弱关联，也就是指在一个软件系统的抽象化和实现化之间使用组合/聚合关系而不是继承关系，从而使两者可以独立的变化。 COMPOSITE Mary 今天过生日。“我过生日，你要送我一件礼物。”“嗯，好吧，去商店，你自己挑。”“这件 T 恤挺漂亮，买，这条裙子好看，买，这个包也不错，买。”“喂，买了三件了呀，我只答应送一件礼物的哦。”“什么呀，T 恤加裙子加包包，正好配成一套呀，小姐，麻烦你包起来。”“……”，MM 都会用 Composite 模式了，你会了没有？ 合成模式 合成模式将对象组织到树结构中，可以用来描述整体与部分的关系。合成模式就是一个处理对象的树结构的模式。合成模式把部分与整体的关系用树结构表示出来。合成模式使得客户端把一个个单独的成分对象和由他们复合而成的合成对象同等看待。 DECORATOR Mary 过完轮到 Sarly 过生日，还是不要叫她自己挑了，不然这个月伙食费肯定玩完，拿出我去年在华山顶上照的照片，在背面写上“最好的的礼物，就是爱你的Fita”，再到街上礼品店买了个像框（卖礼品的 MM 也很漂亮哦），再找隔壁搞美术设计的 Mike 设计了一个漂亮的盒子装起来……，我们都是 Decorator，最终都在修饰我这个人呀，怎么样，看懂了吗？ 装饰模式 装饰模式以对客户端透明的方式扩展对象的功能，是继承关系的一个替代方案，提供比继承更多的灵活性。动态给一个对象增加功能，这些功能可以再动态的撤消。增加由一些基本功能的排列组合而产生的非常大量的功能。 FACADE 我有一个专业的 Nikon 相机，我就喜欢自己手动调光圈、快门，这样照出来的照片才专业，但 MM 可不懂这些，教了半天也不会。幸好相机有 Facade 设计模式，把相机调整到自动档，只要对准目标按快门就行了，一切由相机自动调整，这样 MM 也可以用这个相机给我拍张照片了。 门面模式 外部与一个子系统的通信必须通过一个统一的门面对象进行。门面模式提供一个高层次的接口，使得子系统更易于使用。每一个子系统只有一个门面类，而且此门面类只有一个实例，也就是说它是一个单例模式。但整个系统可以有多个门面类。 FLYWEIGHT 每天跟 MM 发短信，手指都累死了，最近买了个新手机，可以把一些常用的句子存在手机里，要用的时候，直接拿出来，在前面加上 MM 的名字就可以发送了，再不用一个字一个字敲了。共享的句子就是 Flyweight，MM 的名字就是提取出来的外部特征，根据上下文情况使用。 享元模式 FLYWEIGHT 在拳击比赛中指最轻量级。享元模式以共享的方式高效的支持大量的细粒度对象。享元模式能做到共享的关键是区分内蕴状态和外蕴状态。内蕴状态存储在享元内部，不会随环境的改变而有所不同。外蕴状态是随环境的改变而改变的。外蕴状态不能影响内蕴状态，它们是相互独立的。将可以共享的状态和不可以共享的状态从常规类中区分开来，将不可以共享的状态从类里剔除出去。客户端不可以直接创建被共享的对象，而应当使用一个工厂对象负责创建被共享的对象。享元模式大幅度的降低内存中对象的数量。 PROXY 跟 MM 在网上聊天，一开头总是“hi，你好”,“你从哪儿来呀？”“你多大了？”“身高多少呀？”这些话，真烦人，写个程序做为我的 Proxy 吧，凡是接收到这些话都设置好了自动的回答，接收到其他的话时再通知我回答，怎么样，酷吧。 代理模式 代理模式给某一个对象提供一个代理对象，并由代理对象控制对源对象的引用。代理就是一个人或一个机构代表另一个人或者一个机构采取行动。某些情况下，客户不想或者不能够直接引用一个对象，代理对象可以在客户和目标对象直接起到中介的作用。客户端分辨不出代理主题对象与真实主题对象。代理模式可以并不知道真正的被代理对象，而仅仅持有一个被代理对象的接口，这时候代理对象不能够创建被代理对象，被代理对象必须有系统的其他角色代为创建并传入。 行为模式 CHAIN OF RESPONSIBLEITY 晚上去上英语课，为了好开溜坐到了最后一排，哇，前面坐了好几个漂亮的 MM 哎，找张纸条，写上“Hi，可以做我的女朋友吗？如果不愿意请向前传”，纸条就一个接一个的传上去了，糟糕，传到第一排的 MM 把纸条传给老师了，听说是个老处女呀，快跑! 责任链模式 在责任链模式中，很多对象由每一个对象对其下家的引用而接起来形成一条链。请求在这个链上传递，直到链上的某一个对象决定处理此请求。客户并不知道链上的哪一个对象最终处理这个请求，系统可以在不影响客户端的情况下动态的重新组织链和分配责任。处理者有两个选择：承担责任或者把责任推给下家。一个请求可以最终不被任何接收端对象所接受。 COMMAND 俺有一个 MM 家里管得特别严，没法见面，只好借助于她弟弟在我们俩之间传送信息，她对我有什么指示，就写一张纸条让她弟弟带给我。这不，她弟弟又传送过来一个 COMMAND，为了感谢他，我请他吃了碗杂酱面，哪知道他说：“我同时给我姐姐三个男朋友送 COMMAND，就数你最小气，才请我吃面。”，😦 命令模式 命令模式把一个请求或者操作封装到一个对象中。命令模式把发出命令的责任和执行命令的责任分割开，委派给不同的对象。命令模式允许请求的一方和发送的一方独立开来，使得请求的一方不必知道接收请求的一方的接口，更不必知道请求是怎么被接收，以及操作是否执行，何时被执行以及是怎么被执行的。系统支持命令的撤消。 INTERPRETER 俺有一个《泡 MM 真经》，上面有各种泡 MM 的攻略，比如说去吃西餐的步骤、去看电影的方法等等，跟 MM 约会时，只要做一个 Interpreter，照着上面的脚本执行就可以了。 解释器模式 给定一个语言后，解释器模式可以定义出其文法的一种表示，并同时提供一个解释器。客户端可以使用这个解释器来解释这个语言中的句子。解释器模式将描述怎样在有了一个简单的文法后，使用模式设计解释这些语句。在解释器模式里面提到的语言是指任何解释器对象能够解释的任何组合。在解释器模式中需要定义一个代表文法的命令类的等级结构，也就是一系列的组合规则。每一个命令对象都有一个解释方法，代表对命令对象的解释。命令对象的等级结构中的对象的任何排列组合都是一个语言。 ITERATOR 我爱上了 Mary，不顾一切的向她求婚。 Mary：“想要我跟你结婚，得答应我的条件” 我：“什么条件我都答应，你说吧” Mary：“我看上了那个一克拉的钻石” 我：“我买，我买，还有吗？” Mary：“我看上了湖边的那栋别墅” 我：“我买，我买，还有吗？” Mary：“你的小弟弟必须要有 50cm 长” 我脑袋嗡的一声，坐在椅子上，一咬牙：“我剪，我剪，还有吗？” …… 迭代子模式 迭代子模式可以顺序访问一个聚集中的元素而不必暴露聚集的内部表象。多个对象聚在一起形成的总体称之为聚集，聚集对象是能够包容一组对象的容器对象。迭代子模式将迭代逻辑封装到一个独立的子对象中，从而与聚集本身隔开。迭代子模式简化了聚集的界面。每一个聚集对象都可以有一个或一个以上的迭代子对象，每一个迭代子的迭代状态可以是彼此独立的。迭代算法可以独立于聚集角色变化。 MEDIATOR 四个 MM 打麻将，相互之间谁应该给谁多少钱算不清楚了，幸亏当时我在旁边，按照各自的筹码数算钱，赚了钱的从我这里拿，赔了钱的也付给我，一切就 OK 啦，俺得到了四个 MM 的电话。 调停者模式 调停者模式包装了一系列对象相互作用的方式，使得这些对象不必相互明显作用。从而使他们可以松散偶合。当某些对象之间的作用发生改变时，不会立即影响其他的一些对象之间的作用。保证这些作用可以彼此独立的变化。调停者模式将多对多的相互作用转化为一对多的相互作用。调停者模式将对象的行为和协作抽象化，把对象在小尺度的行为上与其他对象的相互作用分开处理。 MEMENTO 同时跟几个 MM 聊天时，一定要记清楚刚才跟 MM 说了些什么话，不然 MM 发现了会不高兴的哦，幸亏我有个备忘录，刚才与哪个 MM 说了什么话我都拷贝一份放到备忘录里面保存，这样可以随时察看以前的记录啦。 备忘录模式 备忘录对象是一个用来存储另外一个对象内部状态的快照的对象。备忘录模式的用意是在不破坏封装的条件下，将一个对象的状态捉住，并外部化，存储起来，从而可以在将来合适的时候把这个对象还原到存储起来的状态。 OBSERVER 想知道咱们公司最新 MM 情报吗？加入公司的 MM 情报邮件组就行了，Tom 负责搜集情报，他发现的新情报不用一个一个通知我们，直接发布给邮件组，我们作为订阅者（观察者）就可以及时收到情报啦 观察者模式 观察者模式定义了一种一队多的依赖关系，让多个观察者对象同时监听某一个主题对象。这个主题对象在状态上发生变化时，会通知所有观察者对象，使他们能够自动更新自己。 STATE 跟 MM 交往时，一定要注意她的状态哦，在不同的状态时她的行为会有不同，比如你约她今天晚上去看电影，对你没兴趣的 MM 就会说“有事情啦”，对你不讨厌但还没喜欢上的 MM 就会说“好啊，不过可以带上我同事么？”，已经喜欢上你的 MM 就会说“几点钟？看完电影再去泡吧怎么样？”，当然你看电影过程中表现良好的话，也可以把 MM 的状态从不讨厌不喜欢变成喜欢哦。 状态模式 状态模式允许一个对象在其内部状态改变的时候改变行为。这个对象看上去象是改变了它的类一样。状态模式把所研究的对象的行为包装在不同的状态对象里，每一个状态对象都属于一个抽象状态类的一个子类。状态模式的意图是让一个对象在其内部状态改变的时候，其行为也随之改变。状态模式需要对每一个系统可能取得的状态创立一个状态类的子类。当系统的状态变化时，系统便改变所选的子类。 STRATEGY 跟不同类型的 MM 约会，要用不同的策略，有的请电影比较好，有的则去吃小吃效果不错，有的去海边浪漫最合适，单目的都是为了得到 MM 的芳心，我的追 MM 锦囊中有好多 Strategy 哦。 策略模式 策略模式针对一组算法，将每一个算法封装到具有共同接口的独立的类中，从而使得它们可以相互替换。策略模式使得算法可以在不影响到客户端的情况下发生变化。策略模式把行为和环境分开。环境类负责维持和查询行为类，各种算法在具体的策略类中提供。由于算法和环境独立开来，算法的增减，修改都不会影响到环境和客户端。 TEMPLATE METHOD 看过《如何说服女生上床》这部经典文章吗？女生从认识到上床的不变的步骤分为巧遇、打破僵局、展开追求、接吻、前戏、动手、爱抚、进去八大步骤（Template method），但每个步骤针对不同的情况，都有不一样的做法，这就要看你随机应变啦(具体实现)； 模板方法模式 模板方法模式准备一个抽象类，将部分逻辑以具体方法以及具体构造子的形式实现，然后声明一些抽象方法来迫使子类实现剩余的逻辑。不同的子类可以以不同的方式实现这些抽象方法，从而对剩余的逻辑有不同的实现。先制定一个顶级逻辑框架，而将逻辑的细节留给具体的子类去实现。 VISITOR 情人节到了，要给每个 MM 送一束鲜花和一张卡片，可是每个 MM 送的花都要针对她个人的特点，每张卡片也要根据个人的特点来挑，我一个人哪搞得清楚，还是找花店老板和礼品店老板做一下 Visitor，让花店老板根据 MM 的特点选一束花，让礼品店老板也根据每个人特点选一张卡，这样就轻松多了； 访问者模式 访问者模式的目的是封装一些施加于某种数据结构元素之上的操作。一旦这些操作需要修改的话，接受这个操作的数据结构可以保持不变。访问者模式适用于数据结构相对未定的系统，它把数据结构和作用于结构上的操作之间的耦合解脱开，使得操作集合可以相对自由的演化。访问者模式使得增加新的操作变的很容易，就是增加一个新的访问者类。访问者模式将有关的行为集中到一个访问者对象中，而不是分散到一个个的节点类中。当使用访问者模式时，要将尽可能多的对象浏览逻辑放在访问者类中，而不是放到它的子类中。访问者模式可以跨过几个类的等级结构访问属于不同的等级结构的成员类。 ","link":"https://faded.auspicious.space/post/chasing-mm-and-design-patterns/"},{"title":"嵌入式知识总汇","content":" 嵌入式知识总汇 嵌入式工程师 硬件核心(Hardware Core) Controller(控制器) CPU AMD x86 x64 Intel x86 x64 MCU MCS-51 HCS12 AVR XMEGA ESP Coldfire ARM STM32 FreeScale K60 K40 PIC32 PIC12/14/16 TM4C123 TM4C129X Stellaris lm4f120 CC3200 Renesas R-Car Boards MIPS Creator CI20 MPU SOC OMAP BCM2835(Raspberry Pi A+、B、B+) BCM2836(Raspberry Pi 2B) CPLD Altera EPM7128S Lattice LC4128V Xilinx XC9500 FPGA Altera Cyclone Lattice MachXO2 MachXO3 PowerPC MPC505、821、850、860、8240、8245 QUICC ⅡMPC826 QUICC Ⅲ MPC8560 DSC TMS320F28x MC56 F83x MIPS Actuator(执行器) 直流电机 有刷直流电机 无刷直流电机 交流电机 单相电机 三相电机 步进电机 直线电机 舵机 伺服电机 Sensor(传感器) RIP(人体红外) Temperature(温度) humidity sensor（湿度传感器） geomagnetic sensor（地磁传感器） acceleration transducer（加速度传感器） optoelectronic switch （光电开关） pneumatic sensor (气流传感器) current sensor (电流传感器) soil sensor (土壤传感器) llumination sensor (光照传感器) laser sensor (激光传感器) Motion(运动) Moisture sensor (土壤温度传感器) 电压传感器 three-axis gyroscope（三轴陀螺仪） Camera 感光元件（图像传感器） baroceptor（气压传感器） gas sensor（气敏传感器） Dust concentration sensor（粉尘浓度传感器）PM2.5 Water level sensor（水位传感器） Pulse sensor（脉搏传感器） EEG sensor（脑皮质电位传感器） Flex Sensor (弯曲传感器） proximity sensor (近距离传感器) Atmospheric Pressure Sensor （气压传感器） Electronic Component(电子元件) 电容 保护装置 端子与连接器 电线 开关 Switch Keypad Relay 电阻 Thermistor (热敏电阻) Varistor(压敏电阻) Photosensitive resistance(光敏电阻) 电磁感应装置 network 电阻排 忆阻器 压电装置、晶体谐振器 电源 二极管 晶体管 MCU/CPU/DSP/SOC peripherals ADC / DAC RTC Timer / Watchdog Timer (定时器) Flash / NAND Flash ROM / EEPROM RAM SDRAM DDR1 DDR2 DDR3 DDR4 Cache PWM I/O / GPIO UART Modem DTMF CAN SPI I2C IEEE CRC Interrupt (中断) Communication(通信) 硬件间无线通信 Bluetooth Xbee / Zigbee Z-Wave 6LoWPAN NFC Wifi Radio IR (红外) 802.11 Sub-GHZ 1Ghz以下RF 硬件通信 UART I2C RS232C/RS485 HDLC SPI/SCI/SI CAN USB FireWire 1-wire 网络协议 HTTP CoAP MQTT XMPP WebSocket UDP TCP 软件编程 Common ISR Driver DMA I2C OS uCOS Contiki TinyOS VXWorks FreeRTOS mbed OS emOS Salvo MQX RIOT rt-thread Linux uCLinux openWRT Windows Windows CE Windows 10 IoT Core non-OS LwIP 电路 基础 焊电路 模拟电路 数字电路 PCB设计 设计工具 Eagle Cadence Protel Altium Mentor Kicad gEAD ZUKEN PADS FreePCB Fritzing PCBmodE 布线/布线 电路仿真 原理仿真软件 Multisim Proteus Pspice 信号仿真软件 ADS HFSS HyperLynx Sigrity MATLAB ","link":"https://faded.auspicious.space/post/embedded-summary/"},{"title":"嵌入式知识总汇","content":" 嵌入式知识总汇 嵌入式工程师 硬件核心(Hardware Core) Controller(控制器) CPU AMD x86 x64 Intel x86 x64 MCU MCS-51 HCS12 AVR XMEGA ESP Coldfire ARM STM32 FreeScale K60 K40 PIC32 PIC12/14/16 TM4C123 TM4C129X Stellaris lm4f120 CC3200 Renesas R-Car Boards MIPS Creator CI20 MPU SOC OMAP BCM2835(Raspberry Pi A+、B、B+) BCM2836(Raspberry Pi 2B) CPLD Altera EPM7128S Lattice LC4128V Xilinx XC9500 FPGA Altera Cyclone Lattice MachXO2 MachXO3 PowerPC MPC505、821、850、860、8240、8245 QUICC ⅡMPC826 QUICC Ⅲ MPC8560 DSC TMS320F28x MC56 F83x MIPS Actuator(执行器) 直流电机 有刷直流电机 无刷直流电机 交流电机 单相电机 三相电机 步进电机 直线电机 舵机 伺服电机 Sensor(传感器) RIP(人体红外) Temperature(温度) humidity sensor（湿度传感器） geomagnetic sensor（地磁传感器） acceleration transducer（加速度传感器） optoelectronic switch （光电开关） pneumatic sensor (气流传感器) current sensor (电流传感器) soil sensor (土壤传感器) llumination sensor (光照传感器) laser sensor (激光传感器) Motion(运动) Moisture sensor (土壤温度传感器) 电压传感器 three-axis gyroscope（三轴陀螺仪） Camera 感光元件（图像传感器） baroceptor（气压传感器） gas sensor（气敏传感器） Dust concentration sensor（粉尘浓度传感器）PM2.5 Water level sensor（水位传感器） Pulse sensor（脉搏传感器） EEG sensor（脑皮质电位传感器） Flex Sensor (弯曲传感器） proximity sensor (近距离传感器) Atmospheric Pressure Sensor （气压传感器） Electronic Component(电子元件) 电容 保护装置 端子与连接器 电线 开关 Switch Keypad Relay 电阻 Thermistor (热敏电阻) Varistor(压敏电阻) Photosensitive resistance(光敏电阻) 电磁感应装置 network 电阻排 忆阻器 压电装置、晶体谐振器 电源 二极管 晶体管 MCU/CPU/DSP/SOC peripherals ADC / DAC RTC Timer / Watchdog Timer (定时器) Flash / NAND Flash ROM / EEPROM RAM SDRAM DDR1 DDR2 DDR3 DDR4 Cache PWM I/O / GPIO UART Modem DTMF CAN SPI I2C IEEE CRC Interrupt (中断) Communication(通信) 硬件间无线通信 Bluetooth Xbee / Zigbee Z-Wave 6LoWPAN NFC Wifi Radio IR (红外) 802.11 Sub-GHZ 1Ghz以下RF 硬件通信 UART I2C RS232C/RS485 HDLC SPI/SCI/SI CAN USB FireWire 1-wire 网络协议 HTTP CoAP MQTT XMPP WebSocket UDP TCP 软件编程 Common ISR Driver DMA I2C OS uCOS Contiki TinyOS VXWorks FreeRTOS mbed OS emOS Salvo MQX RIOT rt-thread Linux uCLinux openWRT Windows Windows CE Windows 10 IoT Core non-OS LwIP 电路 基础 焊电路 模拟电路 数字电路 PCB设计 设计工具 Eagle Cadence Protel Altium Mentor Kicad gEAD ZUKEN PADS FreePCB Fritzing PCBmodE 布线/布线 电路仿真 原理仿真软件 Multisim Proteus Pspice 信号仿真软件 ADS HFSS HyperLynx Sigrity MATLAB ","link":"https://faded.auspicious.space/post/qian-ru-shi-zhi-shi-zong-hui/"},{"title":"负载均衡集群 LVS 详解","content":" 负载均衡集群 LVS 详解 服务器扩展 当服务器遇到性能瓶颈需要进行扩展时，一般来说有两种解决思路：Scale-up 和 Scale out，也称作垂直扩展和水平扩展。 垂直扩展 通常指增加 CPU 和内存，购买昂贵的高性能服务器。 优点： 耗电量相比使用多台服务器要少； 实施简单。 缺点： 价格太昂贵； 由于资源的争用，服务器性能的增长会越来越小； 有很大的硬件故障导致服务不可用的风险； 受限制于供应商，且可扩展升级的空间是有限的。 水平扩展 通常指增加多台普通配置的服务器。 优点： 比起垂直扩展要便宜的多； 有容错能力； 易升级； 有着很大的扩展空间。 缺点： 服务器的维护和管理更加麻烦； 耗电和制冷的费用会比垂直扩展要高； 如果使用付费授权软件，那么会增加 lisence 的费用。 一般来说，随着服务器性能提升，其价格也是指数级上升的，使用水平扩展的方式能够节约很多成本，同时还能够增加整个服务的容错能力。 负载均衡集群 负载均衡集群指使用多台提供相同服务的服务器组成集群系统，提高服务的并发处理能力。负载均衡集群的前端使用一个调度器，将客户端请求平均分配到后端的服务器中，同时调度器可能还具有后端服务器状态检测的功能，将故障的服务器自动下线，使得集群具有一定的容错能力。 使用负载均衡集群能够有效的扩展服务的并发能力，负载均衡集群中的主机间应该尽量的「低耦合」，最好是「无状态」的，这样就能够方便的增加主机实现扩展。 常见的负载均衡器 根据工作在的协议层划分可划分为： 四层负载均衡：根据请求报文中的目标地址和端口进行调度； 七层负载均衡：根据请求报文的内容进行调度，这种调度属于「代理」的方式。 根据软硬件划分： 硬件负载均衡： F5 的 BIG-IP； Citrix 的 NetScaler。 这类硬件负载均衡器通常能同时提供四层和七层负载均衡，但同时也价格不菲。 软件负载均衡： TCP 层：LVS，HaProxy，Nginx； 基于 HTTP 协议：Haproxy，Nginx，ATS（Apache Traffic Server），squid，varnish； 基于 MySQL 协议：mysql-proxy。 LVS LVS 是一个工作在四层的负载均衡器，它的实现和 iptables/netfilter 类似，工作在内核空间的 TCP/IP 协议栈上，LVS 工作在 INPUT Hook Funtion 上，并在 INPUT 设置附加规则，一旦客户端请求的是集群服务，LVS 会强行修改请求报文，将报文发往 POSTROUTING，转发至后端的主机。 和 iptables/netfilter 类似，LVS 也是两段式的： ipvsadm：工作在用户空间，负责定义和管理集群服务的规则； ipvs：工作在内核中，在 2.4.23 之前，必须向内核打补丁，并重新编译内核。在 2.4.23 和 2.6 之后的版本，ipvs 直接内置在内核中。 LVS 集群的设备地址命名： VIP：Virtual IP，LVS 面向用户请求的 IP 地址； RIP：Real server IP，后端服务器用于和 LVS 通信的 IP 地址； DIP：Director IP，LVS 用户和后端服务器通信的 IP 地址； CIP：Client IP，客户端 IP 地址。 LVS 的工作模型 LVS-NAT LVS-NAT 模型类似于 DNAT，工作机制与 DNAT 一样，当客户端请求的是集群服务时，LVS 修改请求报文的目标地址为 RIP，转发至后端的 RealServer，并修改后端响应报文的源地址为 VIP，响应至客户端。 在 LVS-NAT 模型下，Director 进出请求报文都经过 Director，因此 Director 的压力是比较大的。 LVS-NAT 的特性： 集群节点跟 Director 必须在同一个 IP 网络中； RIP 通常是私有地址，仅用于各集群节点间的通信； Director 位于 client 和 Realserver 之间，负责处理进出的所有报文； Realserver 必须将网关指向 DIP； 支持端口映射； 较大规模应用场景中，Director 易成为系统瓶颈（bottleneck）。 LVS-DR DR 值 Direct Routing，直接路由，DR 模型中，Director 和 Realserver 处在同一网络中，对于 Director，VIP 用于接受客户端请求，DIP 用于和 Realserver 通信。对于 Realserver，每个 Realserver 都配有和 Director 相同的 VIP（此 VIP 隐藏，关闭对 ARP 请求的响应），仅用户响应客户端的请求，RIP 用于和 Director 通信。 当客户端请求集群服务时，请求报文发送至 Director 的 VIP（Realserver的 VIP 不会响应 ARP 请求），Director 将客户端报文的源和目标 MAC 地址进行重新封装，将报文转发至 Realserver，Realserver 接收转发的报文。此时报文的源 IP 和目标 IP 都没有被修改，因此 Realserver 接受到的请求报文的目标 IP 地址为本机配置的 VIP，它将使用自己的 VIP 直接响应客户端。 LVS-DR 模型中，客户端的响应报文不会经过 Director，因此 Director 的并发能力有很大提升。 LVS-DR 模型的特性： 保证前端路由器将目标地址为 VIP 的报文通过 ARP 解析后送往 Director。 静态绑定：在前端路由将 VIP 对应的目标 MAC 地址静态配置为Director VIP 接口的 MAC 地址。 arptables：在各 Realserver 上，通过 arptables 规则拒绝其响应对 VIP 的 ARP 广播请求。 修改内核参数：在 Realserver 上修改内核参数，并结合地址的配置方式实现拒绝响应对 VIP 的 ARP 广播请求。 各RIP 必须与 DIP 在同一个物理网络中。 RS 的 RIP 可以使用私有地址，也可以使用公网地址，以方便配置。 Director 仅负责处理入站请求，响应报文由 Realserver 直接发往客户端。 Realserver 不能将网关指向 DIP，而直接使用前端网关。 不支持端口映射。 LVS-TUN 和 DR 模型类似，Realserver 都配有不可见的 VIP，Realserver 的 RIP 是公网地址，且可能和 DIP 不再同一网络中。当请求到达 Director 后，Director 不修改请求报文的源 IP 和目标 IP 地址，而是使用 IP 隧道技术，使用 DIP 作为源 IP，RIP 作为目标 IP 再次封装此请求报文，转发至 RIP 的 Realserver 上，Realserver 解析报文后仍然使用 VIP 作为源地址响应客户端。 LVS-TUN 的特性： 集群节点和可以跨越 Internet； RIP，DIP，VIP 都是公网地址； Director 仅负责处理入站请求，响应报文由 Realserver 直接发往客户端； Realserver 使用自己的网关而不是 Director； Realserver 只能使用支持隧道功能的操作系统； 不支持端口映射。 LVS-FULLNAT FULLNAT 由淘宝研发，目前还没有加入至 CentOS 可用的内核中，使用时需要向内核打补丁。 类似于 DNAT，它修改请求报文的源地址为 DIP，目标地址为 RIP 来实现转发。对于响应报文，源地址修改为 VIP，目标地址修改为 CIP 来实现转发。 特点： RIP，DIP 可以使用私有地址； RIP 和 DIP 可以不再同一网络中，且 RIP 的网关不需要指向 DIP； 支持端口映射； 请求和响应报文都经由 Director。 LVS 的调度算法 当 LVS 接受到一个客户端对集群服务的请求后，它需要进行决策将请求调度至某一台后端主机进行响应。LVS 的调度算法共有 10 种，按类别可以分为动态和静态两种类型。 静态调度算法 静态调度算法调度时不会考虑后端服务器的状态 rr：round robin，轮询，即简单在各主机间轮流调度 wrr：weighted round robin，加权轮询，根据各主机的权重进行轮询 sh：source hash，源地址哈希，对客户端地址进行哈希计算，保存在 Director 的哈希表中，在一段时间内，同一个客户端 IP 地址的请求会被调度至相同的 Realserver。sh 算法的目的是实现 session affinity（会话绑定），但是它也在一定程度上损害了负载均衡的效果。如果集群本身有 session sharing 机制或者没有 session 信息，那么不需要使用 sh 算法 dh：destination hash，和 sh 类似，dh 将请求的目标地址进行哈希，将相同 IP 的请求发送至同一主机，dh 机制的目的是，当 Realserver 为透明代理缓存服务器时，提高缓存的命中率。 动态调度算法 动态调度算法在调度时，会根据后端 Realserver 的负载状态来决定调度选择，Realserver 的负载状态通常由活动链接（active），非活动链接（inactive）和权重来计算。 lc：least connted，最少连接，LVS 根据 overhead = active*256 + inactive 计算服务器的负载状态，每次选择 overhead 最小的服务器 wlc：weighted lc，加权最少连接，LVS 根据 overhead = (active*256+inactive)/weight 来计算服务器负载，每次选择 overhead 最小的服务器，它是 LVS 的默认调度算法 sed：shortest expected delay，最短期望延迟，它不对 inactive 状态的连接进行计算，根据 overhead = (active+1)*256/weight 计算服务器负载，选择 overhead 最小的服务器进行调度 nq：never queue，当有空闲服务器时，直接调度至空闲服务器，当没有空闲服务器时，使用 SED 算法进行调度 LBLC：locality based least connection，基于本地的最少连接，相当于 dh + wlc，正常请求下使用 dh 算法进行调度，如果服务器超载，则使用 wlc 算法调度至其他服务器 LBLCR：locality based least connection with replication，基于本地的带复制功能的最少连接，与 LBLC 不同的是 LVS 将请求 IP 映射至一个服务池中，使用 dh 算法调度请求至对应的服务池中，使用 lc 算法选择服务池中的节点，当服务池中的所有节点超载，使用 lc 算法从所有后端 Realserver 中选择一个添加至服务吃中。 ipvsadm ipvsadm 用于配置 LVS 的调度规则，管理集群服务和 Realserver。 管理集群服务： 添加：-A -t|u|f service-address [-s scheduler] -t: TCP协议的集群 -u: UDP协议的集群 service-address: IP:PORT -f: FWM: 防火墙标记 service-address: Mark Number 修改：-E 删除：-D -t|u|f service-address 例如： # ipvsadm -A -t 10.10.0.1:80 -s rr 管理集群服务中的 RS： 添加：-a -t|u|f service-address -r server-address [-g|i|m] [-w weight] -t|u|f service-address：事先定义好的某集群服务 -r server-address: 某RS的地址，在NAT模型中，可使用IP：PORT实现端口映射； [-g|i|m]: LVS类型 -g: DR -i: TUN -m: NAT [-w weight]: 定义服务器权重 修改：-e 删除：-d -t|u|f service-address -r server-address 例如： # ipvsadm -a -t 10.10.0.2:80 -r 192.168.10.8 -m # ipvsadm -a -t 10.10.0.3:80 -r 192.168.10.9 -m 查看规则： -L|-l -n：数字格式显式主机地址和端口 --stats：统计数据 --rate: 速率 --timeout: 显示tcp、tcpfin和udp的会话超时时长 -c: 显示当前的ipvs连接状况 删除所有集群服务： -C：清空 ipvs 规则 保存规则： -S 如： # ipvsadm -S &gt; /path/to/somefile 载入保存的规则： -R 如： # ipvsadm -R &lt; /path/from/somefile DR 模型的配置 DR 模型的 Realserver 禁止 ARP 响应 对于 Linux 来说，地址是属于主机的，Linux 主机在开机时会通告连接所有网络内的所有其他主机自己的所有 IP 地址和 MAC 地址。 可以利用 Linux 的特性，将VIP配置在 Realserver 的本地回环接口上作为别名，并使用 arp_ignore 和 arp_annouce 内核参数。 禁止 ARP 响应的方式 arptables：红帽系类系统上提供的程序 修改内核参数： arp_ignore：定义接收到 ARP 请i去时的响应级别 0：只要本地配置有响应地址，就给与响应； 1：仅在请求的目标地址配置请i去到达的接口上的时候，才进行响应。 arp_announce：定义主机将自己的地址想外通告时的通告级别 0：将本地任何接口上的任何地址向外通告； 1：向目标网络通告与其网络匹配的地址； 2：仅向本地接口上匹配的网络进行通告。 添加特殊的路由条目 Linux 主机在使用某一接口发出报文时，默认会使用此接口的 IP 作为源 IP 地址。 当请求 Realserver 时，Realserver 的 VIP 是 lo 接口的别名，而 VIP 对外的通信实际需要使用的却是 eth0 接口，因此需要添加路由条目，让主机在使用 VIP 向外通信时，强制使用 lo 端口，因而它会使用 lo 端口的地址作为源 IP 进行响应，并最终由 lo 接口转发至 eth0 接口发出报文。 # /sbin/route add -host $VIP dev lo:0 ","link":"https://faded.auspicious.space/post/a-detailed-explanation-of-load-balancing-cluster-lvs/"},{"title":"快慢指针应用总结","content":" 快慢指针应用总结 快慢指针 快慢指针中的快慢指的是移动的步长，即每次向前移动速度的快慢。例如可以让快指针每次沿链表向前移动 2，慢指针每次向前移动 1 次。 快慢指针的应用 判断单链表是否存在环 如果链表存在环，就好像操场的跑道是一个环形一样。此时让快慢指针都从链表头开始遍历，快指针每次向前移动两个位置，慢指针每次向前移动一个位置；如果快指针到达 NULL，说明链表以 NULL 为结尾，没有环。如果快指针追上慢指针，则表示有环。代码如下： bool HasCircle(Node *head) { if(head == NULL) return false; Node *slow = head, *fast = head; while(fast != NULL &amp;&amp; fast-&gt;next!=NULL) { slow = slow-&gt;next; //慢指针每次前进一步 fast = fast-&gt;next-&gt;next; //快指针每次前进两步 if(slow == fast) //相遇，存在环 return true; } return false; } 在有序链表中寻找中位数 快指针的移动速度是慢指针移动速度的 2 倍，因此当快指针到达链表尾时，慢指针到达中点。 程序还要考虑链表结点个数的奇偶数因素，当快指针移动 x 次后到达表尾（1+2x），说明链表有奇数个结点，直接返回慢指针指向的数据即可。 如果快指针是倒数第二个结点，说明链表结点个数是偶数，这时可以根据“规则”返回上中位数或下中位数或（上中位数+下中位数）的一半。 while (fast &amp;&amp; slow) { if (fast-&gt;next==NULL) return slow -&gt;data; else if (fast-&gt;next!= NULL &amp;&amp; fast-&gt;next-&gt;next== NULL) return (slow -&gt;data + slow -&gt;next-&gt;data)/2; else { fast= fast-&gt;next; fast= fast-&gt;next; slow = slow -&gt;next; } } 输出链表中的倒数第 K 个节点(即正数第 K-1 个节点) 可以定义两个指针，第一个指针从链表的头指针开始遍历向前走 k-1 步，第二个指针保持不动；从第 k 步开始，第二个指针也开始从链表的头指针开始遍历。由于两个指针的距离保持在 k-1，当第一个指针到达链表的尾节点时候，第二个指针正好是倒数第 k 个节点，代码如下： // 查找单链表中倒数第K个结点 ListNode * RGetKthNode(ListNode * pHead, unsigned int k) // 函数名前面的R代表反向 { if(k == 0 || pHead == NULL) // 这里k的计数是从1开始的，若k为0或链表为空返回NULL return NULL; ListNode * pAhead = pHead; ListNode * pBehind = pHead; for (int i=0;i&lt;k-1;i++) { pAhead=pAhead-&gt;next; if(pAhead==null) return null; //当链表长度小于k时候，返回Null } while(pAhead-&gt;next != NULL) // 前后两个指针一起向前走，直到前面的指针指向最后一个结点 { pBehind = pBehind-&gt;next; pAhead = pAhead-&gt;next; } return pBehind; // 后面的指针所指结点就是倒数第k个结点 } 判断链表是否存在环，如果存在，找到环入口 有一个单链表，其中可能有一个环，也就是某个节点的 next 指向的是链表中在它之前的节点，这样在链表的尾部形成一环。 如何判断一个链表是否存在环？设定两个指针 slow，fast，均从头指针开始，每次分别前进 1 步、2 步。如存在环，则两者相遇；如不存在环，fast 遇到 NULL 退出。 如果链表存在环，如果找到环的入口点？当 fast 若与 slow 相遇时，slow 肯定没有走遍历完链表或者恰好遍历一圈。于是我们从链表头与相遇点分别设一个指针，每次各走一步，两个指针必定相遇，且相遇第一点为环入口点。 node* findLoopPort(node *head) { node *fast, *slow; fast = slow = head; while (fast &amp;&amp; fast-&gt;next) { //第一步：判断链表是否存在环 slow = slow-&gt;next; fast = fast-&gt;next-&gt;next; if (slow == fast) { //链表存在环 break; } } if ((fast == NULL) || (fast-&gt;next == NULL)) { //链表不存在环 return NULL; } //第二步：寻找环的入口点 slow = head; //让slow回到链表的起点，fast留在相遇点 while (slow != fast) { //当slow和fast再次相遇时，那个点就是环的入口点 slow = slow-&gt;next; fast = fast-&gt;next; } return slow; } 判断两个单链表是否相交,如果相交，给出相交的第一个点（假设两个链表都不存在环）。 思路： 首先利用快慢指针判断链表是否存在环。 如果都不存在环，则如果两个单向链表有公共节点，也就是两个链表从某一节点开始，他们的 p_next 都指向同一个节点，每个节点只有一个 p-&gt;next。因此从第一个公共节点开始，之后它们所有节点都是重合的。因此，首先两个链表各遍历一次，求出两个链表的长度 L1L_1L1​、L2L_2L2​，然后可以得到它们的长度差 LLL。然后现在长的链表上遍历 LLL 个节点，之后再同步遍历，于是在遍历中，第一个相同的节点就是第一个公共的节点。此时，若两个链表长度分别为 MMM, NNN，则时间复杂度为 O(M+N)O(M+N)O(M+N)。 如果一个存在环，另外一个不存在环，则这两个链表是不可能相交的。 如果利用快慢指针发现两个链表都存在环，则判断任意一个链表上快慢指针相遇的那个节点，在不在另外一个链表上，如果在，则相交，不在，则不相交。 下面讨论两个没有环的链表如果是相交于某一结点的情况： 相交的链表示意图如下所示： 方法一 两个没有环的链表如果是相交于某一结点，如上图所示，这个结点后面都是共有的。所以如果两个链表相交，那么两个链表的尾结点的地址也是一样的。程序实现时分别遍历两个单链表，直到尾结点。判断尾结点地址是否相等即可。时间复杂度为 O(L1+L2)O(L_1+L_2)O(L1​+L2​)。 如何找到第一个相交结点？判断是否相交的时候，记录下两个链表的长度，算出长度差 len，接着先让较长的链表遍历 len 个长度，然后两个链表同时遍历，判断是否相等，如果相等，就是第一个相交的结点。 void Is_2List_Intersect(LinkList L1, LinkList L2) { if (L1 == NULL || L2 == NULL) { exit(ERROR); } LinkList p = L1; LinkList q = L2; int L1_length = 0; int L2_length = 0; int len = 0; while (p-&gt;next) { L1_length ++; p = p-&gt;next; } while (q-&gt;next) { L2_length ++; q = q-&gt;next; } printf(&quot;p: = %d\\n&quot;, p); printf(&quot;q: = %d\\n&quot;, q); printf(&quot;L1_length: = %d\\n&quot;, L1_length); printf(&quot;L2_length: = %d\\n&quot;, L2_length); if (p == q) { printf(&quot; 相交\\n&quot;); /*p重新指向短的链表 q指向长链表*/ if (L1_length &gt; L2_length) { len = L1_length - L2_length; p = L2; q = L1; } else { len = L2_length - L1_length; p = L1; q = L2; } while (len) { q = q-&gt;next; len--; } while (p != q) { p = p-&gt;next; q = q-&gt;next; } printf(&quot;相交的第一个结点是：%d\\n&quot;, p-&gt;data ); } else { printf(&quot;不相交 \\n&quot;); } } 方法二 另外一个方法则是将一个链表首尾相接，然后判断另外一个链表是否有环，如果有环，则两个链表相交。那么求第一个交点则求出有环的的那个链表的环结点即是。 int Is_ListLoop(LinkList L) { LinkList fast, slow; if (L == NULL || L-&gt;next == NULL) { exit(ERROR); } fast = slow = L; while (fast-&gt;next != NULL &amp;&amp; fast-&gt;next-&gt;next != NULL) { slow = slow-&gt;next; fast = fast-&gt;next-&gt;next; if (fast == slow) { return True; } } return False; } int Find_Loop(LinkList L) { LinkList fast, slow; if (L == NULL || L-&gt;next == NULL) { exit(ERROR); } fast = slow = L; while (fast-&gt;next != NULL &amp;&amp; fast-&gt;next-&gt;next != NULL) { slow = slow-&gt;next; fast = fast-&gt;next-&gt;next; if (fast == slow) { break; } } slow = L; while (fast != slow) { slow = slow-&gt;next; fast = fast-&gt;next; } printf(&quot;%d\\n&quot;, slow-&gt;data ); return TRUE; } void Is_2List_Intersect2(LinkList L1, LinkList L2) { if (L1 == NULL || L2 == NULL) { exit(ERROR); } LinkList p = L1; LinkList q = L2; while (p-&gt;next) { p = p-&gt;next; } p-&gt;next = L1-&gt;next; if(Is_ListLoop(L2)) { printf(&quot;相交\\n&quot;); printf(&quot;相交的第一个结点是：&quot;); Find_Loop(L2); } else { printf(&quot;不相交\\n&quot;); } } ","link":"https://faded.auspicious.space/post/fast-and-slow-pointer-application-summary/"},{"title":"漫谈《大型网站技术架构》","content":" 漫谈《大型网站技术架构》 一、大型网站的架构演化 1.1 大型网站软件的特点 高并发、大流量 高可用：系统7*24小时不间断提供服务 海量数据 用户分布广泛 安全环境恶劣 需求变更快，发布频繁 渐进式发展 1.2 大型网站架构演化发展历程 初始阶段的网站架构：一台服务器，上面同时拥有应用程序，数据库，文件，等所有资源。例如 LAMP 架构。 应用和数据服务分离：三台服务器（硬件资源各不相同），分别是应用服务器，文件服务器和数据库服务器。 使用缓存改善网站性能：分为两种，缓存在应用服务器上的本地缓存和缓存在专门的分布式缓存服务器的远程缓存。 使用应用服务器集群改善网站并发处理能力：通过负载均衡调度服务器来将访问请求分发到应用服务器集群中的任何一台机器。 数据库读写分离：数据库采用主从热备，应用服务器在写数据时访问主数据库，主数据库通过主从复制机制将数据更新同步到从数据库。应用服务器使用专门的数据访问模块从而对应用透明。 使用反向代理和 CDN 加速网站响应：这两者基本原理都是缓存。反向代理部署在网站的中心机房，CDN 部署在网络提供商的机房。 使用分布式文件系统和分布式数据库系统：数据库拆分的最后手段，更常用的是业务分库。 使用 NoSQL 和搜索引擎：对可伸缩的分布式有更好的支持。 业务拆分：将整个网站业务拆分成不同的应用，每个应用独立部署维护，应用之间通过超链接建立联系/消息队列进行数据分发/访问同一数据存储系统。 分布式服务：公共业务提取出来独立部署。 1.3 大型网站架构演化的价值观 大型网站架构的核心价值是随网站所需灵活应对。 驱动大型网站技术发展的主要力量是网站的业务发展。 1.4 网站架构设计误区 一味追随大公司的解决方案。 为了技术而技术。 企图用技术解决所有问题。 二、大型网站架构模式 2.1 网站架构模式 分层 -分割 -分布式 分布式应用和服务 分布式静态资源 分布式数据和存储 分布式计算 集群 缓存 CDN 反向代理 本地缓存 分布式缓存 异步 提供系统可用性 加快网站响应速度 消除高并发访问高峰 冗余 服务器冗余运行 数据库冗余备份 自动化 安全 防止 XSS 攻击 SQL 注入 三、大型网站核心架构要素 性能 可用性 伸缩性：不断地向服务器集群加服务器 扩张性 四、瞬时响应：网站的高性能架构 4.1 网站性能测试 不同视角下网站的性能 用户视角网站性能 响应时间 开发人员视角的网站性能 响应时间、并发量 运维人员视角的网站性能 资源 性能测试指标 响应时间 并发数 吞吐量 qps tps hps 性能计数器 性能测试方法 性能测试 负载测试 压力测试 稳定性测试 性能测试报告 性能优化策略 性能分析 性能优化 4.2 Web 前端性能优化 浏览器访问优化 减少 HTTP 请求 使用浏览器缓存 启用压缩 CSS 放在网页最上面 JS 最下面 减少 Cookie 传输 CDN 加速。（Content Delivery Network) 反向代理 4.3 应用服务器性能优化 分布式缓存 缓存的基本原理 合理的使用缓存 频繁修改数据 没有热点的访问 数据不一致与脏读 缓存可用性 缓存预热 缓存穿透 缓存不存在，直接访问数据库 分布式缓存架构 Memcached 异步操作 使用集群 代码优化 多线程 资源复用 数据结构 垃圾回收 4.4 存储性能优化 机械硬盘 vs. 固态硬盘 B+ 树 vs. LSM 树 RAID vs. HDFS 五、万无一失：网站的高可用架构 5.1 网站可用性的度量和考核 网站可行性度量 网站可用性考核 5.2 高可用的网站架构 5.3 高可用的应用 通过负载均衡进行无状态服务的失效转移 应用服务器集群的 Session 管理 Session复制 Session绑定 利用 Cookie 记录 Session Session服务器 5.4 高可用的应用 分级管理 超时设置 异步调用 服务降级 随机拒绝访问（twitter) 幂等性设计 5.5 高可用的数据 CAP 原理 数据持久性 数据可访问性 数据一致性 数据强一致性 数据用户一致性 数据最终一致性 数据备份 失效转移 失效确认 访问转移 数据恢复 5.6 高可用软件质量保障 网站发布 自动化测试 预发布验证 代码控制 主干开发，分支发布 分支开发，主干发布 自动化发布 灰度发布 5.7 网站运行监控 监控数据采集 用户行为日志收集 服务器性能检测 运行数据报告 监控管理 系统报警 失效转移 自动优雅降级 六、永无止尽：网站的伸缩性架构 6.1 网站伸缩性设计 不同功能进行物理分离实现伸缩 单一功能通过集群实现伸缩 6.2 应用服务器集群伸缩设计 HTTP 重定向负载均衡 DNS 域名解析负载均衡 反向代理负载均衡 IP 负载均衡 数据链路层负载均衡 负载均衡算法 轮询 加权轮询 随机 最少链接 原地址散列 6.3 分布式缓存集群的伸缩性设计 Memcached 分布式缓存集群的访问模型 Memcached 分布式缓存集群的伸缩性挑战 分布式缓存的一致性 hash 算法 6.4 数据存储服务器集群的伸缩性设计 关系数据库集群的伸缩性设计 NoSQL 数据库的伸缩性设计 七、随机应变：网站的可扩展性架构 7.1 构建可扩展性的网站架构 7.2 利用分布式消息队列降低系统耦合性 事件驱动架构 分布式消息队列 7.3 利用分布式服务打造可复用的业务平台 Web Service 与企业级分布式服务 大型网站分布式服务的需求与特点 负载均衡 失效转移 高效的远程通信 整合异构系统 对应用最少侵入 版本控制 实时监控 分布式服务框架设计 7.4 可扩展的数据结构 7.5 利用开放平台建设网站生态圈 API 接口 协议转移 安全 审计 路由 流程 八、固若金汤：网站的安全架构 8.1 道高一尺魔高一丈的网站应用攻击与防御 xss攻击 消毒 HTTP Only 注入攻击 开源 错误回显 盲注 消毒 参数绑定 CSRF 攻击 表单token 验证码 Referer Check 其他攻击和漏洞 Error Code HTML 注释 文件上传 路径遍历 Web 应用防火墙 网站安全漏洞扫描 8.2 信息加密技术及密钥安全管理 单向散列加密 对称加密 非对称加密 密钥安全管理 8.3 信息过滤与反垃圾 文本匹配 分类算法 黑名单 8.4 电子商务风险控制 风险 账号风险 买家风险 卖家风险 交易风险 风控 规则引擎 统计模型 ","link":"https://faded.auspicious.space/post/a-random-talk-on-large-website-technical-architecture/"},{"title":"UML—简介","content":" uml学习 概念 类（Class）：使用三层矩形框表示。 第一层显示类的名称，如果是抽象类，则就用斜体显示。 第二层是字段和属性。 第三层是类的方法。 注意前面的符号，+ 表示 public，- 表示 private，# 表示 protected。 接口：使用两层矩形框表示，与类图的区别主要是顶端有 &lt;&gt; 显示 。 第一行是接口名称。 第二行是接口方法。 继承类（extends）：用空心三角形 + 实线来表示。 实现接口（implements）：用空心三角形 + 虚线来表示。 关联（Association）：用实线箭头来表示，例如：燕子与气候。 聚合（Aggregation）：用空心的菱形 + 实线箭头来表示。 聚合：表示一种弱的‘拥有’关系，体现的是 A 对象可以包含 B 对象，但 B 对象不是 A 对象的一部分，例如：公司和员工。 组合（Composition）：用实心的菱形 + 实线箭头来表示。 组合：部分和整体的关系，并且生命周期是相同的。例如：人与手。 依赖（Dependency）：用虚线箭头来表示，例如：动物与氧气。 基数 ：连线两端的数字表明这一端的类可以有几个实例，比如：一个鸟应该有两只翅膀。如果一个类可能有无数个实例，则就用‘n’来表示。关联、聚合、组合是有基数的。 类之间的关系 UML 把类之间的关系分为以下 5 种： 关联：类 A 与类 B 的实例之间存在特定的对应关系； 依赖：类 A 访问类 B 提供的服务； 聚集：类 A 为整体类，类 B 为局部类，类 A 的对象由类 B 的对象组合而成； 泛化：类 A 继承类 B； 实现：类 A 实现了 B 接口 关联（Association） 关联指的是类之间的特定对应关系，在 UML 中用带实线的箭头表示。按照类之间的数量对比，关联可以分为以下三种： 一对一关联； 一对多关联； 多对多关联。 注意：关联还要以分为单向关联和双向关联。 依赖（Dependency） 依赖指的是类之间的调用关系，在 UML 中用带虚线的箭头表示。如果类 A 访问类 B 的属性或者方法， 或者类 A 负责实例化类 B，那么可以说类 A 依赖类 B。和关联关系不同，无须在类 A 中定义类 B 类型的属性。 聚集（Aggregation） 聚集指的是整体与部分之间的关系，在 UML 中用带实线的菱形箭头表示。 聚集关系还可以分为两种类型： 被聚集的子系统允许被拆卸和替换，这是普通聚集关系。 被聚集的子系统不允许被拆卸和替换，这种聚集称为强聚集关系，或者组成关系。 注：强聚集（组成）可用带实线的实心菱形箭头表示。 泛化（Generalization） 泛化指的是类之间的继承关系，在 UML 中用带实线的三角形箭头表示。 实现（Realization） 实现指的是类与接口之间的关系，在 UML 中用带虚线的三角形箭头表示。 ","link":"https://faded.auspicious.space/post/uml-introduction/"},{"title":"现代 IM 系统中消息推送和存储架构的实现","content":" 现代IM系统中消息推送和存储架构的实现 前言 IM 全称是“Instant Messaging”，中文名是即时通讯。在这个高度信息化的移动互联网时代，生活中 IM 类产品已经成为必备品，比较有名的如钉钉、微信、QQ 等以 IM 为核心功能的产品。当然目前微信已经成长为一个生态型产品，但其核心功能还是 IM。还有一些非以 IM 系统为核心的应用，最典型的如一些在线游戏、社交应用，IM 也是其重要的功能模块。可以说，带有社交属性的应用，IM 功能一定是必不可少的。 IM 系统在互联网初期即存在，其基础技术架构在这十几年的发展中更新迭代多次，从早期的 CS、P2P 架构，到现在后台已经演变为一个复杂的分布式系统，涉及移动端、网络、安全和存储等技术的方方面面。其支撑的规模也从早期的少量日活，到现在微信这个巨头最新公布的达到 9 亿的日活的体量。 IM 系统中最核心的部分是消息系统，消息系统中最核心的功能是消息的同步和存储： 消息的同步：将消息完整的、快速的从发送方传递到接收方，就是消息的同步。消息同步系统最重要的衡量指标就是消息传递的实时性、完整性以及能支撑的消息规模。从功能上来说，一般至少要支持在线和离线推送，高级的 IM 系统还支持“多端同步”。 消息的存储：消息存储即消息的持久化保存，这里不是指消息在客户端本地的保存，而是指云端的保存，功能上对应的就是“消息漫游”。“消息漫游”的好处是可以实现账号在任意端登陆查看所有历史消息，这也是高级 IM 系统特有的功能之一。 本篇文章内容主要涉及 IM 系统中的消息系统架构，会介绍一种基于 TableStore 构建的消息同步以及存储系统的架构实现，能够支持消息系统中的高级特性“多端同步”以及“消息漫游”。在性能和规模上，能够做到全量消息云端存储，百万 TPS 以及毫秒级延迟的消息同步能力。 架构设计 本章主要会介绍基于 TableStore 的现代 IM 消息系统的架构设计，在详细介绍架构设计之前，会先介绍一种 Timeline 逻辑模型，来抽象和简化对 IM 消息同步和存储模型的理解。理解了 Timeline 模型后，会介绍如何基于此模型对消息的同步以及存储进行建模。基于 Timeline 模型，在实现消息同步和存储时还会有各方面的技术权衡，例如如何对消息同步常见的读扩散和写扩散两种模型进行对比和选择，以及针对 Timeline 模型的特征如何来选择底层数据库。 传统架构 vs 现代架构 传统架构下，消息是先同步后存储。对于在线的用户，消息会直接实时同步到在线的接收方，消息同步成功后，并不会进行持久化。而对于离线的用户或者消息无法实时同步成功时，消息会持久化到离线库，当接收方重新连接后，会从离线库拉取所有未读消息。当离线库中的消息成功同步到接收方后，消息会从离线库中删除。传统的消息系统，服务端的主要工作是维护发送方和接收方的连接状态，并提供在线消息同步和离线消息缓存的能力，保证消息一定能够从发送方传递到接收方。服务端不会对消息进行持久化，所以也无法支持消息漫游。 现代架构下，消息是先存储后同步。先存储后同步的好处是，如果接收方确认接收到了消息，那这条消息一定是已经在云端保存了。并且消息会有两个库来保存，一个是消息存储库，用于全量保存所有会话的消息，主要用于支持消息漫游。另一个是消息同步库，主要用于接收方的多端同步。消息从发送方发出后，经过服务端转发，服务端会先将消息保存到消息存储库，后保存到消息同步库。完成消息的持久化保存后，对于在线的接收方，会直接选择在线推送。但在线推送并不是一个必须路径，只是一个更优的消息传递路径。对于在线推送失败或者离线的接收方，会有另外一个统一的消息同步方式。接收方会主动的向服务端拉取所有未同步消息，但接收方何时来同步以及会在哪些端来同步消息对服务端来说是未知的，所以要求服务端必须保存所有需要同步到接收方的消息，这是消息同步库的主要作用。对于新的同步设备，会有消息漫游的需求，这是消息存储库的主要作用，在消息存储库中，可以拉取任意会话的全量历史消息。 以上是传统架构和现代架构的一个简单的对比，现代架构上整个消息的同步和存储流程，并没有变复杂太多，但是其能实现多端同步以及消息漫游。现代架构中最核心的就是两个消息库“消息同步库”和“消息存储库”，是消息同步和存储最核心的基础。而本篇文章接下来的部分，都是围绕这两个库的设计和实现来展开。 Timeline 模型 在分析“消息同步库”和“消息存储库”的设计和实现之前，在本章会先介绍一个逻辑模型——Timeline。Timeline 模型会帮助我们简化对消息同步和存储模型的理解，而消息库的设计和实现也是围绕 Timeline 的特性和需求来展开。 如图是 Timeline 模型的一个抽象表述，Timeline 可以简单理解为是一个消息队列，但这个消息队列有如下特性： 每个消息拥有一个顺序 ID（SeqId），在队列后面的消息的 SeqId 一定比前面的消息的 SeqId 大，也就是保证 SeqId 一定是增长的，但是不要求严格递增。 新的消息永远在尾部添加，保证新的消息的 SeqId 永远比已经存在队列中的消息都大。 可根据 SeqId 随机定位到具体的某条消息进行读取，也可以任意读取某个给定范围内的所有消息。 有了这些特性后，消息的同步可以拿 Timeline 来很简单的实现。图中的例子中，消息发送方是 A，消息接收方是 B，同时 B 存在多个接收端，分别是 B1、B2 和 B3。A 向 B 发送消息，消息需要同步到 B 的多个端，待同步的消息通过一个 Timeline 来进行交换。A 向 B 发送的所有消息，都会保存在这个 Timeline 中，B 的每个接收端都是独立的从这个 Timeline 中拉取消息。每个接收端同步完毕后，都会在本地记录下最新同步到的消息的 SeqId，即最新的一个位点，作为下次消息同步的起始位点。服务端不会保存各个端的同步状态，各个端均可以在任意时间从任意点开始拉取消息。 消息漫游也是基于 Timeline，和消息同步唯一的区别是，消息漫游要求服务端能够对 Timeline 内的所有数据进行持久化。 基于 Timeline，从逻辑模型上能够很简单的理解在服务端如何去实现消息同步和存储，并支持多端同步和消息漫游这些高级功能。落地到实现的难点主要在如何将逻辑模型映射到物理模型，Timeline 的实现对数据库会有哪些要求？我们应该选择何种数据库去实现？这些是接下来会讨论到的问题。 消息存储模型 如图是基于 Timeline 的消息存储模型，消息存储要求每个会话都对应一个独立的 Timeline。如图例子所示，A 与 B/C/D/E/F 均发生了会话，每个会话对应一个独立的 Timeline，每个 Timeline 内存有这个会话中的所有消息，服务端会对每个 Timeline 进行持久化。服务端能够对所有会话 Timeline 中的全量消息进行持久化，也就拥有了消息漫游的能力。 消息同步模型 消息同步模型会比消息存储模型稍复杂一些，消息的同步一般有读扩散和写扩散两种不同的方式，分别对应不同的 Timeline 物理模型。 如图是读扩散和写扩散两种不同同步模式下对应的不同的 Timeline 模型，按图中的示例，A 作为消息接收者，其与 B/C/D/E/F 发生了会话，每个会话中的新的消息都需要同步到 A 的某个端，看下读扩散和写扩散两种模式下消息如何做同步。 读扩散 消息存储模型中，每个会话的 Timeline 中保存了这个会话的全量消息。读扩散的消息同步模式下，每个会话中产生的新的消息，只需要写一次到其用于存储的 Timeline 中，接收端从这个 Timeline 中拉取新的消息。优点是消息只需要写一次，相比写扩散的模式，能够大大降低消息写入次数，特别是在群消息这种场景下。但其缺点也比较明显，接收端去同步消息的逻辑会相对复杂和低效。接收端需要对每个会话都拉取一次才能获取全部消息，读被大大的放大，并且会产生很多无效的读，因为并不是每个会话都会有新消息产生。 写扩散 写扩散的消息同步模式，需要有一个额外的 Timeline 来专门用于消息同步，通常是每个接收端都会拥有一个独立的同步 Timeline，用于存放需要向这个接收端同步的所有消息。每个会话中的消息，会产生多次写，除了写入用于消息存储的会话 Timeline，还需要写入需要同步到的接收端的同步 Timeline。在个人与个人的会话中，消息会被额外写两次，除了写入这个会话的存储 Timeline，还需要写入参与这个会话的两个接收者的同步 Timeline。而在群这个场景下，写入会被更加的放大，如果这个群拥有 N 个参与者，那每条消息都需要额外的写 N 次。写扩散同步模式的优点是，在接收端消息同步逻辑会非常简单，只需要从其同步 Timeline 中读取一次即可，大大降低了消息同步所需的读的压力。其缺点就是消息写入会被放大，特别是针对群这种场景。 在 IM 这种应用场景下，通常会选择写扩散这种消息同步模式。IM 场景下，一条消息只会产生一次，但是会被读取多次，是典型的读多写少的场景，消息的读写比例大概是 10:1。若使用读扩散同步模式，整个系统的读写比例会被放大到 100:1。一个优化的好的系统，必须从设计上去平衡这种读写压力，避免读或写任意一维触碰到天花板。所以 IM 系统这类场景下，通常会应用写扩散这种同步模式，来平衡读和写，将 100:1 的读写比例平衡到 30:30。当然写扩散这种同步模式，还需要处理一些极端场景，例如万人大群。针对这种极端写扩散的场景，会退化到使用读扩散。一个简单的 IM 系统，通常会在产品层面限制这种大群的存在，而对于一个高级的 IM 系统，会采用读写扩散混合的同步模式，来满足这类产品的需求。 消息库设计 基于 Timeline 模型，以及 Timeline 模型在消息存储和消息同步的应用，我们看下消息同步库和消息存储库的设计。 如图是基于Timeline的消息库设计。 消息同步库 消息同步库用于存储所有用于消息同步的 Timeline，每个 Timeline 对应一个接收端，主要用作写扩散模式的消息同步。这个库不需要永久保留所有需要同步的消息，因为消息在同步到所有端后其生命周期就可以结束，就可以被回收。但是如前面所介绍的，一个实现简单的多端同步消息系统，在服务端不会保存有所有端的同步状态，而是依赖端自己主动来做同步。所以服务端不知道消息何时可以回收，通常的做法是为这个库里的消息设定一个固定的生命周期，例如一周或者一个月，生命周期结束可被淘汰。 消息存储库 消息存储库用于存储所有会话的 Timeline，每个 Timeline 包含了一个会话中的所有消息。这个库主要用于消息漫游时拉取某个会话的所有历史消息，也用于读扩散模式的消息同步。 消息同步库和消息存储库，对数据库有不同的要求，如何对数据库做选型，在下面会讨论。 数据库选型 消息系统最核心的两个库是消息同步库和消息存储库，两个库对数据库有不同的要求： 消息同步库 消息存储库 数据模型 Timeline 模型 Timeline 模型 写能力 高并发写，十万级 TPS 高并发写，少量读，万级 TPS 读能力 高并发范围读，十万级 TPS 少量范围读，千级 TPS 存储规模 保存一段时间内的同步消息，TB 级，保留千万级的 Timeline 规模 保存全量消息，百 TB 级，保留亿级的 Timeline 规模 总结下来，对数据库的要求有如下几点： 表结构设计能够满足 Timeline 模型的功能要求：不要求关系模型，能够实现队列模型，并能够支持生成自增的 SeqId。 能够支持高并发写和范围读，规模在十万级TPS。 能够保存海量数据，百TB级。 能够为数据定义生命周期。 阿里云表格存储（TableStore）是基于 LSM 存储引擎的分布式 NoSQL 数据库，支持百万 TPS 高并发读写，PB 级数据存储，数据支持 TTL，能够很好的满足以上需求，并且支持自增列，能够非常完美的设计和实现 Timeline 的物理模型。 ","link":"https://faded.auspicious.space/post/implementation-of-message-push-and-storage-architecture-in-modern-im-system/"},{"title":"Meta 标签设置","content":" meta标签设置 meta 标签常用设置： &lt;!-- 根据浏览器的屏幕大小自适应的展现合适的效果 --&gt; &lt;meta name=&quot;applicable-device&quot; content=&quot;pc,mobile&quot; /&gt; &lt;!-- 移动端 浏览器中页面将以原始大小显示，不允许缩放 --&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,initial-scale=1.0,minimum-scale=1.0,maximum-scale=1.0,user-scalable=no&quot; /&gt; &lt;!--自动选择更好的浏览器--&gt; &lt;meta name=&quot;renderer&quot; content=&quot;webkit&quot;&gt; //告诉浏览器这个网址应该用哪个内核渲染，那么浏览器就会在读取到这个标签后，立即切换对应的内核 &lt;!-- 优先使用 IE 最新版本和 Chrome --&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge,chrome=1&quot; /&gt; //它必须显示在网页中除 title 元素和其他 meta 元素以外的所有其他元素之前。如果不是的话，它不起作用 &lt;!-- 描述文档类型 content表示文档类型，这里为text/html，如果JS就是text/javascript，charset表示页面字符集 --&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; &lt;!-- iphone会把一串数字识别为电话号码，点击的时候会提示是否呼叫，屏蔽这功能则把telephone设置为no --&gt; &lt;meta content=&quot;telephone=no&quot; name=&quot;format-detection&quot; /&gt; &lt;!-- iphone的私有标签，默认值为default（白色），可以定为black（黑色）和black-translucent（灰色半透明） --&gt; &lt;meta content=&quot;black&quot; name=&quot;apple-mobile-web-app-status-bar-style&quot;&gt; &lt;!-- iphone设备的是有标签 允许全屏模式浏览，隐藏浏览器导航栏 --&gt; &lt;meta content=&quot;yes&quot; name=&quot;apple-mobile-web-app-capable&quot; /&gt; &lt;!-- 全屏显示 --&gt; &lt;meta content=&quot;yes&quot; name=&quot;apple-touch-fullscreen&quot; /&gt; &lt;!-- UC强制全屏 --&gt; &lt;meta name=&quot;full-screen&quot; content=&quot;yes&quot;&gt; &lt;!-- QQ强制全屏 --&gt; &lt;meta name=&quot;x5-fullscreen&quot; content=&quot;true&quot;&gt; &lt;!-- 屏蔽百度转码 --&gt; &lt;meta http-equiv=&quot;Cache-Control&quot; content=&quot;no-transform&quot; /&gt; &lt;meta http-equiv=&quot;Cache-Control&quot; content=&quot;no-siteapp&quot; /&gt; &lt;!-- 定义网页简短描述 --&gt; &lt;meta name=&quot;description&quot; content=&quot;Cochemist&quot;&gt; &lt;!-- 定义网页关键词 --&gt; &lt;meta name=&quot;keywords&quot; content=&quot;生物化学&quot;&gt; &lt;!-- 定义网页的作者 --&gt; &lt;meta name=&quot;author&quot; content=&quot;sun_Annie&quot;&gt; &lt;!-- 避免HTML页面缓存 --&gt; &lt;meta http-equiv=&quot;pragma&quot; content=&quot;no-cache&quot; /&gt; &lt;meta http-equiv=&quot;cache-control&quot; content=&quot;no-cache&quot;&gt; &lt;meta http-equiv=&quot;expires&quot; content=&quot;0&quot;&gt; &lt;!-- 定义网页的缓存过期时间 --&gt; &lt;meta http-equiv=&quot;expires&quot; content=&quot;Sunday 26 October 2016 00:00 GMT&quot;&gt; //由于这是一个过去的日期，所以这个网页只要一打开，就会直接到网站服务器重新下载页面内容，而不是从cache调用。这是一种防止网页被cache缓存的措施。 概要 标签提供关于 HTML 文档的元数据。元数据不会显示在页面上，但是对于机器是可读的。它可用于浏览器（如何显示内容或重新加载页面），搜索引擎（关键词），或其他 web 服务。 —— W3School 必要属性 属性 值 描述 content some text 定义与http-equiv或name属性相关的元信息 可选属性 属性 值 描述 http-equiv content-type / expire / refresh / set-cookie 把 content 属性关联到 HTTP 头部。 name author / description / keywords / generator / revised / others 把 content 属性关联到一个名称。 content some text 定义用于翻译 content 属性值的格式。 SEO 搜索引擎优化 页面关键词 每个网页应具有描述该网页内容的一组唯一的关键字。 使用人们可能会搜索，并准确描述网页上所提供信息的描述性和代表性关键字及短语。标记内容太短，则搜索引擎可能不会认为这些内容相关。另外标记不应超过 874 个字符。 &lt;meta name=&quot;keywords&quot; content=&quot;your tags&quot; /&gt; 页面描述 每个网页都应有一个不超过 150 个字符且能准确反映网页内容的描述标签。 &lt;meta name=&quot;description&quot; content=&quot;150 words&quot; /&gt; 搜索引擎索引方式 robot terms 是一组使用逗号 , 分割的值，通常有如下几种取值：none，noindex，nofollow，all，index 和 follow。确保正确使用 nofollow 和 noindex 属性值。 &lt;meta name=&quot;robots&quot; content=&quot;index,follow&quot; /&gt; &lt;!-- all：文件将被检索，且页面上的链接可以被查询； none：文件将不被检索，且页面上的链接不可以被查询； index：文件将被检索； follow：页面上的链接可以被查询； noindex：文件将不被检索； nofollow：页面上的链接不可以被查询。 --&gt; 页面重定向和刷新 content 内的数字代表时间（秒），既多少时间后刷新。如果加 url，则会重定向到指定网页（搜索引擎能够自动检测，也很容易被引擎视作误导而受到惩罚）。 &lt;meta http-equiv=&quot;refresh&quot; content=&quot;0;url=&quot; /&gt; 其他 &lt;meta name=&quot;author&quot; content=&quot;author name&quot; /&gt; &lt;!-- 定义网页作者 --&gt; &lt;meta name=&quot;google&quot; content=&quot;index,follow&quot; /&gt; &lt;meta name=&quot;googlebot&quot; content=&quot;index,follow&quot; /&gt; &lt;meta name=&quot;verify&quot; content=&quot;index,follow&quot; /&gt; 移动设备 viewport 能优化移动浏览器的显示。如果不是响应式网站，不要使用 initial-scale 或者禁用缩放。 大部分 4.7-5 寸设备的 viewport 宽设为 360px；5.5 寸设备设为 400px；iPhone6 设为 375px；iPone6 Plus 设为414px。 &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0,maximum-scale=1.0, user-scalable=no&quot;/&gt; &lt;!-- `width=device-width` 会导致 iPhone 5 添加到主屏后以 WebApp 全屏模式打开页面时出现黑边 --&gt; width：宽度（数值 / device-width）（范围从 200 到 10,000，默认为 980 像素）； height：高度（数值 / device-height）（范围从 223 到 10,000）； initial-scale：初始的缩放比例 （范围从 &gt;0 到 10）； minimum-scale：允许用户缩放到的最小比例； maximum-scale：允许用户缩放到的最大比例； user-scalable：用户是否可以手动缩 (no, yes)； minimal-ui：可以在页面加载时最小化上下状态栏。（已弃用） 注意，很多人使用 initial-scale=1 到非响应式网站上，这会让网站以 100% 宽度渲染，用户需要手动移动页面或者缩放。如果和 initial-scale=1 同时使用 user-scalable=no 或 maximum-scale=1，则用户将不能放大/缩小网页来看到全部的内容。 WebApp 全屏模式 伪装 App，离线应用。 &lt;meta name=&quot;apple-mobile-web-app-capable&quot; content=&quot;yes&quot; /&gt; &lt;!-- 启用 WebApp 全屏模式 --&gt; 隐藏状态栏/设置状态栏颜色 只有在开启 WebApp 全屏模式时才生效。content 的值为 default | black | black-translucent。 &lt;meta name=&quot;apple-mobile-web-app-status-bar-style&quot; content=&quot;black-translucent&quot; /&gt; 添加到主屏后的标题 &lt;meta name=&quot;apple-mobile-web-app-title&quot; content=&quot;标题&quot;&gt; 忽略数字自动识别为电话号码 &lt;meta content=&quot;telephone=no&quot; name=&quot;format-detection&quot; /&gt; 忽略识别邮箱 &lt;meta content=&quot;email=no&quot; name=&quot;format-detection&quot; /&gt; 添加智能 App 广告条 Smart App Banner 告诉浏览器这个网站对应的 App，并在页面上显示下载 banner（如下图）。Promoting Apps with Smart App Banners &lt;meta name=&quot;apple-itunes-app&quot; content=&quot;app-id=myAppStoreID, affiliate-data=myAffiliateData, app-argument=myURL&quot;&gt; 其他 HTML head 头标签 &lt;!-- 针对手持设备优化，主要是针对一些老的不识别viewport的浏览器，比如黑莓 --&gt; &lt;meta name=&quot;HandheldFriendly&quot; content=&quot;true&quot;&gt; &lt;!-- 微软的老式浏览器 --&gt; &lt;meta name=&quot;MobileOptimized&quot; content=&quot;320&quot;&gt; &lt;!-- uc强制竖屏 --&gt; &lt;meta name=&quot;screen-orientation&quot; content=&quot;portrait&quot;&gt; &lt;!-- QQ强制竖屏 --&gt; &lt;meta name=&quot;x5-orientation&quot; content=&quot;portrait&quot;&gt; &lt;!-- UC强制全屏 --&gt; &lt;meta name=&quot;full-screen&quot; content=&quot;yes&quot;&gt; &lt;!-- QQ强制全屏 --&gt; &lt;meta name=&quot;x5-fullscreen&quot; content=&quot;true&quot;&gt; &lt;!-- UC应用模式 --&gt; &lt;meta name=&quot;browsermode&quot; content=&quot;application&quot;&gt; &lt;!-- QQ应用模式 --&gt; &lt;meta name=&quot;x5-page-mode&quot; content=&quot;app&quot;&gt; &lt;!-- windows phone 点击无高光 --&gt; &lt;meta name=&quot;msapplication-tap-highlight&quot; content=&quot;no&quot;&gt; 网页相关 申明编码 &lt;meta charset='utf-8' /&gt; 优先使用 IE 最新版本和 Chrome &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge,chrome=1&quot; /&gt; &lt;!-- 关于X-UA-Compatible --&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=6&quot; &gt;&lt;!-- 使用IE6 --&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=7&quot; &gt;&lt;!-- 使用IE7 --&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=8&quot; &gt;&lt;!-- 使用IE8 --&gt; 浏览器内核控制 国内浏览器很多都是双内核（Webkit和Trident），Webkit内核高速浏览，IE 内核兼容网页和旧版网站。而添加 meta 标签的网站可以控制浏览器选择何种内核渲染。浏览器内核控制标签meta说明 &lt;meta name=&quot;renderer&quot; content=&quot;webkit|ie-comp|ie-stand&quot;&gt; 国内双核浏览器默认内核模式如下： 搜狗高速浏览器、QQ 浏览器：IE 内核（兼容模式）； 360 极速浏览器、遨游浏览器：Webkit 内核（极速模式）。 禁止浏览器从本地计算机的缓存中访问页面内容 这样设定，访问者将无法脱机浏览。 &lt;meta http-equiv=&quot;Pragma&quot; content=&quot;no-cache&quot;&gt; Windows 8 &lt;meta name=&quot;msapplication-TileColor&quot; content=&quot;#000&quot;/&gt; &lt;!-- Windows 8 磁贴颜色 --&gt; &lt;meta name=&quot;msapplication-TileImage&quot; content=&quot;icon.png&quot;/&gt; &lt;!-- Windows 8 磁贴图标 --&gt; 站点适配 主要用于 PC-手机页的对应关系。 &lt;meta name=&quot;mobile-agent&quot;content=&quot;format=[wml|xhtml|html5]; url=url&quot;/&gt; &lt;!-- [wml|xhtml|html5]根据手机页的协议语言，选择其中一种； url=&quot;url&quot; 后者代表当前PC页所对应的手机页URL，两者必须是一一对应关系。 --&gt; 转码申明 用百度打开网页可能会对其进行转码（比如贴广告），避免转码可添加如下 meta &lt;meta http-equiv=&quot;Cache-Control&quot; content=&quot;no-siteapp&quot;/&gt; 其他参考文档 COMPLETE LIST OF HTML META TAGS W3C META TAGS METATAGES in HTML5 MDN META TAGS ","link":"https://faded.auspicious.space/post/meta-tag-configuration/"},{"title":"药品前后缀","content":" 药品名称中的那些“百家姓” 看到一个药品名字中含有这些前缀/后缀，就应该知道它的基本作用是什么，以防吃药时犯低级错误哦。 一般情况下，前缀或后缀相同的药品，就是同一类药物，具有同一类药效。例如： 头孢××。头孢类药物是很常用的抗菌药物，俗称为抗生素，例如头孢拉定、头孢呋辛、头孢羟氨苄、头孢曲松、头孢克洛、头孢吡肟等。他们是用来治疗感染性炎症的，比如上呼吸道感染、下呼吸道感染、皮肤软组织感染、泌尿道感染、耳鼻部感染、腹腔感染等等，需要知道的是，不同头孢的治疗侧重点不同，需要根据感染的类型选用。 ××沙星。沙星类药物也是很常用的抗生素之一，例如环丙沙星、氧氟沙星、左氧氟沙星、莫西沙星等。他们也是用来治疗感染性炎症的，广泛用于呼吸系统感染、泌尿系统感染、皮肤软组织感染、肠道感染等。近年来，国家食药总局多次提醒大家关注这一类药物的不良反应，包括关节病变、致周围神经病变作用、中枢神经系统毒性等。同时，18岁以下患者禁用此类药物。 ××拉唑。拉唑类药物是很常用的抑制胃酸药，例如奥美拉唑、埃索美拉唑、兰索拉唑、半托拉唑、雷贝拉唑等。他们是用来治疗胃溃疡、十二指肠溃疡、反流性食管炎等消化道疾病的。需要注意的是，由于这一类药物影响胃酸分泌，同时与其他很多药物共用代谢酶，所以相互作用会比较多，同时使用这一类药物和其他药物的患者，最好审查一下相互作用哦。 ××他汀。他汀类药物是很常用的降脂药，主要用于高胆固醇血症和冠心病等，例如阿托伐他汀、瑞舒伐他汀、辛伐他汀、洛伐他汀、匹伐他汀、氟伐他汀等。不同他汀类药物的降脂强度不同，不可随意换用。另外，长期用药患者需要注意监测转氨酶和肌酸激酶。 ××普利。普利类药物是很常用的降压药之一，学名称作血管紧张素转化酶抑制剂，是治疗高血压的一线药物之一，例如卡托普利、贝那普利、依那普利、福辛普利、培哚普利、雷米普利、赖诺普利等。使用普利类降压药时，需要注意其干咳的副作用。 ××地平。地平类药物也是很常用的降压药，学名称为钙离子通道拮抗剂，也是治疗高血压的一线药物，例如硝苯地平、氨氯地平、尼群地平、拉西地平、尼卡地平等。其中，硝苯地平是最常用的降血压和缓解心绞痛的的药物之一，为了减少服药次数增加疗效稳定性，目前有多种缓控释制剂供选择。 但是，任何事情都有例外，有些药品名称“看起来是一家”，实际上却“不是一家”。例如： ××霉素。药品名称中含有“霉素”两个字的药很多，但是他们并不都是一类的。例如，青霉素和苄星青霉素属于青霉素类抗菌药，庆大霉素、妥布霉素属于氨基糖苷类抗菌药，红霉素、阿奇霉素、克拉霉素属于大环内酯类抗菌药，万古霉素属于糖肽类抗菌药，链霉素、利福霉素常用于抗结核病类药，阿霉素、柔红霉素、丝裂霉素属于细胞毒药物（抗癌药）等。这些药物的药效和作用范围都不相同。 ×格列×。名称中含有“格列”两个字的药也很多，例如降糖药里面的磺脲类降糖药格列美脲、格列喹酮，α-糖苷酶抑制剂伏格列波糖，胰岛素增敏药罗格列酮、吡格列酮等。这些药物虽然都是糖尿病患者的口服降糖药，但是具体作用机制和药物类别并不一样，适用人群和用法也不一样。 ×莫司×。名称中含有“莫司”两个字的药也很多，但是药效作用各不相同，有免疫调节剂他克莫司、西罗莫司，有细胞毒药物司莫司汀、卡莫司汀、洛莫司汀、福莫司汀等，还有降血脂药阿昔莫司。 ","link":"https://faded.auspicious.space/post/prefix-and-suffix-of-drug/"},{"title":"程序员经典语录","content":" 程序员经典语录 A good programmer is someone who always looks both ways before crossing a one-way street. (Doug Linder) 好的程序员即使在过单行道时也总是会环顾两边。 Don't worry if it doesn't work right. If everything did, you'd be out of a job. (Mosher's Law of Software Engineering) 不要担心它能否正常工作。如果一切正常，那么你就会失去工作。 The trouble with programmers is that you can never tell what a programmer is doing until it's too late. (Seymour Cray) 程序员的烦恼是，你永远无法知道一个程序员在做什么，直到为时已晚。 Most of you are familiar with the virtues of a programmer. There are three, of course: laziness, impatience, and hubris. (Larry Wall) 程序员的美德：懒惰，没有耐心以及老子天下第一。 Always code as if the guy who ends up maintaining your code will be a violent psychopath who knows where you live. (Martin Golding) 写代码的时候总是想象维护你代码的家伙是一个知道你住在哪里的暴力精神病患者。 One man's crappy software is another man's full time job. (Jessica Gaston) 一个人写的烂软件将会给另一个人带来一份全职工作。 If builders built buildings the way programmers wrote programs, then the first woodpecker that came along wound destroy civilization. (Gerald Weinberg) 如果建筑工人用程序员写程序的方式建造建筑物，那么来的第一只啄木鸟（找bug）就将摧毁文明。 The most likely way for the world to be destroyed, most experts agree, is by accident. That's where we come in; we're computer professionals. We cause accidents. (Nathaniel Borenstein) 大多数专家认为，世界被破坏的最有可能的方式是，是因为偶然。那就是我们所要进入的领域：我们是计算机专家。我们制造偶然。 It's a curious thing about our industry: not only do we not learn from our mistakes, we also don't learn from our successes. (Keith Braithwaite) 这是我们这个行业的一件咄咄怪事：我们不仅不从错误中学习，我们也不从成功中学习。 Once a new technology starts rolling, if you're not part of the steamroller, you're part of the road. (Stewart Brand) 一旦一种新技术开始滚动碾压道路，如果你不能成为压路机的一部分，那么你就只能成为道路的一部分。 If at first you don't succeed, call it version 1.0 (unknown) 如果第一次你没有成功，那么称之为1.0版，继续加油。 All programmers are playwrights and all computers are lousy actors. (Anonymous Hack Actor) 所有的程序员都剧作家，而所有计算机都是糟糕的演员。 The sooner you get behind in your work, the more time you have to catch up. (Anonymous Scheduler) 工作拉下得越早，赶上去所需要的时间越多。 When a programming language is created that allows programmers to program in simple English, it will be discovered that programmers cannot speak English. (Anonymous Linguist) 当创建一种编程语言允许程序员使用简单英语编程的时候，那么新问题又来了，你会发现程序员不说英语。 Why do we never have time to do it right, but always have time to do it over? (Anonymous) 为什么我们总是没有时间把事情做对，却有时间做完它？ Any fool can write code that a computer can understand. Good programmers write code that humans can understand. 傻瓜写计算机能理解的代码。优秀的程序员写人类能读懂的代码。 Any code of your own that you haven't looked at for six or more months might as well have been written by someone else. (Eagleson's law) 自己的代码六个月或更久不见，还不如别人写的代码。 ","link":"https://faded.auspicious.space/post/classic-quotes-of-programmer/"},{"title":"10 种常见的软件架构模式","content":" 10种常见的软件架构模式 有没有想过要设计多大的企业规模系统？在主要的软件开发开始之前，我们必须选择一个合适的体系结构，它将为我们提供所需的功能和质量属性。因此，在将它们应用到我们的设计之前，我们应该了解不同的体系结构。 什么是架构模式？ 根据维基百科中的定义： 架构模式是一个通用的、可重用的解决方案，用于在给定上下文中的软件体系结构中经常出现的问题。架构模式与软件设计模式类似，但具有更广泛的范围。 在本文中，将简要地解释以下 10 种常见的体系架构模式，以及它们的用法、优缺点。 分层模式 客户端-服务器模式 主从设备模式 管道-过滤器模式 代理模式 点对点模式 事件总线模式 模型-视图-控制器模式 黑板模式 解释器模式 一、分层模式 这种模式也称为多层体系架构模式。它可以用来构造可以分解为子任务组的程序，每个子任务都处于一个特定的抽象级别。每个层都为下一个提供更高层次服务。 一般信息系统中最常见的是如下所列的 4 层。 表示层（也称为 UI 层）； 应用层（也称为服务层）； 业务逻辑层（也称为领域层）； 数据访问层（也称为持久化层）。 使用场景： 一般的桌面应用程序； 电子商务 Web 应用程序。 二、客户端-服务器模式 这种模式由两部分组成：一个服务器和多个客户端。服务器组件将为多个客户端组件提供服务。客户端从服务器请求服务，服务器为这些客户端提供相关服务。此外，服务器持续侦听客户机请求。 使用场景： 电子邮件，文件共享和银行等在线应用程序。 三、主从设备模式 这种模式由两方组成：主设备和从设备。主设备组件在相同的从设备组件中分配工作，并计算最终结果，这些结果是由从设备返回的结果。 使用场景： 在数据库复制中，主数据库被认为是权威的来源，并且要与之同步； 在计算机系统中与总线连接的外围设备（主和从驱动器）。 四、管道-过滤器模式 此模式可用于构造生成和处理数据流的系统。每个处理步骤都封装在一个过滤器组件内。要处理的数据是通过管道传递的。这些管道可以用于缓冲或用于同步。 使用场景： 编译器。连续的过滤器执行词法分析、解析、语义分析和代码生成； 生物信息学的工作流。 五、代理模式 此模式用于构造具有解耦组件的分布式系统。这些组件可以通过远程服务调用彼此交互。代理组件负责组件之间的通信协调。 服务器将其功能（服务和特征）发布给代理。客户端从代理请求服务，然后代理将客户端重定向到其注册中心的适当服务。 使用场景： 消息代理软件，如 Apache ActiveMQ，Apache Kafka，RabbitMQ 和 JBoss Messaging。 六、点对点模式 在这种模式中，单个组件被称为对等点。对等点可以作为客户端，从其他对等点请求服务，作为服务器，为其他对等点提供服务。对等点可以充当客户端或服务器或两者的角色，并且可以随时间动态地更改其角色。 使用场景： 像 Gnutella 和 G2 这样的文件共享网络； 多媒体协议，如 P2PTV 和 PDTP； 像 Spotify 这样的专有多媒体应用程序。 七、事件总线模式 这种模式主要是处理事件，包括 4 个主要组件：事件源、事件监听器、通道和事件总线。消息源将消息发布到事件总线上的特定通道上。侦听器订阅特定的通道。侦听器会被通知消息，这些消息被发布到它们之前订阅的一个通道上。 使用场景： 安卓开发； 通知服务。 八、模型-视图-控制器模式 这种模式，也称为 MVC 模式，把一个交互式应用程序划分为 3 个部分， 模型：包含核心功能和数据； 视图：将信息显示给用户（可以定义多个视图）； 控制器：处理用户输入的信息。 这样做是为了将信息的内部表示与信息的呈现方式分离开来，并接受用户的请求。它分离了组件，并允许有效的代码重用。 使用场景： 在主要编程语言中互联网应用程序的体系架构； 像 Django 和 Rails 这样的 Web 框架。 九、黑板模式 这种模式对于没有确定解决方案策略的问题是有用的。黑板模式由 3 个主要组成部分组成。 黑板——包含来自解决方案空间的对象的结构化全局内存； 知识源——专门的模块和它们自己的表示； 控制组件——选择、配置和执行模块； 所有的组件都可以访问黑板。组件可以生成添加到黑板上的新数据对象。组件在黑板上查找特定类型的数据，并通过与现有知识源的模式匹配来查找这些数据。 使用场景： 语音识别； 车辆识别和跟踪； 蛋白质结构识别； 声纳信号的解释。 十、解释器模式 这个模式用于设计一个解释用专用语言编写的程序的组件。它主要指定如何评估程序的行数，即以特定的语言编写的句子或表达式。其基本思想是为每种语言的符号都有一个分类。 使用场景： 数据库查询语言，比如 SQL； 用于描述通信协议的语言。 体系架构模式的比较 下面给出的表格总结了每种体系架构模式的优缺点。 名称 优点 缺点 分层模式 一个较低的层可以被不同的层所使用。层使标准化更容易，因为我们可以清楚地定义级别。可以在层内进行更改，而不会影响其他层。 不是普遍适用的。在某些情况下，某些层可能会被跳过。 客户端-服务器模式 很好地建立一组服务，用户可以请求他们的服务。 请求通常在服务器上的单独线程中处理。由于不同的客户端具有不同的表示，进程间通信会导致额外开销。 主从设备模式 准确性——将服务的执行委托给不同的从设备，具有不同的实现。 从设备是孤立的：没有共享的状态。主-从通信中的延迟可能是一个问题，例如在实时系统中。这种模式只能应用于可以分解的问题。 管道-过滤器模式 展示并发处理。当输入和输出由流组成时，过滤器在接收数据时开始计算。轻松添加过滤器，系统可以轻松扩展。过滤器可重复使用。 可以通过重新组合一组给定的过滤器来构建不同的管道。 效率受到最慢的过滤过程的限制。从一个过滤器移动到另一个过滤器时的数据转换开销。 代理模式 允许动态更改、添加、删除和重新定位对象，这使开发人员的发布变得透明。 要求对服务描述进行标准化。 点对点模式 支持分散式计算。对任何给定节点的故障处理具有强大的健壮性。在资源和计算能力方面具有很高的可扩展性。 服务质量没有保证，因为节点是自愿合作的。安全是很难得到保证的。性能取决于节点的数量。 事件总线模式 新的发布者、订阅者和连接可以很容易地添加。对高度分布式的应用程序有效。 可伸缩性可能是一个问题，因为所有消息都是通过同一事件总线进行的。 模型-视图-控制器模式 可以轻松地拥有同一个模型的多个视图，这些视图可以在运行时连接和断开。 增加复杂性。可能导致许多不必要的用户操作更新。 黑板模式 很容易添加新的应用程序。扩展数据空间的结构很简单。 修改数据空间的结构非常困难，因为所有应用程序都受到了影响。可能需要同步和访问控制。 解释器模式 高度动态的行为是可行的。对终端用户编程性提供好处。提高灵活性，因为替换一个解释程序很容易。 由于解释语言通常比编译后的语言慢，因此性能可能是一个问题。 ","link":"https://faded.auspicious.space/post/10-common-software-architectural-patterns-in-a-nutshell/"},{"title":"window 对象知识总结","content":" window对象知识总结 window 对象属性 属性名 属性功能 closed 返回窗口是否已被关闭 defaultStatus 设置或返回窗口状态栏中的默认文本 document 对 document 对象的只读引用 history 对 history 对象的只读引用 innerheight 返回窗口的文档显示区的高度 innerwidth 返回窗口的文档显示区的宽度 length 设置或返回窗口中的框架数量 location 用于窗口或框架的 location 对象 name 设置或返回窗口的名称 navigator 对 navigator 对象的只读引用 opener 返回对创建此窗口的窗口的引用 outerheight 返回窗口的外部高度 outerwidth 返回窗口的外部宽度 pageXOffset 设置或返回当前页面相对于窗口显示区左上角的 X 位置 pageYOffset 设置或返回当前页面相对于窗口显示区左上角的 Y 位置 parent 返回父窗口 screen 对 screen 对象的只读引用 self 返回对当前窗口的引用。等价于 window 属性 status 设置窗口状态栏的文本 top 返回最顶层的先辈窗口 window window 属性等价于 self 属性，它包含了对窗口自身的引用 screenLeft/screenTop/screenX/screenY 只读整数。声明了窗口的左上角在屏幕上的的 x 坐标和 y 坐标。IE、Safari 和 Opera 支持 screenLeft 和 screenTop，而 Firefox 和 Safari 支持 screenX 和 screenY window 对象方法 对象名 对象功能 alert() 显示带有一段消息和一个确认按钮的警告框 blur() 把键盘焦点从顶层窗口移开 clearInterval() 取消由 setInterval() 设置的 timeout clearTimeout() 取消由 setTimeout() 方法设置的 timeout close() 关闭浏览器窗口 confirm() 显示带有一段消息以及确认按钮和取消按钮的对话框 createPopup() 创建一个 pop-up 窗口 focus() 把键盘焦点给予一个窗口 moveBy() 可相对窗口的当前坐标把它移动指定的像素 moveTo() 把窗口的左上角移动到一个指定的坐标 open() 打开一个新的浏览器窗口或查找一个已命名的窗口 print() 打印当前窗口的内容 prompt() 显示可提示用户输入的对话框 resizeBy() 按照指定的像素调整窗口的大小 resizeTo() 把窗口的大小调整到指定的宽度和高度 scrollBy() 按照指定的像素值来滚动内容 scrollTo() 把内容滚动到指定的坐标 setInterval() 按照指定的周期（以毫秒计）来调用函数或计算表达式 setTimeout() 在指定的毫秒数后调用函数或计算表达式 window 对象描述 window 对象表示一个浏览器窗口或一个框架。在客户端 JavaScript 中，window 对象是全局对象，所有的表达式都在当前的环境中计算。也就是说，要引用当前窗口根本不需要特殊的语法，可以把那个窗口的属性作为全局变量来使用。例如，可以只写 document，而不必写 window.document 同样，可以把当前窗口对象的方法当作函数来使用，如只写 alert()，而不必写 window.alert() 除了上面列出的属性和方法，window 对象还实现了核心 JavaScript 所定义的所有全局属性和方法 window 对象的 window 属性和 self 属性引用的都是它自己。当你想明确地引用当前窗口，而不仅仅是隐式地引用它时，可以使用这两个属性。除了这两个属性之外，parent 属性、top 属性以及 frame[] 数组都引用了与当前 window 对象相关的其他 window 对象 要引用窗口中的一个框架，可以使用如下语法： frame[i]：当前窗口的框架 self.frame[i]：当前窗口的框架 w.frame[i]：窗口 w 的框架 要引用一个框架的父窗口（或父框架），可以使用下面的语法： parent：当前窗口的父窗口。 self.parent：当前窗口的父窗口。 w.parent：窗口 w 的父窗口。 要从顶层窗口含有的任何一个框架中引用它，可以使用如下语法： top：当前框架的顶层窗口。 self.top：当前框架的顶层窗口。 f.top：框架 f 的顶层窗口。 新的顶层浏览器窗口由方法 window.open() 创建。当调用该方法时，应把 open() 调用的返回值存储在一个变量中，然后使用那个变量来引用新窗口。新窗口的 opener 属性反过来引用了打开它的那个窗口 一般来说，window 对象的方法都是对浏览器窗口或框架进行某种操作；而 alert() 方法、confirm() 方法和 prompt 方法则不同，它们通过简单的对话框与用户进行交互。 全局的 window 对象 JavaScript 中的任何一个全局函数或变量都是 window 的属性。 &lt;script type=&quot;text/javascript&quot;&gt; var name = &quot;aaaa&quot;; document.write(window.name); &lt;/script&gt; window 与 self 对象 self 对象与 window 对象完全相同，self 通常用于确认就是在当前的窗体内 &lt;script type=&quot;text/javascript&quot;&gt; document.write(window == self); //必须相等，永远都相等 document.write(window.top == window.self); //判断当前框架是否是主框架 &lt;/script&gt; window、self、window.self 三者是等价的。 window 的子对象 window 的主对象主要有： JavaScript document 对象 JavaScript frames 对象 JavaScript history 对象 JavaScript location 对象 JavaScript navigator 对象 JavaScript screen 对象 window 函数索引（仅对 IE 有效） 窗体控制函数 moveBy()：从当前位置水平移动窗体 x 个像素，垂直移动窗体 y 个像素，x 为负数，将向左移动窗体，y 为负数，将向上移动窗体。 moveTo()：移动窗体左上角到相对于屏幕左上角的 (x, y) 点，当使用负数做为参数时会把窗体移出屏幕的可视区域。 resizeBy()：相对窗体当前的大小，宽度调整 w 个像素，高度调整 h 个像素。如果参数为负值，将缩小窗体，反之扩大窗体。 resizeTo()：把窗体宽度调整为 w 个像素，高度调整为 h个像素。 &lt;body&gt; &lt;input type=&quot;button&quot; id=&quot;btn1&quot; value=&quot;先设置窗体固定大小！&quot; onclick=&quot;window.resizeTo(500,500);&quot; /&gt; &lt;input type=&quot;button&quot; id=&quot;btn2&quot; value=&quot;再缩小10像素！&quot; onclick=&quot;window.resizeBy(-10,-10);&quot; /&gt; &lt;input type=&quot;button&quot; id=&quot;btn2&quot; value=&quot;上！&quot; onclick=&quot;window.moveBy(0,-5);&quot; /&gt; &lt;input type=&quot;button&quot; id=&quot;btn2&quot; value=&quot;下！&quot; onclick=&quot;window.moveBy(0, 5);&quot; /&gt; &lt;input type=&quot;button&quot; id=&quot;btn2&quot; value=&quot;左！&quot; onclick=&quot;window.moveBy(-5, 0);&quot; /&gt; &lt;input type=&quot;button&quot; id=&quot;btn2&quot; value=&quot;右！&quot; onclick=&quot;window.moveBy(5, 0);&quot; /&gt; &lt;input type=&quot;button&quot; id=&quot;btn2&quot; value=&quot;距离左上角左边100像素，顶部200像素&quot; onclick=&quot;window.moveTo(100, 200);&quot; /&gt; &lt;/body&gt; 窗体滚动轴控制函数 scrollTo()：在窗体中如果有滚动条，将横向滚动条移动到相对于窗体宽度为 x 个像素的位置，将纵向滚动条移动到相对于窗体高度为 y 个像素的位置。 scrollBy()：如果有滚动条，将横向滚动条移动到相对于当前横向滚动条的 x 个像素的位置（就是向左移动 x 像素），将纵向滚动条移动到相对于当前纵向滚动条高度为 y 个像素的位置（就是向下移动 y 像素）。 注意区别，一个是相对当前窗口，一个是相当现在滚动条的位置。 &lt;div style=&quot;height:150%; width:150%; background-color:#ddd&quot;&gt; &lt;input type=&quot;button&quot; id=&quot;btn1&quot; value=&quot;移动滚动条！&quot; onclick=&quot;window.scrollTo(100,100);&quot; /&gt; //相当于设置绝对位置 &lt;input type=&quot;button&quot; id=&quot;btn1&quot; value=&quot;移动滚动条！&quot; onclick=&quot;window.scrollBy(100,100);&quot; /&gt; //相当于累加 &lt;/div&gt; 窗体焦点控制函数 focus()：使窗体或空间获得焦点。 blur()：使窗体或控件失去焦点。 &lt;div&gt; &lt;input type=&quot;button&quot; value=&quot;获得焦点&quot; onclick=&quot;document.getElementById('testInput').focus()&quot; /&gt; &lt;input type=&quot;button&quot; value=&quot;失去焦点&quot; onclick=&quot;document.getElementById('testInput').blur()&quot; /&gt; &lt;input type=&quot;text&quot; value=&quot;text&quot; id=&quot;testInput&quot; onblur=&quot;alert('我已失去焦点')&quot; /&gt; &lt;/div&gt; 新建窗体函数 open()：打开（弹出）一个新的窗体。 close()：关闭窗体。 opener：通过 opener 可以实现跨窗体之间的通讯，但是要保证是在同一域名下，而且一个窗体要包含另一个窗体的 opener。 格式：window.open(url, name, features, replace); open 函数参数说明 url：要载入窗体的 URL。 name：新建窗体的名称（也可以是 HTML target 属性的取值，目标）。 features：代表窗体特性的字符串，字符串中每个特性使用逗号分隔。 replace：一个布尔值，说明新载入的页面是否替换当前载入的页面，此参数通常不用指定。 open 方法示例 &lt;a href=&quot;2.html&quot; target=&quot;2&quot;&gt;在新窗口打开连接&lt;/a&gt; &lt;a href=&quot;#&quot; onclick=&quot;window.open('http://www.google.com','2');&quot;&gt;在已建立连接的页面打开新地址&lt;/a&gt; 首先使用普通 HTML 链接打开一个页面( target 名为 dreamdu)，之后使用 open 函数打开另一个页面，浏览器首先要查找是否有名称为 dreamdu 的窗体，如果有，就在这个窗体中加载 open 的地址。 经过设置的 open window.open ('page.html', 'newwindow', 'height=100, width=400, top=0,left=0, toolbar=no, menubar=no, scrollbars=no, resizable=no,location=no, status=no'); 弹窗方法 方法一：&lt;body onload=&quot;openwin()&quot;&gt; 浏览器读页面时弹出窗口； 方法二：&lt;body onunload=&quot;openwin()&quot;&gt; 浏览器离开页面时弹出窗口。 open 函数第三个参数 features 说明 参数名称 类型 说明 height Number 设置窗体的高度，不能小于100 left Number 说明创建窗体的左坐标，不能为负值 location Boolean 窗体是否显示地址栏，默认值为 no resizable Boolean 窗体是否允许通过拖动边线调整大小，默认值为 no scrollable Boolean 窗体中内部超出窗口可视范围时是否允许拖动，默认值为 no toolbar Boolean 窗体是否显示工具栏，默认值为 no top Number 说明创建窗体的上坐标，不能为负值 status Boolean 窗体是否显示状态栏，默认值为 no width Number 创建窗体的宽度，不能小于 100 特性字符串中的每个特性使用逗号分隔，每个特性之间不允许有空格。 window.open 函数新建立窗体后会返回新建窗体的 window 对象，通过此对象可以控制窗体（移动，改变大小，关闭）。 close 函数 &lt;input type=&quot;button&quot; value=&quot;关闭已经打开的窗体！&quot; onclick=&quot;window.close();&quot; /&gt; self.close(); 配合上 setTimeout() 可以实现，打开的窗口定时关闭的效果。 对话框函数 alert()：弹出消息对话框（对话框中有一个 OK 按钮）； confirm()：弹出消息对话框（对话框中包含一个 OK 按钮与 Cancel 按钮）；confirm() 消息对话框是排它的，也就是在用户点击对话框的按钮前，不能进行任何其它操作。 if (confirm(&quot;确定跳大？&quot;)) { alert(&quot;果断跳大&quot;); } else { alert(&quot;猥琐打钱&quot;); } ``` prompt()：弹出消息对话框（对话框中包含一个 OK 按钮、Cancel 按钮与一个文本输入框）；函数有两个参数： str1：要显示在消息对话框中的文本，不可修改； str2：文本框中的内容，可以修改。 var sResult = prompt(&quot;请在下面输入你的姓名&quot;, &quot;aaa&quot;); if (sResult != null) { alert(sResult + &quot;已经超越神的杀戮&quot;); } else { alert(&quot;无名氏已经超越神的杀戮&quot;); } 时间等待与间隔函数 setTimeout() 函数 / clearTimeout() 函数 在指定的时间后调用函数。 语法： setTimeout(fun, time);：fun：函数体或函数名，time：指定时间，单位为毫秒； clearTimeout(id);：取消指定的 setTimeout 函数将要执行的代码。 setTimeout(function () { document.write(&quot;隔3秒后触发&quot;); }, 3000) //在3秒后输出 setTimeout(fun1, 5000); //在5秒后输出 function fun1() { document.write(&quot;函数名的方式5秒后触发&quot;); } setInterval() 函数 / clearInterval() 函数 在间隔指定的事件后重复调用函数。 语法： setInterval(fun1, time)：fun：函数体或函数名，time：指定的时间，单位为毫秒。会返回一个值，这个值是统计该函数的个数用的，第一个是 1，第二个就是 2，指明是第几个 setInterval 函数。 clearInterval(value)：value：setInterval() 函数返回的值，根据这个值可以停止 setInterval() 的重复。 注意，JavaScript 是单线程的，因此，这个定时函数实际上是通过插入执行队列的方式来实现。 如下面的代码： function fn() { setTimeout(function () { alert('can you see me?'); }, 1000); while (true) { } } alert(); 永远都不会执行，因为线程一直被死循环占用了。 window.location 子对象 解析 URL 对象 location:location 对象的属性有：href，protocal，host，hostname，port，pathname，search，hash。 document.write(location.href + &quot;&lt;br/&gt;&quot;); // http://localhost:4889/javascriptTest.html document.write(location.protocol + &quot;&lt;br/&gt;&quot;); // http: document.write(location.host + &quot;&lt;br/&gt;&quot;); // localhost:4889 document.write(location.hostname + &quot;&lt;br/&gt;&quot;); // localhost document.write(location.port + &quot;&lt;br/&gt;&quot;); // 4889 document.write(location.pathname + &quot;&lt;br/&gt;&quot;); // /javascriptTest.html document.write(location.search + &quot;换行&lt;br/&gt;&quot;); //http://localhost:4889/javascriptTest.html?id=1&amp;name=张三 如果路径是这样，则输出 ?id=1&amp;name=%E5%BC%A0%E4%B8%89 document.write(location.hash); //http: //localhost:4889/javascriptTest.html#kk=你好?id=1&amp;name=张三 如果路径是这样，则输出 #kk=你好?id=1&amp;name=张三 载入新文档 location.reload() // 重新加载页面 location.replace() // 本窗口载入新文档 location.assign() // 本窗口载入新文档 location = &quot;http://www.baidu.com&quot; // 跳转到指定网址 location = &quot;search.html&quot; // 相对路径跳转 location = &quot;#top&quot; // 跳转到页面顶部 浏览历史 history 对象的 back() 与 forward()：与浏览器的“后退”，“前进”功能一样。 history.go(-2); 后退两个历史记录。 浏览器和屏幕信息 navigator.appName // Web 浏览器全称 navigator.appVersion // Web 浏览器厂商和版本的详细字符串 navigator.userAgent // 客户端绝大部分信息 navigator.platform // 浏览器运行所在的操作系统 document.write(navigator.userAgent + &quot;&lt;br/&gt;&quot;); // Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.97 Safari/537.11 document.write(navigator.appName + &quot;&lt;br/&gt;&quot;); // Netscape document.write(navigator.appVersion + &quot;&lt;br/&gt;&quot;); // 5.0 (Windows NT 6.1) AppleWebKit/537.11 (KHTML, like Gecko) document.write(navigator.platform); //Win32 窗口的关系 parent == self：只有顶级窗口才返回 true； parent 和 top 属性允许脚本引用它的窗体的祖先，通常窗体是通过元素创建的，可以用来获取顶级窗口。 event 事件对象 最有用的两个操作：阻止事件冒泡。有时 return false; 不管用，这个或许就管用了。 // IE： window.event.cancelBubble = true;//停止冒泡 window.event.returnValue = false;//阻止事件的默认行为 // Firefox： event.preventDefault(); // 取消事件的默认行为 event.stopPropagation(); // 阻止事件的传播 ","link":"https://faded.auspicious.space/post/window-object-summary/"},{"title":"KeyCode 对照表","content":" Keycode对照表 字母和数字键的键码值(keyCode) 按键 键码 按键 键码 按键 键码 按键 键码 A 65 J 74 S 83 1 49 B 66 K 75 T 84 2 50 C 67 L 76 U 85 3 51 D 68 M 77 V 86 4 52 E 69 N 78 W 87 5 53 F 70 O 79 X 88 6 54 G 71 P 80 Y 89 7 55 H 72 Q 81 Z 90 8 56 I 73 R 82 0 48 9 57 数字键盘上的键的键码值(keyCode) 按键 键码 按键 键码 0 96 8 104 1 97 9 105 2 98 * 106 3 99 + 107 4 100 Enter 108 5 101 - 109 6 102 . 110 7 103 / 111 功能键键码表(keyCode) 按键 键码 按键 键码 F1 112 F7 118 F2 113 F8 119 F3 114 F9 120 F4 115 F10 121 F5 116 F11 122 F6 117 F12 123 控制键键码表(keyCode) 按键 键码 按键 键码 按键 键码 按键 键码 BackSpace 8 Esc 27 Right Arrow 39 -_ 189 Tab 9 Spacebar 32 Dw Arrow 40 .&gt; 190 Clear 12 Page Up 33 Insert 45 /? 191 Enter 13 Page Down 34 Delete 46 `~ 192 Shift 16 End 35 Num Lock 144 [{ 219 Control 17 Home 36 ;: 186 | 220 Alt 18 Left Arrow 37 =+ 187 ]} 221 Cape Lock 20 Up Arrow 38 ,&lt; 188 '&quot; 222 多媒体键码值(keyCode) 按键 键码 音量加 175 音量减 174 停止 179 静音 173 浏览器 172 邮件 180 搜索 170 收藏 171 ","link":"https://faded.auspicious.space/post/keycode-map/"},{"title":"TCP 状态机","content":" TCP状态机 前言 本文将会首先介绍 TCP 的各个状态，然后描述 TCP 三次握手和四次挥手时的状态变化，最后重点介绍 TIME_WAIT 状态。 TCP 连接状态 一个 TCP 连接在它的生命周期内会有不同的状态。 下图说明了 TCP 连接可能会有的状态，以及基于事件的状态转换。事件中有的是应用程序的操作，有的是接收到了网络发过来的请求。 TCP状态及其描述如下表： 状态 描述 LISTEN 等待来自远程 TCP 应用程序的请求 SYN_SENT 发送连接请求后等待来自远程端点的确认。TCP 第一次握手后客户端所处的状态 SYN-RECEIVED 该端点已经接收到连接请求并发送确认。该端点正在等待最终确认。TCP 第二次握手后服务端所处的状态 ESTABLISHED 代表连接已经建立起来了。这是连接数据传输阶段的正常状态 FIN_WAIT_1 等待来自远程 TCP 的终止连接请求或终止请求的确认 FIN_WAIT_2 在此端点发送终止连接请求后，等待来自远程 TCP 的连接终止请求 CLOSE_WAIT 该端点已经收到来自远程端点的关闭请求，此 TCP 正在等待本地应用程序的连接终止请求 CLOSING 等待来自远程 TCP 的连接终止请求确认 LAST_ACK 等待先前发送到远程 TCP 的连接终止请求的确认 TIME_WAIT 等待足够的时间来确保远程 TCP 接收到其连接终止请求的确认 TCP三次握手 当一个 TCP 连接建立时，发生了以下事情： 服务端必须准备接收传入的连接。这通常通过调用 socket，bind 和 listen 来完成，称为被动打开。 客户端通过调用 connect 方法来发起一个主动的打开。客户端 TCP 会发送一个“同步”（SYN）段，它告诉服务器客户端在连接上发送的数据的初始序列号。通常情况下，SYN 没有发送数据，它只包含一个 IP 头，TCP 头和可能的 TCP 选项。 服务器必须确认（ACK）客户端的 SYN，并且服务器还必须发送自己的 SYN，其中包含服务器将在连接上发送的数据的初始序列号。 客户端必须确认服务器的 SYN。 下图显示了 TCP 三次握手的过程，以及客户端和服务端状态的变化。 TCP四次挥手 一个 TCP 连接需要四步断开： 一个应用程序首先执行 close，发送 FIN 段，这个操作被称为主动关闭，这意味着这一端完成数据的发送。 执行 FIN 的另一端执行被动关闭，该端发送 ACK，确认该 FIN。 被动关闭的一端执行 close，向主动关闭的一方发送 FIN。 主动关闭的一方确认收到的 FIN。 下图显示了一次典型的 TCP 四次挥手的过程，以及主动关闭方和被动关闭方的状态变化。在图中是客户端主动断开了连接，这里只是举个例子，服务端一样可以主动断开连接。 TIME_WAIT状态 TIME_WAIT 状态应该是最让人疑惑的一个状态了。在上图中可以看到，执行主动断开的节点最后会进入这个状态，该节点会在此状态保存 2 倍的 MSL（最大段生存期）。 TCP 的每个实现都必须为 MSL 选择一个值。RFC 1122 推荐的值为两分钟，伯克利派的实现使用 30 秒。这也就是说 TIME_WAIT 状态会维持 1 到 4 分钟。MSL 是任何 IP 数据报可以在网络中生存的最长时间。这个时间是有限制的，因为每个数据报都包含一个 8 位的跳数限制，最大值是 255。虽然这是一个跳数限制而不是一个真正的时间限制，但是根据这个限制来假设数据报的最长生命周期依然是有意义的。 网络中数据报丢失的原因通常是路由异常。一旦路由崩溃或者两个路由之间的链路断开，路由协议需要几秒或几分钟才能稳定，并找到一条备用路径。在这段时间内，可能发生路由回路。同时假设丢失是一个 TCP 数据报，则发生 TCP 超时，并且重新发送分组，重传的分组通过一些备用路径达到最终目的地。但是一段时间后（该时间小于 MSL），路由循环被更正，在循环中丢失的数据报被发送到最终目的地。这个原始的数据报被称为丢失的副本或漫游副本。TCP 协议必须处理这些数据报。 维持 TIME_WAIT 有两个原因： 可靠地实现 TCP 的全双工连接终止。 允许旧的重复数据段在网络中过期 在四次挥手中，假设最后的 ACK 丢失了，被动关闭方会重发 FIN。主动关闭端必须维护状态，来允许被动关闭方重发最后的 ACK；如果它没有维护这个状态，将会对重发 FIN 返回 RST，被动关闭方会认为这是个错误。如果 TCP 正在执行彻底终止数据流的两个方向所需的所有工作（即全双工关闭），则必须正确处理这四个段中任何一个的丢失。所以执行主动关闭的一方必须在结束时保持 TIME_WAIT 状态：因为它可能必须重传最后的 ACK。 现在来聊维持 TIME_WAIT 状态的第二个原因。假设在主机 12.106.32.254 的 1500 端口和 206.168.112.219 的 21 端口之间有一个 TCP 连接。此连接关闭后，在相通的地址和端口建立了另外一个连接。由于 IP 地址和端口相同，所以后一种连接被称为先前连接的“化身”。TCP 必须防止连接中的旧副本在稍后再次出现，并被误解为属于同一连接的新“化身”。为此，TCP 将不会启动当前处于 TIME_WAIT 状态的连接的新“化身”。由于 TIME_WAIT 状态的持续时间时两倍的 MSL，因此 TCP 允许一个方向的数据在 MSL 秒内丢失，也允许回复在一个 MSL 秒内丢失。通过强制执行此规则，可以保证当一个 TCP 连接成功建立时，来自先前连接的所有旧的副本在网络中已过期。 ","link":"https://faded.auspicious.space/post/tcp-state-machine/"},{"title":"更好的 SQL 模式的 10 条规则","content":" 更好的 SQL 模式的 10 条规则 在创建新表和数据仓库时，要做很多决定。一些在当时似乎无关紧要的地方，却让你和用户在数据库的生命期内感到痛苦。 我们和成千上万的人们以及他们的数据库一道工作，经历了长期的读写查询，我们差不多看到了每种情况。下面是创建免去痛苦模式的 10 条规则。 只使用小写字母、数字和下划线 不要在数据库、模式、表或列名中使用点（dot）、空格、或连接号【注1】。点用于标示对象，通常以 database.schema.table.column 的方式。 对象名称中包含点将引起混淆。类似地，在对象名字里使用空格将迫使你在查询语句中添加不必要的引号： SELECT &quot;user name&quot; FROM events; -- vs SELECT user_name FROM events; 如果在表或列名里有大写字母，查询语句将难以书写。如果所有字母都是小写的，人们将不必记住 users 表是 Users 还是 users。 当你最终修改数据库或把你的表复制到仓库时，你不需要记住哪个表是大小写敏感的。 使用简单的、自说明的列名 如果 users 表需要 packages 表的外键，就把键命名为 package_id。避免使用简短、晦涩的名字，比如 pkg_fk；其他人不知道它代表什么。自说明的名字让其他人更容易理解模式，随着团队规模的增加，这对于维护效率至关重要。 不要为多态的数据使用有歧义的名字。如果你发现自己创建了形如 item_type 或 item_value 的列，那么你最好使用带有具体名字的、更多的列，比如 photo_count、view_cout、transaction_price。 这样，列的内容就可以常从模式中获悉，而不用依赖于当前行的其它值。 SELECT SUM(item_value) AS photo_count FROM items WHERE item_type = 'Photo Count'; -- vs SELECT SUM(photo_count) FROM items; 不要把包含表的名字做为列名的前缀。通常，让用户表包含形如 user_birthday、user_created_at、user_name 的列，没有多少帮助。 避免使用 column、tag 和 user 之类的保留字做为列名。你将不得不在查询中使用额外的引号，不这么做将让你对错误信息感到困惑。如果保留字出现在列名应该出现的地方，数据库会极大地错误理解查询。 使用简单的、自说明的表名 如果表名由多个单词组成，就使用下划线隔开这些单词。package_deliveries 要比 packagedeliveries 更容易阅读。 如有可能，使用一个单词而不是两个单词：deliveries 更易于读。 SELECT * FROM packagedeliveries; -- vs SELECT * FROM deliveries; 不要给表加前缀来暗示模式。如果你需要把表分组为范围，就把这些表放入一个模式。store_items、store_transactions、store_coupons 之类的表名，和加了前缀的列名一样，通常不值得额外敲键盘。 我们推荐表名使用复数（例如 packages），连接表（join table）名字的两个单词都用复数（例如 packages_users）。单数的表名更有可能偶尔与保留字相撞，并且在查询语句中通常有着较低的可读性。 主键为整数 即使你在用 UUID，它也没有意义（比如对于连接表来说），添加标准的 id 列、自增整数序列。这种 key 使得某些查询更加容易，比如仅仅选取一组数据的第一行。 如果导入的任务需要复制数据，这种 key 将成为救命稻草，因为你能够删除特定行： DELETE FROM my_table WHERE id IN (SELECT ...) AS duplicated_ids; 避免多列主键。它们在尽量编写有效查询时难以推断，且难以修改。要使用整数主键、多列 unique 约束、一些单列索引代替。 与外键保持一致 有很多关于主键和外键的命名风格。我们推荐，最受欢迎的是，任何表 foo，都要拥有一个名叫 id 的主键，所有的主键命名为 foo_id。 另一种受欢迎的风格使用全局唯一键名，表 foo 有个名叫 foo_id 的主键，所有外键也叫 foo_id。如果你使用缩写（users 表的主键用 uid），会引起混淆或命名冲突，故不要缩写。 无论你选择什么风格，就保持下去。不要在有的地方使用 uid，在另外地方使用 user_id 或 users_fk。 SELECT * FROM packages JOIN users ON users.user_id = packages.uid; -- vs SELECT * FROM packages JOIN users ON users.id = packages.user_id; -- or SELECT * FROM packages JOIN users USING (user_id); 还要留意不能明显匹配表的外键。名叫 owner_id 的列名或许是 users 表的外键，或许不是。把列名取为 user_id，如有必要，取为 owner_user_id。 把时间存储为 Datetime 不要把日期保持为 Unix 时间戳或字符串：而是把它们转化为 datetime。虽然 SQL 的 date 数学函数不是最好的，但是你自己处理时间戳甚至更难。使用 SQL date 函数要求每次查询都把时间戳转化为 datetime： SELECT DATE(FROM_unixtime(created_at)) FROM packages; -- vs SELECT DATE(created_at) FROM packages; 不要在单独的列里存储年、月、日。这使得每一条时间序列【注2】查询非常难以编写，将阻碍大多数刚入门的 SQL 用户使用表格中的日期信息。 SELECT DATE(created_year || '-' || created_month || '-' || created_day); -- vs SELECT DATE(created_at); UTC，一直都是 UTC 使用某种时区而非 UTC 将引起永无止境的问题。优秀的工具（包括 Periscope）具备所有你需要的、将数据从 UTC 转换成当前时区的功能。在 Periscope 里，添加 :pst 就轻松地将 UTC 转换成 Pacific Time： SELECT [created_at:pst], email_address FROM users; 数据库的时区应该是 UTC，所有的 datetime 列应该是去除了时区的类型（没有时区的时间戳）。 如果你的数据库的时区不是 UTC，或者你的数据库既有 UTC、又有非 UTC 的 datetime，那么时间序列的分析难度将大为增加。 单一的真实数据来源 对于一条数据，应该有且只有一个真实来源【注3】。视图和汇总应该打上标签。这样，数据的使用人员将明白，在他们使用的数据和真实数据之间存在差异。 SELECT * FROM daily_usage_rollup; 留下废弃的 user_id、user_id_old、user_id_v2 之类的列，将变成混淆的、永无止境的源头。在日常维护中，要确信 drop 掉了已被抛弃的表、和弃用的列。 更喜欢没有 JSON 列的表 你肯定不想要非常宽的表。如果有很多列，且它们有的按顺序命名（比如 answer1、answer2、answer3），今后你就会痛苦。 把这种表拆分成没有重复列的模式，这种模式的形态将特别容易查询。例如，获取 survey 表的、完成的答案的数目： SELECT SUM( (CASE WHEN answer1 IS NOT NULL THEN 1 ELSE 0 END) + (CASE WHEN answer2 IS NOT NULL THEN 1 ELSE 0 END) + (CASE WHEN answer3 IS NOT NULL THEN 1 ELSE 0 END) ) AS num_answers FROM surveys WHERE id = 123; -- vs SELECT COUNT(response) FROM answers WHERE survey_id = 123; 对于分析查询，从 JSON 列提取数据，能够极大地降低查询效率。虽然在生产环境有很多理由使用 JSON 列，但那不是针对分析的。强势地把 JSON 列转换为更简单的数据类型，让分析更加容易、更加快捷。 不要过度规范化 日期、邮编和国家，不需要让它们自己的表带有主键查询。如果你带了，每次查询将包含有少量的相同连接。这会给数据库创建大量重复的 SQL，以及大量额外工作。 SELECT dates.d, COUNT(1) FROM users JOIN dates ON users.created_date_id = dates.id GROUP BY 1; -- vs SELECT DATE(created_at), COUNT(1) FROM users GROUP BY 1; 表是有着它们大量自己的数据的第一类对象。其它数据都应该是更加重要的对象上的、另外的列。 注1 连接号（－，〜），表示连接、起止、流程的符号。“两个相关的名词构成一个意义单位，中间用连接号。”、“相关的时间、地点或数目之间用连接号，表示起止。”、“相关的字母、阿拉伯数字等之间，用连接号，表示产品型号。”、“几个相关的项目表示递进式发展，中间用连接号。”http://zh.wikipedia.org/wiki/连接号 请注意：连接号和连字号是不同的：http://zh.wikipedia.org/wiki/连接号 注2 时间序列是用时间排序的一组随机变量，国内生产毛额（GDP）、消费者物价指数（CPI）、台湾加权股价指数、利率、汇率等等都是时间序列。 时间序列的时间间隔可以是分秒（如高频金融数据），可以是日、周、月、季度、年、甚至更大的时间单位。http://zh.wikipedia.org/wiki/時間序列(經濟學) 注3 In Information Systems design and theory Single Source Of Truth (SSOT) refers to the practice of structuring information models and associated schemata such that every data element is stored exactly once (e.g., in no more than a single row of a single table). http://en.wikipedia.org/wiki/Single_Source_of_Truth ","link":"https://faded.auspicious.space/post/10-rules-for-a-better-sql-schema/"},{"title":"跨页面通信的各种姿势","content":" 跨页面通信的各种姿势 将跨页面通讯类比计算机进程间的通讯，其实方法无外乎那么几种，而 Web 领域可以实现的技术方案主要是类似于以下两种原理： 获取句柄，定向通讯。 共享内存，结合轮询或者事件通知来完成业务逻辑。 由于第二种原理更利于解耦业务逻辑，具体的实现方案比较多样。以下是具体的实现方案，简单介绍下，权当科普： 获取句柄 具体方案 父页面通过 window.open(url, name) 方式打开的子页面可以获取句柄，然后通过 postMessage 完成通讯需求。 // parent.html const childPage = window.open('child.html', 'child') childPage.onload = () =&gt; { childPage.postMessage('hello', location.origin) } // child.html window.onmessage = evt =&gt; { // evt.data } Tips 当指定 window.open 的第二个 name 参数时，再次调用 window.open('****', 'child')会使之前已经打开的同 name 子页面刷新； 由于安全策略，异步请求之后再调用 window.open 会被浏览器阻止，不过可以通过句柄设置子页面的url即可实现类似效果。 // 首先先开一个空白页 const tab = window.open('about:blank') // 请求完成之后设置空白页的url fetch(/* ajax */).then(() =&gt; { tab.location.href = '****' }) 优劣 缺点是只能与自己打开的页面完成通讯，应用面相对较窄；但优点是在跨域场景中依然可以使用该方案。 LocalStorage 具体方案 设置共享区域的 storage，storage 会触发 storage 事件。 // A.html localStorage.setItem('message', 'hello') // B.html window.onstorage = evt =&gt; { // evt.key, evt.oldValue, evt.newValue } Tips 触发写入操作的页面下的 storage listener 不会被触发； storage 事件只有在发生改变的时候才会触发，即重复设置相同值不会触发 listener； safari 隐身模式下无法设置 localStorage 值。 优劣 API 简单直观，兼容性好，除了跨域场景下需要配合其他方案，无其他缺点。 BroadcastChannel 具体方案 和 localStorage 方案基本一致，额外需要初始化。 // A.html const channel = new BroadcastChannel('tabs') channel.onmessage = evt =&gt; { // evt.data } // B.html const channel = new BroadcastChannel('tabs') channel.postMessage('hello') 优劣 和 localStorage 方案没特别区别，都是同域、API 简单，BroadcastChannel 方案兼容性差些（Chrome &gt; 58），但比 localStorage 方案生命周期短（不会持久化），相对干净些。 SharedWorker 具体方案 SharedWorker 本身并不是为了解决通讯需求的，它的设计初衷应该是类似总控，将一些通用逻辑放在SharedWorker 中处理。不过因为也能实现通讯，所以一并写下： // A.html var sharedworker = new SharedWorker('worker.js') sharedworker.port.start() sharedworker.port.onmessage = evt =&gt; { // evt.data } // B.html var sharedworker = new SharedWorker('worker.js') sharedworker.port.start() sharedworker.port.postMessage('hello') // worker.js const ports = [] onconnect = e =&gt; { const port = e.ports[0] ports.push(port) port.onmessage = evt =&gt; { ports.filter(v =&gt; v!== port) // 此处为了贴近其他方案的实现，剔除自己 .forEach(p =&gt; p.postMessage(evt.data)) } } 优劣 相较于其他方案没有优势，此外，API 复杂而且调试不方便。 Cookie 具体方案 一个古老的方案，有点 localStorage 的降级兼容版，我也是整理本文的时候才发现的，思路就是往 document.cookie 写入值，由于 cookie 的改变没有事件通知，所以只能采取轮询脏检查来实现业务逻辑。 方案比较丑陋，势必被淘汰的方案，贴一下原版思路地址，我就不写 demo 了。 communication between browser windows (and tabs too) using cookies 优劣 相较于其他方案没有存在优势的地方，只能同域使用，而且污染 cookie 以后还额外增加 AJAX 的请求头内容。 Server 之前的方案都是前端自行实现，势必受到浏览器限制，比如无法做到跨浏览器的消息通讯，比如大部分方案都无法实现跨域通讯（需要增加额外的 postMessage 逻辑才能实现）。通过借助服务端，还有很多增强方案，也一并说下。 乞丐版 后端无开发量，前端定期保存，在 tab 被激活时重新获取保存的数据，可以通过校验 hash 之类的标记位来提升检查性能。 window.onvisibilitychange = () =&gt; { if (document.visibilityState === 'visible') { // AJAX } } Server-sent Events / Websocket 项目规模小型的时候可以采取这类方案，后端自行维护连接，以及后续的推送行为。 SSE // 前端 const es = new EventSource('/notification') es.onmessage = evt =&gt; { // evt.data } es.addEventListener('close', () =&gt; { es.close() }, false) // 后端，express为例 const clients = [] app.get('/notification', (req, res) =&gt; { res.setHeader('Content-Type', 'text/event-stream') clients.push(res) req.on('aborted', () =&gt; { // 清理clients }) }) app.get('/update', (req, res) =&gt; { // 广播客户端新的数据 clients.forEach(client =&gt; { client.write('data:hello\\n\\n') setTimeout(() =&gt; { client.write('event:close\\ndata:close\\n\\n') }, 500) }) res.status(200).end() }) WebSocket socket.io、sockjs 例子比较多，略 消息队列 项目规模大型时，需要消息队列集群长时间维护长链接，在需要的时候进行广播。 提供该类服务的云服务商很多，或者寻找一些开源方案自建。 例如 MQTT 协议方案（阿里云就有提供），Web 客户端本质上也是 WebSocket，需要集群同时支持ws 和 mqtt 协议，示例如下： // 前端 // 客户端使用开源的Paho // port会和mqtt协议通道不同 const client = new Paho.MQTT.Client(host, port, 'clientId') client.onMessageArrived = message =&gt; { // message. payloadString } client.connect({ onSuccess: () =&gt; { client.subscribe('notification') } }) // 抑或，借助flash（虽然快要被淘汰了）进行mqtt协议连接并订阅相应的频道，flash再通过回调抛出消息 // 后端 // 根据服务商提供的Api接口调用频道广播接口 ","link":"https://faded.auspicious.space/post/ways-to-cross-page-communication/"},{"title":"TCP 详解","content":" 计算机网络：这是一份全面 &amp; 详细 的TCP协议攻略 1 定义 Transmission Control Protocol，即传输控制协议。 属于传输层通信协议； 基于 TCP 的应用层协议有 HTTP、SMTP、FTP、Telnet 和 POP3。 2 特点 面向连接 面向字节流 全双工通信 可靠 具体介绍如下： 特点 具体描述 面向连接 使用 TCP 传输数据前，必须先建立 TCP 连接；传输完成后再释放连接（就像打电话：先拨号建立连接，打完后挂机释放连接） 全双工通信 建立 TCP 连接后，通信双方都能发送数据 可靠 通过 TCP 连接传送的数据：不丢失、无差错、不重复并且按需到达 面向字节流 数据以流的形式进行传输- 流：流入/流出进程的字符序列 - TCP 一次传输的报文长度有限制，若太大则需分块、分次传输- 但由于 TCP 连接的可靠性，接收方可按顺序接受数据块并重新组成分块之前的数据流- 所以 TCP 看起来就像直接互相传输字节流一样，即面向字节流 3 优缺点 优点：数据传输可靠。 缺点：效率慢（因需建立连接、发送确认包等）。 4 应用场景（对应的应用层协议） 要求通信数据可靠时，即数据要准确无误地传递给对方. 如：传输文件：HTTP、HTTPS、FTP 等协议；传输邮件：POP、SMTP 等协议。 万维网：HTTP 协议 文件传输：FTP 协议 电子邮件：SMTP 协议 远程终端接入：TELNET 协议 5 报文段格式 TCP 虽面向字节流，但传送的 数据单元 = 报文段； 报文段 = 首部 + 数据 2部分； TCP 的全部功能体现在它首部中各字段的作用，故下面主要讲解 TCP 报文段的首部。 首部前 20 个字符固定、后面有 4n 个字节是根据需而增加的选项，故TCP 首部最小长度 = 20字节。 字段 作用 备注 序号（报文段序号） 本报文段所发送数据的第 1 个字节的序号 4 字节 确认号（ACK） 期望收到对方下一个报文段的第一个数据字节的序号 - 4 字节- 若确认号 = N，则表明到序号 N-1 为止的所有数据都已正确收到 SYN（同步位） 连接建立时用于同步序号 - SYN = 1、ACK = 0，表明这是一个连接请求报文段- SYN = 1、ACK = 1，表明这是一个连接请求响应报文段 FIN（终止控制位） 释放连接 FIN = 1时，表明此报文段的发送方已发送数据完毕并要求释放连接 6 建立连接过程 TCP 建立连接需三次握手，具体介绍如下： 建立 TCP 连接前： TCP 客户端、服务器都处于关闭状态（CLOSED）； 直到客户端主动打开连接，服务器才被动打开连接处于监听状态（LISTEN），等待接收客户端的请求。 过程 具体描述 报文段信息 状态 第一次握手 客户端向服务器发送一个连接请求的报文段 - 同步标志位设为1：SYN = 1- 随机选择一个起始序号：seq = x- 不携带数据（因为SYN位被设置为1的报文段不能携带数据，但要消耗一个序号） 客户端进入同步已发送状态（SYN_SEND）（等待服务器的确认） 第二次握手 服务器收到请求连接报文段后，若同意建立连接，则向客户端发回连接确认的报文段（为该 TCP 连接分配 TCP 缓存、变量） - 同步标志位设为1：SYN=1- 确认标志位设为1：ACK=1- 随机选择一个起始序号：seq=y- 确认号字段设为：ack=x+1- 不携带数据（原因同上：但要消耗一个序号） 服务器进入同步已接收状态（SYN_RCVD） 第三次握手 客户端收到确认报文段后，向服务器再次发出连接确认报文段（为该 TCP 连接分配 TCP 缓存、变量） - 确认标记位设为1：ACK=1- 序号：seq=x+1- 确认号字段设为：ack=y+1- 可携带数据（因SYN无设为1，若不携带数据则不消耗序号） 客户端、服务器都进入已创建（ESTABLISHED）（可开始发送数据） 成功进行 TCP 的三次握手后，就建立起一条 TCP 连接，即可传送应用层数据 注： 因 TCP 提供的是全双工通信，故通信双方的应用进程在任何时候都能发送数据； 三次握手期间，任何 1 次未收到对面的回复，则都会重发。 特别说明：为什么TCP建立连接需三次握手？ 结论 防止服务器端因接收了早已失效的连接请求报文，从而一直等待客户端请求，最终导致形成死锁、浪费资源。 具体描述 SYN 洪泛攻击： 从上可看出：服务端的 TCP 资源分配时刻 = 完成第二次握手时；而客户端的 TCP 资源分配时刻 = 完成第三次握手时； 这就使得服务器易于受到 SYN 洪泛攻击，即同时多个客户端发起连接请求，从而需进行多个请求的 TCP 连接资源分配。 7 释放连接过程 在通信结束后，双方都可以释放连接，共需四次挥手，具体如下： 释放 TCP 连接前： TCP 客户端、服务器都处于已创建状态（ESTABLISHED）； 直到：客户端主动关闭 TCP 连接。 过程 具体描述 报文段信息 状态 第一次挥手 客户端向服务器发送一个连接释放的报文段（停止再发送数据） - 终止控制位设为1：FIN=1- 报文段序号设为前面传送数据最后一个字节的序号加1：seq=u- 可携带数据（FIN=1的报文即使不携带数据也消耗一个序号） 客户端进入终止等待1状态（FIN-WAIT-1）（等待服务器的确认） 第二次挥手 服务器收到连接释放报文段后，则向客户端发回连接释放确认的报文段 - 确认标记位设为1：ACK=1- 报文段序号设为前面传送数据最后一个字节的序号加1：seq=v- 确认号字段设为：ack=u+1 - 服务器进入关闭等待状态（CLOSE-WAIT）- 客户端收到服务器的确认后，进入终止等待2状态（FIN-WAIT-2），等待服务器发出释放连接请求- 至此，客户端→服务器的 TCP 连接已断开- 即 TCP 连接处于半关闭状态- 即客户端→服务器断开，但服务器→客户端未断开 第三次挥手 若服务器已无要向客户端发送数据，则发出释放连接的报文段 - 终止控制位设为1：FIN=1- 确认标记位设为1：ACK=1- 报文段序号：seq=w重复上次已发送的确认号字段设为：ack=u+1可携带数据（FIN=1的报文即使不携带数据也消耗一个序号） 服务器端进入最后确认状态（LAST-ACK） 第四次握手 客户端收到连接释放报文段后，则向服务器发回连接释放确认的报文段 - 确认标记位设为1：ACK=1报文段序号：seq=u+1- 确认号字段设为：ack=w+1- 可携带数据（FIN=1的报文即使不携带数据也消耗一个序号） - 客户端进入时间等待状态（TIME-WAIT） 特别说明：为什么 TCP 释放连接需四次挥手？ 结论 为了保证通信双方都能通知对方 需释放 &amp; 断开连接，即释放连接后，都无法接收 / 发送消息给对方。 具体描述 延伸疑问 为什么客户端关闭连接前要等待 2 MSL 时间？即 TIME - WAIT 状态的作用是什么？（MSL = 最长报文段寿命（Maximum Segment Lifetime）） 原因1：为了保证客户端发送的最后一个连接释放确认报文能到达服务器，从而使得服务器能正常释放连接。 原因2：防止上文提到的早已失效的连接请求报文出现在本连接中。 客户端发送了最后一个连接释放请求确认报文后，再经过 2 MSL 时间，则可使本连接持续时间内所产生的所有报文段都从网络中消失。即在下一个新的连接中就不会出现早已失效的连接请求报文 8 无差错传输 对比于 UDP，TCP 的传输是可靠的、无差错的。那么为什么 TCP 的传输为什么是可靠的、无差错的呢？下面，我将详细讲解 TCP 协议的无差错传输。 8.1 含义 无差错：即传输信道不出差错； 发送 &amp; 接收效率匹配：即无论发送方以多快的速度发送数据，接收方总来得及处理收到的数据。 8.2 基础：滑动窗口协议 先理解2个基础概念：发送窗口、接收窗口 类型 定义 作用 发送窗口 在任意时刻，发送方维持的一组连续的、允许发送帧的帧序号（发送窗口的大小：还没有收到对方确认信息的情况下发送方最多还可以发送多少个数据帧） 对发送方进行流量控制 接收窗口 任意时刻，接收方维持的一组连续的、允许接收帧的帧序号 - 控制可接收哪些数据帧和不可接收哪些数据帧- 接收方只有当收到的数据帧的序号落入接收窗口内采允许将该数据帧收下；否则一律丢弃 工作原理 对于发送端： 每收到一个确认帧，发送窗口就向前滑动一个帧的距离； 当发送窗口内无可发送的帧时（即窗口内的帧全部是已发送但未收到确认的帧），发送方就会停止发送，直到收到接收方发送的确认帧使窗口移动，窗口内有可以发送的帧，之后才开始继续发送。 具体如下图： 对于接收端： 当收到数据帧后，将窗口向前移动一个位置，并发回确认帧，若收到的数据帧落在接收窗口之外，则一律丢弃。 滑动窗口协议的重要特性 只有接收窗口向前滑动、接收方发送了确认帧时，发送窗口才有可能（只有发送方收到确认帧才是一定）向前滑动。 停止-等待协议、后退 N 帧协议和选择重传协议只是在发送窗口大小和接收窗口大小上有所差别： 停止等待协议：发送窗口大小=1，接收窗口大小=1；即单帧滑动窗口等于停止-等待协议。 后退 N 帧协议：发送窗口大小&gt;1，接收窗口大小=1。 选择重传协议：发送窗口大小&gt;1，接收窗口大小&gt;1。 当接收窗口的大小为 1 时，可保证帧有序接收。数据链路层的滑动窗口协议中，窗口的大小在传输过程中是固定的（注意要与 TCP 的滑动窗口协议区别）。 8.3 实现无差错传输的解决方案 核心思想：采用一些可靠传输协议，使得 出现差错时，让发送方重传差错数据：即出错重传； 当接收方来不及接收收到的数据时，可通知发送方降低发送数据的效率：即速度匹配。 针对上述俩个问题，分别采用的解决方案是：自动重传协议和流量控制和拥塞控制协议。 自动重传请求协议ARQ（针对出错重传） 定义：即 Auto Repeat reQuest，具体介绍如下： 类型： 类型 原理 特点 停等式 ARQ（Stop-and-Wait） （单帧滑动窗口）停止-等待协议+超时重传 发送窗口大小=1、接收窗口大小=1 后退 N 帧 ARQ（Go-Back-N） 多帧滑动窗口+累计确认+后退N帧+超时重传 发送窗口大小&gt;1、接收窗口大小=1 选择重传 ARQ（Selective Repeat） 多帧滑动窗口+累计确认+后退N帧+超时重传 发送窗口大小&gt;1、接收窗口大小&gt;1 下面，将主要讲解上述 3 类协议： 停等式ARQ（Stop-and-Wait） 原理：（单帧滑动窗口）停止 - 等待协议 + 超时重传，即 ：发送窗口大小=1、接收窗口大小=1 停止 - 等待协议的协议原理如下： 发送方每发送一帧，要等到接收方的应答信号后才能发送下一帧 接收方每接收一帧，都要反馈一个应答信号，表示可接下一帧 若接收方不反馈应答信号，则发送方必须一直等待 后退 N 帧协议 也称：连续 ARQ 协议 原理：多帧滑动窗口 + 累计确认 + 后退N帧 + 超时重传，即 ：发送窗口大小&gt;1、接收窗口大小=1 具体描述： 发送方：采用多帧滑动窗口的原理，可连续发送多个数据帧而不需等待对方确认。 接收方：采用累计确认和后退 N 帧的原理，只允许按顺序接收帧。具体原理如下： 示例讲解： 本示例 = 源站向目的站发送数据帧。具体示例如下： 选择重传 ARQ（Selective Repeat） 原理：多帧滑动窗口 + 累计确认 + 后退N帧 + 超时重传，即 ：发送窗口大小&gt;1、接收窗口大小&gt;1。类似于类型 2（后退 N 帧协议），此处仅仅是接收窗口大小的区别，故此处不作过多描述。 特点： 优：因连续发送数据帧而提高了信道的利用率； 缺：重传时又必须把原来已经传送正确的数据帧进行重传（仅因为这些数据帧前面有一个数据帧出了错），将导致传送效率降低。 由此可见，若信道传输质量很差，导致误码率较大时，后退 N 帧协议不一定优于停止-等待协议 流量控制 &amp; 拥塞控制（针对 速度匹配） 流量控制 简介： 实例： 特别注意：死锁问题 拥塞控制 定义：防止过多的数据注入到网络中，使得网络中的路由器和链路不致于过载。 拥塞：对网络中的资源需求 &gt; 该资源所能提供的部分 与 “流量控制”的区别： 类型 范围 面向对象 实际措施 拥塞控制 全局性 整个通信网络（含所有主机和路由器） 防止过多数据注入网络 流量控制 点对点、端到端 发送端 降低发送端的发送速率 具体解决方案： 共分为 2 个解决方案：慢开始和拥塞避免、快重传和快恢复。其中，涉及 4 种算法，即慢开始、拥塞避免、快重传和快恢复。 具体介绍如下： 慢开始 &amp; 拥塞避免 储备知识：拥塞窗口、慢开始算法、拥塞避免算法 拥塞窗口 发送方维持一个状态变量：拥塞窗口（cwnd， congestion window），具体介绍如下： 慢开始算法 原理：当主机开始发送数据时，由小到大逐渐增大 拥塞窗口数值（即发送窗口数值），从而由小到大逐渐增大发送报文段。 目的：开始传输时，试探网络的拥塞情况。 具体措施： 步骤 具体描述 备注 1. 开始发送报文段时 把拥塞窗口（cwnd）设置为一个最大报文段（MSS）的数值 设置得如此小的目的：试探一下网络的拥塞情况 2. 每收到一个对新的报文段的确认后 把拥塞窗口增加至多一个 MSS 的数值 逐步增大发送方的拥塞窗口 cwnd，以便分组注入到网络的速率更加合理 3. 每经过一个传输轮次 拥塞窗口（cwnd）就加倍 - 一个传输轮次：把拥塞窗口（cwnd）所允许发送的报文段都连续发送出去，并收到了对已发送的最后一个字节的确认- 一个传输轮次所经历的时间=往返时间 RTT 示意图： 特别注意： 慢开始的“慢”指：一开始发送报文段时拥塞窗口（cwnd）设置得较小（为1），使得发送方在开始时只发送一个报文段（目的是试探一下网络的拥塞情况），并不是指拥塞窗口（cwnd）的增长速率慢。 拥塞避免算法 原理：使得拥塞窗口（cwnd）按线性规律 缓慢增长：每经过一个往返时间RTT，发送方的拥塞窗口（cwnd）加1。 拥塞避免并不可避免拥塞，只是将拥塞窗口按现行规律缓慢增长，使得网络比较不容易出现拥塞 相比慢开始算法的加倍，拥塞窗口增长速率缓慢得多。 示意图： 解决方案描述（慢开始和拥塞避免） 为了防止拥塞窗口（cwnd）增长过大而引起网络拥塞，采用慢开始和拥塞避免 2 种算法，具体规则如下： 实例说明： 快重传和快恢复 快重传和快恢复的解决方案是对慢开始和拥塞避免算法的改进。 储备知识：快重传算法、快恢复算法。 快重传算法 原理： 接收方：每收到一个失序的报文段后 就立即发出重复确认（为的是使发送方及早知道有报文段没有到达对方），而不要等到自己发送数据时才进行捎带确认。 发送方只要一连收到 3 个重复确认就立即重传对方尚未收到的报文段，而不必继续等待设置的重传计时器到期。 作用： 由于发送方尽早重传未被确认的报文段，因此采用快重传后可以使整个网络吞吐量提高约 20%。 示意图： 快恢复 当发送方连续收到 3 个重复确认后，就： 执行乘法减小算法：把慢开始门限（ssthresh）设置为出现拥塞时发送方窗口值的一半 = 拥塞窗口的一半； 将拥塞窗口（cwnd）值设置为慢开始门限 ssthresh 减半后的数值 = 拥塞窗口的一半； 执行加法增大算法：执行拥塞避免算法，使拥塞窗口缓慢地线性增大。 注： 由于跳过了拥塞窗口（cwnd）从1起始的慢开始过程，所以称为：快恢复； 此处网络不会发生网络拥塞，因若拥塞，则不会收到多个重复确认报文。 解决方案描述（快重传 &amp; 快恢复） 原理：为了优化慢开始和拥塞避免的解决方案，在上述方案中加入快重传和快恢复 两种算法，具体规则如下： 示意图： 至此，关于 TCP 无差错传输的知识讲解完毕。 9 与 UDP 协议的区别 类型 特点 性能 应用场景 首部字节 是否面向连接 传输可靠性 传输形式 传输效率 所需资源 TCP 面向连接 可靠 字节流 慢 多 要求通信数据可靠（如文件传输、邮件传输） 20-60 UDP 无连接 不可靠 数据报文段 快 少 要求通信速度高（如域名转换） 8 个字节（有 4 个字段组成） ","link":"https://faded.auspicious.space/post/tcp-full-introduction/"},{"title":"HTML5 file API 加 Canvas 实现图片前端 JS 压缩并上传","content":" HTML5 file API加canvas实现图片前端JS压缩并上传 图片上传前端压缩的现实意义 对于大尺寸图片的上传，在前端进行压缩除了省流量外，最大的意义是极大的提高了用户体验。 这种体验包括两方面： 由于上传图片尺寸比较小，因此上传速度会比较快，交互会更加流畅，同时大大降低了网络异常导致上传失败风险。 最最重要的体验改进点：省略了图片的再加工成本。很多网站的图片上传功能都会对图片的大小进行限制，尤其是头像上传，限制 5M 或者 2M 以内是非常常见的。然后现在的数码设备拍摄功能都非常出众，一张原始图片超过 2M 几乎是标配，此时如果用户想把手机或相机中的某个得意图片上传作为自己的头像，就会遇到因为图片大小限制而不能上传的窘境，不得不对图片进行再处理，而这种体验其实非常不好的。如果可以在前端进行压缩，则理论上对图片尺寸的限制是没有必要的。 HTML5 file API加canvas实现图片前端JS压缩 要想使用 JS 实现图片的压缩效果，原理其实很简单，核心 API 就是使用 canvas 的 drawImage() 方法。 canvas 的 drawImage() 方法 API 如下： context.drawImage(img, dx, dy); context.drawImage(img, dx, dy, dWidth, dHeight); context.drawImage(img, sx, sy, sWidth, sHeight, dx, dy, dWidth, dHeight); 后面最复杂的语法虽然看上去有 9 大参数，但不用慌，实际上可以看出就 3 个参数： img：就是图片对象，可以是页面上获取的 DOM 对象，也可以是虚拟 DOM 中的图片对象。 dx, dy, dWidth, dHeight：表示在 canvas 画布上规划处一片区域用来放置图片，dx，dy 为 canvas 元素的左上角坐标，dWidth，dHeight 指 canvas 元素上用在显示图片的区域大小。如果没有指定 sx，sy，sWidth，sHeight 这 4 个参数，则图片会被拉伸或缩放在这片区域内。 sx, sy, swidth, sheight：这 4 个坐标是针对图片元素的，表示图片在 canvas 画布上显示的大小和位置。sx，sy 表示图片上 sx，sy 这个坐标作为左上角，然后往右下角的 swidth，sheight 尺寸范围图片作为最终在 canvas 上显示的图片内容。 drawImage() 方法有一个非常怪异的地方，大家一定要注意，那就是 5 参数和 9 参数里面参数位置是不一样的，这个和一般的 API 有所不同。一般 API 可选参数是放在后面。但是，这里的 drawImage() 9 个参数时候，可选参数 sx，sy，swidth，sheight 是在前面的。如果不注意这一点，有些表现会让你无法理解。 下图为 MDN 上原理示意： 对于本文的图片压缩，需要用的是 5 个参数语法。举个例子，一张图片（假设图片对象是 img）的原始尺寸是 4000*3000，现在需要把尺寸限制为 400*300 大小，很简单，原理如下代码示意： var canvas = document.createElement('canvas'); var context = canvas.getContext('2d'); canvas.width = 400; canvas.height = 300; // 核心JS就这个 context.drawImage(img,0,0,400,300); 把一张大的图片，直接画在一张小小的画布上。此时大图片就天然变成了小图片，压缩就这么实现了，是不是简单的有点超乎想象。 当然，若要落地于实际开发，我们还需要做些其他的工作，就是要解决图片来源和图片去向的问题。 如何把系统中图片呈现在浏览器中 HTML5 file API 可以让图片在上传之前直接在浏览器中显示，通常使用 FileReader 方法，代码示意如下： var reader = new FileReader(), img = new Image(); // 读文件成功的回调 reader.onload = function(e) { // e.target.result就是图片的base64地址信息 img.src = e.target.result; }; eleFile.addEventListener('change', function (event) { reader.readAsDataURL(event.target.files[0]); }); 于是，包含图片信息的 context.drawImage() 方法中的 img 图片就有了。 如何把 canvas 画布转换成 img 图像 canvas 天然提供了 2 个转图片的方法，一个是： canvas.toDataURL() 语法如下： canvas.toDataURL(mimeType, qualityArgument); 可以把图片转换成 base64 格式信息，纯字符的图片表示法。 其中： mimeType 表示 canvas 导出来的 base64 图片的类型，默认是 png 格式，也即是默认值是 'image/png'，我们也可以指定为 jpg 格式 'image/jpeg' 或者 webp 等格式。file 对象中的 file.type 就是文件的 mimeType 类型，在转换时候正好可以直接拿来用（如果有 file 对象）。 qualityArgument 表示导出的图片质量，只要导出为 jpg 和 webp 格式的时候此参数才有效果，默认值是 0.92，是一个比较合理的图片质量输出参数，通常情况下，我们无需再设定。 canvas.toBlob() 语法如下： canvas.toBlob(callback, mimeType, qualityArgument) 可以把 canvas 转换成 Blob 文件，通常用在文件上传中，因为是二进制的，对后端更加友好。 和 toDataURL() 方法相比，toBlob() 方法是异步的，因此多了个 callback 参数，这个 callback 回调方法默认的第一个参数就是转换好的 blob 文件信息，本文 demo 的文件上传就是将 canvas 图片转换成二进制的 blob 文件，然后再 ajax 上传的，代码如下： // canvas转为blob并上传 canvas.toBlob(function (blob) { // 图片ajax上传 var xhr = new XMLHttpRequest(); // 开始上传 xhr.open(&quot;POST&quot;, 'upload.php', true); xhr.send(blob); }); 于是，经过“图片→canvas 压缩→图片”三步曲，我们完成了图片前端压缩并上传的功能。 更加完整的核心代码请参见demo页面的左侧，如果对其他交互代码也敢兴趣，请参考页面源代码。 结束语 就在几个月前刚写过一篇文章“使用canvas在前端实现图片水印合成”，实际上所使用的技术和套路和本文是如出一辙的，也是“图片→ canvas 水印→图片”三步曲，区别在于水印合成是连续执行两次 context.drawImage() 方法，一次是原图一次水印图片，以及最后转换成图片的时候什么是 toDataURL() 方法，其他代码逻辑和原理都是一样的。 由此及彼，利用同样的原理和代码逻辑，我们还可以实现其它很多以前前端不太好实现的功能，比方说图片的真剪裁效果，所谓“真剪裁”指不是使用个 overflow:hidden 或者 clip 这些 CSS属性的“伪剪裁”，而是真正意义上就这么大区域图片信息。甚至配合一些前端算法，我们可以直接在前端进行人脸识别，图片自动美化等一系列功能再上传等等。 原理都是一样的，都是利用 canvas 作为中间媒介进行处理。 ","link":"https://faded.auspicious.space/post/html5-canvas-image-compress-upload/"},{"title":"PNG 的故事：获取图片信息和像素内容","content":" PNG 的故事：获取图片信息和像素内容 前言 现在时富媒体时代，图片的重要性对于数十亿互联网用户来说不言而喻，图片本身就是像素点阵的合集，但是为了如何更快更好的存储图片而诞生了各种各样的图片格式：jpeg、png、gif、webp 等，而这次我们要拿来开刀的，就是 png。 简介 首先，png 是什么鬼？我们来看看 wiki 上的一句话简介： Portable Network Graphics (PNG) is a raster graphics file format that supports lossless data compression. 也就是说，png 是一种使用无损压缩的图片格式，而大家熟知的另外一种图片格式——jpeg 则是采用有损压缩的方式。用通俗易懂的方式来讲，当原图片数据被编码成 png 格式后，是可以完全还原成原本的图片数据的，而编码成 jpeg 则会损耗一部分图片数据，这是因为两者的编码方式和定位不同。jpeg 着重于人眼的观感，保留更多的亮度信息，去掉一些不影响观感的色度信息，因此是有损耗的压缩。png 则保留原始所有的颜色信息，并且支持透明／alpha 通道，然后采用无损压缩进行编码。因此对于 jpeg 来说，通常适合颜色更丰富、可以在人眼识别不了的情况下尽可能去掉冗余颜色数据的图片，比如照片之类的图片；而 png 适合需要保留原始图片信息、需要支持透明度的图片。 以下，我们来尝试获取 png 编码的图片数据： 结构 图片是属于 2 进制文件，因此在拿到 png 图片并想对其进行解析的话，就得以二进制的方式进行读取操作。png 图片包含两部分：文件头和数据块。 文件头 png 的文件头就是 png 图片的前 8 个字节，其值为 [0x89, 0x50, 0x4E, 0x47, 0x0D, 0x0A, 0x1A, 0x0A]，人们常常把这个头称之为 “魔数”。玩过 Linux 的同学估计知道，可以使用 file 命令类判断一个文件是属于格式类型，就算我们把这个文件类型的后缀改得乱七八糟也可以识别出来，用的就是判断 “魔数” 这个方法。有兴趣的同学还可以使用 String.fromCharCode 将这个 “魔数” 转成字符串看看，就知道为什么 png 会取这个值作为文件头了。 用代码来判断也很简单： // 读取指定长度字节 function readBytes(buffer, begin, length) { return Array.prototype.slice.call(buffer, begin, begin + length); } let header = readBytes(pngBuffer, 0, 8); // [0x89, 0x50, 0x4E, 0x47, 0x0D, 0x0A, 0x1A, 0x0A] 数据块 去掉了 png 图片等前 8 个字节，剩下的就是存放 png 数据的数据块，我们通常称之为 chunk。 顾名思义，数据块就是一段数据，我们按照一定规则对 png 图片（这里指的是去掉了头的 png 图片数据，下同）进行切分，其中一段数据就是一个数据块。每个数据块的长度是不定的，我们需要通过一定的方法去提取出来，不过我们要先知道有哪些类型的数据块才好判断。 数据块类型 数据块类型有很多种，但是其中大部分我们都不需要用到，因为里面没有存储我们需要用到的数据。我们需要关注的数据块只有以下四种： IHDR：存放图片信息。 PLTE：存放索引颜色。 IDAT：存放图片数据。 IEND：图片数据结束标志。 只要解析这四种数据块就可以获取图片本身的所有数据，因此我们也称这四种数据块为 “关键数据块”。 数据块格式 数据块格式如下： 描述 长度 数据块内容长度 4 字节 数据块类型 4 字节 数据块内容 不定字节 CRC 冗余校验码 4 字节 这样我们就可以轻易的指导当前数据块的长度了，即 数据块内容长度 + 12 字节，用代码实现如下： // 读取32位无符号整型数 function readInt32(buffer, offset) { offset = offset || 0; return (buffer[offset] &lt;&lt; 24) + (buffer[offset + 1] &lt;&lt; 16) + (buffer[offset + 2] &lt;&lt; 8) + (buffer[offset + 3] &lt;&lt; 0); } let length = readInt32(readBytes(4)); // 数据块内容长度 let type = readBytes(4); // 数据块类型 let chunkData = readBytes(length); // 数据块内容 let crc = readBytes(4); // CRC 冗余校验码 这里的 CRC 冗余校验码在我们解码过程中用不到，所以这里不做详解。除此之外，数据块内容长度和数据块内容好解释，不过数据块类型有何作用呢，这里我们先将这个 type 转成字符串类型： // 将buffer数组转为字符串 function bufferToString(buffer) { let str = ''; for(let i=0, len=buffer.length; i&lt;len; i++){ str += String.fromCharCode(buffer[i]); } return str; } type = bufferToString(type); 然后会发现 type 的值是四个大写英文字母，没错，这就是上面提到的数据块类型。上面还提到了我们只需要解析关键数据块，因此遇到 type 不等于 IHDR、PLTE、IDAT、IEND 中任意一个的数据块就直接舍弃好了。当我们拿到一个关键数据块，就直接解析其数据块内容就可以了，即上面代码中的 chunkData 字段。 IHDR 类型为 IHDR 的数据块用来存放图片信息，其长度为固定的 13 个字节： 描述 长度 图片宽度 4 字节 图片高度 4 字节 图像深度 1 字节 颜色类型 1 字节 压缩方法 1 字节 过滤方式 1 字节 扫描方式 1 字节 其中宽高很好解释，直接转成 32 位整数，就是这张 png 图片等宽高（以像素为单位）。压缩方法目前只支持一种（deflate/inflate 压缩算法），其值为 0；过滤方式也只有一种（包含标准的 5 种过滤类型），其值为 0；扫描方式有两种，一种是逐行扫描，值为 0，还有一种是 Adam7 隔行扫描，其值为 1，此次只针对普通的逐行扫描方式进行解析，因此暂时不考虑 Adam7 隔行扫描。 图片深度是指每个像素点中的每个通道（channel）占用的位数，只有 1、2、4、8 和 16 这 5 个值；颜色类型用来判断每个像素点中有多少个通道，只有 0、2、3、4 和 6 这 5 个值： 颜色类型的值 占用通道数 描述 0 1 灰度图像，只有 1 个灰色通道 2 3 rgb 真彩色图像，有 RGB3 色通道 3 1 索引颜色图像，只有索引值一个通道 4 2 灰度图像 + alpha 通道 PLTE 类型为 PLTE 的数据块用来存放索引颜色，我们又称之为 “调色板”。 由 IHDR 数据块解析出来的图像信息可知，图像的数据可能是以索引值的方式进行存储。当图片数据采用索引值的时候，调色板就起作用了。调色板的长度和图像深度有关，假设图像深度的值是 xxx，则其长度通常为 2x×32^x\\times 32x×3。原因是图像深度保存的就是通道占用的位数，而在使用索引颜色的时候，通道里存放的就是索引值，2x2^x2x 就表示这个通道可能存放的索引值有多少个，即调色板里的颜色数。而每个索引颜色是 RGB3 色通道存放的，因此此处还需要 ×3\\times 3×3。 通常使用索引颜色的情况下，图像深度的值即为 8，因而调色板里存放的颜色就只有 256 种颜色，长度为 256 * 3 个字节。再加上 1 位布尔值表示透明像素，这就是我们常说的 png8 图片了。 IDAT 类型为 IDAT 的数据块用来存放图像数据，跟其他关键数据块不同的是，其数量可以是连续的复数个；其他关键数据块在 1 个 png 文件里有且只有 1 个。 这里的数据得按顺序把所有连续的 IDAT 数据块全部解析并将数据联合起来才能进行最终处理，这里先略过。 let dataChunks = []; let length = 0; // 总数据长度 // ... while(/* 存在IDAT数据块 */) { dataChunks.push(chunkData); length += chunkData.length; } IEND 当解析到类型为 IEND 的数据块时，就表明所有的 IDAT 数据块已经解析完毕，我们就可以停止解析了。 IEND 整个数据块的值时固定的：[0x00, 0x00, 0x00, 0x00, 0x49, 0x45, 0x4E, 0x44, 0xAE, 0x42, 0x60, 0x82]，因为 IEND 数据块没有数据块内容，所以其数据块内容长度字段（数据块前 4 个字节）的值也是 0。 解析 解压缩 当我们收集完 IDAT 的所有数据块内容时，我们要先对其进行解压缩： const zlib = require('zlib'); let data = new Buffer(length); let index = 0; dataChunks.forEach((chunkData) =&gt; { chunkData.forEach((item) =&gt; {data[index++] = item}); }); // inflate解压缩 data = zlib.inflateSync(new Buffer(data)); 扫描 上面说过，此次我们只考虑逐行扫描的方式： // 读取8位无符号整型数 function readInt8(buffer, offset) { offset = offset || 0; return buffer[offset] &lt;&lt; 0; } let width; // 解析IHDR数据块时得到的图像宽度 let height; // 解析IHDR数据块时得到的图像高度 let colors; // 解析IHDR数据块时得到的通道数 let bitDepth; // 解析IHDR数据块时得到的图像深度 let bytesPerPixel = Math.max(1, colors * bitDepth / 8); // 每像素字节数 let bytesPerRow = bytesPerPixel * width; // 每行字节数 let pixelsBuffer = new Buffer(bytesPerPixel * width * height); // 存储过滤后的像素数据 let offset = 0; // 当前行的偏移位置 // 逐行扫描解析 for(let i=0, len=data.length; i&lt;len; i+=bytesPerRow+1) { let scanline = Array.prototype.slice.call(data, i+1, i+1+bytesPerRow); // 当前行 let args = [scanline, bytesPerPixel, bytesPerRow, offset]; // 第一个字节代表过滤类型 switch(readInt8(data, i)) { case 0: filterNone(args); break; case 1: filterSub(args); break; case 2: filterUp(args); break; case 3: filterAverage(args); break; case 4: filterPaeth(args); break; default: throw new Error('未知过滤类型！'); } offset += bytesPerRow; } 上面代码前半部分不难理解，就是通过之前解析得到的图像宽高，再加上图像深度和通道数计算得出每个像素占用的字节数和每一行数据占用的字节数。因此我们就可以拆分出每一行的数据和每一个像素的数据。 在得到每一行数据后，就要进行这个 png 编码里最关键的 1 步——过滤。 过滤 早先我们说过过滤方法只有 1 种，其中包含 5 种过滤类型，图像每一行数据里的第一个字节就表示当前行数什么过滤类型。 png 为什么要对图像数据进行过滤呢？ 大多数情况下，图像的相邻像素点的色值时很相近的，而且很容易呈现线性变化（相邻数据的值是相似或有某种规律变化的），因此借由这个特性对图像的数据进行一定程度的压缩。针对这种情况我们常常使用一种叫差分编码的编码方式，即是记录当前数据和某个标准值的差距来存储当前数据。 比如说有这么一个数组 [99, 100, 100, 102, 103]，我们可以将其转存为 [99, 1, 0, 2, 1]。转存的规则就是以数组第 1 位为标准值，标准值存储原始数据，后续均存储以前 1 位数据的差值。 当我们使用了差分编码后，再进行 deflate 压缩的话，效果会更好（deflate 压缩是 LZ77 延伸出来的一种算法，压缩频繁重复出现的数据段的效果是相当不错的，有兴趣的同学可自行去了解）。 好，回到正题来讲 png 的 5 种过滤类型，首先我们要定义几个变量以便于说明： CBAX\\begin{matrix} C &amp; B \\\\ A &amp; X \\end{matrix} CA​BX​ XXX：当前像素。 AAA：当前像素点左边的像素。 BBB：当前像素点上边的像素。 CCC：当前像素点左上边的像素。 过滤类型 0：None 这个没啥好解释的，就是完全不做任何过滤。 function filterNone(scanline, bytesPerPixel, bytesPerRow, offset) { for(let i=0; i&lt;bytesPerRow; i++) { pixelsBuffer[offset + i] = scanline[i]; } } 过滤类型 1：Sub 记录 X−AX - AX−A 的值，即当前像素和左边像素的差值。左边起第一个像素是标准值，不做任何过滤。 function filterSub(scanline, bytesPerPixel, bytesPerRow, offset) { for(let i=0; i&lt;bytesPerRow; i++) { if(i &lt; bytesPerPixel) { // 第一个像素，不作解析 pixelsBuffer[offset + i] = scanline[i]; } else { // 其他像素 let a = pixelsBuffer[offset + i - bytesPerPixel]; let value = scanline[i] + a; pixelsBuffer[offset + i] = value &amp; 0xFF; } } } 过滤类型 2：Up 记录 X−BX - BX−B 的值，即当前像素和上边像素点差值。如果当前行是第 1 行，则当前行数标准值，不做任何过滤。 function filterUp(scanline, bytesPerPixel, bytesPerRow, offset) { if(offset &lt; bytesPerRow) { // 第一行，不作解析 for(let i=0; i&lt;bytesPerRow; i++) { pixelsBuffer[offset + i] = scanline[i]; } } else { for(let i=0; i&lt;bytesPerRow; i++) { let b = pixelsBuffer[offset + i - bytesPerRow]; let value = scanline[i] + b; pixelsBuffer[offset + i] = value &amp; 0xFF; } } } 过滤类型 3：Average 记录 X−(A+B)/2X - (A + B) / 2X−(A+B)/2 的值，即当前像素与左边像素和上边像素的平均值的差值。 如果当前行数第一行：做特殊的 Sub 过滤，左边起第一个像素是标准值，不做任何过滤。其他像素记录该像素与左边像素的二分之一的值的差值。 如果当前行数不是第一行：左边起第一个像素记录该像素与上边像素的二分之一的值的差值，其他像素做正常的 Average 过滤。 function filterAverage(scanline, bytesPerPixel, bytesPerRow, offset) { if(offset &lt; bytesPerRow) { // 第一行，只做Sub for(let i=0; i&lt;bytesPerRow; i++) { if(i &lt; bytesPerPixel) { // 第一个像素，不作解析 pixelsBuffer[offset + i] = scanline[i]; } else { // 其他像素 let a = pixelsBuffer[offset + i - bytesPerPixel]; let value = scanline[i] + (a &gt;&gt; 1); // 需要除以2 pixelsBuffer[offset + i] = value &amp; 0xFF; } } } else { for(let i=0; i&lt;bytesPerRow; i++) { if(i &lt; bytesPerPixel) { // 第一个像素，只做Up let b = pixelsBuffer[offset + i - bytesPerRow]; let value = scanline[i] + (b &gt;&gt; 1); // 需要除以2 pixelsBuffer[offset + i] = value &amp; 0xFF; } else { // 其他像素 let a = pixelsBuffer[offset + i - bytesPerPixel]; let b = pixelsBuffer[offset + i - bytesPerRow]; let value = scanline[i] + ((a + b) &gt;&gt; 1); pixelsBuffer[offset + i] = value &amp; 0xFF; } } } } 过滤类型 4：Paeth 记录 X−PrX - PrX−Pr 的值，这种过滤方式比较复杂，PrPrPr 的计算方式（伪代码）如下： p = a + b - c pa = abs(p - a) pb = abs(p - b) pc = abs(p - c) if pa &lt;= pb and pa &lt;= pc then Pr = a else if pb &lt;= pc then Pr = b else Pr = c return Pr 如果当前行数第一行：做 Sub 过滤。 如果当前行数不是第一行：左边起第一个像素记录该像素与上边像素的差值，其他像素做正常的 Peath 过滤。 function filterPaeth(scanline, bytesPerPixel, bytesPerRow, offset) { if(offset &lt; bytesPerRow) { // 第一行，只做Sub for(let i=0; i&lt;bytesPerRow; i++) { if(i &lt; bytesPerPixel) { // 第一个像素，不作解析 pixelsBuffer[offset + i] = scanline[i]; } else { // 其他像素 let a = pixelsBuffer[offset + i - bytesPerPixel]; let value = scanline[i] + a; pixelsBuffer[offset + i] = value &amp; 0xFF; } } } else { for(let i=0; i&lt;bytesPerRow; i++) { if(i &lt; bytesPerPixel) { // 第一个像素，只做Up let b = pixelsBuffer[offset + i - bytesPerRow]; let value = scanline[i] + b; pixelsBuffer[offset + i] = value &amp; 0xFF; } else { // 其他像素 let a = pixelsBuffer[offset + i - bytesPerPixel]; let b = pixelsBuffer[offset + i - bytesPerRow]; let c = pixelsBuffer[offset + i - bytesPerRow - bytesPerPixel]; let p = a + b - c; let pa = Math.abs(p - a); let pb = Math.abs(p - b); let pc = Math.abs(p - c); let pr; if (pa &lt;= pb &amp;&amp; pa &lt;= pc) pr = a; else if (pb &lt;= pc) pr = b; else pr = c; let value = scanline[i] + pr; pixelsBuffer[offset + i] = value &amp; 0xFF; } } } } 获取像素 到这里，解析的工作就做完了，上面代码里的 pixelsBuffer 数组里存的就是像素的数据了，不过我们要如何获取具体某个像素的数据呢？方式可参考下面代码： let palette; // PLTE数据块内容，即调色板内容 let colorType; // 解析IHDR数据块时得到的颜色类型 let transparentPanel; // 透明像素面板，解析tRNS数据块获得 function getPixel(x, y) { if(x &lt; 0 || x &gt;= width || y &lt; 0 || y &gt;= height) { throw new Error('x或y的值超出了图像边界！'); } let bytesPerPixel = Math.max(1, colors * bitDepth / 8); // 每像素字节数 let index = bytesPerPixel * ( y * width + x); switch(colorType) { case 0: // 灰度图像 return [pixelsBuffer[index], pixelsBuffer[index], pixelsBuffer[index], 255]; case 2: // rgb真彩色图像 return [pixelsBuffer[index], pixelsBuffer[index + 1], pixelsBuffer[index + 2], 255]; case 3: // 索引颜色图像 let paletteIndex = pixelsBuffer[index]; let transparent = transparentPanel[paletteIndex] if(transparent === undefined) transparent = 255; return [palette[paletteIndex * 3 + 0], palette[paletteIndex * 3 + 1], palette[paletteIndex * 3 + 2], transparent]; case 4: // 灰度图像 + alpha通道 return [pixelsBuffer[index], pixelsBuffer[index], pixelsBuffer[index], pixelsBuffer[index + 1]]; case 6: // rgb真彩色图像 + alpha通道 return [pixelsBuffer[index], pixelsBuffer[index + 1], pixelsBuffer[index + 2], pixelsBuffer[index + 3]]; } } 此处用到了非关键数据块 tRNS 的数据，不过这里不做讲解，有兴趣的同学可去官网了解：https://www.w3.org/TR/PNG/#11tRNS（此数据块的结构相当简单） 尾声 png 的解析流程可以由这一张图简单概括： 此文只对 png 图片的格式做了简单的介绍，我们也知道如何对一张 png 图片做简单的解析。上面出现的代码只是 js 代码片段，如果对完整代码有兴趣的同学可以戳这里，虽然代码仓库还在建设过程中，不过关于简单的 png 图片解析部分已经完成。 参考资料： https://www.w3.org/TR/PNG/ http://www.libpng.org/pub/png/ https://en.wikipedia.org/wiki/Portable_Network_Graphics ","link":"https://faded.auspicious.space/post/the-story-of-png-get-images-and-pixel-content/"},{"title":"贝尔实验室的历史","content":" 贝尔实验室的历史 发明家贝尔 贝尔的全名是亚历山大·格拉厄姆·贝尔（Alexander Graham Bell） 亚历山大·贝尔（外国人常常将中间的名字略去）在 1847 年出生在苏格兰的一个声学世家， 23岁（1870年）：移民加拿大； 24岁（1871年）：又来到美国； 29岁（1876年）：试验成功了第一台可用的电话，并在同年获得电话专利； 30岁（1877年）：贝尔便创立了贝尔电话公司，公司以出租电话机收取使用费的方式盈利； 31岁（1878年）：贝尔退出了贝尔电话公司，但他所拥有的电话专利可以让他不断获得可观的专利费； 35岁（1882年）：正式加入美国国籍； 76岁（1922年）：贝尔离开了人世。 AT&amp;T AT&amp;T，即 American Telephone &amp; Telegraph Company，中文则是“美国电话电报公司”，它是当今世界五百强之一。 AT&amp;T的前身便是刚刚提到的于 1877 年创立的“贝尔电话公司”（简称贝尔公司），在经营了 18 年之后，也就是 1895 年，贝尔电话公司决定将“全美范围内的长途业务”分割出来，成立一家独立的公司，起名叫做“AT&amp;T”。AT&amp;T 发展迅猛，在 1899 年，AT&amp;T 便把其前身的贝尔电话公司整合进来，于是 AT&amp;T 便成了贝尔公司的母公司。 贝尔实验室 在 1925 年，AT&amp;T 收购了西方电子公司的研究部门，并成立了一个叫做“贝尔电话实验室公司”（简称便是贝尔实验室）的独立实体，在建立之初，贝尔实验室便致力于数学、物理学、材料科学、计算机编程、电信技术等各方面的研究。 不幸的是，在 1984 年，美国司法部依据《反托拉斯法》对如日中天的 AT&amp;T 进行拆分，形成了新的 AT&amp;T 公司及七个本地电话公司，贝尔实验室也因此缩减形成了贝尔实验室核心团队，主要负责为各个拆分后的公司提供研究开发的服务。 朗讯 在 1995 年到 1996 年间，AT&amp;T 公司又被进行了一轮拆分，贝尔实验室和设备制造部门脱离出来形成了一个新的公司，叫做朗讯科技，而 AT&amp;T 则只保留了通信服务业务，也只保留很小一批研究人员组建了AT&amp;T 实验室。 自从 1996 年从 AT&amp;T 独立出来后，朗讯（Lucent）公司以贝尔实验室作为强力后盾，一致致力于为全球最大的通信服务提供商设计和提供网络。 朗讯公司的总部位于美国新泽西州的茉莉山。 阿尔卡特 阿尔卡特（Alcatel）公司，创立于 1898 年，总部位于法国巴黎，一直专注于电信系统和设备以及相关的电缆和部件领域的研究和生产。 阿朗 在 2006 年，通信行业发生了一件大事，那就是法国阿尔卡特公司和美国朗讯公司发表联合声明，宣布了阿尔卡特公司收购朗讯公司的消息。在合并后的新公司中，阿尔卡特占据 60% 的股份，朗讯占有 40% 的股份。合并后的规模仅次于美国思科。 合并后的公司叫做阿尔卡特-朗讯（Alcatel-Lucent，简称“阿朗”），总部设在法国巴黎。 与此同时，原属朗讯科技的贝尔实验室也一并合并到阿朗。 贝尔实验室的历史 通过上面的背景知识介绍，相信你一定已经了解了贝尔实验室的发展历程，我们再来用简短的文字总结一下： 贝尔发明电话→贝尔建立贝尔电话公司→贝尔电话公司分离出 AT&amp;T 公司专门负责全美长途业务→AT&amp;T 整合原贝尔电话公司→AT&amp;T 收购西方电子研究部门并建立贝尔电话实验室（即贝尔实验室）→AT&amp;T 因垄断被拆分→AT&amp;T再次被拆分，贝尔实验室和设备制造部门被独立出来成立朗讯科技公司→阿尔卡特收购朗讯组成阿朗，贝尔实验室也一起合并→贝尔实验室现在服务于阿朗公司\\begin{aligned} \\text{贝尔发明电话} &amp;\\to \\text{贝尔建立贝尔电话公司} \\\\ &amp;\\to \\text{贝尔电话公司分离出 AT\\&amp;T 公司专门负责全美长途业务}\\\\ &amp;\\to \\text{AT\\&amp;T 整合原贝尔电话公司}\\\\ &amp;\\to \\text{AT\\&amp;T 收购西方电子研究部门并建立贝尔电话实验室（即贝尔实验室）}\\\\ &amp;\\to \\text{AT\\&amp;T 因垄断被拆分}\\\\ &amp;\\to \\text{AT\\&amp;T再次被拆分，贝尔实验室和设备制造部门被独立出来成立朗讯科技公司}\\\\ &amp;\\to \\text{阿尔卡特收购朗讯组成阿朗，贝尔实验室也一起合并}\\\\ &amp;\\to \\text{贝尔实验室现在服务于阿朗公司} \\end{aligned} 贝尔发明电话​→贝尔建立贝尔电话公司→贝尔电话公司分离出 AT&amp;T 公司专门负责全美长途业务→AT&amp;T 整合原贝尔电话公司→AT&amp;T 收购西方电子研究部门并建立贝尔电话实验室（即贝尔实验室）→AT&amp;T 因垄断被拆分→AT&amp;T再次被拆分，贝尔实验室和设备制造部门被独立出来成立朗讯科技公司→阿尔卡特收购朗讯组成阿朗，贝尔实验室也一起合并→贝尔实验室现在服务于阿朗公司​ 所以，当你进入到贝尔实验室的官方首页时，你会发现 LOGO 也是 Alcatel-Lucent 的。 ","link":"https://faded.auspicious.space/post/the-history-of-the-bell-labs/"},{"title":"浏览器 user-agent 详解","content":" 浏览器user-agent详解 特性检测并非浏览器检测 浏览器们的家族史 较古的浏览器 1993年，NCSA 发布了首款 Web 浏览器 Mosaic。它的 user-agent 字串非常简洁： Mosaic/0.9 虽然当时由于它对操作系统和平台的依赖性，但是基本格式还是很简单明了。在文本中，斜杠前面是产品名称(可能会显示为 NCSA Mosaic 或是其他类似的字)，斜杠后面是产品版本号。 Netscape Communications 开发了 Web 浏览器 Mozilla（当时号称“Mosaic 杀手”）。他们首款公开发行版本： Netscape Navigator 2 的 user-agent 字串具有如下格式： Mozilla/Version [Language] (Platform; Encryption) Netscape 按之前的做法在 user-agent 字串的前半部分使用了产品名称和产品版本，但在后面增加了下列信息： Language - 表示应用程序用的是哪个语言； Platform - 表示应用程序是在什么操作系统和/或平台中运行； Encryption - 表示应用程序包含了什么安全加密类型。其中的值可能是U（128 位加密）、I（40 位加密）、N（没加密）。 Netscape Navigator 2 的 user-agent 字串的示例： Mozilla/2.02 [fr] (WinNT; I) 上面的字串指： Netscape Navigator 2.02 、法语 、Windows NT 、40 位加密。在当时，通过 user-agent 字串中的产品名称，可以正确判断使用的是哪个 Web 浏览器。 Netscape Navigator 3 、Internet Explorer 3 1996 年，Netscape Navigator 3 发布，它远远超过 Mosaic 成为当时最流行的 Web 浏览器。而 user-agent 字串只有些小的变化：去掉了语言部分，多了个放操作系统或 CPU 的可选信息。格式如下： Mozilla/Version (Platform; Encryption [; OS-or-CPU description]) 在 Windows 系统中 Netscape Navigator 3 的 user-agent 字串的示例： Mozilla/3.0 (Win95; U) 上面的字串指：Netscape Navigator 3 、Windows 95 、128 位加密。在 Windows 系统中，字串里面不会显示 OS 或 CPU 的信息。 Netscape Navigator 3 发布不久，微软公布了它的首款 Web 浏览器： IE 3 ¹，但是 Netscape 是当时首选浏览器，大多数服务器在加载页面前都会检查 user-agent 是否为该款浏览器。IE 如果不兼容 Netscape user-agent 字串，使用 IE 的用户就根本打不开这些页面，于是造就了如下格式： Mozilla/2.0 (compatible; MSIE Version; Operating System) 在 Windows 95 中 IE 3.02 的 user-agent 字串的示例： Mozilla/2.0 (compatible; MSIE 3.02; Windows 95) 由于当时的浏览器嗅探只查 user-agent 字串中的产品名称部分，结果 IE 摇身一变被识别成了 Mozilla，伪装成 Netscape Navigator。这个做法引发了对浏览器识别的争论。从此以后，浏览器真正的版本埋没在了字串的中间。 Netscape Communicator 4 、Internet Explorer 4 至 8 1997 年8月，Netscape Communicator 4 发布（发布的名称中 Navigator 换成了 Communicator），它的 user-agent 字串格式与 3 版本一致。Windows 98 中 4 版本的 user-agent 字串如下: Mozilla/4.0 (Win98; I) Netscape 浏览器在更新时，版本也相应增加。4.79 版本的 user-agent 字串如下： Mozilla/4.79 (Win98; I) 微软发布 IE 4 时，user-agent 字串更新了版本，格式如下： Mozilla/4.0 (compatible; MSIE Version; Operating System) 在 Windows 98 中 IE 4 的 user-agent 字串的示例： Mozilla/4.0 (compatible; MSIE 4.0; Windows 98) 可以看出，Mozilla 的版本与 IE 实际的版本一致，这样就可以识别第 4 代浏览器了。但遗憾的是，不久 IE 4.5 马上就发布了(只在 Mac 平台)，虽然 Mozilla 版本仍是 4，但是 IE 的版本改成如下： Mozilla/4.0 (compatible; MSIE 4.5; Mac_PowerPC) 此后，IE 的版本一直到 7 都沿用了这个模式。 而 IE 8 的 user-agent 字串添加了呈现引擎（rendering engine）版本： Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0) 新增的呈现引擎非常重要！这样 IE8 以 MSIE 7.0 兼容模式运行时，Trident 版本保持不变，而原先 IE7 的 user-agent 字串不包括 Trident 版本。这样可以区分 IE7 与 IE8 运行的兼容模式。 注意：别指望能从 Mozilla 版本中得到什么靠谱的信息。 Gecko Gecko 是 Firefox 的呈现引擎。Gecko 首次开发是作为 Mozilla 浏览器 Netscape 6 的一部分。Netscape 6 的 user-agent 字串的结构是面向未来的，新版本反应出从 4.x 版本的简单变得较为复杂，它的格式如下： Mozilla/MozillaVersion (Platform; Encryption; OS-or-CPU; Language; PrereleaseVersion)Gecko/GeckoVersion ApplicationProduct/ApplicationProductVersion 为了更好的理解上面的 Gecko user-agent 字串格式，下面来看看各种从基于 Gecko 浏览器中取得的字串。 在 Windows XP 中的 Netscape 6.21： Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:0.9.4) Gecko/20011128 Netscape6/6.2.1 在 Linux 中的 SeaMonkey 1.1a: Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.1b2) Gecko/20060823 SeaMonkey/1.1a 在 Windows XP 中的 Firefox 2.0.0.11 : Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11 Mac OS X 中的 Camino 1.5.1: Mozilla/5.0 (Macintosh; U; Intel Mac OS X; en; rv:1.8.1.6) Gecko/20070809 Camino/1.5.1 上面都是基于 Gecko 的浏览器所取得的 user-agent 字串，区别只是版本有所不同。Mozilla 版本 5.0 是自从首款基于 Gecko 发布后就一直不变，而且以后有可能也不会变²。 WebKit 2003 年，Apple 宣布发布首款他们自主开发的 Web 浏览器：Safari。它的呈现引擎叫 WebKit。它是 Linux 中的 Web 浏览器 Konqueror 呈现引擎 KHTML 的一个分支，几年后，WebKit 的开源吸引了呈现引擎的开发人员。 这款新浏览器和呈现引擎的开发人员也遇到了曾经 IE 3.0 类似的问题：怎样才能溶入主流而不被踢出局？答案是：在 user-agent 字串中放详尽的信息，以便骗取网站的信任使它与其它流行的浏览器兼容。user-agent 字串格式如下： Mozilla/5.0 (Platform; Encryption; OS-or-CPU; Language) AppleWebKit/AppleWebKitVersion (KHTML, like Gecko) Safari/SafariVersion 下面是示例： Mozilla/5.0 (Macintosh; U; PPC Mac OS X; en) AppleWebKit/124 (KHTML, like Gecko) Safari/125.1 这又是个挺长的 user-agent 字串，其中包括的信息既有 Apple WebKit 的版本，也有 Safari 的版本。凡是基于 WebKit 的浏览器都将自己伪装成了 Mozilla 5.0，与基于 Gecko 浏览器完全一样。但 Safari 的版本是浏览器的构建版本号（build number）。Safari 1.25 在 user-agent 字串中号为 125.1（如上所示）。Safari 版本 3 的 user-agent 字串包括了实际的 Safari 版本： Mozilla/5.0 (Macintosh; U; PPC Mac OS X; en) AppleWebKit/522.15.5 (KHTML, like Gecko) Version/3.0.3 Safari/522.15.5 其中的“（KHTML, like Gecko）”在 Safari 1.0 预览版本中就有了，这字串部分是最耐人寻味又饱受诟病。Apple 的野心是为了让开发人员把 Safari 当成 Gecko，所以采取了当初微软 IE user-agent 的类似做法：Safari 是兼容 Mozilla 的，否则 Safari 用户会认为用的浏览器不受支持。 而其它基于 WebKit 的浏览器与 Safari 不同的是，没有上面说的这个情况，所以检测断定浏览器是否基于 WebKit 比看有没有明确标 Safari 更有用。 Konqueror Konqueror 是款在 KDE Linux 桌面环境中的浏览器，基于 KHTML 开源呈现引擎。它只发布了在 Linux 的版本，但是拥有活跃的用户群。为了兼容性最大化，user-agent 字串的格式也紧跟 IE 的后尘： Mozilla/5.0 (compatible; Konqueror/Version; OS-or-CPU) Konqueror 3.2 为了与 WebKit user-agent 字串变化保持一致，它将 KHTML 作为它的标识： Mozilla/5.0 (compatible; Konqueror/Version; OS-or-CPU) KHTML/KHTMLVersion (like Gecko) 如下所示： Mozilla/5.0 (compatible; Konqueror/3.5; SunOS) KHTML/3.5.0 (like Gecko) Konqueror 和 KHTML 的版本号比较一致，唯一的区别就是下点处不同，比如Konquerer 3.5、KHTML 3.5.1。 Chrome Google Chrome 浏览器以 WebKit 作为呈现引擎，JavaScript 引擎却用了另一种。最初发布的版本是 0.2，它的 user-agent 字串格式是在 WebKit 信息的基础上又增加了如下： Mozilla/5.0 (Platform; Encryption; OS-or-CPU; Language) AppleWebKit/AppleWebKitVersion (KHTML, like Gecko) Chrome/ChromeVersion Safari/SafariVersion Chrome 0.2 user-agent 信息的示例如下： Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US) AppleWebKit/525.13 (KHTML, like Gecko) Chrome/0.2.149.29 Safari/525.13 虽我不敢完全保证，但很可能 WebKit 版本和 Safari 版本总会保持同步。 Opera Opera 浏览器默认 user-agent 字串是现代浏览器中最合理的——正确的标识了它自己及其版本。 在 Opera 8.0 前，它的 user-agent 字串格式如下： Opera/Version (OS-or-CPU; Encryption) [Language] 在 Windows XP 中 Opera 7.54 user-agent 字串示例： Opera/7.54 (Windows NT 5.1; U) [en] Opera 8 user-agent 字串的语言部分移到了括号内。 Opera/Version (OS-or-CPU; Encryption; Language) 在 Windows XP 中 Opera 8 user-agent 字串示例： Opera/8.0 (Windows NT 5.1; U; en) 当时 Opera 做为主流浏览器之一，它的 user-agent 字串是唯一使用产品名称和版本完全真实的标识了它自己。但是由于大量的浏览器嗅探代码在 Internet 上像蝗虫飞过般只吃标 Mozilla 产品名的 user-agent 字串，造成了 Opera 的 user-agent 字串发生了完全的改变。 Opera 9 user-agent 字串有两种修改的方式：一种方式是将自己标识为 Firefox 或 IE 浏览器。在这种方式下，user-agent 字串与 Firefox 或 IE 的几乎一样，只不过末尾附加了“Opera”及版本号。如下所示： Mozilla/5.0 (Windows NT 5.1; U; en; rv:1.8.1) Gecko/20061208 Firefox/2.0.0 Opera 9.50 Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; en) Opera 9.50 前 一字串将 Opera 9.5 标识为 Firefox 2。后一字串将 Opera 9.5 标识为 IE 6，在两个字串中都带有 Opera 版本信息。虽然这种方式是作为 Firefox 或 IE 打开的，但也能识别出 Opera。另一种方法则是浏览器 user-agent 字串标识伪装成 Firefox 或 IE，同时也找不到“Opera”字串及其版本信息。这样从字面上去区分 Opera 浏览器便成了“不可能完成的任务”。³ 结论 user-agent 字串史可以说明曾对 user-agent 嗅探说不的原因：IE 想要将自己识别为 Netscape 4，Konqueror 和 WebKit 想要识别为 Firefox，Chrome 想要识别为 Safari。这样使得除 Opera 外所有浏览器的 user-agent 嗅探区别很小，想要从一堆茫茫浏览器海洋中找出有用的标识太少了。关于嗅探要记住：一款浏览器与其它浏览器是兼容的，这样造成了不能完全准确的断定是哪款 浏览器。 比如说 Chrome ，它声称任何可以在 Safari 3 访问的网站 Chrome 也都可以访问，但是对检测 Chrome 没有一点用。为了浏览器的兼容——这便是这个声明的理由。 起初前端工程师们就极力反对浏览器检测，他们认为类似 user-agent 嗅探的方法是很不好的，理由是它并不是一种面向未来的代码，无法适应新版的浏览器。更好的做法是使用特性检测，就像这样： if (navigator.userAgent.indexOf(&quot;MSIE 7&quot;) &gt; -1) { //do something } 而更好的做法是这样： if (document.all) { //do something } 这两种方式并不相同。前者是检测浏览器的特殊名称和版本；后者却是检测浏览器的特性。UA 嗅探能够精确得到浏览器的类型和版本（至少能得知浏览器类型），而特性检测却是去确定浏览器是否拥有某个对象或者支持某个方法。注意这两者是完全不同的。 因为特性检测依赖于哪些浏览器支持，当出现新版本浏览器的时候需要繁琐的确认工作。例如 DOM 标准刚出现的时候，并不是所有浏览器都支持 getElementById() 方法，所以一开始代码可能是这样： if (document.getElementById) { //DOM element = document.getElementById(id); } else if (document.all) { //IE element = document.all[id]; } else if (document.layers) { //Netscape &lt; 6 element = document.layers[id]; } 这是特性检测很好的一个例子，亮点在于当其它浏览器开始支持 getElementById() 方法时不必修改代码。 混合方式 后来前端工程师们考虑改进的写法，代码变化成这样： //AVOID!!! if (document.all) { //IE id = document.uniqueID; } else { id = Math.random(); } 这个代码的问题是通过检测 document.all 属性来确定是否是 IE。当确定是 IE 后，假定使用私有的 document.uniqueID 属性也是安全的。然而，目前所作的只是确定是否支持 document.all，并非是去辨识浏览器是否为 IE。仅仅支持 document.all 的话也不意味着 document.uniqueID 是可用的。 后来人们开始这样写，用下面那行代替上面的： var isIE = navigator.userAgent.indexOf(&quot;MSIE&quot;) &gt; -1; //下面这行代替上面那行 var isIE = !!document.all; 这些变化说明大家对“不要使用UA嗅探”存在误解——不再对浏览器的详细信息进行检测，取而代之的是通过特性的支持来推断。这种基于浏览器特性检测的方式非常不好。 后来前端们发现 document.all 并不可靠，更好的检测 IE 变为： var isIE = !!document.all &amp;&amp; document.uniqueID; 这种实现方式陷入歧途。不仅需要费时费事地去识别浏览器所增加的特性支持，另外也不能确定其它浏览器开始支持相同的特性。 如果你认为这样的代码并未被广泛使用，那么看看来自于老版本的Mootools代码片段吧： //from MooTools 1.1.2 if (window.ActiveXObject) window.ie = window[window.XMLHttpRequest ? 'ie7' : 'ie6'] = true; else if (document.childNodes &amp;&amp; !document.all &amp;&amp; !navigator.taintEnabled) window.webkit = window[window.xpath ? 'webkit420' : 'webkit419'] = true; else if (document.getBoxObjectFor != null || window.mozInnerScreenX != null) window.gecko = true; 注意它是如何使用特性检测的。我可以指出它一系列的问题，比如通过检测 window.ie 会将 IE 8 误认为 IE 7。 余波 随着浏览器的快速发展，使用特性检测变得越来越困难和不可靠。但是 Mootools 1.2.4 仍然使用这一方法，例如：getBoxObjectFor()。 //from MooTools 1.2.4 var Browser = $merge({ Engine: { name: 'unknown', version: 0 }, Platform: { name: (window.orientation != undefined) ? 'ipod' : (navigator.platform.match(/mac|win|linux/i) || ['other'])[0].toLowerCase() }, Features: { xpath: !!(document.evaluate), air: !!(window.runtime), query: !!(document.querySelector) }, Plugins: {}, Engines: { presto: function () { return (!window.opera) ? false : ((arguments.callee.caller) ? 960 : ((document.getElementsByClassName) ? 950 : 925)); }, trident: function () { return (!window.ActiveXObject) ? false : ((window.XMLHttpRequest) ? ((document.querySelectorAll) ? 6 : 5) : 4); }, webkit: function () { return (navigator.taintEnabled) ? false : ((Browser.Features.xpath) ? ((Browser.Features.query) ? 525 : 420) : 419); }, gecko: function () { return (!document.getBoxObjectFor &amp;&amp; window.mozInnerScreenX == null) ? false : ((document.getElementsByClassName) ? 19 : 18); } } }, Browser || {}); 应该怎么做？ 特性检测是个应该避免的方法，尽管直接进行特性检测是个很好的方法，并且大部分情况下能满足需求。一般只要在检测前知道这个特性是否被实现即可，而不会去考虑它们之间的关系。 我并非是说永远不使用浏览器特性检测而是基于 UA 嗅探，因为我相信它还是有很多用途的，然而我不相信它有很多合理的用途。如果你考虑 UA 嗅探的话， 请先贯彻这一思想：唯一安全的方式是针对特定浏览器的特定版本，超出范围之外都是不可靠的——例如新出的浏览器版本。其实这样做也是个明智的办法，因为相 较于向前兼容不确定的新版本而言，向后兼容老版本是最简单的做法。 ","link":"https://faded.auspicious.space/post/an-introduction-to-browser-user-agent/"},{"title":"哪些情况下必须使用 301 重定向","content":" 哪些情况下必须使用301重定向 看了一眼百度百科，301 重定向又叫做页面永久性移走，是站长必备的自动转向技术之一。你已经 get 了吗，你知道哪些情况下必须使用 301 重定向吗？ 如果你想为网站更换域名，请千万要记得利用 301 重定向将原本的域名重定向至现在的域名。 如果你想删除网站中的不合理或无意义的目录，请千万要记得利用 301 重定向到网站首页。 如果你想把其他的一些闲置域名共同指向某一个在用的网站，利用 301 重定向就能够轻松实现。 如果你想实现网站 URL 的规范化，比如 www.kangbinle.cn 和不带 www 的域名的规范化，利用 301 重定向就可以完成。 好了，以上四种情况就是必须使用 301 重定向的时候，各位站长可一定要记得呀，不要等网站的流量出现了问题再去后悔。 ","link":"https://faded.auspicious.space/post/when-should-use-301-redirect/"},{"title":"JVM—GC 垃圾回收器总结","content":" JVM—GC垃圾回收器总结 收集算法（标记-清理、复制、标记-整理、分代收集）是内存回收的方法论，垃圾收集器就是内存回收的具体实现。 主要有 7 个 GC 器，如下图。 1 Serial 收集器 1.1 介绍 Serial 收集器是单线程的收集器。 单线程：不仅仅说明它只会使用一个 CPU 或一条收集线程去完成垃圾收集工作，且在垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束。 Stop the world：是 VM 在后台自动发起和自动完成的，在用户不可见情况下把用户正常工作的线程全部停掉。 1.2 缺点 由于 Stop The World，给用户带来不良体验，比如，计算机每运行一段时间就会暂停响应几分钟来处理垃圾收集。 1.3 优点 简单而高效（与其他收集器的单线程比）； 对于限定单个 CPU 的环境来说，Serial 收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集效率。 1.4 应用场景 VM 运行在 Client 模式下的默认新生代收集器； 在用户的桌面应用场景中，停顿时间完全可以控制在几十毫秒最多一百多毫秒以内，不频繁发生，是可接受的 1.5 Serial / Serial Old 收集器运行示意图 2 ParNew 收集器 2.1 介绍 ParNew 收集器是 Serial 收集器多线程版本（是 GC 线程的多线程，并行）。 并行：Parallel 指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态（多个处理器同时处理多条指令）； 并发：Concurrent 指用户线程与垃圾收集线程同时执行（但并不一定是并行的，可能交替执行），用户程序在继续运行，而垃圾收集程序运行于另一个 CPU 上（同一时刻只能有一条指令执行，多个进程指令是交替执行）。 2.2 缺点 在单 CPU 的环境中绝对不会有比 Serial 收集器更好的效果，甚至存在线程交互的开销。 2.3 优点 除了 Serial 收集器外，只有 ParNew 收集器能与 CMS 收集器配合工作。 CMS（Concurrent Mark Sweep）第一次实现让垃圾收集线程与用户线程（基本上）同时工作。 2.4 应用场景 运行在 Server 模式下的 VM 首选新生代收集器。 2.5 ParNew / Serial Old 收集器运行示意图 2.6 参数控制 使用 -XX:+UseConcMarkSweepGC 选项后默认新生代收集器为 ParNew 收集器； 使用 -XX:+UseParNewGC 选项强制指定使用 ParNew 收集器； 使用 -XX:ParallelGCThreads 参数限制垃圾收集的线程数。 3 Parallel Scavenge 收集器 3.1 介绍 Parallel Scavenge 收集器是一个新生代收集器，使用复制算法的收集器，并行的多线程收集器。更关注吞吐量。 吞吐量：CPU 用于运行用户代码的时间与 CPU 总消耗时间的比值，即： 如虚拟机总共运行 100 分钟，垃圾收集花费 1 分钟，则吞吐量是 99%；吞吐量高效率利用 CPU 时间，尽快完成程序的运算任务，主要适合后台运算而不需要太多交互的任务。 吞吐量=运行用户代码时间(运行用户代码时间+垃圾收集时间)吞吐量=\\frac{运行用户代码时间}{(运行用户代码时间+垃圾收集时间)} 吞吐量=(运行用户代码时间+垃圾收集时间)运行用户代码时间​ 停顿时间：如 CMS 等收集器关注点尽可能缩短垃圾收集时用户线程的停顿时间，停顿时间越短就越适合需要与用户交互的程序，良好的响应速度可以提升用户体验（适合交互）。 3.2 参数控制 用户精确控制吞吐量 使用 -XX:MaxGCPauseMillis 参数：控制最大垃圾收集停顿时间； 使用 -XX:GCTimeRatio 参数：直接设置吞吐量大小； 使用 -XX:+UseAdaptiveSizePolicy 开关参数：GC 自适应的调节策略； MaxGCPauseMillis 参数允许的值是一个大于 0 的毫秒数，收集器尽可能保证内存回收时间不超过设定值。 GC 停顿时间缩短牺牲吞吐量和新生代空间——若将 MaxGCPauseMillis 该值调小带来的问题：系统把新生代调小一些，收集发生更频繁一些，吞吐量下降。 GCTimeRatio 参数值是一个大于 0 且小于 100 的整数，即垃圾收集时间占总时间的比率，相当于吞吐量的倒数。如设置为 19，则最大 GC 时间占 1/(1+19)=5%，默认值为 99。则最大允许 1/(1+99)=1% 的垃圾收集时间。 UseAdaptiveSizePolicy 开关参数：VM 会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或最大吞吐量。自适应调节策略是 Parallel Scavenge 收集器与 ParNew 收集器的重要区别。 3.3 应用场景 主要适合后台运算而不需要太多交互的任务。 4 Serial Old 收集器 4.1 介绍 Serial Old 收集器是 Serial 收集器的老年代版本，是一个单线程收集器，使用“标记-整理”算法。 4.2 应用场景 主要给 Client 模式下的 VM 使用； 若在 Server 模式下用，两大用途： 在 JDK1.5 及之前的版本中与 Parallel Scavenge 收集器搭配使用； 作为 CMS 收集器备选，并在 Concurrent Mode Failure 时使用。 4.3 Serial / Serial Old 收集器运行示意图 5 Parallel Old 收集器 5.1 介绍 Parallel Old 是 Parallel Scavenge 收集器的老年代版本，是一个多线程收集器，使用“标记-整理”算法。在 JDK1.6 开始提供。 5.2 应用场景 注重吞吐量以及 CPU 资源敏感的场合，优先考虑 Parallel Scavenge + Parallel Old 收集器。适合吞吐量优先。 5.3 Parallel Scavenge / Parallel Old 收集器运行示意图 6 CMS 收集器 6.1 介绍 CMS 收集器（Concurrent Mark Sweep）是一种以获取最短回收停顿时间为目标的收集器，是基于“标记-清除”算法。 6.2 CMS 的整个过程有 4 个步骤 初始标记——并发标记——重新标记——并发清除 初始标记：CMS initial mark 仅仅是标记一下 GC Roots 能直接关联的对象，速度快；需要 stop the world； 并发标记：CMS concurrent mark 是进行 GC Roots Tracing 的过程； 重新标记：CMS remark 是修正并发标记期间因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，停顿时间比初始标记长，比并发标记短；需要 stop the world； 并发清除：CMS concurrent sweep，清除算法会在收集结束时产生大量空间碎片，有可能导致没有足够大的连续空间来分配当前对象而触发一次 Full GC。 6.3 缺点 CMS 收集器对 CPU 资源非常敏感； CMS 收集器无法处理浮动垃圾，可能出现“Concurrent Mode Failure”失败（备选用 Serial Old）而导致另一次 Full GC 的产生； CMS 是一款基于“标记-清除”算法的收集器，在收集结束后会产生大量空间碎片。 缺点具体分析： 对 CPU 资源敏感：在并发阶段会占用一部分线程而导致应用程序变慢，总吞吐量降低；（解决方法是“增量式并发收集器”，但不提倡使用，i-CMS 收集器是与单 CPU 年代 PC 机操作系统使用抢占式模拟多任务机制的思想，在并发标记、清理的时候让 GC 线程、用户线程交替执行，尽量减少 GC 线程的独占资源的时间）。 无法处理浮动垃圾：CMS 并发清理阶段用户线程还在运行，会产生新的垃圾，这部分垃圾出现在标记过程之后，CMS 无法在当次收集中处理它们，只好留到下一次 GC 时再处理。CMS 需要预留一部分提供并发收集时的程序运行使用，CMS 收集时老年代不能填满再收集。 收集后产生大量空间碎片：“标记-清除”算法的缺点，解决方案是使用 -XX:+UseCMSCompactAtFullCollection 和 -XX:CMSFullGCsBeforeCompaction 参数。 6.4 优点 并发收集；低停顿（并发低停顿收集器）。 6.5 应用场景 在互联网站或者 B/S 系统的服务端上，重视服务的响应速度，希望系统停顿时间最短，给用户带来较好的体验。 6.6 参数控制 使用 -XX:CMSInitiatingOccupancyFraction 参数：提高触发老年代 CMS 垃圾回收的百分比； 使用 -XX:+UseCMSCompactAtFullCollection 开关参数：默认开启，用于 CMS 收集器要进行 Full GC 时开启内存碎片合并整理过程，非并发的过程； 使用 -XX:CMSFullGCsBeforeCompaction 参数：用于设置执行多少次不压缩的 Full GC 后，紧接着一次带压缩的（默认为 0，表示每次进入 Full GC 时就进行碎片整理）。 6.7 CMS 收集器运行时示意图 7 G1 收集器 7.1 介绍 G1（Garbage-First）收集器是一款面向服务端应用的垃圾收集器，为了代替 JDK1.5 中发布的 CMS 收集器。将整个 Java 堆划分为多个大小相等的独立区域。####（Region），保留新生代和老年代概念，但不再是物理隔离，是一部分 Region 的集合（不需要连续）。 7.2 优点 并发与并行、分代收集、空间整合、可预测的停顿。 并发与并行：G1 能充分利用多 CPU、多核环境下的硬件优势，使用多个 CPU 缩短 storp-the-world 停顿时间；G1 可通过并发的方式使得 Java 程序运行； 分代收集：可以独立管理整个 GC 堆，采用不同的方式处理新创建的对象和已经存活一段时间、熬过多次GC 的旧对象以获取更好的收集效果； 空间整合：整体上基于“标记-整理”算法，局部（两个 Region 之间）基于“复制”算法，G1 运行期间不会产生内存空间碎片，收集后能提供规整的可用内存，有利于程序长时间运行，分配大对象时不会因为无法获得连续内存空间而提前触发下一次 GC； 可预测的停顿：相比于 CMS 的另一优势，能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在垃圾收集上的时间不得超过 N 毫秒。因为可以有计划地避免在整个 Java 堆上进行全区域的垃圾收集。G1 跟踪各个 Region 内垃圾堆积的价值大小（回收所获得的空间大小+回收所需时间的经验值），在后台维护一个优先列表，根据允许的收集时间，回收价值最大的 Region（Garbage-First 的由来）。 7.3 G1 将内存“化整为零”的思路： Region 之间的对象引用以及其他收集器中的新生代与老年代之间的对象引用，VM 都是通过 Remember Set 来避免全堆扫描。G1 中每个 Region 中都有一个与之对应的 Remember Set： VM 发现程序对 Reference 类型的数据进行写操作时，会产生一个 Write Barrier 暂时中断写操作； 检查 Reference 引用的对象是否处于不同的 Region 之中；如果是，便通过 CardTable 把相关引用信息记录到被引用对象所属的 Region 的 Remember Set 之中； 当进行内存回收时，在 GC 根节点的枚举范围中加入 Remember Set 即可保证不对全堆扫描； 7.4 G1收集器运作的步骤 初始标记——并发标记——最终标记——筛选回收 初始标记：initial marking，标记一下 GC Roots 能直接关联的对象，并且修改 TAMS(Next Top at Mark Start) 的值，让下一阶段用户程序并发运行时，能在正确可用的 Region 中创建新对象，需要停顿线程，耗时短； 并发标记：concurrent marking，从 GC Root 开始对堆中对象进行可达性分析，找出存活的对象，这阶段耗时长，但可与用户程序并发执行； 最终标记：final marking，修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，对象变化记录存在线程 Remember Set Logs 中，然后把这些数据合并到 Remember Set 中，该阶段停顿线程，但是可并行执行； 筛选回收：live data counting and evacuation，对各个 Region 的回收价值和成本进行排序，根据用户所期望的 GC 停顿时间来指定回收计划。 7.5 G1 收集器运行示意图 8 安全点 8.1 介绍 Safepoint：在 HotSpot 的实现中，使用一组称为 OopMap 的数据结构，在类加载完成的时候，HotSpot 把对象带内什么偏移量上是什么类型的数据都计算出来，在 JIT 编译过程中，也会在特定的位置记录下栈和寄存器中哪些位置是引用，这些特定的位置就是安全点。 程序执行时并非在所有的地方都能停顿下来开始 GC，只有到达安全点时才能暂停。 安全点的选定：标准是“是否具有让程序长时间执行的特征”，不能太少以至于让 GC 等待时间太长，也不能过于频繁以至于过分增大运行时的负荷。 8.2 安全点的停顿 如何在 GC 发生时让所有线程都“跑”到最近的安全点停顿？两种方案：抢先式中断和主动式中断。 抢先式中断 不需要线程的执行代码主动去配合，在 GC 发生时，首先把所有线程全部中断，如果发现有线程中断的地方不在安全点上，就恢复线程，让它“跑”到安全点上。（现在几乎不用） 主动式中断 当 GC 需要中断线程的时候，不直接对线程操作，仅仅设置一个标志，各个线程执行时主动去轮询这个标志，发现中断标志为真时就自己中断挂起。轮询标志的地方和安全点是重合的，另外再加上创建对象需要分配内存的地方。 8.3 安全点的作用 safepoint 保证程序执行时，在不太长的时间内就会遇到可进入的 GC 的 safepoint。 若程序不执行，则没有分配 CPU 时间，如线程处于 Sleep 或 Blocked 状态，无法响应 JVM 的中断请求，此时需要安全区域解决，在一段代码片段中，引用关系不会发生变化，在这个区域中的任意地方开始 GC 都是安全的。 9 垃圾收集器的参数总结 垃圾收集器总结： 9.1 GC 器的参数 参数 解释 -XX:+UseSerialGC JVM 运行在 Client 模式下的默认值，打开此开关后，使用 Serial+Serial Old 的收集器组合进行内存回收 -XX:+UseParNewGC 打开此开关后，使用 ParNew+CMS+Serial Old 的收集器进行垃圾回收 -XX:+UseConcMarkSweepGC 使用 ParNew+CMS+Serial Old 的收集器组合进行垃圾回收，Serial Old 作为 CMS 出现“Concurrent Mode Failure”失败后的后备收集器使用。 -XX:+UseParallelGC JVM 运行在 Server 模式下的默认值，打开此开关后，使用 Parallel Scavenge+Serial Old 的收集器组合进行回收 -XX:+UseParallelOldGC 使用 Parallel Scavenge + Parallel Old 的收集器组合进行回收 -XX:SurvivorRatio 新生代中 Eden 区域与 Survivor 区域的容量比值，默认为8，代表 Eden:Subrvivor = 8.1 -XX:PretenureSizeThreshold 直接晋级到老年代对象的大小，设置这个参数后，大于这个参数的对象将直接在老年代分配 -XX:MaxTenuringThreshold 晋级到老年代的对象年龄，每次 Minor GC 之后，年龄就加 1，当超过这个参数的值时进入老年代 -XX:UerAdaptiveSizePolicy 动态调整 Java 堆中各区域的大小以及进入老年代的年龄 -XX:+HandlePromotionFailure 是否允许新生代手机担保，进行一次 minor gc 后，另一块 Suvivor 空间不足时，将直接会在老年代中保留 -XX:ParallelGCThreads 设置并行 GC 进行内存回收的线程数 -XX:GCTimeRatio GC 时间占总时间的比例，默认值为 99，即允许 1% 的 GC 时间，仅在使用 Parallel Scavenge 收集器时有效 -XX:CMSInitingOccupancyFraction 设置 CMS 收集器在老年代空间被使用多少后触发垃圾收集，默认值为 68%，仅在 CMS 收集器时有效 -XX:+UseCMSCompactAtFullCollection 由于 CMS 收集器会产生碎片，此参数设置在垃圾收集器后是否需要一次内存碎片整理过程，仅在 CMS 收集器时有效 -XX:+CMSFullGCBeforeCompaction 设置 CMS 收集器在进行若干次垃圾收集后进行一次内存碎片整理过程，通常与 UseCMSCompactAtFullCollection 参数一起使用 -XX:+UseFastAccessorMethods 原始类型优化 -XX:+DisableExplicitGC 是否关闭手动 System.gc -XX:+CMSParallelRemarkEnabled 降低标记停顿 -XX:LargePageSizeInBytes 内存页的大小不可设置过大，会影响 Perm 的大小，-XX:LargePageSizeInBytes=128m 9.2 GC 器的使用 新生代 GC 老年代 -XX:+UseSerialGC Serial 串行 GC Serial Old 串行 GC -XX:+UseParallelGC Parallel Scavenge 并行回收 GC Parallel Old 并行 GC -XX:+UseConcMarkSweepGC ParNew 并行 GC CMS 并发 GC，当出现“Concurrent Mode Failure”时采用 Serial Old 串行 GC -XX:+UseParNewGC ParNew 并行 GC Serial Old 串行 GC -XX:+UseParallelOldGC Parallel Scavenge 并行回收 GC Parallel Old 并发 GC -XX:+UseConcMarkSweepGC-XX:+UseParNewGC Serial 串行 GC CMS 并发 GC 当出现“Concurrent Mode Failure”时采用 Serial Old 串行 GC ","link":"https://faded.auspicious.space/post/a-summary-to-jvm-garbage-collector/"},{"title":"六个字符构建 JavasScript 世界","content":" A Javascript journey with only six characters 前言 无用但有趣的冷知识，通过 [ ] ( ) ! + 构建 Javascript 世界，hope you enjoy it！ JavaScript 是一门非常奇怪，同时也非常棒的语言，我们可以用它写出非常疯狂但却奏效的代码，同时，它也能根据我们使用的方式进行类型转换从而辅助开发。 构建假设 如果将字符串（string）和其他类型参数相加，它会猜测我们需要文本格式，最后结果将返回 string 类型。 如果将其他类型参数加上 + 或 - 前缀，它知道我们需要一个数值类型（Number）， 如果类型转换合法，紧接着就会将参数转换成数值类型。 如果将参数取反（!），它会把类型转换成布尔值。 构建法则 下面让我们从最基础的开始，这里有几条黄金法则： 通过 ! 转换为布尔（Boolean）类型； 通过 + 转换为数值（Number）类型； 与 [] 相加转换为字符（String）类型。 通过上面的黄金法则，让我们实践几个例子： ![] === false; +[] === 0; []+[] === &quot;&quot;; 当然，聪明的你肯定知道可以通过中括号加索引的方式从字符串中获取对应位置的字符： &quot;hello&quot;[0] === &quot;h&quot;; 通过多个数值字符相加，再转换为数值类型，即可获得多位数： +(&quot;1&quot; + &quot;1&quot;) === 11 非常棒，通过上面的铺垫，现在我们已经拿到了字母 a ： ![] === false ![]+[] === &quot;false&quot; +!![] === 1 ------------------------ (![]+[])[+!![]] === &quot;a&quot; // same as &quot;false&quot;[1] 如此，通过一些简单的结合，我们还可以从单词 true 和 false 中获取到 a、e、f、l、r、s、t、u，那剩下的字母呢？ 别忘了，我们还可以通过 [][[]] 这种方式获得 undefined，复用上面的方式，我们能拿到字母 d、i、n。 [][[]] + [] === &quot;undefined&quot; 模拟构建 到目前为止，通过我们拿到的字母，已经可以拼出 fill、filter、find 这些单词，当然，你还可以组合成其他的单词，值得一提的是，上面提出三个单词都属于数组方法，这意味可以在数组实例中直接被调用，例如 [2,1].sort()。 语法上，[2,1][&quot;sort&quot;]() 是与 [2,1].sort() 等价的。 让我们继续看看，使用字母拼凑的数组方法还能得到什么，目前我们还不需要执行这些函数： [][&quot;fill&quot;] 上面的代码最终会产生 function fill() { [native code] }，通过黄金法则，我们将结果再次转换为 String 类型： [][&quot;fill&quot;]+[] === &quot;function fill() { [native code] }&quot; 现在，我们又获得了 c、o、v、(、)、{、}、（空格）。 通过新获取的 c 和 o，现在可以组合单词 constructor，constructor 是 JS 对象中返回构造函数的方法，下面就让我们通过 constructor 从已有对象类型中获取对应字符串形式的构造函数： true[&quot;constructor&quot;] + [] === &quot;function Boolean() { [native code] }&quot; 0[&quot;constructor&quot;] + [] === &quot;function Number() { [native code] }&quot; &quot;&quot;[&quot;constructor&quot;] + [] === &quot;function String() { [native code] }&quot; [][&quot;constructor&quot;] + [] === &quot;function Array() { [native code] }&quot; 通过这些构造函数，字母集合中增添了 B、N、S、A、m、g、y 现在可以构建 toString，同样，可以通过中括号使用的函数，不过这次我们要执行它： (10)[&quot;toString&quot;]() === &quot;10&quot; 但前文中我们已经通过第三条黄金法则熟练地将任何类型转换为字符串类型了，toString 的存在看起来有点鸡肋，没用了？ 别忘了，数值类型的 toString 方法还有第二个参数 radix，radix 决定了数值转换为字符串类型前被转换为的进制，举个例子： (12)[&quot;toString&quot;](10) === &quot;12&quot; // base 10 - normal to us (12)[&quot;toString&quot;](2) === &quot;1100&quot; // base 2, or binary, for 12 (12)[&quot;toString&quot;](8) === &quot;14&quot; // base 8 (octonary) for 12 (12)[&quot;toString&quot;](16) === &quot;c&quot; // hex for 12 机智的你肯定想到了，为什么只写到 16 进制？进制最大可以是 36，这可包括了 0-9、a-z 中的所有字母，现在我们可以拿到我们想要的任何字母： (10)[&quot;toString&quot;](36) === &quot;a&quot; (35)[&quot;toString&quot;](36) === &quot;z&quot; 太棒了，我们已经拿到了全部小写字母，但新问题摆在眼前，标点符号和大写字母该怎么办呢？ 根据 JS 执行的位置，它可能有权限访问特定的预定义对象或数据，如果是在浏览器中运行，那么就可以有访问到一些传统的 HTML 包装方法，例如，bold 是一个字符串方法，可以将字符串用 &lt;b&gt; 标签包裹。 &quot;test&quot;[&quot;bold&quot;]() === &quot;&lt;b&gt;test&lt;/b&gt;&quot; 这样，我们就拿到了 &lt;、&gt; 和 /。 函数运行 你可能在项目开发中使用过 escape 函数，它可以将字符串转换为浏览器可以翻译的 URI 友好格式，这个函数在我们接下来的工作中扮演了重要角色，我们需要用到它。通过拼凑字母得到这个单词，但问题是如何使其执行，它是一个全局函数，不属于任何类型。 那么函数的构造函数是什么呢？其实就是函数对象本身，function Function() { [native code] }。 [][&quot;fill&quot;][&quot;constructor&quot;] === Function 通过 Function，我们可以传入字符串来构建一个函数： Function(&quot;alert('test')&quot;); 运行得到： Function anonymous() { alert('test') } 我们只需要在末尾加上 () 就可以立即执行这个函数，如你所见，我们现在可以真正执行代码了！ 小试牛刀，运行 escape 函数： [][&quot;fill&quot;][&quot;constructor&quot;](&quot;return escape(' ')&quot;)() === &quot;%20&quot; 如果我们给 escape 函数传入 &lt;，会得到 %C，如果想获得盛夏的大写字母，这个 C 至关重要。 [][&quot;fill&quot;][&quot;constructor&quot;](&quot;return escape('&lt;')&quot;)()[2] === &quot;C&quot; 通过 C，我们可以得到 fromCharCode 函数，通过给定的十进制参数，可以得到对应的 Unicode 字符，它属于字符对象，因此调用方式可以参照前文： &quot;&quot;[&quot;constructor&quot;][&quot;fromCharCode&quot;](65) === &quot;A&quot; &quot;&quot;[&quot;constructor&quot;][&quot;fromCharCode&quot;](46) === &quot;.&quot; Javascript 世界 通过 Unicode 速查可以快速找到任何字符对应的数值。 到这里，JavaScript 世界的构建元素已经全部找齐！我们已经能拿到我们需要的任何参数，并将它们连接到一起形成代码并执行，这意味，我们仅通过 [、]、(、)、+、! 实现了 JavaScript 的图灵完备。 想证明一下？不妨在浏览器控制台里执行下面的代码： 如果你是在手机上看的，可以告诉你，上面执行的是 alert(&quot;wtf&quot;) jsFuck 可以自动转换你的代码，这里是过程介绍。 你说了这么多，有用吗？ 如果你非要问我有没有用，我只能说点儿用也没，不过 eBay 前段时间出了个 Bug，网站里允许卖家在页面中插入这些字符构成的 JS 代码，但这种攻击媒介不是很常见。有人说可以用来进行代码混淆，实际上，有更好的混淆方式。 ","link":"https://faded.auspicious.space/post/a-javascript-journey-with-only-six-characters/"},{"title":"WebSocket 协议：5 分钟从入门到精通","content":" WebSocket协议：5分钟从入门到精通 1 内容概览 WebSocket 的出现，使得浏览器具备了实时双向通信的能力。本文由浅入深，介绍了 WebSocket 如何建立连接、交换数据的细节，以及数据帧的格式。此外，还简要介绍了针对 WebSocket 的安全攻击，以及协议是如何抵御类似攻击的。 2 什么是WebSocket HTML5 开始提供的一种浏览器与服务器进行全双工通讯的网络技术，属于应用层协议。它基于 TCP 传输协议，并复用 HTTP 的握手通道。 对大部分 Web 开发者来说，上面这段描述有点枯燥，其实只要记住几点： WebSocket 可以在浏览器里使用； 支持双向通信； 使用很简单。 2.1 有哪些优点 说到优点，这里的对比参照物是 HTTP 协议，概括地说就是：支持双向通信，更灵活，更高效，可扩展性更好。 支持双向通信，实时性更强。 更好的二进制支持。 较少的控制开销。连接创建后，ws 客户端、服务端进行数据交换时，协议控制的数据包头部较小。在不包含头部的情况下，服务端到客户端的包头只有 2~10字节（取决于数据包长度），客户端到服务端的的话，需要加上额外的 4 字节的掩码。而 HTTP 协议每次通信都需要携带完整的头部。 支持扩展。ws 协议定义了扩展，用户可以扩展协议，或者实现自定义的子协议。（比如支持自定义压缩算法等） 对于后面两点，没有研究过 WebSocket 协议规范的同学可能理解起来不够直观，但不影响对 WebSocket 的学习和使用。 2.2 需要学习哪些东西 对网络应用层协议的学习来说，最重要的往往就是连接建立过程、数据交换教程。当然，数据的格式是逃不掉的，因为它直接决定了协议本身的能力。好的数据格式能让协议更高效、扩展性更好。 下文主要围绕下面几点展开： 如何建立连接； 如何交换数据； 数据帧格式； 如何维持连接。 3 入门例子 在正式介绍协议细节前，先来看一个简单的例子，有个直观感受。例子包括了WebSocket服务端、WebSocket客户端（网页端）。完整代码可以在 这里 找到。 这里服务端用了 ws 这个库。相比大家熟悉的 socket.io，ws 实现更轻量，更适合学习的目的。 3.1 服务端 代码如下，监听 8080 端口。当有新的连接请求到达时，打印日志，同时向客户端发送消息。当收到到来自客户端的消息时，同样打印日志。 var app = require('express')(); var server = require('http').Server(app); var WebSocket = require('ws'); var wss = new WebSocket.Server({ port: 8080 }); wss.on('connection', function connection(ws) { console.log('server: receive connection.'); ws.on('message', function incoming(message) { console.log('server: received: %s', message); }); ws.send('world'); }); app.get('/', function (req, res) { res.sendfile(__dirname + '/index.html'); }); app.listen(3000); 3.2 客户端 代码如下，向 8080 端口发起 WebSocket 连接。连接建立后，打印日志，同时向服务端发送消息。接收到来自服务端的消息后，同样打印日志。 &lt;script&gt; var ws = new WebSocket('ws://localhost:8080'); ws.onopen = function () { console.log('ws onopen'); ws.send('from client: hello'); }; ws.onmessage = function (e) { console.log('ws onmessage'); console.log('from server: ' + e.data); }; &lt;/script&gt; 3.3 运行结果 可分别查看服务端、客户端的日志，这里不展开。 服务端输出： server: receive connection. server: received hello 客户端输出： client: ws connection is open client: received world 4 如何建立连接 前面提到，WebSocket 复用了 HTTP 的握手通道。具体指的是，客户端通过 HTTP 请求与 WebSocket 服务端协商升级协议。协议升级完成后，后续的数据交换则遵照 WebSocket 的协议。 4.1 客户端：申请协议升级 首先，客户端发起协议升级请求。可以看到，采用的是标准的 HTTP 报文格式，且只支持 GET 方法。 GET / HTTP/1.1 Host: localhost:8080 Origin: http://127.0.0.1:3000 Connection: Upgrade Upgrade: websocket Sec-WebSocket-Version: 13 Sec-WebSocket-Key: w4v7O6xFTi36lq3RNcgctw== 重点请求首部意义如下： Connection: Upgrade：表示要升级协议 Upgrade: websocket：表示要升级到 websocket 协议。 Sec-WebSocket-Version: 13：表示 websocket 的版本。如果服务端不支持该版本，需要返回一个 Sec-WebSocket-Versionheader，里面包含服务端支持的版本号。 Sec-WebSocket-Key：与后面服务端响应首部的 Sec-WebSocket-Accept 是配套的，提供基本的防护，比如恶意的连接，或者无意的连接。 注意，上面请求省略了部分非重点请求首部。由于是标准的 HTTP请求 ，类似 Host、Origin、Cookie 等请求首部会照常发送。在握手阶段，可以通过相关请求首部进行 安全限制、权限校验等。 4.2 服务端：响应协议升级 服务端返回内容如下，状态代码 101 表示协议切换。到此完成协议升级，后续的数据交互都按照新的协议来。 HTTP/1.1 101 Switching Protocols Connection:Upgrade Upgrade: websocket Sec-WebSocket-Accept: Oy4NRAQ13jhfONC7bP8dTKb4PTU= 备注：每个 header 都以 \\r\\n 结尾，并且最后一行加上一个额外的空行 \\r\\n。此外，服务端回应的 HTTP 状态码只能在握手阶段使用。过了握手阶段后，就只能采用特定的错误码。 4.3 Sec-WebSocket-Accept 的计算 Sec-WebSocket-Accept 根据客户端请求首部的 Sec-WebSocket-Key 计算出来。 计算公式为： 将 Sec-WebSocket-Key 跟 258EAFA5-E914-47DA-95CA-C5AB0DC85B11 拼接。 通过 SHA1 计算出摘要，并转成 base64 字符串。 伪代码如下： $ toBase64( sha1( Sec-WebSocket-Key + 258EAFA5-E914-47DA-95CA-C5AB0DC85B11 ) ) 验证下前面的返回结果： const crypto = require('crypto'); const magic = '258EAFA5-E914-47DA-95CA-C5AB0DC85B11'; const secWebSocketKey = 'w4v7O6xFTi36lq3RNcgctw=='; let secWebSocketAccept = crypto.createHash('sha1') .update(secWebSocketKey + magic) .digest('base64'); console.log(secWebSocketAccept); // Oy4NRAQ13jhfONC7bP8dTKb4PTU= 5 数据帧格式 客户端、服务端数据的交换，离不开数据帧格式的定义。因此，在实际讲解数据交换之前，我们先来看下 WebSocket 的数据帧格式。 WebSocket 客户端、服务端通信的最小单位是帧（frame），由 1 个或多个帧组成一条完整的消息（message）。 发送端：将消息切割成多个帧，并发送给服务端； 接收端：接收消息帧，并将关联的帧重新组装成完整的消息； 本节的重点，就是讲解数据帧的格式。详细定义可参考 RFC6455 5.2 节 。 5.1 数据帧格式概览 下面给出了 WebSocket 数据帧的统一格式。熟悉 TCP/IP 协议的同学对这样的图应该不陌生。 从左到右，单位是比特。比如 FIN、RSV1 各占据 1 比特，opcode 占据 4 比特。 内容包括了标识、操作代码、掩码、数据、数据长度等。（下一小节会展开） 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-------+-+-------------+-------------------------------+ |F|R|R|R| opcode|M| Payload len | Extended payload length | |I|S|S|S| (4) |A| (7) | (16/64) | |N|V|V|V| |S| | (if payload len==126/127) | | |1|2|3| |K| | | +-+-+-+-+-------+-+-------------+ - - - - - - - - - - - - - - - + | Extended payload length continued, if payload len == 127 | + - - - - - - - - - - - - - - - +-------------------------------+ | |Masking-key, if MASK set to 1 | +-------------------------------+-------------------------------+ | Masking-key (continued) | Payload Data | +-------------------------------- - - - - - - - - - - - - - - - + : Payload Data continued ... : + - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - + | Payload Data continued ... | +---------------------------------------------------------------+ 5.2 数据帧格式详解 针对前面的格式概览图，这里逐个字段进行讲解，如有不清楚之处，可参考协议规范，或留言交流。 FIN 1 个比特。如果是 1，表示这是消息（message）的最后一个分片（fragment），如果是 0，表示不是是消息（message）的最后一个分片（fragment）。 RSV1, RSV2, RSV3 各占 1 个比特。一般情况下全为 0。当客户端、服务端协商采用 WebSocket 扩展时，这三个标志位可以非 0，且值的含义由扩展进行定义。如果出现非零的值，且并没有采用 WebSocket 扩展，连接出错。 Opcode 4个比特。操作代码，Opcode 的值决定了应该如何解析后续的数据载荷（data payload）。如果操作代码是不认识的，那么接收端应该断开连接（fail the connection）。可选的操作代码如下： %x0：表示一个延续帧。当 Opcode 为 0 时，表示本次数据传输采用了数据分片，当前收到的数据帧为其中一个数据分片。 %x1：表示这是一个文本帧（frame）。 %x2：表示这是一个二进制帧（frame）。 %x3-7：保留的操作代码，用于后续定义的非控制帧。 %x8：表示连接断开。 %x9：表示这是一个ping操作。 %xA：表示这是一个pong操作。 %xB-F：保留的操作代码，用于后续定义的控制帧。 Mask 1 个比特。表示是否要对数据载荷进行掩码操作。从客户端向服务端发送数据时，需要对数据进行掩码操作；从服务端向客户端发送数据时，不需要对数据进行掩码操作。 如果服务端接收到的数据没有进行过掩码操作，服务端需要断开连接。 如果 Mask 是 1，那么在 Masking-key 中会定义一个掩码键（masking key），并用这个掩码键来对数据载荷进行反掩码。所有客户端发送到服务端的数据帧，Mask 都是 1。 掩码的算法、用途在下一小节讲解。 Payload length 数据载荷的长度，单位是字节。为 7 位，或 7+16 位，或 1+64 位。 假设数 Payload length === x，如果 x 为 0~126：数据的长度为 x 字节。 x 为 126：后续 2 个字节代表一个 16 位的无符号整数，该无符号整数的值为数据的长度。 x 为 127：后续 8 个字节代表一个 64 位的无符号整数（最高位为 0），该无符号整数的值为数据的长度。 此外，如果 payload length 占用了多个字节的话，payload length 的二进制表达采用网络序（big endian，重要的位在前）。 Masking-key 0 或 4字节（32位）。所有从客户端传送到服务端的数据帧，数据载荷都进行了掩码操作，Mask 为 1，且携带了 4 字节的 Masking-key。如果 Mask 为 0，则没有 Masking-key。 备注：载荷数据的长度，不包括 mask key 的长度。 Payload data (x+y) 字节。载荷数据：包括了扩展数据、应用数据。其中，扩展数据 x 字节，应用数据 y 字节。 扩展数据：如果没有协商使用扩展的话，扩展数据数据为 0 字节。所有的扩展都必须声明扩展数据的长度，或者可以如何计算出扩展数据的长度。此外，扩展如何使用必须在握手阶段就协商好。如果扩展数据存在，那么载荷数据长度必须将扩展数据的长度包含在内。 应用数据：任意的应用数据，在扩展数据之后（如果存在扩展数据），占据了数据帧剩余的位置。载荷数据长度 减去 扩展数据长度，就得到应用数据的长度。 5.3 掩码算法 掩码键（Masking-key）是由客户端挑选出来的 32 位的随机数。掩码操作不会影响数据载荷的长度。掩码、反掩码操作都采用如下算法： 首先，假设： original-octet-i：为原始数据的第i字节。 transformed-octet-i：为转换后的数据的第i字节。 j：为 i mod 4 的结果。 masking-key-octet-j：为 mask key 第 j 字节。 算法描述为：original-octet-i 与 masking-key-octet-j 异或后，得到 transformed-octet-i。 j = i MOD 4 transformed-octet-i = original-octet-i XOR masking-key-octet-j 6 数据传递 一旦 WebSocket 客户端、服务端建立连接后，后续的操作都是基于数据帧的传递。 WebSocket 根据 opcode 来区分操作的类型。比如 0x8 表示断开连接，0x0-0x2 表示数据交互。 6.1 数据分片 WebSocket 的每条消息可能被切分成多个数据帧。当 WebSocket 的接收方收到一个数据帧时，会根据 FIN 的值来判断，是否已经收到消息的最后一个数据帧。 FIN=1 表示当前数据帧为消息的最后一个数据帧，此时接收方已经收到完整的消息，可以对消息进行处理。FIN=0，则接收方还需要继续监听接收其余的数据帧。 此外，opcode 在数据交换的场景下，表示的是数据的类型。0x01 表示文本，0x02 表示二进制。而 0x00 比较特殊，表示延续帧（continuation frame），顾名思义，就是完整消息对应的数据帧还没接收完。 6.2 数据分片例子 直接看例子更形象些。下面例子来自 MDN，可以很好地演示数据的分片。客户端向服务端两次发送消息，服务端收到消息后回应客户端，这里主要看客户端往服务端发送的消息。 第一条消息 FIN=1, 表示是当前消息的最后一个数据帧。服务端收到当前数据帧后，可以处理消息。opcode=0x1，表示客户端发送的是文本类型。 第二条消息 FIN=0，opcode=0x1，表示发送的是文本类型，且消息还没发送完成，还有后续的数据帧。 FIN=0，opcode=0x0，表示消息还没发送完成，还有后续的数据帧，当前的数据帧需要接在上一条数据帧之后。 FIN=1，opcode=0x0，表示消息已经发送完成，没有后续的数据帧，当前的数据帧需要接在上一条数据帧之后。服务端可以将关联的数据帧组装成完整的消息。 Client: FIN=1, opcode=0x1, msg=&quot;hello&quot; Server: (process complete message immediately) Hi. Client: FIN=0, opcode=0x1, msg=&quot;and a&quot; Server: (listening, new message containing text started) Client: FIN=0, opcode=0x0, msg=&quot;happy new&quot; Server: (listening, payload concatenated to previous message) Client: FIN=1, opcode=0x0, msg=&quot;year!&quot; Server: (process complete message) Happy new year to you too! 7 连接保持+心跳 WebSocket 为了保持客户端、服务端的实时双向通信，需要确保客户端、服务端之间的 TCP 通道保持连接没有断开。然而，对于长时间没有数据往来的连接，如果依旧长时间保持着，可能会浪费包括的连接资源。 但不排除有些场景，客户端、服务端虽然长时间没有数据往来，但仍需要保持连接。这个时候，可以采用心跳来实现。 发送方-&gt;接收方：ping 接收方-&gt;发送方：pong ping、pong 的操作，对应的是 WebSocket 的两个控制帧，opcode 分别是 0x9、0xA。 举例，WebSocket 服务端向客户端发送 ping，只需要如下代码（采用 ws 模块） ws.ping('', false, true); 8 Sec-WebSocket-Key/Accept 前面提到了，Sec-WebSocket-Key/Sec-WebSocket-Accept 在主要作用在于提供基础的防护，减少恶意连接、意外连接。 作用大致归纳如下： 避免服务端收到非法的 websocket 连接（比如 http 客户端不小心请求连接 websocket 服务，此时服务端可以直接拒绝连接）； 确保服务端理解 websocket 连接。因为 ws 握手阶段采用的是 http 协议，因此可能 ws 连接是被一个 http 服务器处理并返回的，此时客户端可以通过 Sec-WebSocket-Key 来确保服务端认识 ws 协议。（并非百分百保险，比如总是存在那么些无聊的 http 服务器，光处理 Sec-WebSocket-Key，但并没有实现 ws 协议。。。）； 用浏览器里发起 AJAX 请求，设置 header 时，Sec-WebSocket-Key 以及其他相关的 header 是被禁止的。这样可以避免客户端发送 AJAX 请求时，意外请求协议升级（websocket upgrade）； 可以防止反向代理（不理解 ws 协议）返回错误的数据。比如反向代理前后收到两次 ws 连接的升级请求，反向代理把第一次请求的返回给 cache 住，然后第二次请求到来时直接把 cache 住的请求给返回（无意义的返回）。 Sec-WebSocket-Key 主要目的并不是确保数据的安全性，因为 Sec-WebSocket-Key、Sec-WebSocket-Accept 的转换计算公式是公开的，而且非常简单，最主要的作用是预防一些常见的意外情况（非故意的）。 |强调：Sec-WebSocket-Key/Sec-WebSocket-Accept 的换算，只能带来基本的保障，但连接是否安全、数据是否安全、客户端/服务端是否合法的 ws 客户端、ws 服务端，其实并没有实际性的保证。| |-| 9 数据掩码的作用 WebSocket 协议中，数据掩码的作用是增强协议的安全性。但数据掩码并不是为了保护数据本身，因为算法本身是公开的，运算也不复杂。除了加密通道本身，似乎没有太多有效的保护通信安全的办法。 那么为什么还要引入掩码计算呢，除了增加计算机器的运算量外似乎并没有太多的收益（这也是不少同学疑惑的点）。 答案还是两个字：安全。但并不是为了防止数据泄密，而是为了防止早期版本的协议中存在的代理缓存污染攻击（proxy cache poisoning attacks）等问题。 代理缓存污染攻击 下面摘自 2010 年关于安全的一段讲话。其中提到了代理服务器在协议实现上的缺陷可能导致的安全问题。猛击出处。 “We show, empirically, that the current version of the WebSocket consent mechanism is vulnerable to proxy cache poisoning attacks. Even though the WebSocket handshake is based on HTTP, which should be understood by most network intermediaries, the handshake uses the esoteric “Upgrade” mechanism of HTTP [5]. In our experiment, we find that many proxies do not implement the Upgrade mechanism properly, which causes the handshake to succeed even though subsequent traffic over the socket will be misinterpreted by the proxy.” [TALKING] Huang, L-S., Chen, E., Barth, A., Rescorla, E., and C. Jackson, &quot;Talking to Yourself for Fun and Profit&quot;, 2010, 在正式描述攻击步骤之前，我们假设有如下参与者： 攻击者、攻击者自己控制的服务器（简称“邪恶服务器”）、攻击者伪造的资源（简称“邪恶资源”）； 受害者、受害者想要访问的资源（简称“正义资源”）； 受害者实际想要访问的服务器（简称“正义服务器”）； 中间代理服务器。 攻击步骤一 攻击者 浏览器 向 邪恶服务器 发起 WebSocket 连接。根据前文，首先是一个协议升级请求。 协议升级请求 实际到达 代理服务器。 代理服务器 将协议升级请求转发到 邪恶服务器。 邪恶服务器 同意连接，代理服务器 将响应转发给 攻击者。 由于 upgrade 的实现上有缺陷，代理服务器 以为之前转发的是普通的 HTTP 消息。因此，当 协议服务器 同意连接，代理服务器 以为本次会话已经结束。 攻击步骤二 攻击者 在之前建立的连接上，通过 WebSocket 的接口向 邪恶服务器 发送数据，且数据是精心构造的HTTP格式的文本。其中包含了 正义资源 的地址，以及一个伪造的 host（指向 正义服务器）。（见后面报文） 请求到达 代理服务器 。虽然复用了之前的TCP连接，但 代理服务器 以为是新的HTTP请求。 代理服务器 向 邪恶服务器 请求 邪恶资源。 邪恶服务器 返回 邪恶资源。代理服务器 缓存住 邪恶资源（url 是对的，但 host 是 正义服务器 的地址）。 受害者 到这里，受害者可以登场了： 受害者 通过 代理服务器 访问 正义服务器 的 正义资源。 代理服务器 检查该资源的 url、host，发现本地有一份缓存（伪造的）。 代理服务器 将 邪恶资源 返回给 受害者。 受害者 卒。 附：前面提到的精心构造的“HTTP 请求报文”。 Client → Server: POST /path/of/attackers/choice HTTP/1.1 Host: host-of-attackers-choice.com Sec-WebSocket-Key: &lt;connection-key&gt; Server → Client: HTTP/1.1 200 OK Sec-WebSocket-Accept: &lt;connection-key&gt; 当前解决方案 最初的提案是对数据进行加密处理。基于安全、效率的考虑，最终采用了折中的方案：对数据载荷进行掩码处理。 需要注意的是，这里只是限制了浏览器对数据载荷进行掩码处理，但是坏人完全可以实现自己的 WebSocket 客户端、服务端，不按规则来，攻击可以照常进行。 但是对浏览器加上这个限制后，可以大大增加攻击的难度，以及攻击的影响范围。如果没有这个限制，只需要在网上放个钓鱼网站骗人去访问，一下子就可以在短时间内展开大范围的攻击。 10 写在后面 WebSocket 可写的东西还挺多，比如 WebSocket 扩展。客户端、服务端之间是如何协商、使用扩展的。WebSocket 扩展可以给协议本身增加很多能力和想象空间，比如数据的压缩、加密，以及多路复用等。 篇幅所限，这里先不展开，感兴趣的同学可以留言交流。文章如有错漏，敬请指出。 11 相关链接 RFC6455：websocket规范 - https://tools.ietf.org/html/rfc6455 规范：数据帧掩码细节 - https://tools.ietf.org/html/rfc6455#section-5.3 规范：数据帧格式 - https://tools.ietf.org/html/rfc6455#section-5.1 server-example - https://github.com/websockets/ws#server-example 编写websocket服务器 - https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API/Writing_WebSocket_servers 对网络基础设施的攻击（数据掩码操作所要预防的事情）- https://tools.ietf.org/html/rfc6455#section-10.3 Talking to Yourself for Fun and Profit（含有攻击描述） - http://w2spconf.com/2011/papers/websocket.pdf What is Sec-WebSocket-Key for? - https://stackoverflow.com/questions/18265128/what-is-sec-websocket-key-for 10.3. Attacks On Infrastructure (Masking) - https://tools.ietf.org/html/rfc6455#section-10.3 Talking to Yourself for Fun and Profit - http://w2spconf.com/2011/papers/websocket.pdf Why are WebSockets masked? - https://stackoverflow.com/questions/33250207/why-are-websockets-masked How does websocket frame masking protect against cache poisoning? - https://security.stackexchange.com/questions/36930/how-does-websocket-frame-masking-protect-against-cache-poisoning What is the mask in a WebSocket frame? - https://stackoverflow.com/questions/14174184/what-is-the-mask-in-a-websocket-frame ","link":"https://faded.auspicious.space/post/websocket-deep-in/"},{"title":"英特尔至强处理器技术简介","content":" 英特尔至强处理器技术简介 1 至强处理器介绍 至强（Xeon）是英特尔针对服务器和工作站市场的处理器品牌，但也有某些超级计算机采用此处理器。Xeon 采用 x86 架构和（或）x86-64 架构，和采用 IA-64 架构的 Itanium 不同。 至强处理器与常规桌面级 CPU 相采用同一套微结构（微内核），但更关注于核心数量而非时钟频率，并增加了针对服务器和工作站的高级功能，例如 ECC 内存，更多的内核数量，更大的 RAM 和高速缓存，提供企业级的可靠性，可用性和可维护性的 Machine Check Architecture (MCA) 异常处理机制等。此外，某些型号还支持 QPI（快速通道互联）和 UPI（超级通道互联）总线，从而将多个 CPU 连接在一起，从而提供 2 路、4 路、8 路等多路处理能力。 至强（Xeon）处理器目前主要有 6 个系列： 1.1 E3 系列 针对入门级工作站、移动工作站、小型企业服务器等应用的单路处理器，通常为 2/4/8 核，提供必要的性能和视觉功能，处理器架构每年跟随消费级处理器同步更新； 1.2 E5 系列 针对高端工作站的双路 / 四路处理器，最高 22 核，支持 4 通道内存技术和 QPI (快速通道互联)，提供大幅增强的性能和功能，专为下一代数据中心的架构而设计，每年更新，处理器架构落后 E3 一代； 1.3 E7 系列 面向数据要求苛刻的关键任务和数据中心的双路 / 四路 / 八路处理器，最高 24 核，支持 4 通道内存技术和 QPI (快速通道互联)，提供实时分析、任务关键型业务处理以及大数据洞察能力，强调可靠性、可用性和可服务性（RAS）； 1.4 可扩展处理器系列 分为铜牌、银牌、金牌、铂金 4 个等级，分别对应于 E5 和 E7 的不同产品定位，最高 28 核，支持 4 通道内存技术和 UPI（超级通道互联）； 1.5 D 系列 用于空间和功率受限环境的片上系统 (SoC)，最高 16 核，将可扩展平台架构创新引入到片上系统（SoC）处理器，以支持低功耗、高密度解决方案，且集成了基本网络以及安全和加速功能； 1.6 W 系列 针对主流工作站，最高 28 核，基于可扩展平台架构，提供硬件增强的工作负载性能、安全性和可靠性。 此外，至强还包含至强融核系列处理器，目前最新的 Xeon PHI 处理器基于英特尔®集成众核架构（MIC 架构），能为要求最苛刻的高性能计算应用程序提供大规模并行处理和矢量化服务，最高支持 72 核，36M L2 Cache。 2 至强处理器核心技术介绍 2.1 多路互联技术 多路互联技术用于在单块主板上安装多块互相连接的处理器，主要包括: 2.1.1 Intel 的 QPI（快速通道互联）/ UPI（超级通道互联）技术 由英特尔开发并使用的用于替代 FSB（前端总线）并与 AMD 的 HT（HyperTransport）技术竞争的点对点处理器互联架构，最高速度 9.6 GT/s（38.4 GB/s）；2017 年，英特尔通过 SkyLake 微架构发布了基于 QPI 的 UPI（超级通道互联）技术，采用共享地地址空间技术和基于目录的一致性 snoop 协议，通过新的封包格式提高传输效率，最高速度可达 10.4 GT/s（41.6 GB/s），支持低功耗模式，并且不再要求资源预分配； 2.1.2 HT 联盟的 HT（HyperTransport）技术 曾被称作“闪电数据传输”（Lightning Data Transport，LDT），是一种高速、双向、低延时、点对点（P2P）、串行或者并行的高带宽连接总线技术，1999 年由 AMD 提出并发起成立 HyperTransport 开放联盟，于 2001 年 4 月 2 日开始投入使用，广泛用于 AMD、IBM、苹果、Nvidia、MIPS、龙芯、思科、Broadcom 等厂商的处理器上，目前有 1.x，2.0，3.0 和 3.1 等版本，最高速度 51.2 GB/s，支持电源管理； 2.1.3 Nvidia 的 NVLink 技术 NVIDIA 开发并推出的一种串行点对点总线和通信协议，主要使用在 Nvidia GPU 和 IBM Power 处理器上，最高速率为单通道 25 GT/s（25 GB/s），在 IBM Power 9 的 6 通道模式下可达 300 GB/s。 2.2 众核处理器 众核（Manycore）处理器是专为高度并行处理而设计的专用多核处理器，不追求流水线深度、超线程等计数来提高单核性能，而是包含大量简单独立的处理器内核，因此具有更高的吞吐量或更低的功耗，但是具有更高的延迟和较低的单线程性能。 Cache 一致性是限制多核处理器扩展的难点。众核处理器通过消息传递，暂存式内存，DMA，分区化的全局地址空间（Partitioned global address space，PGAS），只读 / 非一致性高速缓存等技巧绕过这个难点。GPU 实际上可以认为是具有多个着色器处理单元的众核处理器。 2.3 多通道内存技术 多通道内存技术是一种可以提升内存数据发送性能的技术，通过在 DRAM 和内存控制器 / 芯片组之间，增加更多的并行通信通道以增加数据发送的带宽。理论上每增加一条通道，数据发送性能相较于单通道而言会增加一倍。通常情况下，多通道对内存的规格和插槽都有要求，只要满足要求才能使能多通道模式。 目前常见的多通道技术多为双通道的设置，例如两组 64-bit DDR 提供 128 位的 DDR 通道。支持四通道技术的处理器包括 Intel / AMD 的高端处理器、包含 ARM CoreLink CCI-500 技术的 Cortex-A72 等处理器，以及高通和三星的高端处理器等。支持八通道技术的有 AMD EPYC、Cavium ThunderX2 等服务器处理器。此外，英特尔 2012 年展示的 Haswell-EX 架构也支持八通道 DDR4。 2.4 多线程技术 多线程技术包括同时多线程（SMT）和时间多线程： 2.4.1 同时多线程 同时多线程（Simultaneous multithreading，SMT）：也称同步多线程，即在一个时钟周期中发出多个线程的多个指令。支持 SMT 计数的处理器包括 IBM Power、MIPS、SUN / Oracle / 富士通Sparc、AMD Bulldozer / Zen微架构等处理器；超线程（HT, Hyper-Threading）是英特尔专有的同步多线程（SMT）实现，通过在 CPU 内部仅复制必要的资源让两个线程可同时运行，从而在同一周期内处理两个线程的工作，模拟实体双核心、双线程运作。 2.4.2 时间多线程 时间多线程（Temporal multithreading）也称交叉多线程，即在一个时钟周期中发出一个指令，交错发出不同线程的多个指令。时间多线程目前仅在 CDC 6000（1960s）、Tera MTA（1988）、XMOS XCore XS1（2007）等 Barrel（桶）处理器上出现。 2.5 MCA 异常处理机制 Intel 服务器处理器提供的硬件错误检测和报告机制，包括系统总线错误，ECC 错误，奇偶校验错误，Cache 错误、TLB 错误等，包括一组用于设置 MCA（Machine Check Architecture） 的 MSR 寄存器和记录硬件错误的附加 MSR 寄存器。 2.6 ECC 内存 在 ECC 技术出现之前，内存中应用最多的另外一种错误检查技术，是奇偶校验位（Parity）技术，仅能发现错误而不能纠正错误。 ECC 内存够实现错误检查和自动纠正技术的内存，可以自动检测和纠正最常见的内部数据损坏，使系统得以正常的操作，不致因错误而中断。通常情况下，ECC 内存保持一个内存系统不受单一位错误的影响，即使用 5 位 ECC 码纠正 8 位数据中的 1 位错误。数据位每增加一倍，ECC 只增加 1 位检验位，即数据位为 16 位时 ECC 位为 6 位，32 位时 ECC 位为 7 位，数据位为 64 位时 ECC 位为 8 位，依此类推。 2.7 向量处理技术 向量处理技术能够直接操作一维数组（向量），与一次只能处理一个数据的标量处理正好相反。向量处理技术可以在特定工作环境中极大地提升性能，尤其是在数值模拟或者相似领域。向量处理技术最早出现于 20 世纪 70 年代早期，并在 70 年代到 90 年代期间成为超级计算机设计的主导方向。由于常规处理器设计性价比的快速下降，基于向量处理的超级计算机在 90 年代末逐渐让出了主导地位。现在，绝大多数商业化 CPU 实现都能够提供某种形式的向量处理指令，用来处理多个向量化的数据集，也就是所谓的 SIMD（单一指令多重数据）。此外，还有多重指令处理多重向量化数据集的 MIMD（多重指令多重数据）技术。 ","link":"https://faded.auspicious.space/post/a-brief-introduction-to-intel-xeon-processor /"},{"title":"TCP 请求头","content":" TCP 请求头 TCP 请求头结构： Source Port：源端口号 （占用 16 位），发送端程序端口。 Destination Port：目的端口号（占用 16 位），接收端程序端口。 Sequence Number：发送数据序号，用来标识从 TCP 发端向 TCP 收端发送的数据字节流，它表示在这个报文段中的的第一个数据字节在数据流中的序号；主要用来解决网络报乱序的问题；（占用 32 位）。 Acknowledgment Number：ACK 确认号，32 位确认序列号包含发送确认的一端所期望收到的下一个序号，因此，确认序号应当是上次已成功收到数据字节序号加 1。不过，只有当标志位中的 ACK 标志（下面介绍）为 1 时该确认序列号的字段才有效。主要用来解决不丢包的问题； 例如：传输一个文件，文件比较大的 TCP 会把该文件拆成多段进行发送， 假如每段1000个字节，第一次的时候 Sequence Number 会随机一个 int 数值，假如为1。 第一次发送 Sequence Number=1， 第一次响应 Acknowledgment Number = 1001 第二次发送 Sequence Number=1001 第二次响应 Acknowledgment Number = 2001 ... ... Data Offset： 数据偏移量（4 位）给出首部中 32 bit 字的数目，需要这个值是因为任选字段的长度是可变的。这个字段占 4 bit（最多能表示 15 个 32 bit 的的字，即 4*15 = 60 个字节的首部长度），因此 TCP 最多有 60 字节的首部。然而，没有任选字段，正常的长度是 20 字节； 如果有额外的 TCP 的 option 选项，还得加上 option 的长度。 Reserved：保留字段，目前还没有使用。 TCP Flags：TCP 控制位（6 位），每一位代表一个控制位，它们中的多个可同时被设置为 1，主要是用于操控 TCP 的状态机的，依次为 URG，ACK，PSH，RST，SYN，FIN。每个标志位的意思如下： URG：此标志表示 TCP 包的紧急指针域（后面马上就要说到）有效，用来保证 TCP 连接不被中断，并且督促中间层设备要尽快处理这些数据； ACK：此标志表示应答域有效，就是说前面所说的 TCP 应答号将会包含在 TCP 数据包中；有两个取值：0 和 1，为 1 的时候表示应答域有效，反之为 0； PSH：这个标志位表示 Push 操作。所谓 Push 操作就是指在数据包到达接收端以后，立即传送给应用程序，而不是在缓冲区中排队； RST：这个标志表示连接复位请求。用来复位那些产生错误的连接，也被用来拒绝错误和非法的数据包； SYN：表示同步序号，用来建立连接。SYN 标志位和 ACK 标志位搭配使用，当连接请求的时候，SYN = 1，ACK = 0；连接被响应的时候，SYN = 1，ACK = 1；这个标志的数据包经常被用来进行端口扫描。扫描者发送一个只有 SYN 的数据包，如果对方主机响应了一个数据包回来 ，就表明这台主机存在这个端口；但是由于这种扫描方式只是进行 TCP 三次握手的第一次握手，因此这种扫描的成功表示被扫描的机器不很安全，一台安全的主机将会强制要求一个连接严格的进行 TCP 的三次握手； FIN： 表示发送端已经达到数据末尾，也就是说双方的数据传送完成，没有数据可以传送了，发送 FIN 标志位的 TCP 数据包后，连接将被断开。这个标志的数据包也经常被用于进行端口扫描。 Window：窗口大小（16 位），表示接收端可用缓冲区大小，根据缓冲区大小和每次包大小，就可以计算出同时处理的 TCP 包的个数。同时处理的包个数越多，则网速越快。 Checksum：校验和，用来检查 TCP 包是否完整（16 位）。 Urgent Pointer：紧急指针，表示应紧急处理的数据位置（16 位）。路由器可以把紧急的数据包优先处理。 Options：可选字段，可变长度，最长为 40 字节。（因为 Data Offset 最多能表示 60 个字节长度的 TCP 头信息，固定的 TCP 头部为 20 字节）。 Padding：填充位。因为 Data Offset 只能表示 TCP 头部的长度 必须是 4 字节的整倍数。如果 Options 选项不足 4 字节的整倍数，就需要 Padding 填充为 4 字节的整倍数。 ","link":"https://faded.auspicious.space/post/tcp-request-header/"},{"title":"HTTP 状态码详解","content":" HTTP协议状态码详解（HTTP Status Code） 1xx（临时响应） 表示临时响应并需要请求者继续执行操作的状态代码。 100（继续） 请求者应当继续提出请求。 服务器返回此代码表示已收到请求的第一部分，正在等待其余部分。 101（切换协议） 请求者已要求服务器切换协议，服务器已确认并准备切换。 2xx （成功） 表示成功处理了请求的状态代码。 200（成功） 服务器已成功处理了请求。 通常，这表示服务器提供了请求的网页。 201（已创建） 请求成功并且服务器创建了新的资源。 202（已接受） 服务器已接受请求，但尚未处理。 203（非授权信息） 服务器已成功处理了请求，但返回的信息可能来自另一来源。 204（无内容） 服务器成功处理了请求，但没有返回任何内容。 205（重置内容） 服务器成功处理了请求，但没有返回任何内容。 206（部分内容） 服务器成功处理了部分 GET 请求。 3xx （重定向） 表示要完成请求，需要进一步操作。 通常，这些状态代码用来重定向。 300（多种选择） 针对请求，服务器可执行多种操作。 服务器可根据请求者 (user agent) 选择一项操作，或提供操作列表供请求者选择。 301（永久移动） 请求的网页已永久移动到新位置。 服务器返回此响应（对 GET 或 HEAD 请求的响应）时，会自动将请求者转到新位置。 302（临时移动） 服务器目前从不同位置的网页响应请求，但请求者应继续使用原有位置来进行以后的请求。 303（查看其他位置） 请求者应当对不同的位置使用单独的 GET 请求来检索响应时，服务器返回此代码。 304（未修改） 自从上次请求后，请求的网页未修改过。 服务器返回此响应时，不会返回网页内容。 305（使用代理） 请求者只能使用代理访问请求的网页。 如果服务器返回此响应，还表示请求者应使用代理。 307（临时重定向） 服务器目前从不同位置的网页响应请求，但请求者应继续使用原有位置来进行以后的请求。 4xx（请求错误） 这些状态代码表示请求可能出错，妨碍了服务器的处理。 400（错误请求） 服务器不理解请求的语法。 401（未授权） 请求要求身份验证。 对于需要登录的网页，服务器可能返回此响应。 403（禁止） 服务器拒绝请求。 404（未找到） 服务器找不到请求的网页。 405（方法禁用） 禁用请求中指定的方法。 406（不接受） 无法使用请求的内容特性响应请求的网页。 407（需要代理授权） 此状态代码与 401（未授权）类似，但指定请求者应当授权使用代理。 408（请求超时） 服务器等候请求时发生超时。 409（冲突） 服务器在完成请求时发生冲突。 服务器必须在响应中包含有关冲突的信息。 410（已删除） 如果请求的资源已永久删除，服务器就会返回此响应。 411（需要有效长度） 服务器不接受不含有效内容长度标头字段的请求。 412（未满足前提条件） 服务器未满足请求者在请求中设置的其中一个前提条件。 413（请求实体过大） 服务器无法处理请求，因为请求实体过大，超出服务器的处理能力。 414（请求的 URI 过长） 请求的 URI（通常为网址）过长，服务器无法处理。 415（不支持的媒体类型） 请求的格式不受请求页面的支持。 416（请求范围不符合要求） 如果页面无法提供请求的范围，则服务器会返回此状态代码。 417（未满足期望值） 服务器未满足“期望”请求标头字段的要求。 5xx（服务器错误） 这些状态代码表示服务器在尝试处理请求时发生内部错误。 这些错误可能是服务器本身的错误，而不是请求出错。 500（服务器内部错误） 服务器遇到错误，无法完成请求。 501（尚未实施） 服务器不具备完成请求的功能。 例如，服务器无法识别请求方法时可能会返回此代码。 502（错误网关） 服务器作为网关或代理，从上游服务器收到无效响应。 503（服务不可用） 服务器目前无法使用（由于超载或停机维护）。 通常，这只是暂时状态。 504（网关超时） 服务器作为网关或代理，但是没有及时从上游服务器收到请求。 505（HTTP 版本不受支持） 服务器不支持请求中所用的 HTTP 协议版本。 4 个新的 HTTP 状态码 RFC 6585 最近刚刚发布，该文档描述了 4 个新的 HTTP 状态码。 HTTP 协议还在变化？是的，HTTP 协议一直在演变，新的状态码对于开发 REST 服务或者说是基于 HTTP 的服务非常有用，下面我们为你详细介绍这四个新的状态码以及是否应该使用。 428 Precondition Required (要求先决条件) 先决条件是客户端发送 HTTP 请求时，如果想要请求能成功必须满足一些预设的条件。 一个好的例子就是 If-None-Match 头，经常在 GET 请求中使用，如果指定了 If-None-Match，那么客户端只在响应中的 ETag 改变后才会重新接收回应。 先决条件的另外一个例子就是 If-Match 头，这个一般用在 PUT 请求上用于指示只更新没被改变的资源，这在多个客户端使用 HTTP 服务时用来防止彼此间不会覆盖相同内容。 当服务器端使用 428 Precondition Required 状态码时，表示客户端必须发送上述的请求头才能执行请求，这个方法为服务器提供一种有效的方法来阻止 'lost update' 问题。 429 Too Many Requests (太多请求) 当你需要限制客户端请求某个服务数量时，该状态码就很有用，也就是请求速度限制。 在此之前，有一些类似的状态码，例如 '509 Bandwidth Limit Exceeded'. Twitter 使用 420 （这不是HTTP定义的状态码） 如果你希望限制客户端对服务的请求数，可使用 429 状态码，同时包含一个 Retry-After 响应头用于告诉客户端多长时间后可以再次请求服务。 431 Request Header Fields Too Large (请求头字段太大) 某些情况下，客户端发送 HTTP 请求头会变得很大，那么服务器可发送 431 Request Header Fields Too Large 来指明该问题。 我不太清楚为什么没有 430 状态码，而是直接从 429 跳到 431，我尝试搜索但没有结果。唯一的猜测是 430 Forbidden 跟 403 Forbidden 太像了，为了避免混淆才这么做的，天知道！ 511 Network Authentication Required (要求网络认证) 对我来说这个状态码很有趣，如果你在开发一个 HTTP 服务器，你不一定需要处理该状态码，但如果你在编写 HTTP 客户端，那这个状态码就非常重要。 如果你频繁使用笔记本和智能手机，你可能会注意到大量的公用 WiFi 服务要求你必须接受一些协议或者必须登录后才能使用。 这是通过拦截 HTTP 流量，当用户试图访问网络返回一个重定向和登录，这很讨厌，但是实际情况就是这样的。 使用这些“拦截”客户端，会有一些讨厌的副作用。在 RFC 中有提到这两个的例子： 如果你在登录 WIFI 前访问某个网站，网络设备将会拦截首个请求，这些设备往往也有自己的网站图标 ‘favicon.ico'。登录后您会发现，有一段时间内你访问的网站图标一直是WIFI登录网站的图标。 如果客户端使用 HTTP 请求来查找文档（可能是 JSON），网络将会响应一个登录页，这样你的客户端就会解析错误并导致客户端运行异常，在现实中这种问题非常常见。 因此 511 状态码的提出就是为了解决这个问题。 如果你正在编写 HTTP 的客户端，你最好还是检查 511 状态码以确认是否需要认证后才能访问。 英文原文 ","link":"https://faded.auspicious.space/post/http-detailed-introduction-to-status-code/"},{"title":"HTTP 协议中 GET 和 POST 的区别","content":" HTTP协议中GET和POST的区别 不完全正确的网红答案 GET 的 URL 会有长度上的限制，则 POST 的数据则可以非常大。 POST 比 GET 安全，GET 请求的数据会附在 URL 之后，POST 把提交的数据则放置在是 HTTP 包的包体中。 为什么是不完全正确的答案 HTTP 协议对 GET 和 POST 都没有对长度的限制：HTTP 协议没有对传输的数据大小进行限制，HTTP 协议规范也没有对 URL 长度进行限制。 而在实际开发中存在的限制主要有： GET：特定浏览器和服务器对 URL 长度有限制，例如 IE 对 URL 长度的限制是 2083 字节(2K+35)。对于其他浏览器，如 Netscape、FireFox 等，理论上没有长度限制，其限制取决于操作系统的支持。因此对于 GET 提交时，传输数据就会受到 URL 长度的限制。 POST：由于不是通过 URL 传值，理论上数据不受 限。但实际各个 WEB 服务器会规定对 POST 提交数据大小进行限制，Apache、IIS6 都有各自的配置。 安全性意义不同：通过 GET 提交数据，用户名和密码将明文出现在 URL 上，因为登录页面有可能被浏览器缓存，其他人查看浏览器的历史纪录，那么别人就可以拿到你的账号和密码了。POST 一般来说都不会被缓存，但有很多抓包工具也是可以窥探到你的数据，真的要安全那就要把传输的信息加密。但这不是 HTTP 协议对 GET 和 POST 做的的安全性区别，是浏览器使用的具体表现出来的。GET 在 HTTP 协议中用于获取数据，POST 在 HTTP 协议中用于修改数据。 在 HTTP 中 GET 和 POST 的原理区别 HTTP 定义了与服务器交互的不同方法，最基本的方法有 4 种，分别是 GET，POST，PUT，DELETE URL 全称是资源定位符，我们可以这样认为：一个 URL 地址，它用于描述一个网络上的资源，而 HTTP 中的 GET，POST，PUT，DELETE 就对应着对这个资源的查 ，改 ，增 ，删 4 个操作。到这里，大家应该有个大概的了解了，GET 一般用于获取 / 查询资源信息，而 POST 一般用于更新资源信息。 根据 HTTP 规范，GET 用于信息获取，而且应该是安全的和幂等的。 GET—安全 所谓安全的意味着该操作用于获取信息而非修改信息。换句话说，GET 请求一般不应产生副作用。就是说，它仅仅是获取资源信息，就像数据库查询一样，不会修改，增加数据，不会影响资源的状态。 注意：这里安全的含义仅仅是指是非修改信息。 GET—幂等 幂等的意味着对同一 URL 的多个请求应该返回同样的结果。幂等（idempotent、idempotence）是一个数学或计算机学概念，常见于抽象代数中。 幂等有以下几种定义： 对于单目运算，如果一个运算对于在范围内的所有的一个数多次进行该运算所得的结果和进行一次该运算所得的结果是一样的，那么我们就称该运算是幂等的。 比如绝对值运算就是一个例子，在实数集中，有 abs(a) = abs(abs(a)) 。 对于双目运算，则要求当参与运算的两个值是等值的情况下，如果满足运算结果与参与运算的两个值相等，则称该运算幂等，如求两个数的最大值的函数，有在实数集中幂等，即 max(x, x) = x。看完上述解释后，应该可以理解 GET 幂等的含义了。 但在实际应用中，以上 2 条规定并没有这么严格。引用别人文章的例子：比如，新闻站点的头版不断更新。虽然第二次请求会返回不同的一批新闻，该操作仍然被认为是安全的和幂等的，因为它总是返回当前的新闻。从根本上说，如果目标是当用户打开一个链接时，他可以确信从自身的角度来看没有改变资源即可。 根据 HTTP 规范，POST 表示可能修改变服务器上的资源的请求。继续引用上面的例子：还是新闻以网站为例，读者对新闻发表自己的评论应该通过 POST 实现，因为在评论提交后站点的资源已经不同了，或者说资源被修改了。 上面大概说了一下 HTTP 规范中，GET 和 POST 的一些原理性的问题。但在实际的做的时候，很多人却没有按照 HTTP 规范去做，导致这个问题的原因有很多，比如说： 很多人贪方便，更新资源时用了 GET，因为用 POST 必须要到 FORM（表单），这样会麻烦一点。 对资源的增，删，改，查操作，其实都可以通过 GET/POST 完成，不需要用到 PUT 和 DELETE。 另外一个是，早期的但是 Web MVC 框架设计者们并没有有意识地将 URL 当作抽象的资源来看待和设计。还有一个较为严重的问题是传统的 Web MVC 框架基本上都只支持 GET 和 POST 两种 HTTP 方法，而不支持 PUT 和 DELETE 方法。 为什么要使用 GET 答案就是：因为 GET 比 POST 更快。 请求过程区别 POST 请求的过程，会先将请求头发送给服务器进行确认，然后才真正发送数据；而 GET 请求的过程，会在连接建立后会将请求头和请求数据一起发送。 POST 请求的过程： 浏览器请求 TCP 连接（第一次握手）； 服务器答应进行 TCP 连接（第二次握手）； 浏览器确认，并发送 POST 请求头（第三次握手，这个报文比较小，所以 HTTP 会在此时进行第一次据发送）； 服务器返回 100 CONTINUE 响应； 浏览器开始发送数据； 服务器返回 200 OK 响应。 GET 请求的过程： 浏览器请求 TCP 连接（第一次握手）； 服务器答应进行 TCP 连接（第二次握手）； 浏览器确认，并发送 GET 请求头和数据（第三次握手，这个报文比较小，所以 HTTP 会在此时进行第一次数据发送）； 服务器返回 200 OK 响应； 也就是说，目测 GET 的总耗是 POST 的 2/3 左右。 GET 会将数据缓存起来，而 POST 不会 可以做个简短的测试，使用 AJAX 采用 GET 方式请求静态数据（比如 HTML 页面，图片）的时候，如果两次传输的数据相同，第二次以后耗费的时间将在 10ms 以内（Chrome 测试），而 POST 每次耗费的时间都差不多。经测试，Chrome 下和 FireFox 下如果检测到 GET 请求的是静态资源，则会缓存，如果是数据，则不缓存，但是 IE 都会缓存起来。 POST 不能进行管道化传输 HTTP 在的一次会话需要先建立 TCP 连接然后才能通信，如果每次连接都只进行一次 HTTP 会话，那这个连接过程占的比例太大了！ 于是出现了持久连接：在 http/1.0+ 中是 connection 首部中添加 keep-alive 值，在 http/1.1 中是在 connection 首部中添加 persistent 值，当然两者不仅仅是命名上的差别，http/1.1 中，持久连接是默认的，除非显示在 connection 中添加 close，否则持久连接不会关闭，而 http/1.0+ 中则恰好相反，除非显示在 connection 首部中添加 keep-alive，否则在接收数据包后连接就断开了。 出现了持久连接还不够，在 http/1.1 中，还有一种称为管道通信的方式进行速度优化：把需要发送到服务器上的所有请求放到输出队列中，在第一个请求发送出去后，不等到收到服务器的应答，第二个请求紧接着就发送出去，但是这样的方式有一个问题：不安全，如果一个管道中有 10 个连接，在发送出 9 个后，突然服务器告诉你，连接关闭了，此时客户端即使收到了前 9 个请求的答复，也会将这 9 个请求的内容清空，也就是说，白忙活了……此时，客户端的这 9 个请求需要重新发送。这对于幂等请求还好（比如 GET，多发送几次都没关系，每次都是相同的结果），如果是 POST 这样的非幂等请求（比如支付的时候，多发送几次就惨了），肯定是行不通的。所以，POST 请求不能通过管道的方式进行通信！ 管道化传输在浏览器端的实现还需考证，貌似默认情况下大部分浏览器（除了 Opera）是不进行管道化传输的，除非手动开启！ 所以，在可以使用 GET 请求通信的时候，不要使用 POST 请求，这样用户体验会更好，当然，如果有安全性要求的话，POST 会更好。 ","link":"https://faded.auspicious.space/post/the-difference-between-get-and-post-in-http/"},{"title":"TCP——发送窗口","content":" 理解TCP发送窗口 窗口是 TCP 中一个极为重要的概念，它直接关系到 TCP 的一个关键功能——流量控制。今天我简单介绍下 TCP 发送窗口，从较为微观的角度去理解 TCP 是如何限制发送端可发送的数据量的。 我们知道 TCP header 中有一个 Window Size 字段，它其实是指接收端的窗口，即接收窗口，用来告知发送端自己所能接收的数据量，从而达到一部分流控的目的。假设你现在有 10MB 的数据要通过 TCP 发送，或许你点个按钮就开始发送了，然后就认为 TCP 仅仅是简单的把数据从一端挪到另一端（宏观上的确如此）。其实 TCP 在整个发送过程中，也在度量当前的网络状态，目的是为了维持一个健康稳定的发送过程。因此，这 10MB 数据是在某些机制的控制下进行传输的，其中一种重要机制就是窗口机制。发送端的发送窗口是基于接收端的接收窗口来计算的，我们可以把这 10MB 数据分为如下四类来看（见图）： 已发送且已应答（Sent / Acked）； 已发送但尚未应答（Send / UnAcked）； 未发送，但位于当前发送窗口之内（Unsent / Inside）； 未发送，但位于当前发送窗口之外（Unsent / Outside）。 已发送且已应答（Sent / Acked） Sent/Acked 数据的第一个字节是 ISN+1，ISN 是指在 TCP 建立连接时由 SYN 分段所选择的第一个编号。SYN Flag 被当作是一个字节的数据，特地会被应答一次。因此，TCP 连接上发送的数据的第一个字节编号就是 ISN+1，被应答的数据的最后一个字节编号为【ACK编号-1】。例如，A 发送 1000 个字节给 B，假设 ISN=1，则所要发送数据的第一个字节的编号为 2，全部发送到 B 之后，B 会应答 1002，意思是说前 1001（包括 ISN）个字节我都收到了，请给我第 1002 个字节。所以，被应答的数据的最后一个字节的编号为 1001。 已发送但尚未应答（Send / UnAcked） Send/UnAcked 数据的状态可能是正在传输的过程中，或是被网络丢弃了，或是已到达接收端但应答尚未被发送（因为 Delayed-Ack），又或是应答正在传输过程中。 为了区分 Sent/UnAcked 数据和 Unsent/Inside 数据，TCP 维护一个叫做 SND.NEXT 的变量，它是下一个即将被发送的字节的编号。所以 SND.NEXT 的值将是下一个即将被发送的 TCP 分段的 Sequence Number 字段的值。Send/UnAcked 数据的第一个字节是接收端上一次接受的应答分段的 Acknowledge Number 字段的值。 未发送，但位于当前发送窗口之内（Unsent / Inside） Unsent/Inside 数据是接收端允许发送端发送的数据，发送端可以发送窗口内的所有数据，无需等待应答以及窗口更新。换句话说，如果发送端停止发送并等待应答，那就说明已经没有 Unsent/Inside 的数据了。 然而，如果遇到拥塞，发送端的流控机制，即 slow start 和 congestion avoidance 会阻止发送端发送所有位于接收窗口内的数据。在这种情况下，这些机制会主宰等待应答之前的可发送数据量。 未发送，但位于当前发送窗口之外（Unsent / Outside） Unsent/Outside 数据是位于当前发送窗口意外的数据，代表将来要发送的数据，但根据目前的接收窗口它们是不允许被发送的。接收端会丢弃无法保存在接收缓存区中的数据，并用当前的应答编号来应答发送端。 发送窗口的移动 发送窗口有一个左侧边缘和一个右边边缘。当收到一个带有更高 Ack number 的应答时，发送窗口的左侧边缘就会向右移动（close）。当收到的应答的 Ack number + Window &gt; 之前的 Ack number + Window 时，发送窗口的右侧边缘会向右移动（open）。 另外，发送窗口仅 close 但不 open 也是有可能的。比如发送端收到一个应答，它的 Ack number 增加了，但是窗口变小了，最终 Ack number + Window 并没有变化。这种情况发生在接收端收到了数据，但还没有把数据给应用层，因此 Ack number 会增加，但是窗口大小会减少同样多的值。 ","link":"https://faded.auspicious.space/post/tcp-slide-window/"},{"title":"Git——原理","content":" 聊聊Git原理 Git 给自己的定义是一套内存寻址文件系统，当你在一个目录下执行 git init 命令时，会生成一个 .git 目录，它的目录结构是这样的： .git/ ├── branches ├── config ├── description ├── HEAD ├── hooks │ ├── applypatch-msg.sample │ ├── commit-msg.sample │ ├── post-update.sample │ ├── pre-applypatch.sample │ ├── pre-commit.sample │ ├── prepare-commit-msg.sample │ ├── pre-push.sample │ ├── pre-rebase.sample │ └── update.sample ├── info │ └── exclude ├── objects │ ├── info │ └── pack └── refs ├── heads └── tags 其中 branches 目录已经不再使用，description 文件仅供 GitWeb 程序使用，config 文件保存了项目的配置。 需要我们重点关注的是 HEAD 和 index 文件以及 objects 和 refs 目录。其中 index 中保存了暂存区的一些信息，这里不做过多介绍。 objects 目录 这个目录是用来存储 Git 对象的（包括 tree 对象、commit 对象和 blob 对象），对于一个初始的 Git 仓库，objects 目录下只有 info 和 pack 两个子目录，并没有常规文件。随着项目的进行，我们创建的文件，以及一些操作记录，都会作为 Git 对象被存储在这个目录下。 在该目录下，所有对象都会生成一个文件，并且有对应的 SHA-1 校验和，Git 会创建以校验和前两位为名称的子目录，并以剩下的 38 位为名称来保存文件。 接下来让我们一起看一下当我们进行一次提交时，Git 具体做了哪些事情。 $ echo 'test content'&gt;test.txt $ git add . 执行上述命令后，objects 目录结构如下： .git/objects/ ├── d6 │ └── 70460b4b4aece5915caf5c68d12f560a9fe3e4 ├── info └── pack 这里多了一个文件夹，如上面所述，这个就是 Git 为我们创建的一个对象，我们可以使用底层命令来看一下这个对象的类型以及它存储的是什么。 $ git cat-file -t d670460b4b4aece5915caf5c68d12f560a9fe3e4 blob $ git cat-file -p d670460b4b4aece5915caf5c68d12f560a9fe3e4 test content 可以看到，这是一个 blob 对象，存储内容就是我们刚刚创建的文件的内容。接下来继续执行提交操作。 $ git commit -m 'test message' [master (root-commit) 2b00dca] test message 1 file changed, 1 insertion(+) create mode 100644 test.txt $ tree .git/objects/ .git/objects/ ├── 2b │ └── 00dcae50af70bb5722033b3fe75281206c74da ├── 80 │ └── 865964295ae2f11d27383e5f9c0b58a8ef21da ├── d6 │ └── 70460b4b4aece5915caf5c68d12f560a9fe3e4 ├── info └── pack 此时 objects 目录下又多了两个对象。再用 cat-file 命令来查看一下这两个文件。 $ git cat-file -t 2b00dcae50af70bb5722033b3fe75281206c74da commit $ git cat-file -p 2b00dcae50af70bb5722033b3fe75281206c74da tree 80865964295ae2f11d27383e5f9c0b58a8ef21da author jackeyzhe &lt;jackeyzhe59@163.com&gt; 1534670725 +0800 committer jackeyzhe &lt;jackeyzhe59@163.com&gt; 1534670725 +0800 test message $ git cat-file -t 80865964295ae2f11d27383e5f9c0b58a8ef21da tree $ git cat-file -p 80865964295ae2f11d27383e5f9c0b58a8ef21da 100644 blob d670460b4b4aece5915caf5c68d12f560a9fe3e4 test.txt 可以看到一个是 commit 对象，一个是 tree 对象。commit 对象通常包括 4 部分内容： 工作目录快照的 Hash，即 tree 的值； 提交的说明信息； 提交者的信息； 父提交的 Hash 值； 由于我是第一次提交，所以这里没有父提交的 Hash 值。 tree 对象可以理解为 UNIX 文件系统中的目录，保存了工作目录的 tree 对象和 blob 对象的信息。接下来我们再来看一下 Git 是如何进行版本控制的。 echo 'version1'&gt;version.txt $ git add . $ git commit -m 'first version' [master 702193d] first version 1 file changed, 1 insertion(+) create mode 100644 version.txt $ echo 'version2'&gt;version.txt $ git add . $ git commit -m 'second version' [master 5333a75] second version 1 file changed, 1 insertion(+), 1 deletion(-) $ tree .git/objects/ .git/objects/ ├── 1f │ └── a5aab2a3cf025d06479b9eab9a7f66f60dbfc1 ├── 29 │ └── 13bfa5cf9fb6f893bec60ac11d86129d56fcbe ├── 2b │ └── 00dcae50af70bb5722033b3fe75281206c74da ├── 53 │ └── 33a759c4bdcdc6095b4caac19743d9445ca516 ├── 5b │ └── dcfc19f119febc749eef9a9551bc335cb965e2 ├── 70 │ └── 2193d62ffd797155e4e21eede20897890da12a ├── 80 │ └── 865964295ae2f11d27383e5f9c0b58a8ef21da ├── d6 │ └── 70460b4b4aece5915caf5c68d12f560a9fe3e4 ├── df │ └── 7af2c382e49245443687973ceb711b2b74cb4a ├── info └── pack $ git cat-file -p 1fa5aab2a3cf025d06479b9eab9a7f66f60dbfc1 100644 blob d670460b4b4aece5915caf5c68d12f560a9fe3e4 test.txt 100644 blob 5bdcfc19f119febc749eef9a9551bc335cb965e2 version.txt $ git cat-file -p 2913bfa5cf9fb6f893bec60ac11d86129d56fcbe 100644 blob d670460b4b4aece5915caf5c68d12f560a9fe3e4 test.txt 100644 blob df7af2c382e49245443687973ceb711b2b74cb4a version.txt Git 将没有改变的文件的 Hash 值直接存入 tree 对象，对于有修改的文件，则会生成一个新的对象，将新的对象存入 tree 对象。我们再来看一下 commit 对象的信息。 $ git cat-file -p 5333a759c4bdcdc6095b4caac19743d9445ca516 tree 2913bfa5cf9fb6f893bec60ac11d86129d56fcbe parent 702193d62ffd797155e4e21eede20897890da12a author jackeyzhe &lt;jackeyzhe59@163.com&gt; 1534672270 +0800 committer jackeyzhe &lt;jackeyzhe59@163.com&gt; 1534672270 +0800 second version $ git cat-file -p 702193d62ffd797155e4e21eede20897890da12a tree 1fa5aab2a3cf025d06479b9eab9a7f66f60dbfc1 parent 2b00dcae50af70bb5722033b3fe75281206c74da author jackeyzhe &lt;jackeyzhe59@163.com&gt; 1534672248 +0800 committer jackeyzhe &lt;jackeyzhe59@163.com&gt; 1534672248 +0800 first version 此时的 commit 对象已经有 parent 信息了，这样我们就可以顺着 parent 一步步往回进行版本回退了。不过这样是比较麻烦的，我们一般习惯用的是 git log 查看提交记录。 refs 目录 在介绍 refs目录之前，我们还是先来看一下该目录结构： $ tree .git/refs/ .git/refs/ ├── heads │ └── master └── tags 2 directories, 1 file $ cat .git/refs/heads/master 5333a759c4bdcdc6095b4caac19743d9445ca516 在一个刚刚被初始化的 Git 仓库中，refs 目录下只有 heads 和 tags 两个子目录，由于我们刚刚有过提交操作，所以 git 为我们自动生成了一个名为 master 的引用。master 的内容是最后一次提交对象的 Hash 值。看到这里大家一定在想，如果我们对每次提交都创建一个这样的引用，不就不需要记住每次提交的 Hash 值了，只要看看引用的值，复制过来就可以退回到对应版本了。没错，这样是可以方便的退回，但是这样做的意义不大，因为我们并不需要频繁的退回，特别是比较古老的版本，退回的概率更是趋近于 0。Git 用这个引用做了更有意义的事，那就是分支。 当我新建一个分支时，git 就会在 .git/refs/heads 目录下新建一个文件。当然新建的引用还是指向当前工作目录的最后一次提交，一般情况下我们不会主动去修改这些引用文件，不过如果一定要修改，Git 为我们提供了一个 update-ref 命令。可以改变引用的值，使其指向不同的 commit 对象。 tags 目录下的文件存储的是标签对应的 commit，当为某次提交打上一个 tag 时，tags 目录下就会被创建出一个命名为 tag 名的文件，值是此次提交的 Hash 值。 HEAD 新建分支的时候，Git 是怎么知道我们当前是在哪个分支的，Git 又是如何实现分支切换的呢？答案就在 HEAD 这个文件中。 $ cat .git/HEAD ref: refs/heads/master $ git checkout test Switched to branch 'test' $ cat .git/HEAD ref: refs/heads/test 很明显，HEAD 文件存储的就是我们当前分支的引用，当我们切换分支后再次进行提交操作时，Git 就会读取 HEAD 对应引用的值，作为此次 commit 的 parent。我们也可以通过 symbolic-ref 命令手动设置 HEAD 的值，但是不能设置 refs 以外的形式。 Packfiles 到这里我们在文章开头所说的重点关注的目录和文件都介绍完毕了。但是作为一个文件系统，还存在一个问题，那就是空间。前文介绍过，当文件修改后进行提交时，Git 会创建一份新的快照。这样长久下去，必定会占用很大的存储空间。而比较古老的版本的价值已经不大，所以要想办法清理出足够的空间供用户使用。 好消息是，Git 拥有自己的 gc（垃圾回收）方法。当仓库中有太多松散对象时，Git 会调用 git gc 命令（当然我们也可以手动调用这个命令），将这些对象进行打包。打包后会出现两个新文件：一个 idx 索引文件和一个 pack 文件。索引文件包含了 packfile 的偏移信息，可以快速定位到文件。打包后，每个文件最新的版本的对象存的是完整的文件内容。而之前的版本只保存差异。这样就达到了压缩空间的目的。 ","link":"https://faded.auspicious.space/post/git-principle/"},{"title":"为什么 Nginx 的性能要比 Apache 高得多","content":" 为什么Nginx的性能要比Apache高得多 这得益于 Nginx 使用了最新的 epoll（Linux 2.6 内核）和 kqueue（freebsd）网络 I/O 模型， 而 Apache 则使用的是传统的 select 模型。目前 Linux 下能够承受高并发访问的 Squid、Memcached 都采用的是 epoll 网络 I/O 模型。 处理大量的连接的读写，Apache 所采用的 select 网络 I/O 模型非常低效。 下面用一个比喻来解析 Apache 采用的 select 模型和 Nginx 采用的 epoll 模型进行之间的区别： 假设你在大学读书，住的宿舍楼有很多间房间，你的朋友要来找你。select 版宿管大妈就会带着你的朋友挨个房间去找，直到找到你为止。而 epoll 版宿管大妈会先记下每位同学的房间号，你的朋友来时，只需告诉你的朋友你住在哪个房间即可，不用亲自带着你的朋友满大楼找人。如果来了 10000 个人，都要找自己住这栋楼的同学时，select 版和 epoll 版宿管大妈，谁的效率更高，不言自明。同理，在高并发服务器中，轮询 I/O 是最耗时间的操作之一，select 和 epoll 的性能谁的性能更高，同样十分明了。 epoll - I/O event notification facility 在 Linux 的网络编程中，很长的时间都在使用 select 来做事件触发。在 Linux 新的内核中，有了一种替换它的机制，就是epoll。相比于 select，epoll 最大的好处在于它不会随着监听 fd 数目的增长而降低效率。因为在内核中的 select 实现中，它是采用轮询来处理的，轮询的 fd 数目越多，自然耗时越多。并且，在 linux/posix_types.h 头文件有这样的声明： #define __FD_SETSIZE 1024 表示 select 最多同时监听 1024 个 fd，当然，可以通过修改头文件再重编译内核来扩大这个数目，但这似乎并不治本。 epoll 的接口非常简单，一共就三个函数： int epoll_create(int size); 创建一个 epoll 的句柄，size 用来告诉内核这个监听的数目一共有多大。这个参数不同于 select() 中的第一个参数，给出最大监听的 fd+1 的值。 需要注意的是，当创建好 epoll 句柄后，它就是会占用一个 fd 值，在 Linux 下如果查看 /proc/进程id/fd/，是能够看到这个 fd 的，所以在使用完 epoll 后，必须调用 close() 关闭，否则可能导致 fd 被耗尽。 int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); epoll 的事件注册函数，它不同于 select() 是在监听事件时告诉内核要监听什么类型的事件， 而是在这里先注册要监听的事件类型。第一个参数是 epoll_create() 的返回值，第二个参数表示动作，用三个宏来表示： EPOLL_CTL_ADD：注册新的 fd 到 epfd 中； EPOLL_CTL_MOD：修改已经注册的 fd 的监听事件； EPOLL_CTL_DEL：从 epfd 中删除一个 fd； 第三个参数是需要监听的 fd，第四个参数是告诉内核需要监听什么事，struct epoll_event 结构如下： typedef union epoll_data { void *ptr; int fd; __uint32_t u32; __uint64_t u64; } epoll_data_t; struct epoll_event { __uint32_t events; /* Epoll events */ epoll_data_t data; /* User data variable */ }; events 可以是以下几个宏的集合： EPOLLIN：表示对应的文件描述符可以读（包括对端 SOCKET 正常关闭）； EPOLLOUT：表示对应的文件描述符可以写； EPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）； EPOLLERR：表示对应的文件描述符发生错误； EPOLLHUP：表示对应的文件描述符被挂断； EPOLLET： 将 EPOLL 设为边缘触发（Edge Triggered）模式，这是相对于水平触发（Level Triggered）来说的。 EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个 socket 的话，需要再次把这个 socket 加入到 EPOLL 队列里。 int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout); 等待事件的产生，类似于 select() 调用。参数 events 用来从内核得到事件的集合，maxevents 告之内核这个 events 有多大，这个 maxevents 的值不能大于创建 epoll_create() 时的 size，参数 timeout 是超时时间（毫秒，0 会立即返回，-1 将不确定，也有说法说是永久阻塞）。该函数返回需要处理的事件数目，如返回 0 表示已超时。 关于 ET、LT 两种工作模式： 可以得出这样的结论: ET 模式仅当状态发生变化的时候才获得通知,这里所谓的状态的变化并不包括缓冲区中还有未处理的数据, 也就是说,如果要采用 ET 模式,需要一直 read/write 直到出错为止，很多人反映为什么采用 ET 模式只接收了一部分数据就再也得不到通知了,大多因为这样；而 LT 模式是只要有数据没有处理就会一直通知下去的。 那么究竟如何来使用 epoll 呢？其实非常简单。 通过在包含一个头文件 #include &lt;sys/epoll.h&gt; 以及几个简单的 API 将可以大大的提高你的网络服务器的支持人数。 首先通过 create_epoll(int maxfds) 来创建一个 epoll 的句柄，其中 maxfds 为你 epoll 所支持的最大句柄数。这个函数会返回一个新的 epoll句柄，之后的所有操作将通过这个句柄来进行操作。在用完之后，记得用 close() 来关闭这个创建出来的 epoll 句柄。 之后在你的网络主循环里面，每一帧的调用 epoll_wait(int epfd, epoll_event events, int max events, int timeout) 来查询所有的网络接口，看哪一个可以读，哪一个可以写了。基本的语法为： nfds = epoll_wait(kdpfd, events, maxevents, -1); 其中 kdpfd 为用 epoll_create 创建之后的句柄，events 是一个 epoll_event* 的指针，当 epoll_wait 这个函数操作成功之后，epoll_events 里面将储存所有的读写事件。max_events 是当前需要监听的所有 socket 句柄数。最后一个 timeout 是 epoll_wait 的超时，为 0 的时候表示马上返回，为 -1 的时候表示一直等下去，直到有事件范围，为任意正整数的时候表示等这么长的时间，如果一直没有事件，则范围。一般如果网络主循环是单独的线程的话，可以用 -1 来等，这样可以保证一些效率，如果是和主逻辑在同一个线程的话，则可以用 0 来保证主循环的效率。epoll_wait 范围之后应该是一个循环，遍利所有的事件。 几乎所有的 epoll 程序都使用下面的框架： for( ; ; ) { nfds = epoll_wait(epfd,events,20,500); for(i=0;i&lt;nfds;++i) { if(events[i].data.fd==listenfd) //有新的连接 { connfd = accept(listenfd,(sockaddr *)&amp;clientaddr, &amp;clilen); //accept这个连接 ev.data.fd=connfd; ev.events=EPOLLIN|EPOLLET; epoll_ctl(epfd,EPOLL_CTL_ADD,connfd,&amp;ev); //将新的fd添加到epoll的监听队列中 } else if( events[i].events&amp;EPOLLIN ) //接收到数据，读socket { n = read(sockfd, line, MAXLINE)) &lt; 0 //读 ev.data.ptr = md; //md为自定义类型，添加数据 ev.events=EPOLLOUT|EPOLLET; epoll_ctl(epfd,EPOLL_CTL_MOD,sockfd,&amp;ev);//修改标识符，等待下一个循环时发送数据，异步处理的精髓 } else if(events[i].events&amp;EPOLLOUT) //有数据待发送，写socket { struct myepoll_data* md = (myepoll_data*)events[i].data.ptr; //取数据 sockfd = md-&gt;fd; send( sockfd, md-&gt;ptr, strlen((char*)md-&gt;ptr), 0 ); //发送数据 ev.data.fd=sockfd; ev.events=EPOLLIN|EPOLLET; epoll_ctl(epfd,EPOLL_CTL_MOD,sockfd,&amp;ev); //修改标识符，等待下一个循环时接收数据 } else { //其他的处理 } } } ","link":"https://faded.auspicious.space/post/why-nginx-outperform-apache/"},{"title":"TCP——三次握手和四次挥手","content":" TCP 三次握手 和 四次挥手 概述 我们都知道 TCP 是 可靠的数据传输协议，UDP 是不可靠传输，那么 TCP 它是怎么保证可靠传输的呢？那我们就不得不提 TCP 的三次握手和四次挥手。 三次握手 下图为三次握手的流程图： 下面通过我们 Wireshark 抓包工具来分析三次握手： 第一次握手 建立连接。客户端发送连接请求报文段，将 SYN 位置为 1，Sequence Number 为 x；（x 是随机生成的一个 int 数值）然后，客户端进入 SYN_SEND 状态，等待服务器的确认； 第二次握手 服务器收到 SYN 报文段。服务器收到客户端的 SYN 报文段，需要对这个 SYN 报文段进行确认，设置 Acknowledgment Number 为 x+1(Sequence Number+1)；同时，自己自己还要发送 SYN 请求信息，将 SYN 位置为1，Sequence Number 为 y （y 是随机生存的一个 int 数值）；服务器端将上述所有信息放到一个报文段（即 SYN + ACK 报文段）中，一并发送给客户端，此时服务器进入 SYN_RECV 状态； 第三次握手 客户端收到服务器的 SYN + ACK 报文段。然后将 Acknowledgment Number 设置为 y + 1，向服务器发送 ACK 报文段，这个报文段发送完毕以后，客户端和服务器端都进入 ESTABLISHED 状态，完成 TCP 三次握手。 四次挥手 第一次挥手 Client（可以使客户端，也可以是服务器端），设置 Sequence Number 和 Acknowledgment Number，向 Server 发送一个 FIN 报文段；此时，Client 进入 FIN_WAIT_1 状态；这表示 Client 没有数据要发送给 Server 了； 客户端发送第一次挥手后，就不能在向服务端发送数据了。 第二次挥手 Server 收到了 Client 发送的 FIN 报文段，向 Client 回一个 ACK 报文段，Acknowledgment Number 为 Sequence Number 加 1；Client 进入 FIN_WAIT_2 状态；Server 告诉 Client ，我“同意”你的关闭请求； Server 第一次响应后，还可以继续向 Client 发送数据，这里只是告诉 Client ，我收到你发送的关闭请求。 第三次挥手 Server 向 Client 发送 FIN 报文段，请求关闭连接，同时 Server 进入 CLOSE_WAIT 状态； 当 Server 的数据响应完成后，再告诉 Client，我这边也可以关闭请求了， 这时 Server 就不能再向 Client 发送数据了。 第四次挥手 Client 收到 Server 发送的 FIN 报文段，向 Server 发送 ACK 报文段，然后 Client 进入 TIME_WAIT 状态；Server 收到 Client 的 ACK 报文段以后，就关闭连接；此时，Client 等待 2 MSL 后依然没有收到回复，则证明 Server 端已正常关闭，那好，Client 也可以关闭连接了。 什么是 MSL MSL 是 Maximum Segment Lifetime 英文的缩写，中文可以译为“报文最大生存时间”，他是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。因为 TCP 报文（segment）是 IP 数据报（datagram）的数据部分，具体称谓请参见《数据在网络各层中的称呼》一文，而 IP 头中有一个 TTL 域，TTL 是 Time To Live 的缩写，中文可以译为“生存时间”，这个生存时间是由源主机设置初始值但不是存的具体时间，而是存储了一个 IP 数据报可以经过的最大路由数，每经过一个处理他的路由器此值就减 1，当此值为 0 则数据报将被丢弃，同时发送 ICMP 报文通知源主机。RFC 793 中规定 MSL 为 2 分钟，实际应用中常用的是 30 秒，1 分钟和 2 分钟等。 2 MSL 即两倍的 MSL，TCP 的 TIME_WAIT 状态也称为 2 MSL 等待状态，当 TCP 的一端发起主动关闭，在发出最后一个 ACK 包后，即第 3 次握手完成后发送了第四次握手的 ACK 包后就进入了 TIME_WAIT 状态，必须在此状态上停留两倍的 MSL 时间，等待 2 MSL 时间主要目的是怕最后一个 ACK 包对方没收到，那么对方在超时后将重发第三次握手的 FIN 包，主动关闭端接到重发的 FIN 包后可以再发一个 ACK 应答包。在 TIME_WAIT 状态时两端的端口不能使用，要等到 2 MSL 时间结束才可继续使用。当连接处于 2 MSL 等待阶段时任何迟到的报文段都将被丢弃。不过在实际应用中可以通过设置 SO_REUSEADDR 选项达到不必等待 2 MSL 时间结束再使用此端口。 TTL 与 MSL 是有关系的但不是简单的相等的关系，MSL 要大于等于 TTL。 为什么要三次握手？ TCP 建立连接，其实通过两次握手就可以建立连接了，为什么要三次呢？是不是多此一举呢？ 《计算机网络》中是这样说的 为了防止已失效的连接请求报文段突然又传送到了服务端，因而产生错误。在书中同时举了一个例子，如下：“已失效的连接请求报文段”的产生在这样一种情况下： Client 发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后的某个时间才到达 Server。本来这是一个早已失效的报文段。但 Server 收到此失效的连接请求报文段后，就误认为是 Client 再次发出的一个新的连接请求。于是就向 Client 发出确认报文段，同意建立连接。假设不采用“三次握手”，那么只要 Server 发出确认，新的连接就建立了。由于现在 Client 并没有发出建立连接的请求，因此不会理睬 Server 的确认，也不会向 Server 发送数据。但 Server 却以为新的运输连接已经建立，并一直等待 Client 发来数据。这样， Server 的很多资源就白白浪费掉了。采用“三次握手”的办法可以防止上述现象发生。例如刚才那种情况， Client 不会向 Server 的确认发出确认。 Server 由于收不到确认，就知道 Client 并没有要求建立连接。” 网络故障 比如，现在网络出现了故障，只能发请求数据包，而接收不到响应数据包，那么只要发送一次请求，服务器就建立请求，这样肯定也是不对的，网络请求有来有回才能完成通讯。所以三次握手是必不可少的。 为什么要四次挥手呢？ TCP 协议是一种面向连接的、可靠的、基于字节流的运输层通信协议。TCP 是全双工模式，这就意味着，当 Client 发出 FIN 报文段时，只是表示 Client 已经没有数据要发送了，Client 告诉 Server，它的数据已经全部发送完毕了；但是，这个时候 Client 还是可以接受来自 Server 的数据；当 Server 返回 ACK 报文段时，表示它已经知道 Client 没有数据发送了，但是 Server 还是可以发送数据到 Client 的；当 Server 也发送了 FIN 报文段时，这个时候就表示 Server 也没有数据要发送了，就会告诉 Client，我也没有数据要发送了，之后彼此就会愉快的中断这次 TCP 连接。如果要正确的理解四次分手的原理，就需要了解四次分手过程中的状态变化。 ","link":"https://faded.auspicious.space/post/tcp-three-handshake-and-four-waves/"},{"title":"芙蓉女儿诔","content":" 芙蓉女儿诔---注音版 芙蓉女儿诔（lěi） 贾宝玉（《红楼梦》第七十八回） 维太平不易之元，蓉桂竞芳之月，无可奈何之日，怡红院浊玉，谨以群花之蕊，冰鲛之縠（hú），沁芳之泉，枫露之茗（míng），四者虽微，聊以达诚申信，乃致祭于白帝宫中抚司秋艳芙蓉女儿之前曰： 窃思女儿自临浊世，迄今凡十有六载。其先之乡籍姓氏，湮（yān）沦（lún）而莫能考者久矣。而玉得于衾（qīn）枕栉（zhì）沐（mù）之间，栖息宴游之夕，亲昵（nì）狎（xiá）亵（xiè），相与共处者，仅五年八月有（yòu）奇（jī）。 忆女儿曩（nǎng）生之昔，其为质则金玉不足喻其贵，其为性则冰雪不足喻其洁，其为神则星日不足喻其精，其为貌则花月不足喻其色。姊妹悉慕媖（yīng）娴（xián），妪（yù）媪（ǎo）咸仰惠德。 孰料鸠（jiū）鸩（zhèn）恶（wù）其高，鹰鸷（zhì）翻遭罦（fú）罬（zhuó）；薋（cí）葹（shī）妒其臭（xiù），茝（chǎi）兰竟被芟（shān）鉏（chú）！花原自怯，岂奈狂飙（biāo）？柳本多愁，何禁骤雨？偶遭蛊（gǔ）虿（chài）之谗，遂抱膏肓（huāng）之疚（jiù）。故而樱唇红褪，韵吐呻吟；杏脸香枯，色陈顑（kǎn）颔（hàn）。诼（zhuó）谣謑（xī）诟（gòu），出自屏帏（wéi）；荆棘蓬榛（zhēn），蔓延户牖（yǒu）。岂招尤则替，实攘诟而终。既忳（tún）幽沉于不尽，复含罔屈于无穷。高标见嫉，闺（guī）帏（wéi）恨比长沙；直烈遭危，巾帼惨于羽野。自蓄辛酸，谁怜夭折？仙云既散，芳趾难寻。洲迷聚窟，何来却死之香？海失灵槎（chá），不获回生之药。 眉黛烟青，昨犹我画；指环玉冷，今倩谁温？鼎炉之剩药犹存，襟泪之余痕尚渍（zì）。镜分鸾（luán）别，愁开麝月之奁（lián）；梳化龙飞，哀折檀（tán）云之齿。委金钿（diàn）于草莽，拾翠盒（è 盍代勺中之丶）于尘埃。楼空鳷（zhī）鹊，徒悬七夕之针；带断鸳鸯，谁续五丝之缕？ 况乃金天属节，白帝司时，孤衾（qīn）有梦，空室无人。桐阶月暗，芳魂与倩影同销；蓉帐香残，娇喘共细言皆绝。连天衰草，岂独蒹（jiān）葭（jiā）；匝（zā）地悲声，无非蟋蟀。露苔晚砌，穿帘不度寒砧（zhēn）；雨荔秋垣（yuán），隔院希闻怨笛。芳名未泯（mǐn），檐前鹦鹉犹呼；艳质将亡，槛外海棠预老。捉迷屏后，莲瓣无声；斗草庭前，兰芳枉（wǎng）待。抛残绣线，银笺（jiān）彩缕谁裁？摺（zhé）断冰丝，金斗御香未熨。 昨承严命，既趋车而远涉芳园；今犯慈威，复拄杖而近抛孤柩（jiù）。及闻槥（huì）棺被燹（xiǎn），惭违共穴之盟；石椁（guǒ）成灾，愧迨（dài）同灰之诮（qiào）。 尔乃西风古寺，淹滞青燐（lín），落日荒丘，零星白骨。楸（qiū）榆飒（sà）飒（sà），蓬艾萧萧。隔雾圹（kuàng）以啼猿，绕烟塍（chéng）而泣鬼。自为红绡（xiāo）帐里，公子情深；始信黄土陇（lǒng）中，女儿命薄！汝南泪血，斑斑洒向西风；梓（zǐ）泽馀（yú）衷，默默诉凭冷月。 呜呼！固鬼蜮（yù）之为灾，岂神灵而亦妒。箝诐（bì）奴之口，讨岂从宽？剖（pōu）悍妇之心，忿犹未释！在君之尘缘虽浅，然玉之鄙意岂终。因蓄惓（quán）惓（quán）之思，不禁谆（zhūn）谆（zhūn）之问。 始知上帝垂旌（jīng），花宫待诏（zhào），生侪（chái）兰蕙（huì），死辖芙蓉。听小婢（bì）之言，似涉无稽（jī）；据浊玉之思，则深为有据。何也？昔叶法善摄魂以撰碑，李长吉被诏而为记，事虽殊，其理则一也。故相物以配才，苟非其人，恶乃滥乎其位？始信上帝委托权衡，可谓至洽至协，庶不负其所秉赋也。因希其不昧之灵，或陟（zhì）降（jiàng）于兹（zī），特不揣（chuǎi）鄙俗之词，有污慧听。乃歌而招之曰： 天何如是之苍苍兮，乘玉虬（qiú）以游乎穹（qióng）窿（lóng）耶？ 地何如是之茫茫兮，驾瑶（yáo）象以降乎泉壤耶？ 望伞盖之陆离兮，抑箕（jī）尾之光耶？ 列羽葆（bǎo）而为前导兮，卫危虚于傍耶？ 驱丰隆以为比从兮，望舒月以临耶？ 听车轨而伊轧兮，御鸾（luán）鹥（yī）以征耶？ 闻馥郁而薆（ài）然兮，纫（rèn）蘅杜以为纕（xiāng）耶？ 炫裙裾（jū）之烁（shuò）烁（shuò）兮，镂明月以为珰（dāng）耶？ 籍葳（wēi）蕤（ruí）而成坛畤（zhì）兮，檠（qíng）莲焰以烛兰膏耶？ 文瓟（bó）瓠（hú）以为觯（zhì）斝（jiǎ）兮，漉醽（líng）醁（lù）以浮桂醑（xǔ）耶？ 瞻云气而凝盼兮，仿佛有所觇（chān）耶？ 俯窈窕而属耳兮，恍惚有所闻耶？ 期汗漫而无夭阏（yān）兮，忍捐弃余于尘埃耶？ 倩风廉之为余驱车兮，冀联辔（pèi）而携归耶？ 余中心为之慨然兮，徒噭（jiào）噭（jiào）而何为耶？ 君偃（yǎn）然而长寝（qǐn）兮，岂天运之变于斯耶？ 既窀（zhūn）穸（xī）且安稳兮，反其真而复奚（xī）化耶？ 余犹桎（zhì）梏（gù）而悬附兮，灵格余以嗟（jiē）来耶？ 来兮止兮，君其来耶！ 若夫鸿蒙而居，寂静以处，虽临于兹，余亦莫睹。搴（qiān）烟萝而为步幛（zhàng），列枪蒲而森行伍。警柳眼之贪眠，释莲心之味苦。素女约于桂岩，宓（fú）妃迎于兰渚（zhǔ）。弄玉吹笙（shēng），寒簧击敔（yǔ）。征嵩（sōng）岳之妃，启骊（lí）山之姥（mǔ）。龟呈洛浦之灵，兽作咸池之舞。潜赤水兮龙吟，集珠林兮凤翥（zhù）。爰（yuán）格爰（yuán）诚，匪簠（fǔ）匪筥（jǔ）。发轫（rèn）乎霞城，返旌（jīng）乎玄圃。既显微而若通，复氤（yīn）氲（yūn）而倏阻。离合兮烟云，空蒙兮雾雨。尘霾（mái）敛兮星高，溪山丽兮月午。何心意之忡忡，若寤（wù）寐（mèi）之栩栩？余乃欷（xī）歔（xū）怅望，泣涕彷徨。人语兮寂历，天籁兮篔（yún）筜（dāng）。鸟惊散而飞，鱼唼（shà）喋（zhá）以响。志哀兮是祷（dǎo），成礼兮期祥。呜呼哀哉！尚飨（xiǎng）！ ","link":"https://faded.auspicious.space/post/hibiscus-girl-dirge/"},{"title":"在网页上使用 jpg、png、gif 和 svg","content":" 在网页上使用JPG、PNG和SVG：新手指南 | Cheesecake Labs 如今，图像已经成为网络不可或缺的一部分。但情况并非一贯如此。直到 1993 年，Mosaic 浏览器才在网页内容中中加入图像。有些图像格式像 GIF 和 JPEG 当时已经存在，而 PNG 和 SVG 直到 90 年代才出现。图像用途多样，如：显示图片、品牌、插图、图表以及许多其他内容。 由于多样的使用情况和图片格式，有时选择正确的格式可能会令人困惑。标志应该是 SVG 还是 PNG？截图呢？JPEG 还是 PNG？在不生成过大文件的前提下，文件质量能有多高？了解每个图像格式如何工作还有它们各自的利弊可以帮助回答这些问题。 在过去几年中，通过数字设计和前端开发，研究和测试不同的工具帮助我澄清了这些问题。在本文中，我将展示一下每种格式如何工作、它们在哪个方面存在优势，以及在网页使用时的压缩与保存方法。 JPEG JEPG 由联合图像专家小组（Joint Photographic Experts Group）于 1992 年创建，并以创建者命名。JPEG 是一种有损光栅图像格式，这意味着每次保存被压缩的 JPEG 时，一些信息将被不可逆转地丢失。 JPEG 利用人眼感知的缺陷——对亮度比对颜色更敏感——使用了一种压缩算法来丢弃我们不太擅长获取的信息，因此被命名为“有损格式”。应用于给定图像的压缩量将直接与所得文件的质量和大小相关。 JPEG 压缩的技术方面远远不止这些，如果你想更深入，请查看这篇文章 作者：大卫·奥斯丁（David Austin）。 JPEG 的用途 因为 JPEG 适用于亮度和色彩压缩，所以在照片，以及其他写实或者带阴影的图像（如绘画和 3D 渲染）上使用效果良好。这就是为什么它是多年来最流行的存储图片的格式。出于同样的原因，JPEG 不适宜用在矢量图片，如徽标，几何图形，截图等方面。 照片，以及复杂的或带阴影的图像，如绘画，是使用 JPEG 的很好的例子。 压缩 JPEG 作为有损格式，JPEG 文件的压缩级别与最终图像质量直接相反。在像 Photoshop 这样的工具中保存 JPEG 时，你会看到一个从 0 到 100 的质量设置。Photoshop 设置了一些图像质量范围： 低 — 10% 中 — 30% 高 — 60% 非常高 — 80% 最佳 — 100% 最佳 100% (61 KB)，非常高 80% (29 KB)。 高 60% (16 KB)，中 30% (7 KB)。 低 10% (6 KB)，最低 0% (3 KB)。 网络建议使用在 50％ 到 60％ 质量之间的 JPEG，因为它能保证不错的图像质量和较小的文件尺寸。从 JPEG 资源中删除元数据也可以减少文件大小。还有如 TinyJPG 的在线工具，以及桌面应用程序如 ImageOptim (Mac) 和 RIOT (Windows)都可以用来压缩图片。在 Photoshop 里，可以通过在“导出”中选择“元数据：无”或“存储为 Web 所用格式（旧版）”来完成压缩。模糊图像或图像部分区域也会产生较小的文件可在此处查看。 请注意，由于 JPEG 的有损方式，即使以 100％ 的质量保存相同的文件，因为压缩算法会在同一图像上一次又一次地应用，多次之后也会导致图像质量的降低。但这一变化可能不会显示在文件大小的改变上。 PNG 可移植网络图形（Portable Network Graphics）也是一种自 1995 年以来就一直存在的光栅图像格式。它与 JPEG 不同，因为它是一种无损格式，并且是目前网络上最常见的无损格式。这意味着由于它的压缩算法，当文件被保存和压缩时，不会丢失任何信息。 PNG 有很多很酷的特性，如： Alpha 透明度——意味着每个像素可以具有不同的透明度； 8 位文件可以使用基于调色板的颜色模型（也称为索引颜色）——这意味着如果减少颜色数量，文件可能更小； 依据 libPNG 的说法，PNG 压缩效率比 GIF 高 25％； 二维隔行扫描——图像会在加载过程中逐步显现，而不是只有当图像完全加载时，才能显示图像。必须谨慎使用此选项，因为它会增加文件大小。 有关 PNG 更多特性、历史和技术信息的完整列表，请查看 libpng 的页面。 PNG 的用途 PNG 对于线条图，徽标，图标和只有几种颜色的图像非常好用。另一方面，用在大量颜色的照片和图像时，将生成巨大的文件。PNG 的另一个好用的地方是透明背景。在这种情况下，即使是复杂的图片仍然可以使用 PNG，因为 JPEG 中不存在透明度功能。 PNG 可以很好地用在线条作品，徽标和图标上。 (漫画作者：xkcd) 压缩 PNG 因为 PNG 中的压缩算法是无损的，你可以选择性地减少它的颜色，从而通过外部工具减小图片尺寸。 Pngquant 就是一个很好的工具，它可以在保持 Alpha 水平的同时减少文件大小。请注意，这一过程会创建一个 8 位文件，即该文件最多可以有 256 种颜色。可能看起来不多，但是用这么多颜色足以获得很好的效果。 左边的 24 位图像 (149 KB) 和右边 8 位，256 色图像 (54 KB)——缩小了 63.7% 对于大多数 PNG 使用场景（线图，图形，图标），256色是足够的。因此，可以通过减少调色板中的颜色数量来进一步减少文件大小。 使用 GUI 工具是个不错的选择，如 Pngyu 或 ImageAlpha，这些工具允许你预览生成的文件。 下面的例子显示了如何在不会显著影响质量的前提下，将调色板减少到 32 种颜色。在类似的例子中，图像很难被自动化地压缩——因为需要不断预览和测试来达到最佳效果——同时使用最少的颜色和产生最小的文件尺寸。就像 JPEG 一样，也有用于压缩 PNG 的在线工具，如：TinyPNG。 在这个示例中，徽标可以从原始的 24 位 PNG（10 KB）减少到 8 位，32 色版本（2 KB，缩减 80％），并且没有丢失任何明显的细节。 GIF 图形交换格式（Graphics Interchange Format）也是一种位图格式，并且比本文中提到的其它格式都出现地更早。它在 1989 年由 Steve Wilhite 创建, 它在 PNG 创建前都是最流行的 8 位图像格式。GIF 与 PNG 具有类似的特性，但有一些缺点： 仅支持 256 种颜色； 一维交错——图像边加载边现实，但不够平滑； 与 PNG 相比压缩性差； “非此即彼”透明度——像素只能是 100％ 透明或 100％ 可见； 有歧义的发音？ SVG 可伸缩矢量图形（Scalable Vector Graphics）与前面的两个栅格格式不同，顾名思义，它是矢量格式。这意味着它不会存储基于像素的数据，而是协调生成图形的信息。 SVG 矢量使用带有标签的基于 XML 的结构，就像 HTML 一样。由于此标签结构，你可以通过 ID 识别 SVG 元素，并操纵它们。这带来了很多可能性，例如使用 JavaScript 和 CSS 修改和动画化元素或创建响应式图形。 请看这个例子：#1 – 咖啡机 – CSS3制作SVG动画 ，作者乔纳森·席尔瓦（Jonathan Silva）(@jonathansilva) 发表于 CodePen。 就像其它矢量格式，SVG 图片能不丢失任何细节或像素化地放大到任何大小。打个比方，同一个图标，可以以多种尺寸使用，并且在任何屏幕分辨率（比如 Retina 显示器）中都将看起来很清晰，而不需要存成多个文件。 矢量图片（右）能够在保持图片的质量的前提下任意放大。 SVG 的用途 SVG 在线条艺术，标志，图标，插画和数据可视化方面发挥出色。但它不适用于写实图像和有许多细节的复杂图片。在一些情况下，SVG 和 PNG 都能很好地达到同一个目的。对于线条艺术，SVG 通常能生成较小的文件。但是这不是必然的，实际情况会根据矢量图像究竟有多少个锚点，它甚至可能会生成比 PNG 更大的文件。 SVG 真正出色的地方是数据可视化。由于可以使用 JavaScript 来操纵和创建矢量动画，诸如 D3 之类的库提供了无限的可能性。 徽标，图标和数据可视化是 SVG 使用的优秀范例。 压缩 SVG 大多数情况下，在网页上使用工具如 SVGz（GZipped SVG）来压缩 SVG 文件是不必要的。你可以（并且应该）在服务器上应用此 Gzipping，但也是多此一举。能做的应该是通过清除 SVG 文件里矢量图形中不必要的锚点、元素和属性来减少文件大小。锚点绘制了矢量图像，因此，你需要确保已移除的锚点不会影响矢量图形的最终形状。如果您使用 Adobe Illustrator 编辑 SVG，请确保使用 导出&gt;导出为... 而不是 文件&gt;另存为... 进行保存，因为这样才能生成一个最小化的文件，其它优点。在 Sketch 里, 注意不要使用不必要的文件夹，因为它们也会作为额外的标签被 SVG 保存。 清理不必要的节点是缩减 SVG 尺寸的一种途径。 元素是包含在 SVG 文件内的所有内容，包括开始和结束标签。矢量编辑软件，如 Adobe Illustrator 和 Sketch 可能会到处含有非必要元素和属性的 SVG。SVG 压缩器可用于删除这种多余的信息。Compressor 和 SVGOMG 等在线工具可以完成此工作。如果你是开发人员，而且不习惯清理和压缩 SVG，可以用自动执行工具 SVGO，如果你是设计师，请与该项目的开发人员谈谈 SVG 的最小化，以避免进行将这一项可以轻松自动化的工作手动完成。 在下面的例子里，这个从 Sketch 里导出的图标有 1,364 B。同一个图标在清理和压缩后，就只剩 460 B——缩小了 66%。 请看这个例子：来自 Sketch 的 SVG 作者布鲁诺·穆勒（Bruno Müller）(@brunomuler) on CodePen。 对比：优化后的 SVG。 文末思考 如同任何其他技术一般，图像格式也在不断发展。作为网页设计师和开发人员，我们的主要限制是浏览器支持。几年前，在 IE6 为主流浏览器的时代，PNG 还不能使用 Alpha 透明度。在不久的将来，也许我们会使用新的格式，如 Google's Webp 或者其它仍未被创建出来的图片格式。 了解如何使用和优化每种图片格式将确保更好的用户体验。因为用户将能够更早地预览和接收内容，减少带宽的使用。它还将为设计人员提供了动画和响应式页面的新机会。 我希望这篇文章有助于澄清一些网络上关于图像格式的诸多不确定性的问题。如果你还有任何问题或建议，请在下方发表评论或与我联系。另外，如果觉得本文对你有帮助，不要忘了分享。 ","link":"https://faded.auspicious.space/post/jpg-png-gif-and-svg-on-the-web-a-beginner-guide/"},{"title":"进程间通信—IPC","content":" 看图理解进程间通信IPC 什么是进程间通讯 进程间通信（Inter-Process Communication 或 Interprocess Communication，简写 IPC）是指两个或两个以上进程（或线程）之间进行数据或信号交互的技术方案。 通常，IPC 一般包含客户端和服务器，客户端请求数据，服务器响应请求（比如分布式计算中就是这样）。 有哪些 IPC 方法 IPC 方法适用的环境 IPC 方法 操作系统或环境 文件（File） 多数操作系统 信号（Singal） 多数操作系统 套接字（Socket） 多数操作系统 Unix 域套接字（Unix domain socket） 所有 POSIX 操作系统 消息队列（Message queue） 多数操作系统 管道（Pipe） 所有 POSIX 系统，Windows 命名管道（Named pipe 或 FIFO） 所有 POSIX 系统，Windows，Amiga OS 2.0+ 共享内存（Shared memory） 所有 POSIX 系统，Windows 消息传递（Message passing） 用于 RPC、RMI、MPI 规范、Java RMI、CORBA、DDS、MSMQ、MailSlots、QNX 等 内存映射文件（Memory-mapped file） 所有 POSIX 系统，Windows 文件（File） 存储在磁盘上的记录，或由文件服务器按需合成的记录，可以由多个进程访问。 信号（Signal） 系统消息从一个进程发送到另一个进程，一般不用于传输数据，而是用于远程传输命令。 套接字（Socket） 通过网络接口将数据量发送到本机的不同进程或远程计算机。 Unix域套接字（Unix domain socket） 用于在同一台机器上运行的进程之间的通信。虽然因特网域套接字可用于同一目的，但 Unix 域套接字的效率更高。Unix 域套接字仅仅复制数据；它们并不执行协议处理，不需要添加或删除网络报头，无需计算检验和，不要产生顺序号，无需发送确认报文。 消息队列（Message queue） 类似于套接字的数据流，但消息有自己的结构，它允许多个进程只需要读写消息队列，而不需要直接相互连接。 管道（Pipe） 管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。 命名管道（Named pipe 或 FIFO） 命名管道可在同一台计算机的不同进程之间或在跨越一个网络的不同计算机的不同进程之间，支持可靠的、单向或双向的数据通信。 共享内存（Shared memory） 允许多个进程访问同一个内存块，该内存块作为一个共享缓冲区，供进程间相互通信。 消息传递（Message passing） 一般在并发模型中，允许多个程序使用消息队列或者托管通道通信。 内存映射文件（Memory-mapped file） 类似于标准的文件，内存映射文件映射到 RAM，可以直接对内存地址进行更改，而不是更改输出流。 ","link":"https://faded.auspicious.space/post/inter-process-communication/"},{"title":"MBR、开机流程与主引导分区","content":" MBR、开机流程与主引导分区 预备知识 CMOS：主板上记录了硬件参数的储存器。 BIOS：写入到主板 ROM 中的程序，开机时执行的第一个程序。 MBR：可引导设备的第一个扇区（一般情况下是硬盘中的第一扇区）中的主引导分区。 Boot loader：读取操作系统内核文件来执行的程序。 开始 学过计算机的肯定都知道操作系统是一个软件，我们平时用的软件都是基于操作系统提供的接口运行的。那么操作系统又是怎么运行的呢？ Step 1: BIOS 在预备知识中我们就知道了，开机运行的第一个程序就是 BIOS，那么开机引导操作系统的切入点也肯定是它了。 BIOS 识别 CMOS 读取硬件信息，从中找出可开机设备，一般自然是硬盘了。 当然，可开机设备不一定是硬盘，或者说你有两个硬盘，这里就不做讨论了。 Step 2: MBR &amp; Boot loader BIOS 找到第一个扇区之后就查找 MBR 的位置，这是最基本的引导程序（Boot loader），这个程序一旦启动，BIOS 的任务才算圆满。 MBR 中的引导程序运行，这个时候用户可以对开机选项进行操作，例如转交引导加载的任务给其他引导程序（之后还会提到）。 Step 3: Operating System 什么都做完了，当然该操作系统登场，负责提供其基本功能。 补充 磁盘分区表 Step 2 中提到了第一个扇区。实际上，这个扇区不止包含 MBR，它还包含了一个分区表，用来对硬盘进行分割，文件系统的最小单位是柱面，所以它是以记录柱面号来分割硬盘的。 例如第一分区是 1~100 柱面，那么分区记录项第一个的内容就是 1 和 100，其他以此类推。 我们所谓的&quot;分区&quot;实际上就是对这个分区表的记录进行修改。 由于分区表只有 64 bytes，所以最多只能有 4 个分区，这四个分区被称为主（Primary）分区。 那么你肯定要有疑问：分区可不一定只有 4 个啊。 是的，这种情况下就有一个相对于主引导分区的概念——扩展（Extended）分区。 如果我们拿主分区的其中一个存放另外一个分区表储存更多的分区信息，那我们就可以拥有更多分区。不是么。有时间理解一下下图吧。 这里需要注意的地方是，如果有 4 个主分区，我们就再也没有办法存放另外的分区信息，所以一般情况下会留下一个分区存放扩展分区信息，主分区因此最多只有 3 个。 扩展分区只能有一个（操作系统限制），扩展分区持续切割，形成新的分区，这就是逻辑分区（logical partition）。我们平时能作为数据访问的分区是主分区和逻辑分区，扩展分区不能格式化。 多重引导 这里需要说明一下，每个分区都拥有自己的启动扇区，可以用来存放引导程序，并且该引导程序可以将管理权交给另一引导程序（其他分区的引导扇区）或者自己引导所在的分区，是的，可开机的内核文件不是在引导扇区内，而是在各分区内。 意思就是，你可以引导不止一种操作系统，根据你在 boot menu 的选择，可以引导任何在 MBR 指向中的系统。 Linux 安装的时候可以选择安装在分区的启动扇区，或者 MBR，Linux 的 Loader 可以手动转换引导程序。 Windows 会覆盖掉 MBR 和自己所在的分区。你没有办法保留之前 MBR 中对 Linux 引导程序的指向。 上述两个原因表明了为何需要先安装 Windows 操作系统，再安装 Linux，否则将不会在开机的时候看到 Linux 引导选项。 GUID 分区方案 相对于 MBR 分区方案，GUID有以下优点： 支持 2TB 以上的大硬盘。 每个磁盘的分区个数几乎没有限制。为什么说“几乎”呢？是因为 Windows 系统最多只允许划分 128 个分区。不过也完全够用了。 分区大小几乎没有限制。又是一个“几乎”。因为它用 64 位的整数表示扇区号，即 0 ~18,446,744,073,709,551,616。夸张一点说，一个 64 位整数能代表的分区大小已经是个“天文数字”了，若干年内你都无法见到这样大小的硬盘，更不用说分区了。 分区表自带备份。在磁盘的首尾部分分别保存了一份相同的分区表。其中一份被破坏后，可以通过另一份恢复。 每个分区可以有一个名称（不同于卷标）。 GUID 扩展了分区表头，并且兼容了 MBR（第一扇区还是留有 MBR 的空间，为了兼容不支持 GUID 的硬盘），分区信息存放于分区表中，由 GUID HEADER 中的信息标识引导程序 efi 所在的分区。 开机过程（以下是我个人的理解，欢迎指正）： BIOS 过程同 MBR 分区方案一致。 之后在 GUID HEADER 中查找 .efi 引导程序。 主引导程序扫描所有分区的引导扇区并运行其中的分区引导程序。 分区引导程序读取分区内容并引导操作系统。 ","link":"https://faded.auspicious.space/post/mbr-and-boot-loader/"},{"title":"如何优雅地链式取值","content":" 如何优雅地链式取值 开发中，链式取值是非常正常的操作，如： res.data.goods.list[0].price 但是对于这种操作报出类似于 Uncaught TypeError: Cannot read property 'goods' of undefined 这种错误也是再正常不过了，如果说是 res 数据是自己定义，那么可控性会大一些，但是如果这些数据来自于不同端（如前后端），那么这种数据对于我们来说我们都是不可控的，因此为了保证程序能够正常运行下去，我们需要对此校验： if (res.data.goods.list[0] &amp;&amp; res.data.goods.list[0].price) { // your code } 如果再精细一点，对于所有都进行校验的话，就会像这样： if (res &amp;&amp; res.data &amp;&amp; res.data.goods &amp;&amp; res.data.goods.list &amp;&amp; res.data.goods.list[0] &amp;&amp; res.data.goods.list[0].price){ // your code } 不敢想象，如果数据的层级再深一点会怎样，这种实现实在是非常不优雅，那么如果优雅地来实现链式取值呢？ optional chaining 这是一个出于 stage 2 的 ecma 新语法，目前已经有了 babel 的插件 babel-plugin-transform-optional-chaining，这种语法在 swift 中有，可以看下官方给的实例： a?.b // undefined if `a` is null/undefined, `a.b` otherwise. a == null ? undefined : a.b a?.[x] // undefined if `a` is null/undefined, `a[x]` otherwise. a == null ? undefined : a[x] a?.b() // undefined if `a` is null/undefined a == null ? undefined : a.b() // throws a TypeError if `a.b` is not a function // otherwise, evaluates to `a.b()` a?.() // undefined if `a` is null/undefined a == null ? undefined : a() // throws a TypeError if `a` is neither null/undefined, nor a function // invokes the function `a` otherwise 通过函数解析字符串 我们可以通过函数解析字符串来解决这个问题，这种实现就是 lodash 的 _.get 方法。 var object = { a: [{ b: { c: 3 } }] }; var result = _.get(object, 'a[0].b.c', 1); console.log(result); // output: 3 实现起来也非常简单，只是简单的字符串解析而已： function get (obj, props, def) { if((obj == null) || obj == null || typeof props !== 'string') return def; const temp = props.split('.'); const fieldArr = [].concat(temp); temp.forEach((e, i) =&gt; { if(/^(\\w+)\\[(\\w+)\\]$/.test(e)) { const matchs = e.match(/^(\\w+)\\[(\\w+)\\]$/); const field1 = matchs[1]; const field2 = matchs[2]; const index = fieldArr.indexOf(e); fieldArr.splice(index, 1, field1, field2); } }) return fieldArr.reduce((pre, cur) =&gt; { const target = pre[cur] || def; if(target instanceof Array) { return [].concat(target); } if(target instanceof Object) { return Object.assign({}, target) } return target; }, obj) } var c = {a: {b : [1,2,3] }} get(c ,'a.b') // [1,2,3] get(c, 'a.b[1]') // 2 get(c, 'a.d', 12) // 12 使用解构赋值 这个思路是来自 github 上 You-Dont-Need-Lodash-Underscore 这个仓库，看到这个的时候真的佩服。 const c = {a:{b: [1,2,3,4]}} const { a: result } = c; // result : {b: [1,2,3,4]} const {a: { c: result = 12 }} = c // result: 12 当然，这个时候为了保证不报 uncaught Typeerror，我们仍然需要定义默认值， 就像这样, 貌似如果不加 lint 可读性堪忧。 const {a: {c: {d: result2} = {}}} = c 使用 Proxy 这个是组内同事提到的，一个简单实现如下： function pointer(obj, path = []) { return new Proxy(() =&gt; {}, { get (target, property) { return pointer(obj, path.concat(property)) }, apply (target, self, args) { let val = obj; let parent; for(let i = 0; i &lt; path.length; i++) { if(val === null || val === undefined) break; parent = val; val = val[path[i]] } if(val === null || val === undefined) { val = args[0] } return val; } }) } 我们可以这样使用： let c = {a: {b: [1, ,2 ,3]}} pointer(c).a(); // {b: [1,2,3]} pointer(c).a.b(); // [1,2,3] pointer(d).a.b.d('default value'); // default value 这差不多就是心中所谓的优雅了。 综上，在实际工作中，使用方法四会是最优雅，可读性也非常强，但考虑到浏览器的话，可能方法二会更加常用，当然，如果你所要取的值层级不是太深，你组内的同事要严格的 lint，方法三也不失为一种好的选择。 ","link":"https://faded.auspicious.space/post/how-to-elegantly-optional-chaining/"},{"title":"& * # 这三个是什么符号？","content":"&amp; 这个符号的名字是 ampersand，含义就是“and”。听到这个词，要意识到老师在说啥。 * 这个符号的名是 asterisk，一种符号，在文章中一般表示，底下有脚注。 # 这个符号的名字是 hash，是不是很难受的感觉，在英文里的意思是表示数字，比如我家住 18 号 401，写地址可以写成 #18, 401，另外在美国还用于表示磅这个重量单位，比如 2# of sugar，大概两斤糖； ～ 这个符号的名字最难找，叫 tilde，从西班牙语或者葡萄牙语过来的名字，参看维基百科； ` 这个符号，我一般念做 back tick，不过我也没查到，我看百科里叫 back quote； ^ 这个符号，叫 caret，在 ASCII 里面是一个Space Symbol。 ","link":"https://faded.auspicious.space/post/names-of-three-sign/"},{"title":"TCP——套接口编程","content":" 值得收藏的TCP套接口编程文章 TCP 客户端-服务器典型事件 下图是 TCP 客户端与服务器之间交互的一系列典型事件时间表： 首先启动服务器，等待客户端连接。 启动客户端，连接到服务器。 客户端发送一个请求给服务器，服务器处理请求，响应客户端。 循环步骤3。 客户端给服务器发一个文件结束符，关闭客户端连接。 服务器也关闭连接。 套接口编程基本函数 socket 函数 为了执行网络 I/O，一个进程（无论是服务端还是客户端）必须做的第一件事情就是调用 socket 函数。 #include &lt;sys/socket.h&gt; /* basic socket definitions */ int socket(int family, int type, int protocol);/* 返回：非负描述字——成功，-1——出错 */ family——协议族 族 解释 AF_INET IPv4 协议 AF_INET6 IPv6 协议 AF_LOCAL Unix 域协议 AF_ROUTE 路由套接口 AF_KEY 密钥套接口 type——套接口类型 类型 解释 SOCK_STREAM 字节流套接口 SOCK_DGRAM 数据报套接口 SOCK_RAW 原始套接口 有效的 family 和 type 组合（简略版） AF_INET AF_INET6 SOCK_STREAM TCP TCP SOCK_DGRAM UDP UDP SOCK_RAW IPv4 IPv6 socket 函数返回一个套接口描述字，简称套接字（sockfd）。获取套接字无需指定地址，只需要指定协议族和套接口类型（如上表中的组合）。 connect 函数 TCP 客户用 connect 函数来建立一个与 TCP 服务器的连接。 #include &lt;sys/socket.h&gt; /* basic socket definitions */ int connect(int sockfd, const struct sockaddr * servaddr, socklen_t addrlen);/* 返回：0——成功，-1——出错 */ 参数 sockfd 便是 socket 函数返回的套接口描述字。 套接口地址结构 servaddr 必须包含服务器的 IP 地址和端口号。 客户端不必非要绑定一个端口（调用 bind 函数），内核会选择源 IP 和一个临时端口。 connect 函数会触发 TCP 三次握手。有可能出现下面的错误情况： 客户端未收到 SYN 分节的响应： 第一次发出未收到，间隔 6s 再发一次，再没收到，隔 24s 再发一次，总共等待 75s 还没收到则返回错误（ETIMEDOUT）。可以用时间日期程序验证一下： 查看本地网络信息： JACKIELUO-MC0:intro jackieluo$ ifconfig en0: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500 ether f4:0f:24:2a:72:a6 inet6 fe80::1830:dbd:1b29:2989%en0 prefixlen 64 secured scopeid 0x6 inet 192.168.0.101 netmask 0xffffff00 broadcast 192.168.0.255 nd6 options=201&lt;PERFORMNUD,DAD&gt; media: autoselect status: active 将程序指向本地地址 192.168.0.101（确保时间日期服务器程序已运行），成功： JACKIELUO-MC0:intro jackieluo$ ./daytimetcpcli 192.168.0.101 Sat Oct 6 17:06:55 2018 将程序指向本地子网地址 192.168.0.102，其主机ID（102）不存在，等待几分钟后超时返回： JACKIELUO-MC0:intro jackieluo$ ./daytimetcpcli 192.168.0.102 connect error: Operation timed out 收到 RST： 即服务器主机在指定端口上没有等待连接的进程，这称为“hard error”，客户端一接收到 RST，马上返回错误（ECONNREFUSED）。验证： 关闭之前本机运行的 daytimetcpsrv 进程 将程序指向本地地址192.168.0.101： JACKIELUO-MC0:intro jackieluo$ ./daytimetcpcli 192.168.0.101 connect error: Connection refused 发出的 SYN 在路由器上引发了目的不可达 ICMP 错误： 这个错误被称为“soft error”，最终返回 EHOSTUNREACH 或者 ENETUNREACH。 bind 函数 函数 bind 为套接口分配一个本地协议地址，包括 IP 地址和端口号。 #include &lt;sys/socket.h&gt; /* basic socket definitions */ int bind(int sockfd, const struct sockaddr * servaddr, socklen_t addrlen);/* 返回：0——成功，-1——出错 */ 客户端可以不调用这个函数，由内核选择一个本地 IP 的临时端口就好。 服务器一般都会调用 bind 函数绑定 IP 地址和端口，供客户端调用。一个例外是 RPC（远程过程调用）服务器，它由内核为其选择临时端口。然后通过 RPC 端口映射器进行注册，客户端与该服务器连接之前，先通过端口映射器获取服务器的端口。 进程可以把一个特定的 IP 地址捆绑到它的套接口上。对于客户端，它发送的请求，源IP地址就是这个地址；对于服务器，如果绑定了 IP 地址，则只接受目的地为此 IP 地址的客户连接。 如果服务器不把 IP 地址绑定到套接口上，那么内核把客户端发送 SYN 所在分组的目的 IP 地址作为服务器的源 IP 地址。（即服务器收到 SYN 的 IP） 给函数 bind 指定用于捆绑的 IP 地址和 / 或端口号的结果： IP地址 端口 结果 0 内核选择 IP 地址和端口 非 0 内核选择 IP 地址，进程指定端口 本地 IP 地址 0 进程选择 IP 地址，内核指定端口 本地 IP 地址 非 0 进程选择 IP 地址和端口 listen 函数 函数 listen 仅被 TCP 服务器调用。 #include &lt;sys/socket.h&gt; /* basic socket definitions */ int listen(int sockfd, int backlog);/* 返回：0——成功，-1——出错 */ 调用函数 socket 函数创建的套接口，默认是主动方，下一步应是调用 connect，CLOSED 的下一个状态是 SYN_SENT（见 TCP 状态转换图）。而函数 listen 将套接口转换成被动方，告诉内核，应接受指向此套接口的连接请求，CLOSED 状态变成 LISTEN。 函数 listen 的第二个参数 backlog 表示内核为此套接口排队的最大连接数。对于给定的监听套接口，内核会维护两个队列： 未完成连接队列（incomplete connection queue）SYN 分节已由客户发出，到达服务器，正在进行 TCP 的三路握手。此时这些套接口处于 SYN_RCVD 状态。 已完成连接队列（completed connection queue）SYN 分节已由客户发出，到达服务器，并且已完成三路握手。此时这些套接口处于 ESTABLISHED 状态。 当来自客户的 SYN 到达时，TCP 在未完成连接队列中创建一个新条目，直到三路握手中，第三个分节（客户对服务 SYN 的 ACK）到达，这个条目移到已完成连接队列的队尾。 当进程调用 accept 函数时，已完成连接队列的头部条目返回给进程。 两个队列之和不能超过 backlog。 当一个客户 SYN 到达时，若这两个队列都是满的，TCP 就忽略此分节，且不发送 RST。客户 TCP 将重发 SYN，期望不久就能在队列中找到空闲位置。 TCP 为监听套接口维护的两个队列。 accept 函数 函数 accept 由 TCP 服务器调用，从已完成连接队列头部返回下一个已完成连接，若该队列为空，则进程睡眠（假定套接口为默认的阻塞方式）。 #include &lt;sys/socket.h&gt; /* basic socket definitions */ int accept(int sockfd, struct sockaddr *cliaddr, socklen_t *addrlen);/* 返回：非负描述字——成功，-1——出错 */ 函数 accept 的第一个参数和返回值都是套接口描述字。其中， 第一个参数，称为监听套接口描述字，即由函数 socket 返回，也用于 bind，listen 的第一个参数。 返回值，称为已连接套接口描述字。 通常一个服务器，只生成一个监听套接口描述字，直到其关闭。而内核为每个被接受的客户连接，创建一个已连接套接口，当客户连接完成时，关闭该已连接套接口。 注意到 intro/daytimetcpsrv.c 中，后两个参数传的都是空指针，这是因为我们不关注客户的身份，无需知道客户的协议地址。 connfd = Accept(listenfd, (SA *) NULL, NULL); 稍作修改，不再传入空指针，见 intro/daytimetcpsrv1.c： socklen_t len; struct sockaddr_in servaddr, cliaddr; ... connfd = Accept(listenfd, (SA *) &amp;cliaddr, &amp;len); printf(&quot;connection from %s, port %d\\n&quot;, Inet_ntop(AF_INET, &amp;cliaddr.sin_addr, buff, sizeof(buff)), ntohs(cliaddr.sin_port)); kill 掉之前的 daytimetcpsrv 进程： $ sudo lsof -i -P | grep -i &quot;listen&quot; daytimetc 80986 root 3u IPv4 0xae12d925e4528793 0t0 TCP *:13 (LISTEN) $ sudo kill -9 80986 编译运行新的服务端程序： $ make daytimetcpsrv1.c daytimetcpsrv1 $ ./daytimetcpsrv1 重复执行客户端程序，发几个请求： $ ./daytimetcpcli 127.0.0.1 Wed Sep 26 14:11:20 2018 $ ./daytimetcpcli 127.0.0.1 Wed Sep 26 14:17:06 2018 查看服务端打印： connection from 127.0.0.1, port 58201 connection from 127.0.0.1, port 58342 注意到，由于客户端程序没有调用 bind 函数，内核为它的协议地址选择了源 ip 作为 IP 地址，临时端口号也发生了变化。 fork 和 exec 函数 #include &lt;unistd.h&gt; pid_t fork(void);/* 返回：在子进程中为0，在父进程中为子进程ID，-1——出错 */ fork 函数调用一次，却返回两次。 在调用它的进程（即父进程），它返回一次，返回值是派生出来的子进程的进程 ID。 父进程可能有很多子进程，必须通过返回值跟踪记录子进程 ID。 在子进程，它还返回一次，返回值为 0。 子进程只有一个父进程，总可以通过 getppid 来得到父进程的 ID。 通过返回值可以判断当前进程是子进程还是父进程。 父进程在调用 fork 之前打开的所有描述字在函数 fork 返回后都是共享的。网络服务器会利用这一特性： 父进程调用 accept。 父进程调用 fork，已连接套接口就在父进程与子进程间共享。（一般来说就是子进程读、写已连接套接口，而父进程关闭已连接套接口）。 fork 有两个典型应用： 一个进程为自己派生一个拷贝，并发执行任务，这也是典型的并发网络服务器模型。 一个进程想执行其他的程序，于是调用 fork 生成一个拷贝，利用子进程调用 exec 来执行新的程序。典型应用是 shell。 以文件形式存储在硬盘上的可执行程序若要被执行，需要由一个现有进程调用 exec 函数。我们将调用 exec 的进程称为调用进程，新程序的进程 ID 并不改变，仍处于当前进程。 小结 客户和服务器，从调用 socket 开始，返回一个套接口描述字。客户调用 connect，服务器调用 bind、listen、accept。最后套接口由 close 关闭。 多数 TCP 服务器是调用 fork 来实现并发处理多客户请求的。多数 UDP 服务器则是迭代的。 ","link":"https://faded.auspicious.space/post/tcp-suite-programming/"},{"title":"Git——超详实教程与命令大全","content":" 超详实Git简明教程与命令大全 Git（wiki: en chs ）是一个免费开源的分布式版本控制系统，由 Linux 内核作者 Linus Torvalds 开发，大型开源项目 Linux Kernel、Android、Chromium、Mono、DotNet、UE4 等都使用 Git 管理项目著名 github 网站使用 Git 托管所有项目代码，Git 的代码也托管在 github 上，链接为：github.com/git 与集中式版本控制系统（开源软件：SVN；免费软件：CVS；商业软件：微软的 VSS、IBM 的 Rational ClearCase）相比。 Git 优点： 本地是版本库的完整镜像，因此支持离线工作； 绝大多数操作都只需要访问本地文件和资源，而且与每个提交都是所有文件的完整副本，因此速度非常快； 强大快捷的分支功能，非常适合非线性开发过程。 Git 缺点： 只能全量整体，而不能以子目录和分支为单位进行更新、提交等操作； 子目录和分支不能单独进行权限控制； 由于每个提交都是所有文件的完整副本，因此更占磁盘空间。 注：SVN 等集中式版本控制系统存储每个文件与初始化版本的差异。 注：Git 每个提交都是所有文件的完整副本，使得 Git 在回溯到某个提交时，不会对所有文件执行差异计算还原，因此速度会非常快。 这使得源代码、配置文件等更适合用 Git 来管理，而资源等较大的二进制文件则容易导致版本库体积膨胀。 在项目实践中，对于资源等较大的二进制文件可以采用 Git-LFS 来管理，UE4 则是使用自己开发的 GitDependencies 来管理。 基本概念 origin：默认远程版本库名。 master：默认分支名。 origin/master：远程默认分支名。 HEAD：当前分支顶端 Commit 的别名，即当前分支最近的一个提交的 SHA-1 哈希值。 ORIG_HEAD：上次 HEAD 指针的位置。注：当执行 git reset / git pull / git merge 命令时，git 会把老的 HEAD 拷贝到文件 .git/ORIG_HEAD 中，在后续命令中可以使用 ORIG_HEAD 引用这个提交。 commit（提交）：每个 commit 都是全部文件的完整快照，并用一个 commitID（基于文件的内容或目录结构计算出来的 40 位十六进制的 SHA-1 哈希值） 来唯一标志。从某个角度上来说，Git 维护的就是一个 commitID 有向无环图。 detached HEAD：HEAD 没有指向任何分支的状态。一般有以下几种情况会出现这种情况： 使用 checkout 命令跳到某个没有分支指着的 commit 时； rebase 处理冲突时所处的状态； 切换到某个远程分支 cache 上时。 在 Git 中，在执行命令时，一定要清楚：你在哪？对谁执行这个命令？ 本文使用 git 版本为：git version 2.13.0.windows 运行命令行建议使用：git bash（可通过右键菜单 Git Bash here 来启动），主要有3个原因： 在 Windows 的 cmd 下执行 git log 等需要显示多页内容的命令时，会导致 cmd 卡死（有时按 Q 键也没法退出）； git bash 中可以使用 MinGW 中自带的 Linux 环境下常用的命令工具； git bash 着色做得更好，利于阅读。 图解常见操作 Working Directory：即工作区。操作系统层面的目录树结构，也可以理解为一个 tree 目录对象。 Stage(Index)：即暂存区，为等待 Commit 的文件列表。是以扁平的文件清单实现的，不过从理解层面上也可以理解为 tree 目录对象。 Local Repository(History)：本地版本库。有向无环图，其每一个节点都是一个 tree 目录对象。 Remote Repository：远程版本库。有向无环图，其每一个节点都是一个 tree目录对象。 注：图中 git checkout -- &lt;file&gt; 1、2 步骤的含义是当在暂存区中有修改时，优先使用暂存区中的修改覆盖工作区。 SVN 命令对比一览 svn git 说明 svn checkout git clone 检出项目 svn update git fetchgit pull 更新 svn commit git commitgit push 提交 svn add git add 添加 svn mv git mv 移动 svn rm git rm 删除 svn status git status 查看状态 svn log git log 查看 log svn diff git diff 查看差异 svn revert git checkoutgit resetgit revert 撤销、丢弃修改 svn copy git checkout -b/-Bgit branch 创建分支 svn switch git checkout 切换分支 svn copy git tag 创建tag svn merge git mergegit rebase 分支合并 文件存储机制 Git 存储使用的是一个内容寻址的文件系统，其核心部分是一个简单的键值对（key-value）数据库，当向数据库中插入任意类型的内容，它会返回一个 40 位十六进制的 SHA-1 哈希值用作索引。在版本库中，Git 维护的数据结构有：以下 4 种对象及索引，并通过保存 commitID 有向无环图的 log 日志来维护与管理项目的修订版本和历史信息。 blob — 1 个 blob 保存 1 个文件的 1 个版本的数据。 tree — 表示 1 个目录，记录着目录里所有文件 blob 哈希值、文件名子目录名及其他元数据。通过递归引用其他目录树，从而建立一个包含文件和子目录的完整层次结构。 commit — 1 个提交对象保存版本库中每一次变化的元数据，每个提交对象指向一个版本的 git 目录树对象。 tag — 分为轻量标签和附注标签。轻量标签实际上是一个特定提交的引用，附注标签是存储在 git 中的一个完整可被校验的对象（保存在 .git/refs/tags 中），还包含打标签者的名字、E-mail、日志、注释等信息。 git 使用 zlib 将头部信息（对象类型：blob 或 tree 或 commit + 1 个空格 + 数据内容长度 + 1 个空字节）和对象数据拼接一起的内容进行压缩存储成一个文件。 压缩的文件被十六进制的 SHA-1 哈希值命名，该文件可以用 pigz.exe -dz &lt;文件路径&gt; 来解压查看。注：Windows 版的 pigz.exe 可以从这儿下载。 40 位十六进制的 SHA-1 哈希值 = sha1(&quot;blob/tree/commit &quot; + filesize + &quot;\\0&quot; + data)， 如：sha1(&quot;blob 7\\0foobar\\n&quot;) = &quot;323fae03f4606ea9991df8befbb2fca795e648fa&quot; 注：\\n 的二进制为 0a。 底层命令—剖析 Git 对象 find .git/objects -type f // 用 find 命令查看 .git/objects 目录（递归子目录）中的所有文件。 git rev-list --objects --all // 查看所有 git 对象的 SHA-1 哈希值与文件名的对应关系。 git rev-list --objects --all | grep 83c4fbc43a6f187d4e8a247a1c9aced872b2315d // 查看 SHA-1 哈希值为 83c4fbc43a6f187d4e8a247a1c9aced872b2315d 的文件名。 echo &quot;Hello World!&quot; | git hash-object --stdin // 计算内容为 Hello World! 文件的 SHA-1 哈希值。 echo &quot;Hello World!&quot; | git hash-object -w --stdin // 计算内容为 Hello World! 文件的 SHA-1 哈希值并写入到当前 git 本地版本库中。 git hash-object README.txt // 查看 README.txt 的 SHA-1 哈希值。 git hash-object -w README.txt // 查看 README.txt 的 SHA-1 哈希值并写入到当前 git 本地版本库中。 git cat-file -p master^^{tree} // 查看 master 分支 HEAD 指针 git 目录（tree 对象）下的各子目录（tree 对象）和文件（blob 对象）的 SHA-1 哈希值。 100644 blob 7abd3a56703ad4a7120571967f5d06607b5e5502 README.txt 040000 tree 9f448c40e684dc38109574007c661277c815fb7e ss 注：040000：表示目录；100644：表示一般文件；100755：表示可执行文件；120000：表示符号链接。 git cat-file -p 7abd3a56703ad4a7120571967f5d06607b5e5502 // 查看 SHA-1 哈希值为 7abd3a56703ad4a7120571967f5d06607b5e5502 文件的内容。 git show 7abd3a56703ad4a7120571967f5d06607b5e5502 // 查看 SHA-1 哈希值为 7abd3a56703ad4a7120571967f5d06607b5e5502 文件的内容。 git cat-file -t f3961f5 // 查看 f3961f5 提交对象的类型：显示为 commit。 git cat-file -p f3961f5 // 查看 f3961f5 提交对象的信息：包含 git 目录（tree 对象）、上次提交对象的 SHA-1 哈希值及提交时 Author、Date 和注释信息。 tree ead34240822030a3f71df4fc351057d80d7d83f8 parent 33d5bbc5d61b024aab5078e40548c4e3da808e0e author nicochen &lt;nicochen@tencent.com&gt; 1537258258 +0800 committer nicochen &lt;nicochen@tencent.com&gt; 1537258258 +0800 123 desc txt git cat-file -p tag1.0 // 查看轻量标签或附注标签 tag1.0 信息。 git cat-file tag tag1.0 // 查看附注标签 tag1.0 信息。 git ls-tree ead34240822030a3f71df4fc351057d80d7d83f8 // 查看 tree 目录对象 ead34240822030a3f71df4fc351057d80d7d83f8 中包含的 blob 文件对象和 tree 目录对象。 git ls-tree HEAD // 查看 HEAD 所指向 tree 目录对象中包含的 blob 文件对象和 tree 目录对象。 git verify-pack -v .git/objects/pack/pack-a9282552b62cbe3f255fbb20374695a17c1ba2a2.idx // 查看pack-a9282552b62cbe3f255fbb20374695a17c1ba2a2.pack 压缩包中的内容。 git update-index n.txt // 将修改状态的 n.txt 文件添加到暂存区。 git update-index --add n.txt // 将未追踪状态或修改状态的 n.txt 文件添加到暂存区。 git update-index --add --cacheinfo 100644 5d11580eed65ffd34b6786274a60460b3582aa7d n.txt // 使用类型为 100644、SHA-1 哈希值为 5d11580eed65ffd34b6786274a60460b3582aa7d 的信息将追踪状态或修改状态的 n.txt 添加到暂存区。 git write-tree // 将整个暂存区内容生成一个 tree 对象，并输出其 SHA-1 哈希值。 echo &quot;add n.txt&quot; | git commit-tree 31b7ca405196ca9e8fb4d5404b315bef9f2c841f -p HEAD // 用 git write-tree 得到的31b7ca405196ca9e8fb4d5404b315bef9f2c841f 树对象创建一个注释为 add n.txt 的提交对象，并将提交对象的父亲设置为当前 HEAD。 git update-ref refs/heads/master 372aa8e425b57ca30e2974b8e7737133caaa0b7f // 若当前分支为 master，更新 HEAD 指向上面 git commit-tree 命令得到的 372aa8e425b57ca30e2974b8e7737133caaa0b7f 提交对象，此时用 git log 就可以看到这条 commit 记录。 git write-tree --prefix=ss // 将暂存区中 ss 目录下的内容生成一个 tree 对象，并输出其 SHA-1 哈希值。 git update-ref -d refs/remotes/origin/v1.0 // 删除 v1.0 远程分支 cache。 git update-index --chmod=+x engine_mac.sh // 为 engine_mac.sh 增加可执行权限（Linux、Unix、Mac OS X 系统上需要）。 命令大全 查看命令帮助 git config --help // 查看 git config 命令详细用法 git help config // 功能同上 配置 git config --global user.name &quot;kekec&quot; // 配置提交用户名。 git config --global user.email &quot;kekec@qq.com&quot; // 配置 E-mail 信息。 git config --global core.editor vim // 配置默认文本编辑器，当 Git 需要你输入信息时会调用它。 git config --global alias.st status // 为 status 配置别名 st，这样 git status 就可以写成 git st。 git config --list // 查看当前仓库的所有配置信息（包括分支相关的信息） git config user.name // 查看当前仓库的用户名信息 git config -e --global // 编辑全局配置文件（用户名和 E-mail 信息就记录在其中） 所在目录：c:/users/&lt;用户名&gt;/.gitconfig。 git config -e // 编辑当前仓库的配置文件 所在目录：.git\\config。 创建版本库 git init // 在当前目录创建一个空的 git 代码库。 git init MyGame // 在当前目录创建一个名为 MyGame 的文件夹，然后在其中创建一个空的 git 代码库。 .git目录结构如下： hooks：不同操作时执行的 hook 脚本。 info/exclude：与 .gitignore 文件（该文件需放在 .git 文件夹的同级目录中，Windows 下可通过命令行 type nul &gt; .gitignore 来创建）一样，用作文件过滤。不同的是：该文件不会提交到版本库，因此过滤只对本地生效，不影响其他人。 # 忽略所有.so 结尾的文件 *.so # 但 game.so 除外 !game.so # 仅仅忽略项目根目录下的 README.md 文件，不包括 subdir/README.md /README.md # 忽略 .svn/ 目录下的所有文件 .svn/ # 会忽略 doc/notes.txt 但不包括 doc/server/arch.txt doc/*.txt # 忽略 doc/ 目录下所有扩展名为 txt 的文件 doc/**/*.txt logs/refs/heads：各个本地分支的版本 log 记录。 logs/refs/remotes：各个远程分支 cache 的 log 记录。 logs/refs/stash：储藏区数据。 logs/HEAD：git 操作记录。 objects：2 级文件索引（把 SHA-1 哈希值拆成了：2位+38位），存储 commit 数据、blob 文件数据和 tree 目录数据。 objects/pack：pack 文件为存储 commit、tree 目录及 blob 文件的压缩数据；idx 文件为 pack 文件中各数据对象的索引。 objects/info/packs：该文件记录所有 git 库的 pack 文件列表。 refs/heads：各个本地分支 HEAD。 refs/remotes：各个远程分支 cache 的 HEAD。 refs/tags：各个附注标签的信息。 COMMIT_EDITMSG：上一次提交的注释。 config：版本库相关的配置信息。 description：仓库描述信息，供 gitweb 程序使用。 index：暂存区相关的信息。 HEAD：指向当前分支的最近提交（如：ref: refs/heads/master）。 ORIG_HEAD：执行 git merge / git pull / git reset 操作时，会把调整为新值之前的先前版本的 HEAD 记录到 OERG_HEAD 中，用于恢复或回滚之前的状态。 FETCH_HEAD：git fech 将所有抓取分支的 HEAD 记录到 .git/FETCH_HEAD 中。 MERGEHEAD：正在合并进 HEAD 的 commit id。 packed-refs：远程版本库 cache 和远程标签 cache。 日志与文件状态 git reflog // 查看操作记录。 注：每条操作记录使用 HEAD@{n} 来标识。 git show HEAD@{5} // 查看索引为 5 的操作记录的详细信息。 git status // 查看当前所处的分支暂存区和工作区的文件（会显示当前所处分支）。 注1：处于暂存区的文件状态:：staged（已暂存）；处于工作区的文件状态:：untrack（未跟踪）、modified（已修改）； 注2：工作区中的空目录不会被 git 追踪。 git status -s --ignored // 以简洁模式查看暂存区和工作区的文件（全部显示，不执行文件过滤）。 git status -uno // 查看暂存区和工作区的非 untrack（未跟踪）状态文件。 git status -uall // 查看暂存区和工作区的状态文件（递归子目录显示出里面的文件）。 git log // 查看本地版本库提交记录（会显示当前所处分支，HEAD 指针指向哪个分支的哪条提交）。 git log --stat // 查看本地版本库提交记录（会显示当前所处分支，HEAD 指针指向哪个分支的哪条提交和每次提交的文件变更简略统计信息）。 git log -- README.md // 查看 README.md 文件的本地版本库提交记录。 git log --graph -- README.md // 以图形化方式查看 README.md 文件的本地版本库提交记录。 git log -p README.md // 查看 README.md 文件的本地版本库提交记录（显示出每次的修改内容）。 git log --grep &quot;test&quot; // 显示注释中含有 test 字符串的提交。 git log --author=kekec // 查看本地版本库中作者为 kekec 的提交记录。 git log -S &quot;SplitPath(FString&amp; str)&quot; // 查看 SplitPath(FString&amp; str) 内容是什么时候加到项目中那些文件中去的。 git log --since=2.weeks // 查看最近 2 周的提交记录。 git log --since=&quot;2 weeks 3 days 2 hours 30 minutes 59 seconds ago&quot; // 查看 2 周 3 天 2 小时 30 分 59 秒以前的提交记录。 git log --after=&quot;2018-10-7&quot; --before=&quot;2018-10-12&quot; // 查看 2018.10.7~2018.10.12 之间的提交记录。 git log --since=&quot;2018-10-7&quot; --until=&quot;2018-10-12&quot; // 功能同上：git log --after=&quot;2018-10-7&quot; --before=&quot;2018-10-12&quot;。 注：--since、--until 标记和 --after、--before 标记分别是等价的。 git whatchanged README.md // 查看 README.md 文件的本地版本库提交记录（包括文件改名）。 git log --follow README.md // 功能同上：git whatchanged README.md。 git log -3 // 查看最近 3 条本地版本库提交记录。 git log -3 --pretty --oneline // 查看最近 3 条本地版本库提交记录（简洁模式，一行显示一个提交）。 git log --graph --oneline // 以图形化简洁模式查看当前分支的本地版本库提交记录。 git log release --graph --oneline // 以图形化简洁模式查看 release 分支的本地版本库提交记录。 git log --graph --oneline --no-merges // 以图形化简洁模式查看当前分支的本地版本库提交记录（过滤 merge 过来的提交）。 git log --graph --oneline --merges // 以图形化简洁模式查看当前分支的本地版本库提交记录（只显示有 2 个及以上父亲节点的提交）。 git log --graph --oneline --name-only // 以图形化简洁模式查看当前分支的本地版本库提交记录（并显示每次提交的文件名称清单）。 git log --graph --oneline --name-status // 以图形化简洁模式查看当前分支的本地版本库提交记录（并显示每次提交的文件状态、名称清单）。 git log --graph --oneline --stat // 以图形化简洁模式查看当前分支的本地版本库提交记录（并显示每次提交的文件变化统计、各文件名及增删记录）。 git log --graph --oneline --shortstat // 以图形化简洁模式查看当前分支的本地版本库提交记录（并显示每次提交的文件变化统计及增删记录）。 git log --graph --oneline --decorate --all // 以图形化简洁模式查看所有分支的本地版本库提交记录树。 git log --graph --pretty=format:&quot;%H - %an, %ad : %s&quot; // 自定义格式图形化查看所有分支的本地版本库提交记录树。 %H 提交对象（commit）的完整哈希字串； %h 提交对象的简短哈希字串； %T 树对象（tree）的完整哈希字串； %t 树对象的简短哈希字串； %P 父对象（parent）的完整哈希字串； %p 父对象的简短哈希字串； %an 作者（author）的名字； %ae 作者的电子邮件地址； %ad 作者修订日期（可以用 --date= 选项定制格式）； %ar 作者修订日期，按多久以前的方式显示； %cn 提交者（committer）的名字； %ce 提交者的电子邮件地址； %cd 提交日期； %cr 提交日期，按多久以前的方式显示； %s 提交说明； git log master..v5.0 // 查看 v5.0 分支还未合并到 master 分支上的提交记录列表。 git log v5.0..master // 查看 master 分支还未合并到 v5.0 分支上的提交记录列表。 git log master...v5.0 // git log master..v5.0 + git log v5.0..master。 git shortlog -sn // 统计各个提交者的次数。 git blame README.md // 显示 README.md 最近一次的修改信息。 git show 3a6c702376168aa15a2f3d7bc98000d07a70d023 README.md // 查看 README.md 文件的 3a6c702376168aa15a2f3d7bc98000d07a70d023 提交的修改内容。 git show HEAD // 查看最近一次提交的修改内容。 git show --name-only HEAD // 查看最近一次提交的文件列表（不显示具体的修改内容）。 标签（查看/新建/切换/删除） git tag // 列出所有的标签。 git tag -l 'tag1*' // 列出所有 tag1 开头的标签。 git tag tag1.0 // 创建名为 tag1.0 的轻量标签。 git tag -a tag1.0 -m &quot;tag1.0 desc&quot; // 添加 tag1.0 desc 注释并创建名为 tag1.0 的附注标签。 git tag tag2.0 abffefc5d82078cbaea7fcbb5106ab0c21cbeba9 // 在 abffefc5d82078cbaea7fcbb5106ab0c21cbeba9 提交处创建名为 tag2.0 的轻量标签。 git tag -a tag2.0 -m &quot;tag2.0 desc&quot; abffefc // 在 abffefc 提交处创建名为 tag2.0 的附注标签。 git tag -d tag2.0 // 删除名为 tag2.0 的标签。 git show tag1.0 // 查看名为 tag1.0 相关的信息。 git ls-remote --tags // 查看所有远端的标签。 分支（查看/新建/切换/删除） git branch // 列出所有本地分支。 git branch -r // 列出所有远程分支 cache。 git branch -a // 列出所有本地分支和远程分支 cache。 git branch -av // 列出所有本地分支和远程分支 cache（含简单说明）。 git branch -vv // 查看所有本地分支和远程分支 cache 之间的追踪关系。 git branch v1.0 // 在当前分支的 HAED 指针下创建名为 v1.0 的分支（创建完不会切到 v1.0 分支上）。 git branch --track v1.0 origin/v1.0 // 若 v1.0 分支不存在则先新建，然后将其与远程分支 origin/v1.0 建立追踪关系。 远程分支 origin/v1.0 要存在，否则命令执行失败。 执行完不会切到 v1.0 分支上。 git branch v2.0 372aa8e425b57ca30e2974b8e7737133caaa0b7f // 在 372aa8e425b57ca30e2974b8e7737133caaa0b7f 提交处创建名为 v2.0 的分支（创建完不会切到 v2.0 分支上）。 git branch -m v1.0 x1.0 // 将分支 v1.0 重命名为 x1.0。 git checkout v1.0 // 切换到 v1.0 分支上（v1.0 分支不存在则命令执行失败）。 git checkout -b v1.0 // 创建并切换到 v1.0 分支上（v1.0 分支存在则命令执行失败）。 git checkout -B v1.0 // 不存在则创建，并切换到 v1.0 分支上。 git checkout -b v1.0 5a95f2d // 在 5a95f2d 提交处创建并切换到 v1.0 的分支上。 git checkout -b v1.0 tag1.0 // 在标签 tag1.0 处创建并切换到 v1.0 的分支上。 git checkout -t origin/v1.0 // 创建并切换到 origin/v1.0 远程分支 cache 的名为 v1.0 本地分支上，并建立两者追踪关系（本地分支 v1.0 存在则命令执行失败）。 git checkout -b x1.0 -t origin/v1.0 // 创建并切换到 origin/v1.0 远程分支 cache 的名为 x1.0 本地分支上，并建立两者追踪关系（本地分支 x1.0 存在则命令执行失败） 注1：切换分支前，必须处理工作区（未追踪的文件不用处理）和暂存区的修改才能切换成功 注2：切换成功后，工作区会被设置成分支的内容 注3：不允许在远程分支 cache 上提交，需要创建对应关联的本地分支，然后在本地分支上进行提交。 git checkout -f v1.0 // 强制切换到 v1.0 分支上，丢弃暂存区和工作区中的所有文件的修改（工作区中未追踪的文件不受影响）。 git checkout -f -B v1.0 origin/v1.0 // 不存在则创建，强制切换到 v1.0 分支上，丢弃暂存区和工作区中的所有文件的修改，并将 HEAD 指向 origin/v1.0 处（工作区中未追踪的文件不受影响）。 git checkout - // 切换到上一次分支。 git branch -d v2.0 // 删除名为 v2.0 的分支（必须先切到其他分支上才能执行删除操作）。 git branch -D v2.0 // 强制删除名为 v2.0 的分支（必须先切到其他分支上才能执行删除操作）。 git branch -dr origin/v2.0 // 删除远程分支 origin/v2.0 cache。 文件（增加/删除/提交/撤销） git add README.md // 将当前目录下的 README.md 文件加入到暂存区。 git add . // 将当前目录下（递归子目录）所有文件加入到暂存区。 git add -u . // 将当前目录下（递归子目录）所有追踪状态的文件加入到暂存区。 git add Doc/\\*.txt // 将当前目录的 Doc 文件夹下（递归子目录）所有 txt 后缀的文件加入到暂存区。 git rm README.md // 删除工作区文件，并且将这次删除放入暂存区（若 README.md 在工作区或暂存区中有修改，命令会执行失败）。 git rm -f README.md // 强制删除工作区文件，并且将这次删除放入暂存区（即使 README.md 在工作区或暂存区中有修改，也会执行删除操作）。 git rm --cached README.md // 不删除工作区对应的文件，只将 README.md 删除放入暂存区以供提交。 git mv README.md test.md // 将 README.md 改名为 test.md，并且将这个改名放入暂存区。 git commit -m &quot;desc&quot; // 添加 desc 注释并将暂存区中的所有修改提交到本地仓库。 git commit README.md -m &quot;desc&quot; // 添加 desc 注释并将暂存区中的 README.md 的修改提交到本地仓库。 git commit --amend -m &quot;desc&quot; // 添加 desc 注释使用当前提交覆盖上一次的提交（若上一次提交包含 1.txt 和 2.txt 的修改，当前提交只包含 1.txt 的修改；执行命令后，本地版本库中为本次的 1.txt 和上一次 2.txt）。若没有提交内容，则用来改写上一次提交的日志信息。 git commit -m &quot;desc&quot; --amend README.txt // 添加 desc 注释使用 README.txt 的当前提交覆盖上一次的提交。 git commit -a -m &quot;desc&quot; // 添加 desc 注释并将工作区和暂存区中的所有修改提交到本地仓库。 git commit -am &quot;desc&quot; // 功能同上。 git commit -c b5cad94d229e72bd7aff5fe2c6f022b29c30e7a8 // 拿 372aa8e425b57ca30e2974b8e7737133caaa0b7f 提交的信息（作者、提交者、注释、时间戳等）来提交当前修改。 git reset -- README.md // 丢弃暂存区中的 README.md 文件的修改。 git reset README.md // 功能如上，丢弃暂存区中的 README.md 文件的修改。 git reset b5cad94 README.md // 使用本地版本库 b5cad94 提交处的 README.md 版本覆盖暂存区中的 README.md。 git reset // 丢弃暂存区中的所有文件的修改（工作区不受影响）。 git reset --mixed // --mixed 为缺省参数，命令与上面 git reset 一样。 git reset --hard // 丢弃暂存区和工作区中的所有文件的修改（工作区中未追踪的文件不受影响）。 git reset --soft b5cad94d229e72bd7aff5fe2c6f022b29c30e7a8 // 仅将当前分支的。HEAD 指向 372aa8e425b57ca30e2974b8e7737133caaa0b7f 提交（暂存区和工作区中的所有文件的修改都不丢弃）。 git reset --soft HEAD~ // 仅将当前分支的 HEAD 指向上一次提交（暂存区和工作区中的所有文件的修改都不丢弃）。 git reset --soft HEAD~2 // 仅将当前分支的 HEAD 指向上两次提交（暂存区和工作区中的所有文件的修改都不丢弃）。 git reset --merge &lt;commit&gt; // 在被污染的工作区中回滚 merge 或者 pull。 $ git pull (1) Auto-merging nitfol Merge made by recursive. nitfol | 20 +++++---- ... $ git reset --merge ORIG_HEAD (2) 即便你已经在本地更改了一些你的工作区，你也可安全的 git pull，前提是你知道将要 pull 的内容不会覆盖你的工作区中的内容。 git pull 完后，你发现这次 pull 下来的修改不满意，想要回滚到 pull 之前的状态，我们可以执行 git reset --hard ORIG_HEAD，但是这个命令有个副作用就是清空你的工作区，即丢弃你的本地未 add 的那些改变。 为了避免丢弃工作区中的内容，可以使用 git reset --merge ORIG_HEAD，注意其中的--hard 换成了 --merge，这样就可以避免在回滚时清除工作区。 git reset --keep &lt;commit&gt; // 保留工作区并丢弃一些之前的提交。 假设你正在编辑一些文件，并且已经提交，接着继续工作，但是现在你发现当前在工作区中的内容应该属于另一个分支，与之前的提交没有什么关系。此时，可以开启一个新的分支，并且保留着工作区中的内容。 $ git tag start $ git checkout -b branch1 $ edit $ git commit ... (1) $ edit $ git checkout -b branch2 (2) $ git reset --keep start (3) 这次是把在 branch1 中的改变提交了。 此时发现，之前的提交不属于这个分支，此时新建了 branch2 分支，并切换到了 branch2 上。 此时可以用 reset --keep 把在 start 之后的提交清除掉，但是保持工作区不变。 git checkout -- README.md // -- 符号非常重要，否则就变成了切换到 README.md 分支了。 // 当 README.md 在暂存区中有修改时，使用暂存区中的修改覆盖工作区中的 README.md。 // 当 README.md 不在暂存区中时，使用本地版本库中的 HEAD 指针处的修改覆盖工作区中的 README.md。 git checkout -- . // 使用暂存区和本地版本库来恢复当前目录（递归子目录）下的所有文件。 注：若暂存区中有修改，优先使用暂存区。 git checkout HEAD README.md // 使用本地版本库中的 HEAD 处提交覆盖暂存区和工作区中的 README.md。 git checkout 9a387f22ff949fa16336508adc2284384bd6a890 README.md // 使用本地版本库中的 9a387f22ff949fa16336508adc2284384bd6a890 修改覆盖暂存区和工作区中的 README.md。 git checkout -b v2.0 tag2.0 // 在名为 tag2.0 的提交处创建并切换到 v2.0 分支上（v2.0 分支存在则命令执行失败）。 git revert --no-edit 3a6c702376168aa15a2f3d7bc98000d07a70d023 // 回滚 3a6c702376168aa15a2f3d7bc98000d07a70d023 提交，然后提交到本地仓库。 git revert HEAD~ // 回滚 HEAD 的上一次提交，然后会弹出 vim 环境编辑注释（输入 :q 直接使用默认注释内容、输入 :q! 放弃修改使用默认注释内容、输入 :x 或 :wq 保存当前修改的注释内容），然后提交到本地仓库。 git revert -n HEAD~3 // 回滚掉 HEAD~3 处的提交，不自动提交到本地仓库。 git revert -n HEAD~2..HEAD // 回滚掉 (HEAD~2, HEAD] 之间的 2 次提交，不自动提交到本地仓库。 注：git reset 是把 HEAD 向后移动来删除提交，而 git revert 是用一次新的提交来回滚之前的提交（HEAD 会继续前进）。 查看差异 git diff README.md // 查看当前目录下的 README.md 在工作区和暂存区之间的差异。 git diff --cached README.md // 查看当前目录下的 README.md 在暂存区和本地仓库最后一次提交之间的差异。 git diff --cached 372aa8e425b57ca30e2974b8e7737133caaa0b7f README.md // 查看当前目录下的 README.md 在暂存区和本地仓库的 372aa8e425b57ca30e2974b8e7737133caaa0b7f 提交之间的差异。 git diff HEAD README.md // 查看当前目录下的 README.md 在工作区和本地仓库 HEAD 指针处提交之间的差异。 git diff 372aa8e425b57ca30e2974b8e7737133caaa0b7f README.md // 查看当前目录下的 README.md 在工作区和本地仓库的 372aa8e425b57ca30e2974b8e7737133caaa0b7f 提交之间的差异。 git diff 372aa8e425b57ca30e2974b8e7737133caaa0b7f HEAD README.md // 查看当前目录下的 README.md 在本地仓库的 372aa8e425b57ca30e2974b8e7737133caaa0b7f 提交和最后一次提交之间的差异。 git diff 372aa8e425b57ca30e2974b8e7737133caaa0b7f HEAD // 查看本地仓库的 372aa8e425b57ca30e2974b8e7737133caaa0b7f 提交和最后一次提交之间的差异。 git diff 372aa8e b5cad94 README.md // 查看当前目录下的 README.md 在本地仓库的 372aa8e 提交和 b5cad94 提交之间的差异 注：可以将 git diff 换成 git difftool 来使用外部 diff 工具（可以在 c:/users/&lt;用户名&gt;/.gitconfig 文件配置 beyond compare 作为默认的 difftool 和 mergetool）来查看差异。[diff] tool = bc3 [difftool] prompt = false [difftool &quot;bc3&quot;] cmd = &quot;\\&quot;e:/program files (x86)/beyond compare 3/bcomp.exe\\&quot; \\&quot;$LOCAL\\&quot; \\&quot;$REMOTE\\&quot;&quot; 分支合并 git merge-base Master Feature // 查看 Master 和 Feature 分支的最优共同 commit 父节点。 git merge Feature // 将 Feature 分支 merge 合并到当前分支 Master（无冲突时会直接提交）。 git merge -m &quot;merge test&quot; Feature // 将 Feature 分支 merge 合并到当前分支 Master（无冲突时使用 merge test 注释直接提交）。 git merge --no-commit Feature // 将 Feature 分支 merge 合并到当前分支 Master（不自动提交）。 git rebase Feature // 将 Feature 分支 rebase 合并到当前分支 Master。 注1：git rebase 会先找出共同的祖先节点，从祖先节点把 Feature 分支的提交记录全都剪切下来，然后合到 Master 分支（合并前后 commitID 会不一样）。 注2：相对来说，git merge 处理冲突更直接，但会增加一些冗余的提交记录；而 git rebase 能够保证清晰线性的提交记录，但这也将合并的操作没有被记录下来。 注3：最好是用 git rebase 合并远程分支到本地，git merge 合并 Feature 分支到 Master 分支。 注4：在合并 Feature 分支到 Master 分支前，务必先执行 git pull -r origin Feature 来进行远程分支与本地分支的 rebase 合并。 注5：处于冲突状态（conflict）的文件为 UU（可通过 git status -s --ignored 来查找），手动处理完冲突后，然后使用 git add 该文件，最后继续执行 git merge/rebase --continue 来完成合并的提交工作。 注6：README.md 文件冲突内容如下&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD 123 456 789 000 111 222 333 444 555 ss // 当前分支的内容 ======= 123 456 789 000 ss tt // Feature 分支的内容 &gt;&gt;&gt;&gt;&gt;&gt;&gt; Feature 注7：可以使用 git mergetool 来使用外部 merge 工具（可以在 c:/users/&lt;用户名&gt;/.gitconfig 文件配置 beyond compare 作为默认的 mergetool）来处理冲突。 修改完当前文件后，可再次调用 git mergetool 来处理下一个冲突，直至全部处理完毕，然后使用 git add 该文件，最后继续执行 git merge/rebase --continue 来完成合并的提交工作。[merge] tool = bc3 [mergetool] prompt = false [mergetool &quot;bc3&quot;] cmd = &quot;\\&quot;e:/program files (x86)/beyond compare 3/bcomp.exe\\&quot; \\&quot;$LOCAL\\&quot; \\&quot;$REMOTE\\&quot; \\&quot;$BASE\\&quot; \\&quot;$MERGED\\&quot;&quot; git rebase /i Feature // 将 Feature 分支采用手动交互方式 rebase 合并到当前分支 Master。 pick 07c5abd Introduce OpenPGP and teach basic usage pick de9b1eb Fix PostChecker::Post#urls pick 3e7ee36 Hey kids, stop all the highlighting pick fa20af3 git interactive rebase, squash, amend # Rebase 8db7e8b..fa20af3 onto 8db7e8b # # Commands: # p, pick = use commit # r, reword = use commit, but edit the commit message # e, edit = use commit, but stop for amending # s, squash = use commit, but meld into previous commit # f, fixup = like &quot;squash&quot;, but discard this commit's log message # x, exec = run command (the rest of the line) using shell # # These lines can be re-ordered; they are executed from top to bottom. # # If you remove a line here THAT COMMIT WILL BE LOST. # # However, if you remove everything, the rebase will be aborted. # # Note that empty commits are commented out git merge/rebase --abort // 撤销当前 merge 或 rebase 操作。 git merge/rebase --skip // 强制使用 Feature 分支的内容。 git merge/rebase --continue // 手动处理完冲突后使用 git add 该文件，最后继续执行 git merge/rebase --continue 来完成合并的提交工作。 git merge origin/master // fetch 完之后，可以将远程分支 cache master 分支 merge 合并到当前分支上。 git rebase origin/master // fetch 完之后，可以将远程分支 cache master 分支 rebase 合并到当前分支上。 git rebase --onto master 76cada~ // 将当前分支从 [76cada, HEAD] 区间段的提交 rebase 合并到 master 上。 git cherry-pick 9a341e // 将 9a341e 提交合入当前分支。若不冲突，则直接使用 9a341e 的提交信息进行 commit，否则要先进行冲突处理，然后继续执行 git cherry-pick --continue 来完成合并的提交工作。 git cherry-pick 371c2…971209 // 将 (371c2, 971209] 提交合入当前分支（每个提交都会在当前分支上创建一个 commit）。 git cherry-pick 371c2~…971209 // 将 [371c2, 971209] 提交合入当前分支（每个提交都会在当前分支上创建一个 commit）。 git cherry-pick -n 9a341e d2f99e // 将 9a341e 和 d2f99e 提交合入当前分支（不提交），后续需要手动 commit。 git cherry-pick --abort // 撤销当前 cherry-pick 操作。 git cherry-pick --quit // 清理当前操作状态，不撤销修改强制退出 cherry-pick 操作过程。 git cherry-pick --continue // 手动处理完冲突后，最后继续执行 git cherry-pick --continue 来完成合并的提交工作。 查看远程版本库 git remote -v // 显示远程仓库的 URL。 注：由于 git 是分布式的，所有远程仓库可能有很多个origin https://github.com/kekec/Test.git (fetch) origin https://github.com/kekec/Test.git (push) git remote -ls // 查看远程仓库 URL 和分支信息From https://github.com/kekec/Test.git fae0fc82d711425daa897a63137d7e1af09512ba HEAD fae0fc82d711425daa897a63137d7e1af09512ba refs/heads/master git remote // 查看远程仓库名称，一般为 origin。 git remote rename origin test // 将远程仓库名称从 origin 修改为 test。 git remote show origin // 显示远程仓库的信息。* remote origin Fetch URL: https://github.com/kekec/Test.git Push URL: https://github.com/kekec/Test.git HEAD branch: master Remote branches: master tracked v3.1 tracked Local branch configured for 'git pull': master merges with remote master Local refs configured for 'git push': master pushes to master (fast-forwardable) v3.1 pushes to v3.1 (up to date) git remote rm origin // 删除 .git/config 文件中添加 remote origin 相关的信息。 git remote add origin https://github.com/kekec/Test.git // 在 .git/config 文件中添加 remote origin 指向的远程仓库 URL（若已存在，则命令执行失败）。[remote &quot;origin&quot;] url = https://github.com/kekec/Test.git fetch = +refs/heads/*:refs/remotes/origin/* git remote set-url origin https://github.com/kekec/Test.git // 修改 .git/config 文件中添加 remote origin 指向的远程仓库 URL。 git remote prune origin // 对于远程仓库不存在的分支，清除对应的远程分支 cache。 远程操作 git clone https://github.com/kekec/Test.git // 将 https://github.com/kekec/Test.git 上的当前分支克隆到本地（会创建一个名为 Test 目录，远程仓库名称使用默认名 origin）。 git clone https://github.com/kekec/Test.git MyProject // 将 https://github.com/kekec/Test.git 上的当前分支克隆到本地（会创建一个名为 MyProject 目录，远程仓库名称使用默认名 origin）。 git clone -b v1.0 https://github.com/kekec/Test.git // 将 https://github.com/kekec/Test.git 上的 v1.0 分支克隆到本地（会创建一个名为 Test 目录，远程仓库名称使用默认名 origin）。 git clone -b v1.0 https://github.com/kekec/Test.git d:\\MyGame // 将 https://github.com/kekec/Test.git 上的 v1.0 分支克隆到 d:\\MyGame 目录（会在 d:\\MyGame 中创建一个名为 Test 目录，远程仓库名称使用默认名 origin）。 git clone -o TestPrj https://github.com/kekec/Test.git // 将 https://github.com/kekec/Test.git 上的当前分支克隆到本地（会创建一个名为 Test 目录，并将远程仓库名称设置为 TestPrj）。 git fetch origin master // 从远程仓库拉取 master 分支状态的变化信息（工作区文件不会更新）。 git fetch // 从远程仓库拉取所有分支和 tag 状态的变化信息（工作区文件不会更新）。 git fetch -p // 从远程仓库拉取所有分支和 tag 状态的变化信息，并清除已被删除的远程分支和 tag 在本地的缓存（工作区文件不会更新）。 git fetch origin --tags // 从远程仓库拉取所有 tag 到本地（工作区文件不会更新）。 git pull &lt;远程仓库名&gt; &lt;远程分支名&gt;:&lt;本地分支名&gt;。 git pull origin master // 先执行 fetch，然后将远程 origin/master 分支 merge 合并到当前分支（最后会更新 origin/master，origin/HEAD 指针到最新提交）。 git pull https://github.com/kekec/Test.git master // 先执行 fetch，将远程 origin/master 分支 merge 合并到当前分支（最后不会更新 origin/master，origin/HEAD 指针到最新提交）。 git pull origin v1.0:master // 先执行 fetch，然后将远程 origin/v1.0 分支 merge 合并到本地 master 分支。 git pull origin // 先执行 fetch，然后将对应的远程分支 merge 合并到当前分支（当前分支需要预存远程分支的追踪关系）。 git pull // 先执行 fetch，然后将对应的远程分支 merge 合并到当前分支（当前分支需要预存远程分支的追踪关系，而且当前分支只有一个远程仓库）。 git pull -p // 先执行 fetch，然后将对应的远程分支 merge 合并到当前分支，并清除已被删除的远程分支和 tag 在本地的缓存。 git pull -r origin master // 先执行 fetch，然后将远程 origin/master 分支 rebase 合并到 master 分支。 git push &lt;远程仓库名&gt; &lt;本地分支名&gt;:&lt;远程分支名&gt;。 git push -u origin master // 将本地仓库的修改 push 到 origin 所指向的远程仓库 URL 的 master 分支上，并在 .git/config 文件中记录当前分支与远程分支 master 的对应关系。 git push origin // 将当前分支更新推送给对应的远端分支。 git push // 将当前分支更新推送给对应的远端分支（当前分支只有一个远程仓库，可以省略仓库名 origin）。 git push origin -f // 使用当前分支更新强行覆盖对应的远端分支（合入远端分支有冲突时，也使用当前分支更新）。 git push origin v1.0 // 将本地分支 v1.0 更新推送给对应的远端分支 remotes/origin/v1.0。 git push origin --all // 将本地所有分支更新推送给各自对应的远端分支。 git push origin tag1.0 // 将本地标签 tag1.0 更新到远端标签 tag1.0。 git push origin --tags // 将本地所有标签更新到对应的远端标签。 git push origin :v1.0 // 删除远端分支 v1.0。 git push origin :refs/tags/tag1.0 // 删除远程标签 tag1.0。 git push origin -d v1.0 // 删除远端分支 v1.0 功能同上。 储藏区 git stash // 将工作区中所有文件的修改备份压栈到储藏区，然后丢弃工作区与暂存区的所有文件的修改。 git stash pop // 使用储藏区的栈顶处备份（stash@{0}）来恢复当前分支的工作区，并将栈顶备份移除。 git stash apply stash@{1} // 使用储藏区的栈顶下面一个备份（stash@{1}）来恢复当前分支的工作区，但不移除储藏区中任何备份。 git stash list // 查看储藏区栈列表。 git stash show -p stash@{0} // 查看储藏区的栈顶处备份中各个文件的内容。 git stash drop // 直接移除储藏区的栈顶处备份（不用于恢复当前分支的工作区）。 git stash clear // 清除储藏区栈列表。 工作区 git clean -nd // 探测工作区中哪些文件和目录（未追踪状态）会被删除。 git clean -fd // 删除工作区中未追踪状态的文件和目录。 暂存区 git ls-files // 查询暂存区中的文件列表（递归子目录）。 git ls-files -s // 查看暂存区中所有文件的 blob 数据块信息。 git ls-files -s -- README.md // 查看暂存区中的 README.md 文件的 blob 数据块信息。 其他命令 git fsck --full // 列出所有未引用的 blob、tree、commit 对象。 git archive --format zip --output d:/file.zip master // 将当前 master 分支所有文件使用 zip 压缩方式打包到 d:/file.zip。 Git 瘦身 git count-objects -v // 查看 git 对象的统计信息。 find .git/objects -type f -print0 | xargs -0 du -hk | sort -nr | head -5 // 查找 git 库中最大的 5 个文件（du -hk 中的 k 代表单位为 KB）。 find .git/objects -type f -size +1M -print0 | xargs -0 du -hm | sort -nr | head -5 // 查找 git 库中 size 超过 1M 的最大的 5 个文件（du -hm 中的 m 代表单位为 MB）。 git verify-pack -v .git/objects/pack/pack-b340eea7566df839294b71ec91a327ca2ece0b94.idx | sort -k 3 -nr | head -5 // 对压缩存储的 git 库查找最大的 5 个文件。 git filter-branch --force --index-filter 'git rm --cached --ignore-unmatch FramePro.cpp' --prune-empty --tag-name-filter cat -- --all // 从 git 库的历史记录中彻底清理 FramePro.cpp。 git for-each-ref --format='delete %(refname)' refs/original | git update-ref --stdin // 清理所有废弃的 ref 引用。 git gc --prune=now 将所有的对象压缩存储到 pack 二进制文件中，以节省空间和提高效率。 移除与任何提交都不相关的陈旧对象。 git reflog expire --expire=now --all // 清除所有操作记录日志。 除了使用 git 原生命令外，可以使用专门的工具 BFG（Java 实现）来对 Git 库瘦身。 经典 Gitflow master 分支存储了正式发布的历史（master 分支上的所有提交都会分配一个版本号）。 develop 分支作为功能的集成分支。 每个新功能位于一个自己的 Feature 分支，该分支使用 develop 分支作为父分支。当新功能完成时，合并回 develop 分支。新功能提交应该从不直接与 master 分支交互。 一旦 develop 分支上有了做一次发布（或者说快到了既定的发布日）的足够功能，就从 develop 分支上 fork 一个 release分支。 新建的分支用于开始发布循环，所以从这个时间点开始之后新的功能不能再加到这个分支上。 这个分支只应该做 Bug 修复、文档生成和其它面向发布任务。 对外发布的工作完成后，发布分支会合并到 master 分支并分配一个版本号打好 Tag。另外，这些从新建发布分支以来的做的修改要合并回 develop 分支。 hotfix 分支用于生成快速给产品发布版本（production releases）打补丁，修复完成，修改应该马上合并回 master 分支（打好 Tag）和 develop 分支（当前的发布分支）。 参考 Pro Git 第二版pdf Git 原理入门 常用 Git 命令清单 Git 远程操作详解 Git 工作流程 Git 使用规范流程 Git 分支管理策略 GIT 基本概念和用法总结 Git 工作流指南：Gitflow 工作流 ","link":"https://faded.auspicious.space/post/git-super-detailed-tutorials-and-commands/"},{"title":"Git——25个进阶技巧","content":" 25个 Git 进阶技巧 基本技巧 1. 安装后的第一步 在安装好 git 后，你第一件该做的事是设置你的名字和电子邮箱，因为每次提交都要用到这些信息： $ git config --global user.name &quot;Some One&quot; $ git config --global user.email &quot;someone@gmail.com&quot; 2. Git 是基于指针的 保存在 git 里的一切都是文件。当你创建一个提交的时候，会建立一个包含你的提交信息和相关数据（名字，邮件地址，日期/时间，前一个提交，等等）的文件，并把它链接到一个树文件中。这个树文件中包含了对象或其他树的列表。这里的提到的对象（或二进制大对象）是和本次提交相关的实际内容（它也是一个文件，另外，尽管文件名并没有包含在对象里，但是存储在树中）。所有这些文件都使用对象的 SHA-1 哈希值作为文件名。 用这种方式，分支和标签就是简单的文件（基本上是这样），包含指向该提交的 SHA-1 哈希值。使用这些索引会带来优秀的灵活性和速度，比如创建一个新分支就是简单地用分支名字和所分出的那个提交的 SHA-1 索引来创建一个文件。当然，你不需要自己做这些，而只要使用 Git 命令行工具（或者 GUI），但是实际上就是这么简单。 你也许听说过叫 HEAD 的索引。这只是简单的一个文件，包含了你当前指向的那个提交的 SHA-1 索引值。如果你正在解决一次合并冲突然后看到了 HEAD，这并不是一个特别的分支或分支上的一个必需的特殊位置，只是标明你当前所在位置。 所有的分支指针都保存在 .git/refs/heads 里，HEAD 在 .git/HEAD 里，而标签保存在 .git/refs/tags 里——自己可以随便进去看看。 3. 两个爸爸（父节点） - 你没看错！ 在历史中查看一个合并提交的信息时，你将看到有两个父节点（不同于工作副本上的常规提交的情况）。第一个父节点是你所在的分支，第二个是你合并过来的分支。 4. 合并冲突 目前我相信你碰到过合并冲突并且解决过。通常是编辑一下文件，去掉 &lt;&lt;&lt;&lt;，====，&gt;&gt;&gt;&gt; 标志，保留需要留下的代码。有时能够看到这两个修改之前的代码会很不错，比如，在这两个现在冲突的分支之前的改动。下面是一种方式： $ git diff --merge diff --cc dummy.rb index 5175dde,0c65895..4a00477 --- a/dummy.rb +++ b/dummy.rb @@@ -1,5 -1,5 +1,5 @@@ class MyFoo def say - puts &quot;Bonjour&quot; - puts &quot;Hello world&quot; ++ puts &quot;Annyong Haseyo&quot; end end 如果是二进制文件，比较差异就没那么简单了...通常你要做的就是测试这个二进制文件的两个版本来决定保留哪个（或者在二进制文件编辑器里手工复制冲突部分）。从一个特定分支获取文件拷贝（比如说你在合并 master 和 feature123 两个分支）： $ git checkout master flash/foo.fla # 或者... $ git checkout feature132 flash/foo.fla $ # 然后... $ git add flash/foo.fla 另一种方式是通过 git 输出文件——你可以输出到另外的文件名，然后当你决定了要用哪个后，再将选定的正确文件复制为正常的文件名： $ git show master:flash/foo.fla &gt; master-foo.fla $ git show feature132:flash/foo.fla &gt; feature132-foo.fla $ # 检出master-foo.fla和feature132-foo.fla $ # 假如说我们决定来自feature132的文件是正确的 $ rm flash/foo.fla $ mv feature132-foo.fla flash/foo.fla $ rm master-foo.fla $ git add flash/foo.fla 更新：感谢 Carl 在原博客文章上评论里的提醒，你实际上可以用 git checkout —ours flash/foo.fla 和 git checkout —theirs flash/foo.fla 来检出特定版本的文件，而不用记住你在合并的分支名字。就我个人来说喜欢更精确一点，但这也是一种方式... 记着在解决完冲突后要将文件加入提交（像我上面做的那样）。 服务器，分支和标签 5. 远端服务器 git 的一个超强大的功能就是可以有不止一个远端服务器（实际上你一直都在一个本地仓库上工作）。你并不是一定都要有这些服务器的写权限，你可以有多个可以读取的服务器（用来合并他们的工作）然后写入到另外一个仓库。添加一个新的远端服务器很简单： $ git remote add john git@github.com:johnsomeone/someproject.git 如果你想查看远端服务器的信息可以这样做： # 显示每个远端服务器的URL $ git remote -v # 提供更多详细信息 $ git remote show name 你随时都可以查看本地分支和远端分支的差异： $ git diff master..john/master 你也可以查看没有在远端分支上的 HEAD 的改动： $ git log remote/branch.. # 注意：..后面没有结束的特定引用 6. 标签 在 git 里有两种类型的标签——轻量级标签和带注释标签。记住技巧 2 里说过 git 是基于指针的，这两者之间的差异也很简单。轻量级标签只是一个简单的指向一次提交的带名字指针。你随时都可以将它指向另一个提交。带注释标签是一个指向标签对象的带名字指针，带有自己的信息和历史。因为有自己的信息，它可以根据需要用 GPG 签名。 建立这两种类型的标签都很简单（只有一个命令行开关的差异） $ git tag to-be-tested $ git tag -a v1.1.0 # 会提示输入标签的信息 7. 建立分支 在 git 里建立分支非常简单（而且像闪电一样快，因为它只需要创建一个小于 100 字节的文件）。用普通方式建立新分支并切换过去： $ git branch feature132 $ git checkout feature132 当然，如果你确定自己直接切换到新建的分支，可以用一个命令实现： $ git checkout -b feature132 如果你想重命名一个本地分支也很简单（可以显示发生了什么的较长的方式）： $ git checkout -b twitter-experiment feature132 $ git branch -d feature132 更新：你也可以（像 Brian Palmer 在原博客文章的评论里提出的）只用 git branch 的 -m 开关在一个命令里实现（像 Mike 提出的，如果你只指定了一个分支参数，就会重命名当前分支）： $ git branch -m twitter-experiment $ git branch -m feature132 twitter-experiment 8. 合并分支 也许在将来的某个时候，你希望将改动合并。有两种方式： $ git checkout master $ git merge feature83 # 或者... $ git rebase feature83 merge 和 rebase 之间的差别是 merge 会尝试处理改动并建立一个新的混合了两者的提交。rebase 会尝试把你从一个分支最后一次分离后的所有改动，一个个加到该分支的 HEAD 上。不过，在已经将分支推到远端服务器后不要再 rebase 了 - 这会引起冲突/问题。 如果你不确定在哪些分支上还有独有的工作——所以你也不知道哪些分支需要合并而哪些可以删除，git branch 有两个开关可以帮你： # 显示已经全部合并到当前分支的分支 $ git branch --merged # 显示没有合并到当前分支的分支 $ git branch --no-merged 9. 远端分支 如果你在本地有一个分支希望推到远端服务器上，你可以用一行命令推送上去： $ git push origin twitter-experiment:refs/heads/twitter-experiment # origin是我们服务器的名字，而twitter-experiment是分支名字 更新：感谢 Erlend 在原博客文章上的评论——这个实际上和 git push origin twitter-experiment 效果一样，不过使用完整的语法，你可以在两者之间使用不同的分支名（这样本地分支可以是 add-ssl-support 而远端是 issue-1723）。 如果你想在远端服务器上删除一个分支（注意分支名前面的冒号）： $ git push origin :twitter-experiment 如果你想查看所有远端分支的状态可以这样做： $ git remote show origin 这个命令可能会列出服务器上一些以前有过但现在已经不在了的分支。如果碰到这种情况你可以用下面的命令从你本地分支里清理掉： $ git remote prune 最后，如果你想在本地跟踪一个远端分支，普通的方式是： $ git branch --track myfeature origin/myfeature $ git checkout myfeature 不过，新版的 git 在使用 -b 标记检出分支时会自动设定跟踪： $ git checkout -b myfeature origin/myfeature 在储藏点，索引和文件系统中保存内容 10. 储藏 在 git 里你可以把当前工作状态放进一个储藏堆栈中，然后可以再取出来。最简单的情形是下面这样： $ git stash # 做点其他事情... $ git stash pop 许多人建议使用 git stash apply 来代替 pop，不过如果这样做的话最后会遗留一个很长的储藏列表。而 pop 会在全部加载后自动从堆栈中移除。如果使用过 git stash apply，你也可以使用下面的命令从堆栈上移除最后一项： $ git stash drop git 会基于当前的提交信息自动创建评论。如果你更希望有自定义信息的话（因为它可能和前一个提交没有任何联系）： $ git stash save &quot;My stash message&quot; 如果你希望从列表中取出一个特定的储藏点（不一定非得是最后一个）可以先列出它们然后用下面的方式取出： $ git stash list stash@{0}: On master: Changed to German stash@{1}: On master: Language is now Italian $ git stash apply stash@{1} 11. 交互式添加 在 subversion 的世界里你只能修改文件然后提交所有改动。而在 git 里你有强大得多的方式来提交部分文件或者甚至是部分补丁。提交部分文件或文件中的部分改动你需要进入交互式模式： $ git add -i staged unstaged path *** Commands *** 1: status 2: update 3: revert 4: add untracked 5: patch 6: diff 7: quit 8: help What now&gt; 这会让你进入一个基于菜单的交互式提示。你可以使用命令中的数字或高亮的字母（如果你在终端里打开了高亮的话）来进入相应的模式。然后就只是输入你希望操作的文件的数字了（你可以使用这样的格式，1 或者 1-4 或 2,4,7）。 如果你想进入补丁模式（交互式模式下按 p 或 5），你也可以直接进入： $ git add -p diff --git a/dummy.rb b/dummy.rb index 4a00477..f856fb0 100644 --- a/dummy.rb +++ b/dummy.rb @@ -1,5 +1,5 @@ class MyFoo def say - puts &quot;Annyong Haseyo&quot; + puts &quot;Guten Tag&quot; end end Stage this hunk [y,n,q,a,d,/,e,?]? 你可以看到下方会有一些选项供选择用来添加该文件的这个改动、该文件的所有改动，等等。使用 ? 命令可以详细解释这些选项。 12. 从文件系统里保存/取回改动 有些项目（比如 Git 项目本身）在 git 文件系统中直接保存额外文件而并没有将它们加入到版本控制中。 让我们从在 git 中存储一个随机文件开始： $ echo &quot;Foo&quot; | git hash-object -w --stdin 51fc03a9bb365fae74fd2bf66517b30bf48020cb 这样这个目标文件就已经保存到数据库中了，但是如果你没有设定一个指向它的指针的话它会被当做垃圾回收。最简单的方式是设定一个标签： $ git tag myfile 51fc03a9bb365fae74fd2bf66517b30bf48020cb 注意这里我们使用了标签 myfile。当我们需要使用这个文件的时候可以这样做： $ git cat-file blob myfile 这个对于一些工具文件很有用，开发者可能会用到（密码，GPG 密钥，等等）但是又不希望每次都检出到硬盘（尤其是在实际工作中）。 日志以及有哪些改动？ 13. 查看日志 长时间使用 Git 的话，不会没用过 git log 来查看最近的提交。不过，有一些技巧来更好地应用。比如，你可以使用下面的命令来查看每次提交的具体改动： $ git log -p 或者你可以仅仅查看有哪些文件改动： $ git log --stat 有个很不错的别名你可以试试，会显示简短提交名和一个不错的分支图并在一行里显示提交信息（有点像 gitk，但是是在命令行下）： $ git config --global alias.lol &quot;log --pretty=oneline --abbrev-commit --graph --decorate&quot; $ git lol * 4d2409a (master) Oops, meant that to be in Korean * 169b845 Hello world 14. 搜索日志 如果你想找特定提交者可以这样做： $ git log --author=Andy 更新：感谢 Johannes 的评论，我已经去掉了之前这里的一些有混淆的地方。 或者你想在提交信息里找一些相关字段： $ git log --grep=&quot;Something in the message&quot; 也有一个更强大的叫做 pickaxe 的命令用来查找包含了删除或添加的某个特定内容的提交（比如，该内容第一次出现或被删除）。这可以告诉你什么时候增加了一行（但这一行里的某个字符后面被改动过就不行了）： $ git log -S &quot;TODO: Check for admin status&quot; 假如你改动了一个特定的文件，比如 lib/foo.rb $ git log lib/foo.rb 比如说你有一个 feature/132 分支和 feature/145 分支，然后你想看看这两个分支上不在 master 分支里的提交（注意符号 ^ 是不在的意思）： $ git log feature/132 feature/145 ^master 你也可以使用 ActiveSupport 格式的日期来缩小到某个日期范围： $ git log --since=2.months.ago --until=1.day.ago 默认情况下会用 OR 来组合查询，但你可以轻易地改为 AND（如果你有超过一条的查询标准） $ git log --since=2.months.ago --until=1.day.ago --author=andy -S &quot;something&quot; --all-match 15. 查看/修改版本 有很多方式可以用来引用一个版本，看你记得哪个： $ git show 12a86bc38 # 根据版本 $ git show v1.0.1 # 根据标签 $ git show feature132 # 根据分支名 $ git show 12a86bc38^ # 一次提交的父节点 $ git show 12a86bc38~2 # 一次提交的祖父节点 $ git show feature132@{yesterday} # 时间相关 $ git show feature132@{2.hours.ago} # 时间相关 注意和之前部分有些不同，末尾 ^ 的意思是该提交的父节点——开始位置 ^ 的意思是不在这个分支。 16. 选择范围 最简单的方式： $ git log origin/master..new # [old]..[new] - 所有你还没有推送的提交 你也可以省略 [new]，将使用当前的 HEAD。 时光回溯和后悔药 17. 重置改动 如果你还没有提交的话可以用下面的命令轻松地取消改动： $ git reset HEAD lib/foo.rb 通常会使用 unstage 的别名，因为上面的看上去有些不直观。 $ git config --global alias.unstage &quot;reset HEAD&quot; $ git unstage lib/foo.rb 如果你已经提交了该文件，你可以做两件事 - 如果是最后一次提交你还可以改正： $ git commit --amend 这会取消最后一次提交，把工作分支回退到提交前标记了所有改动的状态，而且提交信息也都准备好可以修改或直接提交。 如果你已经提交过多次而且希望全部回退，你可以将分支重置到合适的位置。 $ git checkout feature132 $ git reset --hard HEAD~2 如果你实际上希望将分支指向一个完全不同的 SHA1（也许你要将一个分支的 HEAD 替换到另一个分支，或者之后的某次提交）你可以使用下面的较长的方式： $ git checkout FOO $ git reset --hard SHA 实际上有一个快速的方式（不需要先把你的工作分支切换到 FOO 再前进到 SHA）： $ git update-ref refs/heads/FOO SHA 18. 提交到了错误的分支 好吧，假如说你已经提交到了 master，但却应该创建一个叫 experimental 的主题分支更合适。要移动这些改动，你可以在当前位置创建分支，回退 HEAD 再检出新分支： $ git branch experimental # 创建一个指向当前master的位置的指针 $ git reset --hard master~3 # 移动master分支的指针到3个版本之前 $ git checkout experimental 如果你的改动是在分支的分支的分支上会更复杂。那样你需要做的是将分支基础切换到其他地方： $ git branch newtopic STARTPOINT $ git rebase oldtopic --onto newtopic 19. 交互式切换基础 这是一个我之前看过展示却没真正理解过的很赞的功能，现在觉得它就很简单了。假如说你提交了3次但是你希望更改顺序或编辑（或者合并）： $ git rebase -i master~3 然后这会启动你的编辑器并带有一些指令。你所要做的就是修改这些指令来选择/插入/编辑（或者删除）提交和保存/退出。然后在编辑完后你可以用 git rebase --continue 命令来让每一条指令生效。 如果你有修改，将会切换到你提交时所处的状态，之后你需要使用命令 git commit --amend 来编辑。 注意：在 rebase 的时候千万不要提交 - 只能先添加然后使用参数 --continue，--skip 或 --abort。 20. 清理 如果你提交了一些内容到你的分支（也许你从 SVN 导入了一些旧仓库），然后你希望把某个文件从历史记录中全部删掉： $ git filter-branch --tree-filter 'rm -f *.class' HEAD 如果你已经推送到 origin 了，但之后提交了一些垃圾改动，你也可以在推送前在本地系统里这样做： $ git filter-branch --tree-filter 'rm -f *.class' origin/master..HEAD 其他技巧 21. 你查看过的前一个引用 如果你知道自己之前查看过一个 SHA-1，但是随后做了一些重置/回退的操作，你可以使用 reflog 命令来列出最近查看过的 SHA-1 记录： $ git reflog $ git log -g # 和上面一样，但是使用'log'格式输出 22. 分支命名 一个可爱的小技巧 - 别忘了分支名并不限于 a-z 和 0-9。名字中可以用/和.将非常方便用来建立伪命名空间或版本，例如： $ # 生成版本132的改动历史 $ git shortlog release/132 ^release/131 $ # 贴上v1.0.1的标签 $ git tag v1.0.1 release/132 23. 找出谁是凶手 通常找出来谁改动了某个文件里的某行代码会很有用。实现这个功能的最简单命令是： $ git blame FILE 有时候这些改动来自其他文件（如果你合并了两个文件，或者你移动了某个函数）所以你可以使用下面的命令： $ # 显示内容来自哪个文件 $ git blame -C FILE 有时候通过点击各个改动然后回到很早很早以前来跟踪改动会很不错。有一个很好的内建 GUI 命令来做这个： $ git gui blame FILE 24. 数据维护 通常 git 不需要经常维护，它把自己照顾的很好。不过，你可以通过下面的命令查看数据统计： $ git count-objects -v 如果占用很多空间的话，你可以选择在你的本地仓库做垃圾回收。这不会影响推送或其他人，却会让一些命令运行更快而且减少空间占用： $ git gc 经常运行完整性检查也很有意义： $ git fsck --full 你也可以在末尾加上 --auto 参数（如果你在服务器上通过 crontab 经常/每天都运行这个命令的话），然后它只会在必要的时候才执行 fsck` 动作。 在检查的时候，看到 dangling 或 unreachable 是正常的，通常这是由回退 HEAD 或切换基础的结果。而看到 missing 或 sha1 mismatch 就不对了...找专业人士帮忙吧！ 25. 恢复遗失的分支 如果你使用 -D 参数删除了 experimental 分支，可以用下面的命令重新建立： $ git branch experimental SHA1_OF_HASH 如果你最近访问过的话，你通常可以用 git reflog 来找到 SHA1 哈希值。 另一种方式是使用 git fsck —lost-found。其中一个 dangling 的提交就是丢失的 HEAD（它只是已删除分支的 HEAD，而 HEAD 被引用为当前的 HEAD 所以它并不处于 dangling 状态）。 ","link":"https://faded.auspicious.space/post/git-25-advanced-skills/"},{"title":"Git——4 个阶段的撤销更改","content":"错误修改了代码不要紧，这里教你如何恢复以前的正确代码。 Git的4个阶段的撤销更改 虽然 git 诞生距今已有 12 年之久，网上各种关于 git 的介绍文章数不胜数，但是依然有很多人（包括我自己在内）对于它的功能不能完全掌握。以下的介绍只是基于我个人对于 git 的理解，并且可能生编硬造了一些不完全符合 git 说法的词语。目的只是为了让 git 通俗化，使初学者也能大概了解如何快速上手 git。同时，下面所有讨论，我们都假设只使用一个分支，也就是主分支 master 的情况，虽然这种作法并不符合 git 规范，但是现实情况中绝大部分用户是直接在 master 分支上进行工作的，所以在这里我们不去引入更加复杂的各种分支的情况，也不涉及标签 tag 的操作，只讲在最简单的主分支上如何回退。 基本概念 3 个步骤 正常情况下，我们的工作流就是 3 个步骤，对应上图中的 3 个箭头线： git add . git commit -m &quot;comment&quot; git push git add . 把所有文件放入暂存区； git commit 把所有文件从暂存区提交进本地仓库； git push 把所有文件从本地仓库推送进远程仓库。 4 个区 git 之所以令人费解，主要是它相比于 svn 等等传统的版本管理工具，多引入了一个暂存区（Stage）的概念，就因为多了这一个概念，而使很多人疑惑。其实，在初学者来说，每个区具体怎么工作的，我们完全不需要关心，而只要知道有这么 4 个区就够了： 工作区（Working Area） 暂存区（Stage） 本地仓库（Local Repository） 远程仓库（Remote Repository） 5 种状态 以上 4 个区，进入每一个区成功之后会产生一个状态，再加上最初始的一个状态，一共是 5 种状态。以下我们把这 5 种状态分别命名为： 未修改（Origin） 已修改（Modified） 已暂存（Staged） 已提交（Committed） 已推送（Pushed） 检查修改 了解了基本概念之后，我们来谈一谈犯错误之后如何撤销的问题。首先，我们要了解如何检查这 3 个步骤当中每一个步骤修改了什么，然后才好判断有没有修改成功。检查修改的二级命令都相同，都是 diff，只是参数有所不同。 已修改，未暂存 git diff 首先，我们来看一下，如果我们只是简单地在浏览器里保存了一下文件，但是还没有做 git add . 之前，我们如何检查有哪些修改。我们先随便拿一个文件来做一下实验： 我们在文件开头的第 2 行胡乱加了 4 个数字 1234，存盘，这时文件进入了已修改状态，但是还没有进入暂存区，我们运行 git diff，结果如下： diff --git a/index.md b/index.md index 73ff1ba..1066758 100644 --- a/index.md +++ b/index.md @@ -1,5 +1,5 @@ --- -layout: main +1234layout: main color: black --- git diff 的结果告诉我们哪些文件已经做了哪些修改。 已暂存，未提交 git diff --cached 现在我们把修改放入暂存区看一下。先执行 git add .，然后执行 git diff，你会发现没有任何结果： 这说明 git diff 这个命令只检查我们的工作区和暂存区之间的差异，如果我们想看到暂存区和本地仓库之间的差异，就需要加一个参数 git diff --cached： diff --git a/index.md b/index.md index 73ff1ba..1066758 100644 --- a/index.md +++ b/index.md @@ -1,5 +1,5 @@ --- -layout: main +1234layout: main color: black --- 这时候我们看到的差异是暂存区和本地仓库之间的差异。 已提交，未推送 git diff master origin/master 现在，我们把修改从暂存区提交到本地仓库，再看一下差异。先执行 git commit，然后再执行 git diff --cached，没有差异，执行 git diff master origin/master，可以看到差异： 在这里，master 就是你的本地仓库，而 origin/master 就是你的远程仓库，master 是主分支的意思，因为我们都在主分支上工作，所以这里两边都是 master，而 origin 就代表远程。 撤销修改 了解清楚如何检查各种修改之后，我们开始尝试各种撤销操作。 恢复已修改，未暂存 如果我们只是在编辑器里修改了文件，但还没有执行 git add .，这时候我们的文件还在工作区，并没有进入暂存区，我们可以用： git checkout . 或者 git reset --hard 来进行撤销操作。 可以看到，在执行完 git checkout . 之后，修改已被撤销，git diff 没有任何内容了。 一对反义词 &gt; git add . 的反义词是 git checkout .。做完修改之后，如果你想向前走一步，让修改进入暂存区，就执行 git add .，如果你想向后退一步，撤销刚才的修改，就执行 git checkout .。 恢复已暂存，未提交 你已经执行了 git add .，但还没有执行 git commit -m &quot;comment&quot;。这时候你意识到了错误，想要撤销，你可以执行： git reset git checkout . 或者 git reset --hard git reset 只是把修改退回到了 git add . 之前的状态，也就是说文件本身还处于已修改未暂存状态，你如果想退回未修改状态，还需要执行 git checkout .。 或许你已经注意到了，以上两个步骤都可以用同一个命令 git reset --hard 来完成。是的，就是这个强大的命令，可以一步到位地把你的修改完全恢复到未修改的状态。 恢复已提交，未推送 你的手太快，你既执行了 git add .，又执行了 git commit，这时候你的代码已经进入了你的本地仓库，然而你后悔了，怎么办？不要着急，还有办法。 git reset --hard origin/master 还是这个 git reset --hard 命令，只不过这次多了一个参数 origin/master，正如我们上面讲过的，origin/master 代表远程仓库，既然你已经污染了你的本地仓库，那么就从远程仓库把代码取回来吧。 已推送 很不幸，你的手实在是太快了，你既 git add 了，又 git commit 了，并且还 git push 了，这时你的代码已经进入远程仓库。如果你想恢复的话，还好，由于你的本地仓库和远程仓库是等价的，你只需要先恢复本地仓库，再强制 push 到远程仓库就好了： git reset --hard HEAD^ git push -f 总结 以上 4 种状态的撤销我们都用到了同一个命令 git reset --hard，前 2 种状态的用法甚至完全一样，所以只要掌握了 git reset --hard 这个命令的用法，从此你再也不用担心提交错误了。 ","link":"https://faded.auspicious.space/post/git-four-stages-of-reset/"},{"title":"JavaScript——发现闭包的强大威力","content":" [译]发现 JavaScript 中闭包的强大威力 闭包是一个可以访问外部作用域的内部函数，即使这个外部作用域已经执行结束。 1 作用域 作用域决定这个变量的生命周期及其可见性。 当我们创建了一个函数或者 {} 块，就会生成一个新的作用域。需要注意的是，通过 var 创建的变量只有函数作用域，而通过 let 和 const 创建的变量既有函数作用域，也有块作用域。 2 嵌套作用域 在 JavasScript 中函数里面可以嵌套函数，如下： (function autorun(){ let x = 1; function log(){ console.log(x); } log(); })(); log() 即是一个嵌套在 autorun() 函数里面的函数。在 log() 函数里面可以通过外部函数访问到变量 x。此时，log() 函数就是一个闭包。 闭包就是内部函数，我们可以通过在一个函数内部或者 {} 块里面定义一个函数来创建闭包。 2.1 外部函数作用域 内部函数可以访问外部函数中定义的变量，即使外部函数已经执行完毕。如下： (function autorun(){ let x = 1; setTimeout(function log(){ console.log(x); }, 10000); })(); 并且，内部函数还可以访问外部函数中定义的形参，如下： (function autorun(p){ let x = 1; setTimeout(function log(){ console.log(x);//1 console.log(p);//10 }, 10000); })(10); 2.2 外部块作用域 内部函数可以访问外部块中定义的变量，即使外部块已执行完毕，如下： { let x = 1; setTimeout(function log(){ console.log(x); }, 10000); } 3 词法作用域 词法作用域是指内部函数在定义的时候就决定了其外部作用域。 看如下代码： (function autorun(){ let x = 1; function log(){ console.log(x); }; function run(fn){ let x = 100; fn(); } run(log);//1 })(); log() 函数是一个闭包，它在这里访问的是 autorun() 函数中的 x 变量，而不是 run 函数中的变量。 ❗️闭包的外部作用域是在其定义的时候已决定，而不是执行的时候。 autorun() 的函数作用域即是 log() 函数的词法作用域。 4 作用域链 每一个作用域都有对其父作用域的引用。当我们使用一个变量的时候，JavaScript 引擎 会通过变量名在当前作用域查找，若没有查找到，会一直沿着作用域链一直向上查找，直到 global 全局作用域。 示例如下： let x0 = 0; (function autorun1(){ let x1 = 1; (function autorun2(){ let x2 = 2; (function autorun3(){ let x3 = 3; console.log(x0 + &quot; &quot; + x1 + &quot; &quot; + x2 + &quot; &quot; + x3);//0 1 2 3 })(); })(); })(); 我们可以看到，autorun3() 这个内部函数可以访问其自身局部变量 x3 ，也可以访问外部作用域中的 x1 和 x2 变量，以及全局作用域中的 x0 变量。即：闭包可以访问其外部（父）作用域中的定义的所有变量。 4.1 外部作用域执行完毕后 当外部作用域执行完毕后，内部函数还存活（仍在其他地方被引用）时，闭包才真正发挥其作用。譬如以下几种情况： 在异步任务例如 timer 定时器，事件处理，Ajax 请求中被作为回调。 被外部函数作为返回结果返回，或者返回结果对象中引用该内部函数。 考虑如下的几个示例： 4.1.1 Timer (function autorun(){ let x = 1; setTimeout(function log(){ console.log(x); }, 10000); })(); 变量 x 将一直存活着直到定时器的回调执行或者 clearTimeout() 被调用。 如果这里使用的是 setInterval()，那么变量 x 将一直存活到 clearInterval() 被调用。 译者注：原文中说变量 x 一直存活到 setTimeout() 或者 setInterval() 被调用是错误的。 4.1.2 Event (function autorun(){ let x = 1; $(&quot;#btn&quot;).on(&quot;click&quot;, function log(){ console.log(x); }); })(); 当变量 x 在事件处理函数中被使用时，它将一直存活直到该事件处理函数被移除。 4.1.3 Ajax (function autorun(){ let x = 1; fetch(&quot;http://&quot;).then(function log(){ console.log(x); }); })(); 变量 x 将一直存活到接收到后端返回结果，回调函数被执行。 在已上几个示例中，我们可以看到，log() 函数在父函数执行完毕后还一直存活着，log() 函数就是一个闭包。 除了 timer 定时器，事件处理，Ajax 请求等比较常见的异步任务，还有其他的一些异步 API 比如 HTML5 Geolocation，WebSockets，requestAnimationFrame() 也将使用到闭包的这一特性。 变量的生命周期取决于闭包的生命周期。被闭包引用的外部作用域中的变量将一直存活直到闭包函数被销毁。如果一个变量被多个闭包所引用，那么直到所有的闭包被垃圾回收后，该变量才会被销毁。 5 闭包与循环 闭包只存储外部变量的引用，而不会拷贝这些外部变量的值。 查看如下示例： function initEvents(){ for(var i=1; i&lt;=3; i++){ $(&quot;#btn&quot; + i).click(function showNumber(){ alert(i);//4 }); } } initEvents(); 在这个示例中，我们创建了 3 个闭包，皆引用了同一个变量 i，且这三个闭包都是事件处理函数。由于变量 i 随着循环自增，因此最终输出的都是同样的值。 修复这个问题最简单的方法是在 for 语句块中使用 let 变量声明，这将在每次循环中为 for 语句块创建一个新的局部变量。如下： function initEvents(){ for(let i=1; i&lt;=3; i++){ $(&quot;#btn&quot; + i).click(function showNumber(){ alert(i); // 1 2 3 }); } } initEvents(); 但是，如果变量声明在 for 语句块之外的话，即使用了 let 变量声明，所有的闭包还是会引用同一个变量，最终输出的还是同一个值。 6 闭包与封装性 封装性意味着信息隐藏。 6.1 函数与私有状态 通过闭包，我们可以创建拥有私有状态的函数，闭包使得状态被封装起来。 6.2 工厂模式与私有原型对象 我们先来看一个通过原型创建对象的常规方式，如下： let todoPrototype = { toString : function() { return this.id + &quot; &quot; + this.userName + &quot;: &quot; + this.title; } } function Todo(todo){ let newTodo = Object.create(todoPrototype); Object.assign(newTodo, todo); return newTodo; } 在这个例子中，todoPrototype 原型对象是一个全局对象。 我们可以通过闭包，只用创建原型对象一次，也能够被所有 Todo 函数调用所公用，并且保证其私有性。示例如下： let Todo = (function createTodoFactory(){ let todoPrototype = { toString : function() { return this.id + &quot; &quot; + this.userName + &quot;: &quot; + this.title; } } return function(todo){ let newTodo = Object.create(todoPrototype); Object.assign(newTodo, todo); return newTodo; } })(); let todo = Todo({id : 1, title: &quot;This is a title&quot;, userName: &quot;Cristi&quot;, completed: false }); 这里，Todo() 就是一个拥有私有状态的函数。 6.3 工厂模式与私有构造函数 查看如下代码： let Todo = (function createTodoFactory(){ function Todo(spec){ Object.assign(this, spec); } return function(spec){ let todo = new Todo(spec); return Object.freeze(todo); } })(); 这里，Todo() 工厂函数就是一个闭包。通过它，不管是否使用 new，我们都可以创建不可变对象，原型对象也只用创建一次，并且它是私有的。 let todo = Todo({title : &quot;A description&quot;}); todo.title = &quot;Another description&quot;; // Cannot assign to read only property 'title' of object todo.toString = function() {}; //Cannot assign to read only property 'toString' of object 而且，在内存快照中，我们可以通过构造函数名来识别这些示例对象。 6.4 翻译功能与私有 map 通过闭包，我们可以创建一个 map，在所有翻译调用中被使用，且是私有的。 示例如下： let translate = (function(){ let translations = {}; translations[&quot;yes&quot;] = &quot;oui&quot;; translations[&quot;no&quot;] = &quot;non&quot;; return function(key){ return translations[key]; } })(); translate(&quot;yes&quot;); //oui 6.5 自增生成器函数 通过闭包，我们可以创建自增生成器函数。同样，内部状态是私有的。示例如下： function createAGenerate(count, increment) { return function(){ count += increment; return count; } } let generateNextNumber = createAGenerate(0, 1); console.log(generateNextNumber()); //1 console.log(generateNextNumber()); //2 console.log(generateNextNumber()); //3 let generateMultipleOfTen = createAGenerate(0, 10); console.log(generateMultipleOfTen()); //10 console.log(generateMultipleOfTen()); //20 console.log(generateMultipleOfTen()); //30 译者注：原文中依次输出0,1,2,0,10,20是有误的，感谢@Round的指正 6.6 对象与私有状态 以上示例中，我们可以创建一个拥有私有状态的函数。同时，我们也可以创建多个拥有同一私有状态的函数。基于此，我们还可以创建一个拥有私有状态的对象。 示例如下： function TodoStore(){ let todos = []; function add(todo){ todos.push(todo); } function get(){ return todos.filter(isPriorityTodo).map(toTodoViewModel); } function isPriorityTodo(todo){ return task.type === &quot;RE&quot; &amp;&amp; !task.completed; } function toTodoViewModel(todo) { return { id : todo.id, title : todo.title }; } return Object.freeze({ add, get }); } TodoStore() 函数返回了一个拥有私有状态的对象。在外部，我们无法访问私有的 todos 变量，并且 add 和 get 这两个闭包拥有相同的私有状态。在这里，TodoStore() 是一个工厂函数。 6.7 闭包 vs 纯函数 闭包就是那些引用了外部作用域中变量的函数。 为了更好的理解，我们将内部函数拆成闭包和纯函数两个方面： 闭包是那些引用了外部作用域中变量的函数。 纯函数是那些没有引用外部作用域中变量的函数，它们通常返回一个值并且没有副作用。 在上述例子中，add() 和 get() 函数是闭包，而 isPriorityTodo() 和 toTodoViewModel() 则是纯函数。 7 闭包在函数式编程中的应用 闭包在函数式编程中也应用广泛。譬如，underscore 源码中 函数相关小节 中的所有函数都利用了闭包这一特性。 A function decorator is a higher-order function that takes one function as an argument and returns another function, and the returned function is a variation of the argument function — Javascript Allongé 装饰器函数也使用了闭包的特性。 我们来看如下 not 这个简单的装饰器函数： function not(fn){ return function decorator(...args){ return !fn.apply(this, args); } } decorator() 函数引用了外部作用域的 fn 变量，因此它是一个闭包。 如果你想知道更多关于装饰器相关的知识，可以查看这篇文章。 8 垃圾回收 在 JavaScript 中，局部变量会随着函数的执行完毕而被销毁，除非还有指向他们的引用。当闭包本身也被垃圾回收之后，这些闭包中的私有状态随后也会被垃圾回收。通常我们可以通过切断闭包的引用来达到这一目的。 在这个例子中，我们首先创建了一个 add() 闭包。 let add = (function createAddClosure(){ let arr = []; return function(obj){ arr.push(obj); } })(); 随后，我们又定义了两个函数： addALotOfObjects() 往闭包变量 arr 中加入对象。 clearAllObjects() 将闭包函数置为 null 。 并且两个函数皆作为事件处理函数： function addALotOfObjects(){ for(let i=1; i&lt;=10000;i++) { add(new Todo(i)); } } function clearAllObjects(){ if(add){ add = null; } } $(&quot;#add&quot;).click(addALotOfObjects); $(&quot;#clear&quot;).click(clearAllObjects); 当我点击 Add 按钮时，将往闭包变量 arr 中加入 10000 个 todo 对象，内存快照如下： 当我点击 Clear 按钮时，我们将闭包引用置为 null。随后，闭包变量 arr 将被垃圾回收，内存快照如下： 9 避免全局变量 在 JavaScript 中，我们很容易创建出全局变量。任何定义在函数和 {} 块之外的变量都是全局的，定义在全局作用域中的函数也是全局的。 这里以定义创建不同对象的工厂函数为例。为了避免将所有的工厂函数都放在全局作用域下，最简单的方法就是将他们挂在 app 全局变量下。 示例如下： let app = Loader(); app.factory(function DataService(args){ return {}}); app.factory(function Helper(args){ return {}}); app.factory(function Mapper(args){ return {}}); app.factory(function Model(args){}); app.factory() 方法还可以将不同的工厂函数归类到不同的模块中。下面这个示例就是将 Timer 工厂函数归类到 tools 模块下。 app.factory(&quot;tools&quot;)(function Timer(args){ return {}}); 我们可以在 app 对象上暴露一个 start 方法来作为应用的入口点，通过回调函数中 factories 参数来访问这些工厂函数。这里 start() 函数只能被调用一次，如下： app.start(function startApplication(factories){ let helper = factories.Helper(); let dataService = factories.DataService(); let model = factories.Model({ dataService : dataService, helper : helper, timer : factories.tools.Timer() }); }); A Composition Root is a (preferably) unique location in an application where modules are composed together—Mark Seemann 9.1 loader 对象 让我们来将 app 完善为一个 loader 对象，示例如下： function Loader(){ let modules = Object.create(null); let started = false; function getNamespaceModule(modulesText){ let parent = modules; if(modulesText){ let parts = modulesText.split('.'); for(let i=0; i&lt;parts.length; i++){ let part = parts[i]; if (typeof parent[part] === &quot;undefined&quot;) { parent[part] = Object.create(null); } parent = parent[part]; } } return parent; } function addFunction(namespace, fn){ if(typeof(fn) !== &quot;function&quot;) { throw &quot;Only functions can be added&quot;; } let module = getNamespaceModule(namespace); let fnName = fn.name; module[fnName] = fn; } function addNamespace(namespace){ return function(fn){ addFunction(namespace, fn) } } function factory(){ if(typeof(arguments[0]) === &quot;string&quot;){ return addNamespace(arguments[0]); } else { return addFunction(null, arguments[0]); } } function start(startApplication){ if(started){ throw &quot;App can be started only once&quot;; } startApplication(Object.freeze(modules)); started = true; } return Object.freeze({ factory, start }); }; let app = Loader(); factory() 方法用于添加新的工厂函数到内部变量 modules 中。 start() 方法则会调用回调函数，在回调函数中访问内部变量。 通过 factory() 定义工厂函数，将 start() 作为整个应用中调用各种工厂函数生成不同对象的唯一入口点，这是如此简洁优雅的方式。 在这里，factory 和 start 都是闭包。 10 总结 闭包是一个可以访问外部作用域中变量的内部函数。 这些被引用的变量直到闭包被销毁时才会被销毁。 闭包使得 timer 定时器，事件处理，Ajax 请求等异步任务更加容易。 可以通过闭包来达到封装性。 最后，想获得更多关于 JavaScript 函数相关知识，可以查看以下文章： Discover Functional Programming in JavaScript with this thorough introduction Discover the power of first class functions How point-free composition will make you a better functional programmer Here are a few function decorators you can write from scratch Make your code easier to read with Functional Programming ","link":"https://faded.auspicious.space/post/javascript-discover-the-power-of-closures/"},{"title":"忘记 jQuery 使用原生接口","content":"jQuery VS JavaScript原生API 1 选择元素 // jQuery var els = $('.el'); //==========================================================// // 原生方法 var els = document.querySelectorAll('.el'); // 函数法 var $ = function (el) { return document.querySelectorAll(el); } var els = $('.el'); 2 创建元素 // jQuery var newEl = $('&lt;div/&gt;'); //==========================================================// // 原生方法 var newEl = document.createElement('div'); 3 添加 / 移除 / 切换类 // jQuery $('.el').addClass('class'); $('.el').removeClass('class'); $('.el').toggleClass('class'); //==========================================================// // 原生方法 document.querySelector('.el').classList.add('class'); document.querySelector('.el').classList.remove('class'); document.querySelector('.el').classList.toggle('class'); 4 判断是否包含类 // jQuery $('.el').hasClass('className'); $('.el').has('.className'); //也可以用来 判断是否包含某个元素 //==========================================================// // 原生方法(1) _hasClass(document.querySelector('.el'), className); function _hasClass( elements,cName ){ return !!elements.className.match( new RegExp( &quot;(\\\\s|^)&quot; + cName + &quot;(\\\\s|$)&quot;) ); }; // 原生方法(2) if(el.classList.contains(&quot;someClass&quot;)){} 5 添加事件监听器 // jQuery $('.el').on('event', function() { }); //==========================================================// // 原生方法 [].forEach.call(document.querySelectorAll('.el'), function (el) { el.addEventListener('event', function() { }, false); }); 原生－DOM绑定事件－优化1 参考HERE //DOM绑定事件-之自执行 var BindEvent = (function () { if ('addEventListener' in document) { return function (dom, event, handle, ex) { dom.addEventListener(event, handle, ex || false); } } else if ('attachEvent' in document) { return function (dom, event, handle) { dom.attachEvent('on' + event, handle); } } else { return function (dom, event, handle) { dom['on' + event] = handle; } } })();``` ## 原生－DOM绑定事件－优化2 ```javascript //DOM绑定事件-之惰性加载(调用方去触发BindEvent之时才去做初始化)// var BindEvent = function (dom, event, handle, ex) { if ('addEventListener' in document) { BindEvent = function (dom, event, handle, ex) { dom.addEventListener(event, handle, ex || false); } } else if ('attachEvent' in document) { trueBindEvent = function (dom, event, handle) { dom.attachEvent('on' + event, handle); } } else { BindEvent = function (dom, event, handle) { dom['on' + event] = handle; } } BindEvent(dom, event, handle, ex); }; 6 设置 / 获取属性 // jQuery $('.el').filter(':first').attr('key', 'value'); $('.el').filter(':first').attr('key'); //==========================================================// // 原生方法 document.querySelector('.el').setAttribute('key', 'value'); document.querySelector('.el').getAttribute('key'); 7 附加内容（Append） // jQuery $('.el').append($('&lt;div/&gt;')); //==========================================================// // 原生方法 document.querySelector('.el').appendChild(document.createElement('div')); 8 克隆元素 // jQuery var clonedEl = $('.el').clone(); //==========================================================// // 原生方法 var clonedEl = document.querySelector('.el').cloneNode(true); 9 移除元素 // jQuery $('.el').remove(); //==========================================================// // 原生方法 remove('.el'); function remove(el) { var toRemove = document.querySelector(el); toRemove.parentNode.removeChild(toRemove); } 10 获取父元素 // jQuery $('.el').parent(); //==========================================================// // 原生方法 document.querySelector('.el').parentNode; 11 上一个 / 下一个元素 // jQuery $('.el').prev(); $('.el').next(); //==========================================================// // 原生方法 document.querySelector('.el').previousElementSibling; document.querySelector('.el').nextElementSibling; 12 修改CSS属性 总是通过 Javascript 修改和检索 CSS 属性，这样会比使用 jQuery CSS 函数更加简单快速，并且没有任何不必要的代码。 //----设置CSS属性---- /* jQuery */ $(el).css({ background: &quot;#FF0000&quot;, &quot;box-shadow&quot;: &quot;1px 1px 5px 5px red&quot;, width: &quot;100px&quot;, height: &quot;100px&quot;, display: &quot;block&quot; }); //==========================================================// /* 原生 */ var el = document.querySelector(&quot;.main-content&quot;); el.style.background = &quot;#FF0000&quot;; el.style.width = &quot;100px&quot;; el.style.height = &quot;100px&quot;; el.style.display = &quot;block&quot;; el.style.boxShadow = &quot;1px 1px 5px 5px red&quot;; 13 XHR 或 Ajax // jQuery $.get('url', function (data) { }); $.post('url', {data: data}, function (data) { }); //==========================================================// // 原生方法 // get var xhr = new XMLHttpRequest(); xhr.open('GET', url); xhr.onreadystatechange = function (data) { } xhr.send(); // post var xhr = new XMLHttpRequest() xhr.open('POST', url); xhr.onreadystatechange = function (data) { } xhr.send({data: data}); ","link":"https://faded.auspicious.space/post/how-to-forget-about-jquery-and-start-using-native/"},{"title":"JavaScript——闭包实际场景应用","content":"1. 函数防抖 比如要缩放窗口 触发 onresize 事件 需要在这时候做一件事情,但是我们希望拖动的时候只触发一次,比如： window.onresize = function () { console.log('onresize')//只想触发一次 } 一般方法 window.onresize = function () { debounce(fn, 1000) } var fn = function () { console.log('fn') } var time = '' function debounce(fn, timeLong) { if (time) { clearTimeout(time) time = '' } time = setTimeout(function () { fn() }, timeLong) } 闭包 window.onresize = debounce(fn, 500) function debounce(fn) { var timer = null return function () { if (timer) { //timer第一次执行后会保存在内存里 永远都是执行器 直到最后被触发 clearTimeout(timer) timer = null } timer = setTimeout(function () { fn() }, 1000) } } var fn = function () { console.log('fn') } 2 使用闭包设计单例模式 class CreateUser { constructor(name) { this.name = name; this.getName(); } getName() { return this.name; } } // 代理实现单例模式 var ProxyMode = (function () { var instance = null; return function (name) { if (!instance) { instance = new CreateUser(name); } return instance; } })(); // 测试单体模式的实例 var a = ProxyMode(&quot;aaa&quot;); var b = ProxyMode(&quot;bbb&quot;); // 因为单体模式是只实例化一次，所以下面的实例是相等的 console.log(a === b); //true 3 为多个组件独立属性 假如我现在要在页面中使用 Echarts画 6 个线状图，需要 6 个容器。需要为每个容器元素声明一个独立 id，不然会混乱。 constructor(){ this.state = { id: &quot;EchartsLine&quot; + Util.clourse() }; } componentDidMount() { this.myEChart = echarts.init(document.getElementById(this.state.id));//不同 id } &lt;div id={this.state.id} className='echarts-line'&gt;&lt;/div&gt; clourse(){ let clourse = (function () { var a = 1; return function () { return a++; } })(this); this.clourse = clourse; } //使用数字命名 不用害怕被篡改 4 设置私有变量 内部属性在 Java 里使用 private 就可以，但是 JS 还没有这个东东。 let _width = Symbol(); class Private { constructor(s) { this[_width] = s } foo() { console.log(this[_width]) } } var p = new Private(&quot;50&quot;); p.foo(); console.log(p[_width]); //可以拿到 // 赋值到闭包里 let sque = (function () { let _width = Symbol(); class Squery { constructor(s) { this[_width] = s } foo() { console.log(this[_width]) } } return Squery })(); let ss = new sque(20); ss.foo(); console.log(ss[_width]) 5 拿到正确的值 for (var i = 0; i &lt; 10; i++) { setTimeout(function () { console.log(i) // 10 个 10 }, 1000) } 遇到这种问题 如何用解决呢？ for (var i = 0; i &lt; 10; i++) { ((j) =&gt; { setTimeout(function () { console.log(j)//1-10 }, 1000) })(i) } 原理是 声明了10个自执行函数，保存当时的值到内部。 ","link":"https://faded.auspicious.space/post/javascript-closure-application-scenarios/"},{"title":"JavaScript——闭包简介","content":" 闭包详解一 一、什么是闭包 《JavaScript高级程序设计》这样描述： 闭包是指有权访问另一个函数作用域中的变量的函数； 《JavaScript权威指南》这样描述： 从技术的角度讲，所有的JavaScript函数都是闭包：它们都是对象，它们都关联到作用域链。 《你不知道的JavaScript》这样描述： 当函数可以记住并访问所在的词法作用域时，就产生了闭包，即使函数是在当前词法作用域之外执行。 我最认同的是《你不知道的JavaScript》中的描述，虽然前面的两种说法都没有错，但闭包应该是基于词法作用域书写代码时产生的自然结果，是一种现象！你也不用为了利用闭包而特意的创建，因为闭包的在你的代码中随处可见，只是你还不知道当时你写的那一段代码其实就产生了闭包。 二、讲解闭包 上面已经说到，当函数可以记住并访问所在的词法作用域时，就产生了闭包，即使函数是在当前词法作用域之外执行。 看一段代码 function fn1() { var name = 'iceman'; function fn2() { console.log(name); } fn2(); } fn1(); 如果是根据《JavaScript高级程序设计》和《JavaScript权威指南》来说，上面的代码已经产生闭包了。fn2 访问到了 fn1 的变量，满足了条件“有权访问另一个函数作用域中的变量的函数”，fn2 本身是个函数，所以满足了条件“所有的JavaScript函数都是闭包”。 这的确是闭包，但是这种方式定义的闭包不太好观察。 再看一段代码： function fn1() { var name = 'iceman'; function fn2() { console.log(name); } return fn2; } var fn3 = fn1(); fn3(); 这样就清晰地展示了闭包： fn2 的词法作用域能访问 fn1的作用域； 将 fn2 当做一个值返回； fn1 执行后，将 fn2 的引用赋值给 fn3； 执行 fn3，输出了变量 name。 我们知道通过引用的关系，fn3 就是 fn2 函数本身。执行 fn3 能正常输出 name，这不就是 fn2 能记住并访问它所在的词法作用域，而且 fn2 函数的运行还是在当前词法作用域之外了。 正常来说，当 fn1 函数执行完毕之后，其作用域是会被销毁的，然后垃圾回收器会释放那段内存空间。而闭包却很神奇的将 fn1 的作用域存活了下来，fn2 依然持有该作用域的引用，这个引用就是闭包。 总结：某个函数在定义时的词法作用域之外的地方被调用，闭包可以使该函数极限访问定义时的词法作用域。 注意：对函数值的传递可以通过其他的方式，并不一定值有返回该函数这一条路，比如可以用回调函数： function fn1() { var name = 'iceman'; function fn2() { console.log(name); } fn3(fn2); } function fn3(fn) { fn(); } fn1(); 本例中，将内部函数 fn2 传递给 fn3，当它在 fn3 中被运行时，它是可以访问到 name变量的。 所以无论通过哪种方式将内部的函数传递到所在的词法作用域以外，它都回持有对原始作用域的引用，无论在何处执行这个函数都会使用闭包。 三、再次解释闭包 以上的例子会让人觉得有点学院派了，但是闭包绝不仅仅是一个无用的概念，你写过的代码当中肯定有闭包的身影，比如类似如下的代码： function waitSomeTime(msg, time) { setTimeout(function () { console.log(msg) }, time); } waitSomeTime('hello', 1000); 定时器中有一个匿名函数，该匿名函数就有涵盖 waitSomeTime 函数作用域的闭包，因此当 1 秒之后，该匿名函数能输出 msg。 另一个很经典的例子就是 for 循环中使用定时器延迟打印的问题： for (var i = 1; i &lt;= 10; i++) { setTimeout(function () { console.log(i); }, 1000); } 在这段代码中，我们对其的预期是输出 1~10，但却输出 10 次 11。这是因为 setTimeout 中的匿名函数执行的时候，for 循环都已经结束了，for 循环结束的条件是 i 大于 10，所以当然是输出 10 次 11 咯。 究其原因：i 是声明在全局作用中的，定时器中的匿名函数也是执行在全局作用域中，那当然是每次都输出 11 了。 原因知道了，解决起来就简单了，我们可以让i在每次迭代的时候，都产生一个私有的作用域，在这个私有的作用域中保存当前i的值。 for (var i = 1; i &lt;= 10; i++) { (function () { var j = i; setTimeout(function () { console.log(j); }, 1000); })(); } 这样就达到我们的预期了呀，让我们用一种比较优雅的写法改造一些，将每次迭代的i作为实参传递给自执行函数，自执行函数中用变量去接收： for (var i = 1; i &lt;= 10; i++) { (function (j) { setTimeout(function () { console.log(j); }, 1000); })(i); } 四、闭包的应用 闭包的应用比较典型是定义模块，我们将操作函数暴露给外部，而细节隐藏在模块内部： function module() { var arr = []; function add(val) { if (typeof val == 'number') { arr.push(val); } } function get(index) { if (index &lt; arr.length) { return arr[index] } else { return null; } } return { add: add, get: get } } var mod1 = module(); mod1.add(1); mod1.add(2); mod1.add('xxx'); console.log(mod1.get(2)); ","link":"https://faded.auspicious.space/post/javascript-closure-introduction/"},{"title":"JavaScript——7 个角度吃透 Lodash 防抖节流原理","content":" 浅出篇 7 个角度吃透 Lodash 防抖节流原理 节流函数 Throttle 我们先来看一张图，这张图充分说明了 Throttle（节流）和 Debounce（防抖）的区别，以及在不同配置下产生的不同效果，其中 mousemove 事件每 50 ms 触发一次，即下图中的每一小隔是 50 ms。今天这篇文章就从下面这张图开始介绍。 角度 1 lodash.throttle(fn, 200, {leading: true, trailing: true}) mousemove 第一次触发 先来看下 throttle 源码 function throttle(func, wait, options) { // 首尾调用默认为 true let leading = true let trailing = true if (typeof func !== 'function') { throw new TypeError('Expected a function') } // options 是否是对象 if (isObject(options)) { leading = 'leading' in options ? !!options.leading : leading trailing = 'trailing' in options ? !!options.trailing : trailing } // maxWait 为 wait 的防抖函数 return debounce(func, wait, { leading, trailing, 'maxWait': wait, }) } 所以 throttle(fn, 200, {leading: true, trailing: true}) 返回内容是 debounce(fn, 200, {leading: true, trailing: true, maxWait: 200})，多了 maxWait: 200 这部分。 先打个预防针，后面即将开始比较难的部分，看下 debounce 入口函数。 // 入口函数，返回此函数 function debounced(...args) { // 获取当前时间 const time = Date.now() // 判断此时是否应该执行 func 函数 const isInvoking = shouldInvoke(time) // 赋值给闭包，用于其他函数调用 lastArgs = args lastThis = this lastCallTime = time // 执行 if (isInvoking) { // 无 timerId 的情况有两种： // 1、首次调用 // 2、trailingEdge 执行过函数 if (timerId === undefined) { return leadingEdge(lastCallTime) } // 如果设置了最大等待时间，则立即执行 func // 1、开启定时器，到时间后触发 trailingEdge 这个函数。 // 2、执行 func，并返回结果 if (maxing) { // 循环定时器中处理调用 timerId = startTimer(timerExpired, wait) return invokeFunc(lastCallTime) } } // 一种特殊情况，trailing 设置为 true 时，前一个 wait 的 trailingEdge 已经执行了函数 // 此时函数被调用时 shouldInvoke 返回 false，所以要开启定时器 if (timerId === undefined) { timerId = startTimer(timerExpired, wait) } // 不需要执行时，返回结果 return result } 对于 debounce(fn, 200, {leading: true, trailing: true, maxWait: 200}) 来说，会经历如下过程。 shouldInvoke(time) 中，因为满足条件 lastCallTime === undefined，所以返回 true。 lastCallTime = time，所以 lastCallTime 等于当前时间，假设为 0。 timerId === undefined 满足，执行 leadingEdge(lastCallTime) 方法。 // 执行连续事件刚开始的那次回调 function leadingEdge(time) { // 1、设置上一次执行 func 的时间 lastInvokeTime = time // 2、开启定时器，为了事件结束后的那次回调 timerId = startTimer(timerExpired, wait) // 3、如果配置了 leading 执行传入函数 func // leading 来源自 !!options.leading return leading ? invokeFunc(time) : result } 在 leadingEdge(time) 中，设置 lastInvokeTime 为当前时间即 0，开启 200 毫秒定时器，执行 invokeFunc(time) 并返回。 // 执行 Func 函数 function invokeFunc(time) { // 获取上一次执行 debounced 的参数 const args = lastArgs // 获取上一次的 this const thisArg = lastThis // 重置 lastArgs = lastThis = undefined lastInvokeTime = time result = func.apply(thisArg, args) return result } 在 invokeFunc(time) 中，执行 func.apply(thisArg, args)，即 fn 函数第一次执行，并把结果赋值给 result，便于后续触发时直接返回。同时重置 lastInvokeTime 为当前时间即 0，清空 lastArgs 和 lastThis。 第一次触发已经完成，注意此时 lastCallTime 和 lastInvokeTime 都为 0，200 毫秒的定时器还在运行中。 mousemove 第二次触发 50 毫秒后第二次触发到来，此时当前时间 time 为 50，wait 为 200， maxWait 为 200，maxing 为 true，lastCallTime 和 lastInvokeTime 都为 0，timerId 定时器存在，我们来看下执行步骤。 function shouldInvoke(time) { // 当前时间距离上一次调用 debounce 的时间差 const timeSinceLastCall = time - lastCallTime // 当前时间距离上一次执行 func 的时间差 const timeSinceLastInvoke = time - lastInvokeTime // 下述 4 种情况返回 true return ( lastCallTime === undefined || (timeSinceLastCall &gt;= wait) || (timeSinceLastCall &lt; 0) || (maxing &amp;&amp; timeSinceLastInvoke &gt;= maxWait) ) } shouldInvoke(time) 中，timeSinceLastCall 为 50，timeSinceLastInvoke 为 50，4 种条件都不满足，返回 false。 此时 isInvoking 为 false，同时 timerId === undefined 不满足，直接返回第一次触发时的 result。 第二次触发完成，并不会执行 fn，只会返回上次执行的结果 result。 第三次和第四次触发时，效果一样，就不再重复了。 mousemove 第五次触发 距第一次触发 200 毫秒后第五次触发到来，此时当前时间 time 为 200，wait 为 200， maxWait 为 200，maxing 为 true，lastCallTime 为 150，lastInvokeTime 为 0，timerId 定时器存在，我们来看下执行步骤。 shouldInvoke(time) 中，timeSinceLastInvoke 为 200，满足 (maxing &amp;&amp; timeSinceLastInvoke &gt;= maxWait)，所以返回 true。 // debounced 方法中执行到这部分 if (maxing) { // 循环定时器中处理调用 timerId = startTimer(timerExpired, wait) return invokeFunc(lastCallTime) } 满足 maxing 条件，重新开启 200 毫秒的定时器，并执行 invokeFunc(lastCallTime) 函数。 invokeFunc(time) 中，重置 lastInvokeTime 为当前时间即 200，清空 lastArgs 和 lastThis。 第六、七、八次触发时，同第二次触发效果一致，就不再重复了。 mousemove 停止触发 假设第八次触发之后就停止了滚动，在第八次触发时 time 为 350，所以如果有第九次触发，那么此时是应该执行 fn 的，但是此时 mousemove 已经停止了触发，那么还会执行 fn 吗？答案是依旧执行，因为最开始设置了 {trailing: true}。 // 开启定时器 function startTimer(pendingFunc, wait) { // 没传 wait 时调用 window.requestAnimationFrame() if (useRAF) { // 若想在浏览器下次重绘之前继续更新下一帧动画 // 那么回调函数自身必须再次调用 window.requestAnimationFrame() root.cancelAnimationFrame(timerId); return root.requestAnimationFrame(pendingFunc) } // 不使用 RAF 时开启定时器 return setTimeout(pendingFunc, wait) } 在第五次触发时开启了 200 毫秒的定时器，所以在时间 time 到 400 时会执行 pendingFunc，此时的 pendingFunc 就是 timerExpired 函数，来看下具体的代码。 // 定时器回调函数，表示定时结束后的操作 function timerExpired() { const time = Date.now() // 1、是否需要执行 // 执行事件结束后的那次回调，否则重启定时器 if (shouldInvoke(time)) { return trailingEdge(time) } // 2、否则 计算剩余等待时间，重启定时器，保证下一次时延的末尾触发 timerId = startTimer(timerExpired, remainingWait(time)) } 此时在 shouldInvoke(time) 中，time 为 400，lastInvokeTime 为 200，timeSinceLastInvoke 为 200，满足 (maxing &amp;&amp; timeSinceLastInvoke &gt;= maxWait)，所以返回 true。 // 执行连续事件结束后的那次回调 function trailingEdge(time) { // 清空定时器 timerId = undefined // trailing 和 lastArgs 两者同时存在时执行 // trailing 来源自 'trailing' in options ? !!options.trailing : trailing // lastArgs 标记位的作用，意味着 debounce 至少执行过一次 if (trailing &amp;&amp; lastArgs) { return invokeFunc(time) } // 清空参数 lastArgs = lastThis = undefined return result } 之后执行 trailingEdge(time)，在这个函数中判断 trailing 和 lastArgs，此时这两个条件都是 true，所以会执行 invokeFunc(time)，最终执行函数 fn。 这里需要说明以下两点： 如果设置了 {trailing: false}，那么最后一次是不会执行的。对于 throttle 和 debounce 来说，默认值是 true，所以如果没有特意指定 trailing，那么最后一次是一定会执行的。 对于 lastArgs 来说，执行 debounced 时会赋值，即每次触发都会重新赋值一次，那什么时候清空呢，在 invokeFunc(time) 中执行 fn 函数时重置为 undefined，所以如果 debounced 只触发了一次，即使设置了 {trailing: true} 那也不会再执行 fn 函数，这个就解答了上篇文章留下的第一道思考题。 角度 2 lodash.throttle(fn, 200, {leading: true, trailing: false}) 在角度 1 之 mousemove 停止触发这部分中说到，如果不设置 trailing 和设置 {trailing: true} 效果是一样的，事件回调结束后都会再执行一次传入函数 fn，但是如果设置了 {trailing: false}，那么事件回调结束后是不会再执行 fn 的。 此时的配置对比角度 1 来说，区别在于设置了 {trailing: false}，所以实际效果对比 1 来说，就是最后不会额外再执行一次，效果见第一张图。 角度 3 lodash.throttle(fn, 200, {leading: false, trailing: true}) 此时的配置和角度 1 相比，区别在于设置了 {leading: false}，所以直接看 leadingEdge(time) 方法就可以了。 // 执行连续事件刚开始的那次回调 function leadingEdge(time) { // 1、设置上一次执行 func 的时间 lastInvokeTime = time // 2、开启定时器，为了事件结束后的那次回调 timerId = startTimer(timerExpired, wait) // 3、如果配置了 leading 执行传入函数 func // leading 来源自 !!options.leading return leading ? invokeFunc(time) : result } 在这里，会开启 200 毫秒的定时器，同时因为 leading 为 false，所以并不会执行 invokeFunc(time)，只会返回 result，此时的 result 值是 undefined。 这里开启一个定时器的目的是为了事件结束后的那次回调，即如果设置了 {trailing: true} 那么最后一次回调将执行传入函数 fn，哪怕 debounced 函数只触发一次。 这里指定了 {leading: false}，那么 leading 的初始值是什么呢？在 debounce 中是 false，在 throttle 中是 true。所以在 throttle 中不需要刚开始就触发时，必须指定 {leading: false}，在 debounce 中就不需要了，默认不触发。 防抖函数 Debounce 角度 4 lodash.debounce(fn, 200, {leading: false, trailing: true}) 此时相比较 throttle 来说，缺少了 maxWait 值，所以具体触发过程中的判断就不一样了，来详细看一遍。 在入口函数 debounced 中，执行 shouldInvoke(time)，前面讨论过因为第一次触发所以会返回 true，之后执行 leadingEdge(lastCallTime)。 // 执行连续事件刚开始的那次回调 function leadingEdge(time) { // 1、设置上一次执行 func 的时间 lastInvokeTime = time // 2、开启定时器，为了事件结束后的那次回调 timerId = startTimer(timerExpired, wait) // 3、如果配置了 leading 执行传入函数 func // leading 来源自 !!options.leading return leading ? invokeFunc(time) : result } 在 leadingEdge 中，因为 leading 为 false，所以并不执行 fn，只开启 200 毫秒的定时器，并返回 undefined。此时 lastInvokeTime 为当前时间，假设为 0。 // 判断此时是否应该执行 func 函数 function shouldInvoke(time) { // 当前时间距离上一次调用 debounce 的时间差 const timeSinceLastCall = time - lastCallTime // 当前时间距离上一次执行 func 的时间差 const timeSinceLastInvoke = time - lastInvokeTime // 下述 4 种情况返回 true return ( lastCallTime === undefined || (timeSinceLastCall &gt;= wait) || (timeSinceLastCall &lt; 0) || (maxing &amp;&amp; timeSinceLastInvoke &gt;= maxWait) ) } 之后每次触发时，timeSinceLastCall 总是为 50 毫秒，maxing 为 false，所以 shouldInvoke(time) 总是返回 false，并不会执行传入函数 fn，只返回 result，即为 undefined。 到现在为止，fn 一次还没有执行，200 毫秒后，定时器回调函数触发，执行 timerExpired 函数。 // 定时器回调函数，表示定时结束后的操作 function timerExpired() { const time = Date.now() // 1、是否需要执行 // 执行事件结束后的那次回调，否则重启定时器 if (shouldInvoke(time)) { return trailingEdge(time) } // 2、否则 计算剩余等待时间，重启定时器，保证下一次时延的末尾触发 timerId = startTimer(timerExpired, remainingWait(time)) } 此时存在两种情况，第一种是 mousemove 事件一直在触发，根据前面介绍 shouldInvoke(time) 会返回 false，之后就将计算剩余等待时间，重启定时器。时间计算公式为 wait - (time - lastCallTime)，即 200 - 50，所以只要 shouldInvoke(time) 返回 false，就每隔 150 毫秒后执行一次 timerExpired()。 第二种情况是 mousemove 事件不再触发，因为 timerExpired() 在循环执行，所以肯定会存在一种情况满足 timeSinceLastCall &gt;= wait，即 shouldInvoke(time) 返回 true，终结 timerExpired() 的循环，并执行 trailingEdge(time)。 // 执行连续事件结束后的那次回调 function trailingEdge(time) { // 清空定时器 timerId = undefined // trailing 和 lastArgs 两者同时存在时执行 // trailing 来源自 'trailing' in options ? !!options.trailing : trailing // lastArgs 标记位的作用，意味着 debounce 至少执行过一次 if (trailing &amp;&amp; lastArgs) { return invokeFunc(time) } // 清空参数 lastArgs = lastThis = undefined return result } 在 trailingEdge 中 trailing 和 lastArgs 都是 true，所以会执行 invokeFunc(time)，即执行传入函数 fn。 所以整个过程中只在最后执行一次传入函数 fn，效果同上面第一张图所示。 角度 5 lodash.debounce(fn, 200, {leading: true, trailing: false}) 此时相比角度 4 来说，差异在于 {leading: true, trailing: false}，但是 wait 和 maxWait 都和角度 4 一致，所以只存在下面 2 种区别，效果同上面第一张图所示。 区别 1：leadingEdge 中会执行传入函数 fn 区别 2：trailingEdge 中不再执行传入函数 fn 角度 6 lodash.debounce(fn, 200, {leading: true, trailing: true}) 此时相比角度 4 来说，差异仅仅在于设置了 {leading: true}，所以只存在一个区别，那就是在 leadingEdge 中会执行传入函数 fn，当然在 trailingEdge 中依旧执行传入函数 fn，所以会出现在 mousemove 事件触发过程中首尾都会执行的情况，效果同上面第一张图所示。 当然一种情况除外，那就是 mousemove 事件永远只触发一次的情况，关键在于 lastArgs 变量。 对于 lastArgs 变量来说，在入口函数 debounced 中赋值，即每次触发都会重新赋值一次，那什么时候清空呢，在 invokeFunc(time) 中重置为 undefined，所以如果 debounced 只触发了一次，而且在 {leading: true} 时执行过一次 fn，那么即使设置了 {trailing: true} 也不会再执行传入函数 fn。 角度 7 lodash.debounce(fn, 200, {leading: false, trailing: true, maxWait: 400}) 此时 wait 为 200，maxWait 为 400，maxing 为 true，我们来看下执行过程。 第一次触发时，因为 {leading: false}，所以肯定不会执行 fn，此时开启了一个 200 毫秒的定时器。 // 判断此时是否应该执行 func 函数 function shouldInvoke(time) { // 当前时间距离上一次调用 debounce 的时间差 const timeSinceLastCall = time - lastCallTime // 当前时间距离上一次执行 func 的时间差 const timeSinceLastInvoke = time - lastInvokeTime // 下述 4 种情况返回 true return ( lastCallTime === undefined || (timeSinceLastCall &gt;= wait) || (timeSinceLastCall &lt; 0) || (maxing &amp;&amp; timeSinceLastInvoke &gt;= maxWait) ) } 之后每隔 50 毫秒触发一次，每次都会执行 shouldInvoke(time) 函数，只有在第 400 毫秒时，才会满足 maxing &amp;&amp; timeSinceLastInvoke &gt;= maxWait，返回 true。 // 计算仍需等待的时间 function remainingWait(time) { // 当前时间距离上一次调用 debounce 的时间差 const timeSinceLastCall = time - lastCallTime // 当前时间距离上一次执行 func 的时间差 const timeSinceLastInvoke = time - lastInvokeTime // 剩余等待时间 const timeWaiting = wait - timeSinceLastCall // 是否设置了最大等待时间 // 是（节流）：返回「剩余等待时间」和「距上次执行 func 的剩余等待时间」中的最小值 // 否：返回剩余等待时间 return maxing ? Math.min(timeWaiting, maxWait - timeSinceLastInvoke) : timeWaiting } 但是在这之前的第 200 毫秒，定时器触发回调函数，执行 timerExpired，因为此时 shouldInvoke(time) 返回 false，所以会重新计算剩余等待时间并重启计时器，其中 timeWaiting 是 150 毫秒，maxWait - timeSinceLastInvoke 是 200 毫秒，所以计算结果是 150 毫秒。 150 毫秒之后，即自开始之后的第 350 毫秒时，会重新计算时间，其中 timeWaiting 依旧是 150 毫秒，maxWait - timeSinceLastInvoke 是 50 毫秒，所以重新开启 50 毫秒的定时器，即在第 400 毫秒时触发。 此时会发现定时器触发的时间是第 400 毫秒，shouldInvoke(time) 中返回 true 的时间也是在第 400 毫秒，为什么要这样呢？这样会冲突吗？首先定时器剩余时间判断和 shouldInvoke(time) 判断中，只要有一处满足执行 fn 条件，就会立马执行，同时 lastInvokeTime 值也会发生改变，所以另一处判断就不会生效了。另外本身定时器是不精准的，所以通过 Math.min(timeWaiting, maxWait - timeSinceLastInvoke) 取最小值的方式来减少误差。 于此同时，需要在 debounced 入口函数添加这么一句 if (timerId === undefined) {timerId = startTimer(timerExpired, wait)}，避免 trailingEdge 执行后定时器被清空。 最终效果和节流是一样的，只是时间间隔变大了而已，具体效果同第一张图所示。 ","link":"https://faded.auspicious.space/post/javascript-a-simple-explanation-to-debounce-and-throttle-in-lodash/"},{"title":"JavaScript——防抖和节流","content":" JS简单实现防抖和节流 1 防抖 - debounce 其中一种解决方案就是每次用户停止输入后，延迟超过 500ms 时，才去搜索此时的 string，这就是防抖。 1.1 原理 将若干个函数调用合成为一次，并在给定时间过去之后仅被调用一次。 1.2 代码实现 function debounce(fn, delay) { // 维护一个 timer，用来记录当前执行函数状态 let timer = null; return function() { // 通过 'this' 和 'arguments' 获取函数的作用域和变量 let context = this; let args = arguments; // 清理掉正在执行的函数，并重新执行 clearTimeout(timer); timer = setTimeout(function() { fn.apply(context, args); }, delay); } } let flag = 0; // 记录当前函数调用次数 // 当用户滚动时被调用的函数 function foo() { flag++; console.log('Number of calls: %d', flag); } // 在 debounce 中包装我们的函数，过 2 秒触发一次 document.body.addEventListener('scroll', debounce(foo, 2000)); 1.3 解释 debounce 函数封装后，返回内部函数。 每一次事件被触发，都会清除当前的 timer 然后重新设置超时并调用。这会导致每一次高频事件都会取消前一次的超时调用，导致事件处理程序不能被触发。 只有当高频事件停止，最后一次事件触发的超时调用才能在 delay 时间后执行。 2 节流 - throttle 另一种解决方案比防抖要宽松些，这时我们不想用户一味的输入，而是给用户一些搜索提示，所以在当中限制每过 500ms 就查询一次此时的 string，这就是节流。 2.1 原理 节流函数不管事件触发有多频繁，都会保证在规定时间内一定会执行一次真正的事件处理函数。 2.2 代码实现 代码实现有两种，一种是时间戳，另一种是定时器。 2.2.1 时间戳实现 function throttle(func, delay){ let prev = Date.now(); return function(){ const context = this; const args = arguments; const now = Date.now(); if(now - prev &gt;= delay){ func.apply(context, args); prev = Date.now(); } } } 当高频事件触发时，第一次应该会立即执行（给事件绑定函数与真正触发事件的间隔如果大于 delay 的话），而后再怎么频繁触发事件，也都是会每 delay 秒才执行一次。而当最后一次事件触发完毕后，事件也不会再被执行了。 2.2.2 定时器实现 当触发事件的时候，我们设置一个定时器，再触发事件的时候，如果定时器存在，就不执行；直到 delay 秒后，定时器执行执行函数，清空定时器，这样就可以设置下个定时器。 fucntion throttle(func, delay){ let timer = null; return funtion(){ let context = this; let args = arguments; if(!timer){ timer = setTimeout(function(){ func.apply(context, args); timer = null; }, delay); } } } 当第一次触发事件时，肯定不会立即执行函数，而是在 delay 秒后才执行。 之后连续不断触发事件，也会每 delay 秒执行一次。 当最后一次停止触发后，由于定时器的 delay 延迟，可能还会执行一次函数。 2.2.3 综合使用时间戳与定时器 完成一个事件触发时立即执行，触发完毕还能执行一次的节流函数。 function throttle(func, delay){ let timer = null; let startTime = Date.now(); return function(){ let curTime = Date.now(); let remaining = delay - (curTime - startTime); const context = this; const args = arguments; clearTimeout(timer); if(remaining &lt;= 0){ func.apply(context,args); startTime = Date.now(); }else{ timer = setTimeout(func, remaining); } } } 需要在每个 delay 时间中一定会执行一次函数，因此在节流函数内部使用开始时间、当前时间与 delay 来计算 remaining，当 remaining &lt;= 0 时表示该执行函数了，如果还没到时间的话就设定在 remaining 时间后再触发。当然在 remaining 这段时间中如果又一次发生事件，那么会取消当前的计时器，并重新计算一个 remaining 来判断当前状态。 ","link":"https://faded.auspicious.space/post/javascript-debounce-throttle/"},{"title":"什么是 P 问题、NP 问题和 NPC 问题","content":" 什么是P问题、NP问题和NPC问题 [转载] 什么是P问题、NP问题和NPC问题 1 总结 Problem Introduction P 能在多项式时间里找到一个解决算法 NP 能在多项式时间里验证一个解是否正确 NPC 1. 首先必须是一个NP问题 2. 所有的NP问题都能 reduce 成该问题 NP-Hard 只需要满足 NPC 问题的第二个条件即可 下面的一些说法或许是众多 OIer 最大的误区之一。 你会经常看到网上出现“这怎么做，这不是 NP 问题吗”、“这个只有搜了，这已经被证明是 NP 问题了”之类的话。你要知道，大多数人此时所说的 NP 问题其实都是指的 NPC 问题。他们没有搞清楚 NP 问题和 NPC 问题的概念。NP 问题并不是那种“只有搜才行”的问题，NPC 问题才是。好，行了，基本上这个误解已经被澄清了。下面的内容都是在讲什么是 P 问题，什么是 NP 问题，什么是 NPC 问题，你如果不是很感兴趣就可以不看了。接下来你可以看到，把 NP 问题当成是 NPC 问题是一个多大的错误。 2 时间复杂度 还是先用几句话简单说明一下时间复杂度。时间复杂度并不是表示一个程序解决问题需要花多少时间，而是当问题规模扩大后，程序需要的时间长度增长得有多快。也就是说，对于高速处理数据的计算机来说，处理某一个特定数据的效率不能衡量一个程序的好坏，而应该看当这个数据的规模变大到数百倍后，程序运行时间是否还是一样，或者也跟着慢了数百倍，或者变慢了数万倍。不管数据有多大，程序处理花的时间始终是那么多的，我们就说这个程序很好，具有 O(1)O(1)O(1) 的时间复杂度，也称常数级复杂度；数据规模变得有多大，花的时间也跟着变得有多长，这个程序的时间复杂度就是 O(n)O(n)O(n)，比如找 nnn 个数中的最大值；而像冒泡排序、插入排序等，数据扩大 2 倍，时间变慢 4 倍的，属于 O(n2)O(n^2)O(n2) 的复杂度。还有一些穷举类的算法，所需时间长度成几何阶数上涨，这就是 O(an)O(a^n)O(an) 的指数级复杂度，甚至 O(n!)O(n!)O(n!) 的阶乘级复杂度。不会存在 O(2n2)O(2n^2)O(2n2) 的复杂度，因为前面的那个“2”是系数，根本不会影响到整个程序的时间增长。同样地，O(n3+n2)O(n^3+n^2)O(n3+n2) 的复杂度也就是 O(n3)O(n^3)O(n3) 的复杂度。因此，我们会说，一个 O(0.01∗n3)O(0.01*n^3)O(0.01∗n3) 的程序的效率比 O(100n2)O(100n^2)O(100n2) 的效率低，尽管在 nnn 很小的时候，前者优于后者，但后者时间随数据规模增长得慢，最终 O(n3)O(n^3)O(n3) 的复杂度将远远超过 O(n2)O(n^2)O(n2)。我们也说，O(n100)O(n^{100})O(n100) 的复杂度小于 O(1.01n)O(1.01^n)O(1.01n) 的复杂度。 容易看出，前面的几类复杂度被分为两种级别，其中后者的复杂度无论如何都远远大于前者：一种是 O(1)O(1)O(1)，O(log⁡(n))O(\\log(n))O(log(n))，O(na)O(n^a)O(na)等，我们把它叫做多项式级的复杂度，因为它的规模 n 出现在底数的位置；另一种是 O(an)O(a^n)O(an) 和 O(n!)O(n!)O(n!) 型复杂度，它是非多项式级的，其复杂度计算机往往不能承受。当我们在解决一个问题时，我们选择的算法通常都需要是多项式级的复杂度，非多项式级的复杂度需要的时间太多，往往会超时，除非是数据规模非常小。 自然地，人们会想到一个问题：会不会所有的问题都可以找到复杂度为多项式级的算法呢？很遗憾，答案是否定的。有些问题甚至根本不可能找到一个正确的算法来，这称之为“不可解问题”(Undecidable Decision Problem)。The Halting Problem 就是一个著名的不可解问题，在我的 Blog 上有过专门的介绍和证明。再比如，输出从 1 到 n 这 n 个数的全排列。不管你用什么方法，你的复杂度都是阶乘级，因为你总得用阶乘级的时间打印出结果来。有人说，这样的“问题”不是一个“正规”的问题，正规的问题是让程序解决一个问题，输出一个“YES”或“NO”（这被称为判定性问题），或者一个什么什么的最优值（这被称为最优化问题）。那么，根据这个定义，我也能举出一个不大可能会有多项式级算法的问题来：Hamilton 回路。问题是这样的：给你一个图，问你能否找到一条经过每个顶点一次且恰好一次（不遗漏也不重复）最后又走回来的路（满足这个条件的路径叫做 Hamilton 回路）。这个问题现在还没有找到多项式级的算法。事实上，这个问题就是我们后面要说的 NPC 问题。 3 P 问题 下面引入 P 类问题的概念：如果一个问题可以找到一个能在多项式的时间里解决它的算法，那么这个问题就属于 P 问题。P 是英文单词多项式的第一个字母。哪些问题是 P 类问题呢？通常 NOI 和 NOIP 不会出不属于 P 类问题的题目。我们常见到的一些信息奥赛的题目都是 P 问题。道理很简单，一个用穷举换来的非多项式级时间的超时程序不会涵盖任何有价值的算法。 4 NP 问题 接下来引入 NP 问题的概念。这个就有点难理解了，或者说容易理解错误。在这里强调（回到我竭力想澄清的误区上），NP 问题不是非 P 类问题。NP 问题是指可以在多项式的时间里验证一个解的问题。NP 问题的另一个定义是，可以在多项式的时间里猜出一个解的问题。比方说，我 RP 很好，在程序中需要枚举时，我可以一猜一个准。现在某人拿到了一个求最短路径的问题，问从起点到终点是否有一条小于 100 个单位长度的路线。它根据数据画好了图，但怎么也算不出来，于是来问我：你看怎么选条路走得最少？我说，我 RP 很好，肯定能随便给你指条很短的路出来。然后我就胡乱画了几条线，说就这条吧。那人按我指的这条把权值加起来一看，嘿，神了，路径长度 98，比 100 小。于是答案出来了，存在比 100 小的路径。别人会问他这题怎么做出来的，他就可以说，因为我找到了一个比 100 小的解。在这个题中，找一个解很困难，但验证一个解很容易。验证一个解只需要 O(n)O(n)O(n) 的时间复杂度，也就是说我可以花 O(n)O(n)O(n) 的时间把我猜的路径的长度加出来。那么，只要我 RP 好，猜得准，我一定能在多项式的时间里解决这个问题。我猜到的方案总是最优的，不满足题意的方案也不会来骗我去选它。这就是 NP 问题。当然有不是 NP 问题的问题，即你猜到了解但是没用，因为你不能在多项式的时间里去验证它。下面我要举的例子是一个经典的例子，它指出了一个目前还没有办法在多项式的时间里验证一个解的问题。很显然，前面所说的 Hamilton 回路是 NP 问题，因为验证一条路是否恰好经过了每一个顶点非常容易。但我要把问题换成这样：试问一个图中是否不存在 Hamilton 回路。这样问题就没法在多项式的时间里进行验证了，因为除非你试过所有的路，否则你不敢断定它“没有 Hamilton 回路”。 之所以要定义 NP 问题，是因为通常只有 NP 问题才可能找到多项式的算法。我们不会指望一个连多项式地验证一个解都不行的问题存在一个解决它的多项式级的算法。相信读者很快明白，信息学中的号称最困难的问题——“NP 问题”，实际上是在探讨 NP 问题与 P 类问题的关系。 很显然，所有的 P 类问题都是 NP 问题。也就是说，能多项式地解决一个问题，必然能多项式地验证一个问题的解——既然正解都出来了，验证任意给定的解也只需要比较一下就可以了。关键是，人们想知道，是否所有的 NP 问题都是 P 类问题。我们可以再用集合的观点来说明。如果把所有 P 类问题归为一个集合 P 中，把所有 NP 问题划进另一个集合 NP 中，那么，显然有 P 属于 NP。现在，所有对 NP 问题的研究都集中在一个问题上，即究竟是否有 P=NP ？通常所谓的“NP 问题”，其实就一句话：证明或推翻 P=NP。 NP 问题一直都是信息学的巅峰。巅峰，意即很引人注目但难以解决。在信息学研究中，这是一个耗费了很多时间和精力也没有解决的终极问题，好比物理学中的大统一和数学中的歌德巴赫猜想等。 目前为止这个问题还“啃不动”。但是，一个总的趋势、一个大方向是有的。人们普遍认为，P=NP 不成立，也就是说，多数人相信，存在至少一个不可能有多项式级复杂度的算法的 NP 问题。人们如此坚信 P≠NP 是有原因的，就是在研究 NP 问题的过程中找出了一类非常特殊的 NP 问题叫做 NP-完全问题，也即所谓的 NPC 问题。C 是英文单词“完全”的第一个字母。正是 NPC 问题的存在，使人们相信 P≠NP。下文将花大量篇幅介绍 NPC 问题，你从中可以体会到 NPC 问题使 P=NP 变得多么不可思议。 5 规约 为了说明 NPC 问题，我们先引入一个概念——约化(Reducibility，有的资料上叫“归约”)。 简单地说，一个问题 A 可以约化为问题 B 的含义即是，可以用问题 B 的解法解决问题 A，或者说，问题 A 可以“变成”问题 B。《算法导论》上举了这么一个例子。比如说，现在有两个问题：求解一个一元一次方程和求解一个一元二次方程。那么我们说，前者可以约化为后者，意即知道如何解一个一元二次方程那么一定能解出一元一次方程。我们可以写出两个程序分别对应两个问题，那么我们能找到一个“规则”，按照这个规则把解一元一次方程程序的输入数据变一下，用在解一元二次方程的程序上，两个程序总能得到一样的结果。这个规则即是：两个方程的对应项系数不变，一元二次方程的二次项系数为 0。按照这个规则把前一个问题转换成后一个问题，两个问题就等价了。同样地，我们可以说，Hamilton 回路可以约化为 TSP(Travelling Salesman Problem，旅行商问题)：在Hamilton 回路问题中，两点相连即这两点距离为 0，两点不直接相连则令其距离为 1，于是问题转化为在 TSP 中，是否存在一条长为 0 的路径。Hamilton 回路存在当且仅当 TSP 中存在长为 0 的回路。 “问题 A 可约化为问题 B”有一个重要的直观意义：B 的时间复杂度高于或者等于 A 的时间复杂度。也就是说，问题 A 不比问题 B 难。这很容易理解。既然问题 A 能用问题 B 来解决，倘若 B 的时间复杂度比 A 的时间复杂度还低了，那 A 的算法就可以改进为 B 的算法，两者的时间复杂度还是相同。正如解一元二次方程比解一元一次方程难，因为解决前者的方法可以用来解决后者。 很显然，约化具有一项重要的性质：约化具有传递性。如果问题 A 可约化为问题 B，问题 B 可约化为问题 C，则问题 A 一定可约化为问题 C。这个道理非常简单，就不必阐述了。 现在再来说一下约化的标准概念就不难理解了：如果能找到这样一个变化法则，对任意一个程序 A 的输入，都能按这个法则变换成程序 B 的输入，使两程序的输出相同，那么我们说，问题 A 可约化为问题 B。 当然，我们所说的“可约化”是指的可“多项式地”约化(Polynomial-time Reducible)，即变换输入的方法是能在多项式的时间里完成的。约化的过程只有用多项式的时间完成才有意义。 好了，从约化的定义中我们看到，一个问题约化为另一个问题，时间复杂度增加了，问题的应用范围也增大了。通过对某些问题的不断约化，我们能够不断寻找复杂度更高，但应用范围更广的算法来代替复杂度虽然低，但只能用于很小的一类问题的算法。再回想前面讲的 P 和 NP 问题，联想起约化的传递性，自然地，我们会想问，如果不断地约化上去，不断找到能“通吃”若干小 NP 问题的一个稍复杂的大 NP 问题，那么最后是否有可能找到一个时间复杂度最高，并且能“通吃”所有的 NP 问题的这样一个超级 NP 问题？答案居然是肯定的。也就是说，存在这样一个 NP 问题，所有的 NP 问题都可以约化成它。换句话说，只要解决了这个问题，那么所有的 NP 问题都解决了。这种问题的存在难以置信，并且更加不可思议的是，这种问题不只一个，它有很多个，它是一类问题。这一类问题就是传说中的 NPC 问题，也就是 NP-完全问题。NPC 问题的出现使整个 NP 问题的研究得到了飞跃式的发展。我们有理由相信，NPC 问题是最复杂的问题。再次回到全文开头，我们可以看到，人们想表达一个问题不存在多项式的高效算法时应该说它“属于 NPC 问题”。此时，我的目的终于达到了，我已经把 NP 问题和 NPC 问题区别开了。到此为止，本文已经写了近 5000 字了，我佩服你还能看到这里来，同时也佩服一下自己能写到这里来。 6 NPC 问题 NPC 问题的定义非常简单。同时满足下面两个条件的问题就是 NPC 问题。 首先，它得是一个 NP 问题； 然后，所有的 NP 问题都可以约化到它。 证明一个问题是 NPC 问题也很简单。先证明它至少是一个 NP 问题，再证明其中一个已知的 NPC 问题能约化到它（由约化的传递性，则 NPC 问题定义的第二条也得以满足；至于第一个 NPC 问题是怎么来的，下文将介绍），这样就可以说它是 NPC 问题了。 既然所有的 NP 问题都能约化成 NPC 问题，那么只要任意一个 NPC 问题找到了一个多项式的算法，那么所有的 NP 问题都能用这个算法解决了，NP 也就等于 P 了。因此，给 NPC 找一个多项式算法太不可思议了。因此，前文才说，“正是 NPC 问题的存在，使人们相信 P≠NP”。我们可以就此直观地理解，NPC 问题目前没有多项式的有效算法，只能用指数级甚至阶乘级复杂度的搜索。 7 NP-Hard 问题 顺便讲一下 NP-Hard 问题。NP-Hard 问题是这样一种问题，它满足 NPC 问题定义的第二条但不一定要满足第一条（就是说，NP-Hard 问题要比 NPC 问题的范围广）。NP-Hard 问题同样难以找到多项式的算法，但它不列入我们的研究范围，因为它不一定是 NP 问题。即使 NPC 问题发现了多项式级的算法，NP-Hard 问题有可能仍然无法得到多项式级的算法。事实上，由于 NP-Hard 放宽了限定条件，它将有可能比所有的 NPC 问题的时间复杂度更高从而更难以解决。 8 NPC 问题示例 不要以为 NPC 问题是一纸空谈。NPC 问题是存在的。确实有这么一个非常具体的问题属于 NPC 问题。下文即将介绍它。 下文即将介绍逻辑电路问题。这是第一个 NPC 问题。其它的 NPC 问题都是由这个问题约化而来的。因此，逻辑电路问题是 NPC 类问题的“鼻祖”。 逻辑电路问题是指的这样一个问题：给定一个逻辑电路，问是否存在一种输入使输出为 True。 什么叫做逻辑电路呢？一个逻辑电路由若干个输入，一个输出，若干“逻辑门”和密密麻麻的线组成。看下面一例，不需要解释你马上就明白了。 +----------+ | Input 1 +---+ +----------+ | | +----------+ +--&gt;+ | | OR +---+ +--&gt;+ | | | +----------+ | +----------+ +----------+ | +---&gt;+ | | Input 2 +---+ | AND +---&gt; Output +----------+ +---&gt;+ | +----------+ | +----------+ | | | +--&gt;+ NOT +---+ | | | +----------+ | +----------+ | Input 3 +---+ +----------+ 这是个较简单的逻辑电路，当输入 1、输入 2、输入 3分别为 True、True、False 或 False、True、False 时，输出为True。 有输出无论如何都不可能为 True 的逻辑电路吗？有。下面就是一个简单的例子。 +----------+ | Input 1 +---+ +----------+ | | +----------+ +---+ | | AND +---+ +---+ | | | +----------+ | +----------+ | +---&gt;+ | | | AND +---&gt; Output | +---&gt;+ | | +----------+ | +----------+ | | | | +---+ NOT +---+ | | | +----------+ | +----------+ | Input 2 +---+ +----------+ 上面这个逻辑电路中，无论输入是什么，输出都是 False。我们就说，这个逻辑电路不存在使输出为 True 的一组输入。 回到上文，给定一个逻辑电路，问是否存在一种输入使输出为 True，这即逻辑电路问题。 逻辑电路问题属于 NPC 问题。这是有严格证明的。它显然属于 NP 问题，并且可以直接证明所有的 NP 问题都可以约化到它（不要以为 NP 问题有无穷多个将给证明造成不可逾越的困难）。证明过程相当复杂，其大概意思是说任意一个 NP 问题的输入和输出都可以转换成逻辑电路的输入和输出（想想计算机内部也不过是一些 0 和 1 的运算），因此对于一个 NP 问题来说，问题转化为了求出满足结果为 True 的一个输入（即一个可行解）。 有了第一个 NPC 问题后，一大堆 NPC 问题就出现了，因为再证明一个新的 NPC 问题只需要将一个已知的 NPC 问题约化到它就行了。后来，Hamilton 回路成了 NPC 问题，TSP 问题也成了 NPC 问题。现在被证明是 NPC 问题的有很多，任何一个找到了多项式算法的话所有的 NP 问题都可以完美解决了。因此说，正是因为 NPC 问题的存在，P=NP 变得难以置信。P=NP 问题还有许多有趣的东西，有待大家自己进一步的挖掘。攀登这个信息学的巅峰是我们这一代的终极目标。现在我们需要做的，至少是不要把概念弄混淆了。 ","link":"https://faded.auspicious.space/post/what-is-p-problem-np-problem-and-npc-problem/"},{"title":"NP 完全性理论","content":"1 计算模型 1.1 随机存取机 RAM 1.2 随机存取存储程序机 RASP 1.3 图灵机 1.4 图灵机模型与 RAM 模型的关系 1.5 问题变换与计算复杂性归约 1.1 随机存取机RAM 1.1.1 RAM 的结构 1.1.2 RAM 程序 一个RAM程序定义了从输入带到输出带的一个映射。可以对这种映射关系作 2 种不同的解释。 解释一：把 RAM 程序看成是计算一个函数 若一个 RAM 程序 PPP 总是从输入带前 nnn 个方格中读入 nnn 个整数 x1,x2,…,xnx_1, x_2,\\ldots,x_nx1​,x2​,…,xn​，并且在输出带的第一个方格上输出一个整数 yyy 后停机，那么就说程序 PPP 计算了函数 f(x1,x2,…,xn)=yf(x_1, x_2,\\ldots,x_n)=yf(x1​,x2​,…,xn​)=y。 解释二：把 RAM 程序当作一个语言接受器。 将字符串 S=a1a2…anS=a_1a_2\\ldots a_nS=a1​a2​…an​ 放在输入带上。在输入带的第一个方格中放入符号 a1a_1a1​，第二个方格中放入符号 a2a_2a2​，……，第 nnn 个方格中放入符号 ana_nan​。然后在第 n+1n+1n+1 个方格中放入000，作为输入串的结束标志符。如果一个 RAM 程序 PPP 读了字符串 SSS 及结束标志符 000 后，在输出带的第一格输出一个 111 并停机，就说程序 PPP 接受字符串SSS。 1.1.3 RAM程序的耗费标准 标准一：均匀耗费标准 在均匀耗费标准下，每条 RAM 指令需要一个单位时间；每个寄存器占用一个单位空间。以后除特别注明，RAM 程序的复杂性将按照均匀耗费标准衡量。 标准二：对数耗费标准 对数耗费标准是基于这样的假定，即执行一条指令的耗费与以二进制表示的指令的操作数长度成比例。在 RAM 计算模型下，假定一个寄存器可存放一个任意大小的整数。 1.2 随机存取存储程序机 RASP 1.2.1 RASP的结构 RASP 的整体结构类似于 RAM，所不同的是 RASP 的程序是存储在寄存器中的。每条 RASP 指令占据 2 个连续的寄存器。第一个寄存器存放操作码的编码，第二个寄存器存放地址。RASP 指令用整数进行编码。 1.2.2 RASP程序的复杂性 不管是在均匀耗费标准下，还是在对数耗费标准下，RAM 程序和 RASP 程序的复杂性只差一个常数因子。在一个计算模型下 T(n)T(n)T(n) 时间内完成的输入-输出映射可在另一个计算模型下模拟，并在 kT(n)kT(n)kT(n) 时间内完成。其中 kkk 是一个常数因子。空间复杂性的情况也是类似的。 1.3 图灵机 1.3.1 多带图灵机 根据有限状态控制器的当前状态及每个读写头读到的带符号，图灵机的一个计算步可实现下面 3 个操作之一或全部。 改变有限状态控制器中的状态。 清除当前读写头下的方格中原有带符号并写上新的带符号。 独立地将任何一个或所有读写头，向左移动一个方格（L）或向右移动一个方格（R）或停在当前单元不动（S）。 k 带图灵机可形式化地描述为一个 7 元组 (Q,T,I,δ,b,q0,qf)(Q,T,I,\\delta,b,q_0,q_f)(Q,T,I,δ,b,q0​,qf​)，其中: QQQ 是有限个状态的集合。 TTT 是有限个带符号的集合。 III 是输入符号的集合，I⊆TI\\subseteq TI⊆T。 bbb 是惟一的空白符，b∈T−Ib \\in T-Ib∈T−I。 q0q_0q0​ 是初始状态。 qfq_fqf​ 是终止（或接受）状态。 δ\\deltaδ 是移动函数。它是从 Q×TkQ\\times T^kQ×Tk 的某一子集映射到 Q×(T×{L,R,S})kQ\\times(T\\times \\lbrace L, R, S\\rbrace)^kQ×(T×{L,R,S})k 的函数。 与 RAM 模型类似，图灵机既可作为语言接受器，也可作为计算函数的装置。 图灵机 M 的时间复杂性 T(n)T(n)T(n) 是它处理所有长度为 nnn 的输入所需的最大计算步数。如果对某个长度为 nnn 的输入，图灵机不停机，T(n)T(n)T(n) 对这个 nnn 值无定义。 图灵机的空间复杂性 S(n)S(n)S(n) 是它处理所有长度为 nnn 的输入时，在 k 条带上所使用过的方格数的总和。如果某个读写头无限地向右移动而不停机，S(n)S(n)S(n) 也无定义。 1.4 图灵机模型与 RAM 模型的关系 图灵机模型与 RAM 模型的关系是指同一问题在这 2 种不同计算模型下的复杂性之间的关系。 定理 8-3 对于问题 PPP 的任何长度为 nnn 的输入，设求解问题 PPP 的算法 AAA 在 k 带图灵机模型 TM 下的时间复杂性为 T(n)T(n)T(n)，那么，算法 AAA 在 RAM 模型下的时间复杂性为 O(T2(n))O(T^2(n))O(T2(n))。 定理 8-4 对于问题 PPP 的任何长度为 nnn 的输入，设求解问题 PPP 的算法 AAA 在 RAM 模型下，不含有乘法和除法指令，且按对数耗费标准其时间复杂性为 T(n)T(n)T(n)，那么，算法 AAA 在 k 带图灵机模型 TM 下的时间复杂性为 O(T2(n))O(T^2(n))O(T2(n))。 1.5 问题变换与计算复杂性归约 通过问题变换的技巧，可以将 2 个不同问题的计算复杂性联系在一起。这样就可以将一个问题的计算复杂性归结为另一个问题的计算复杂性，从而实现问题的计算复杂性归约。 具体地说，假设有 2 个问题 A 和 B，将问题 A 变换为问题 B 是指： 将问题 A 的输入变换为问题 B 的适当输入。 解出问题 B。 把问题 B 的输出变换为问题 A 的正确解。 若用 O(τ(n))O(\\tau(n))O(τ(n)) 时间能完成上述变换的第 1 步和第 3 步，则称问题 A 是τ(n)\\tau(n)τ(n) 时间可变换到问题 B，且简记为 A∝τ(n)BA\\propto _{\\tau(n)}BA∝τ(n)​B。其中的 nnn 通常为问题 A 的规模(大小)。 当 τ(n)\\tau(n)τ(n) 为 nnn 的多项式时，称问题 A 可在多项式时间内变换为问题 B。特别地，当 τ(n)\\tau(n)τ(n) 为 nnn 的线性函数时，称问题 A 可线性地变换为问题 B。 问题的变换与问题的计算复杂性归约的关系： 命题 1（计算时间下界归约）：若已知问题 A 的计算时间下界为 T(n)T(n)T(n)，且问题 AAA 是τ(n)\\tau(n)τ(n) 可变换到问题 B，即 A∝τ(n)BA \\propto _{\\tau(n)}BA∝τ(n)​B，则 T(n)−O(τ(n))T(n)-O(\\tau(n))T(n)−O(τ(n)) 为问题 B 的一个计算时间下界。 命题 2（计算时间上界归约）：若已知问题 B 的计算时间上界为 T(n)T(n)T(n)，且问题 A 是 τ(n)\\tau(n)τ(n) 可变换到问题 B，即 A∝τ(n)BA\\propto _{\\tau(n)}BA∝τ(n)​B，则 T(n)+O(τ(n))T(n)+O(\\tau(n))T(n)+O(τ(n)) 是问题 A 的一个计算时间上界。 在命题 1 和命题 2 中，当 τ(n)=O(T(n))\\tau(n)=O(T(n))τ(n)=O(T(n)) 时，问题 A 的下界归约为问题 B 的下界，问题 B 的上界归约为问题 A 的上界。 2 P 类与 NP 类问题 2.1 非确定性图灵机 2.2 P类与NP类语言 2.3 多项式时间验证 2.1 非确定性图灵机 在图灵机计算模型中，移动函数 δ\\deltaδ 是单值的，即对于 Q×TkQ\\times T^kQ×Tk中的每一个值，当它属于 δ\\deltaδ 的定义域时，Q×(T×{L，R，S})kQ \\times (T \\times \\lbrace L，R，S\\rbrace)^kQ×(T×{L，R，S})k 中只有惟一的值与之对应，称这种图灵机为确定性图灵机，简记为 DTM(Deterministic Turing Machine)。 非确定性图灵机（NDTM）：一个 k 带的非确定性图灵机 M 是一个 7 元组：(Q,T,I,δ,b,q0,qf)(Q, T, I, \\delta, b, q_0, q_f)(Q,T,I,δ,b,q0​,qf​)。与确定性图灵机不同的是非确定性图灵机允许移动函数 δ\\deltaδ 具有不确定性，即对于 Q×TkQ\\times T^kQ×Tk 中的每一个值 (q;x1,x2,…,xk)(q; x_1, x_2,\\ldots, x_k)(q;x1​,x2​,…,xk​)，当它属于 δ\\deltaδ 的定义域时，Q×(T×{L,R,S})kQ \\times (T \\times \\lbrace L, R, S \\rbrace)^kQ×(T×{L,R,S})k 中有惟一的一个子集 δ(q;x1,x2,…,xk)\\delta(q; x_1, x_2,\\ldots, x_k)δ(q;x1​,x2​,…,xk​) 与之对应。可以在 δ(q;x1,x2,…,xk)\\delta(q; x_1, x_2, \\ldots, x_k)δ(q;x1​,x2​,…,xk​) 中随意选定一个值作为它的函数值。 2.2 P 类与 NP 类语言 2.2.1 P 类和 NP 类语言的定义： P = {L∣L\\lbrace L|L{L∣L 是一个能在多项式时间内被一台 DTM 所接受的语言}\\rbrace} NP = {L∣L\\lbrace L|L{L∣L 是一个能在多项式时间内被一台 NDTM 所接受的语言}\\rbrace} 由于一台确定性图灵机可看作是非确定性图灵机的特例，所以可在多项式时间内被确定性图灵机接受的语言也可在多项式时间内被非确定性图灵机接受。故 P⊆NPP \\subseteq NPP⊆NP。 2.2.2 NP 类语言举例——无向图的团问题 该问题的输入是一个有 nnn 个顶点的无向图 G=(V,E)G=(V, E)G=(V,E) 和一个整数 kkk。要求判定图 GGG 是否包含一个 kkk 顶点的完全子图（团），即判定是否存在 V′⊆V,∣V′∣=kV&#x27; \\subseteq V, \\vert V&#x27;\\vert=kV′⊆V,∣V′∣=k，且对于所有的 u,v∈V′u, v\\in V&#x27;u,v∈V′，有 (u,v)∈E(u, v)\\in E(u,v)∈E。 若用邻接矩阵表示图 GGG，用二进制串表示整数 kkk，则团问题的一个实例可以用长度为 n2+log⁡k+1n^2+\\log{k}+1n2+logk+1 的二进位串表示。因此，团问题可表示为语言： CLIQUE = {w#v∣w，v∈{0,1}∗\\lbrace w\\# v\\vert w，v\\in \\lbrace 0, 1\\rbrace ^*{w#v∣w，v∈{0,1}∗，以 www 为邻接矩阵的图 GGG 有一个 kkk 顶点的团，其中 vvv 是 kkk 的二进制表示。}\\rbrace} 接受该语言 CLIQUE 的非确定性算法：用非确定性选择指令选出包含 kkk 个顶点的候选顶点子集 VVV，然后确定性地检查该子集是否是团问题的一个解。算法分为 3 个阶段： 算法的第一阶段将输入串 w#vw\\#vw#v 分解，并计算出 n=∣w,∣n= \\sqrt{\\vert w, \\vert}n=∣w,∣​，以及用 vvv 表示的整数 kkk。若输入不具有形式 w#vw\\#vw#v 或 ∣w∣\\vert w\\vert∣w∣ 不是一个平方数就拒绝该输入。显而易见，第一阶段可 O(n2)O(n^2)O(n2) 在时间内完成。 在算法的第二阶段中，非确定性地选择V的一个 kkk 元子集 V′⊆VV&#x27;\\subseteq VV′⊆V。 算法的第三阶段是确定性地检查 V′V&#x27;V′ 的团性质。若 V′V&#x27;V′ 是一个团则接受输入，否则拒绝输入。这显然可以在 O(n4)O(n^4)O(n4) 时间内完成。因此，整个算法的时间复杂性为 O(n4)O(n^4)O(n4)。 非确定性算法在多项式时间内接受语言 CLIQUE，故 CLIQUE ∈\\in∈ NP 2.3 多项式时间验证 多项式时间可验证语言类 VP 可定义为： VP={L∣L∈∑∗\\text{VP}=\\lbrace L\\vert L\\in \\sum*VP={L∣L∈∑∗，∑\\sum∑ 为一有限字符集，存在一个多项式 ppp 和一个多项式时间验证算法 A(X,Y)A(X, Y)A(X,Y) 使得对任意 X∈∑∗X\\in \\sum*X∈∑∗，X∈LX\\in LX∈L 当且仅当存在 Y∈∑∗,∣Y∣≤p(∣X∣)Y\\in \\sum*, \\vert Y\\vert \\le p(\\vert X\\vert )Y∈∑∗,∣Y∣≤p(∣X∣) 且 A(X,Y)=1}A(X, Y)=1\\rbraceA(X,Y)=1}。 定理8-5：VP=NP 例如（哈密顿回路问题）：一个无向图 GGG 含有哈密顿回路吗? 无向图 GGG 的哈密顿回路是通过 GGG 的每个顶点恰好一次的简单回路。可用语言 HAM-CYCLE 定义该问题如下： HAM-CYCLE={G∣G含有哈密顿回路}\\text{HAM-CYCLE}=\\lbrace G \\vert G 含有哈密顿回路\\rbrace HAM-CYCLE={G∣G含有哈密顿回路} 3 NP 完全问题 3.1 多项式时间变换 3.2 Cook 定理 3.1 多项式时间变换 设 L1⊆∑1∗,L2⊆∑2∗L_1\\subseteq \\sum_1^*, L_2\\subseteq \\sum_2^*L1​⊆∑1∗​,L2​⊆∑2∗​， 是 2 个语言。所谓语言 L1L_1L1​ 能在多项式时间内变换为语言 L2L_2L2​(简记为 L1∝pL2L_1 \\propto _p L_2L1​∝p​L2​) 是指存在映身 f:⊆∑1∗→∑2∗f: \\subseteq \\sum_1^* \\to \\sum_2^*f:⊆∑1∗​→∑2∗​，且 fff 满足： 有一个计算 fff 的多项式时间确定性图灵机； 对于所有 x∈∑1∗,x∈L1x\\in \\sum_1^*, x\\in L_1x∈∑1∗​,x∈L1​，当且仅当 f(x)∈L2f(x)\\in L_2f(x)∈L2​。 定义： 语言 L 是 NP 完全的当且仅当 L∈NPL\\in \\text{NP}L∈NP； 对于所有 L′∈NPL&#x27; \\in \\text{NP}L′∈NP 有 L′∝pLL&#x27; \\propto_p LL′∝p​L。 如果有一个语言 LLL 满足上述性质 2，但不一定满足性质 1，则称该语言是 NP 难的。所有 NP 完全语言构成的语言类称为 NP 完全语言类，记为 NPC。 定理8-6：设 LLL 是 NP 完全的，则 L∈PL\\in PL∈P 当且仅当 P＝NPP＝\\text{NP}P＝NP； 若 L∝pL1L\\propto_p L_1L∝p​L1​，且 L1∈NPL_1\\in \\text{NP}L1​∈NP，则 L1L_1L1​ 是 NP 完全的。 定理 8-6 的 2 可用来证明问题的 NP 完全性。但前提是：要有第一个 NP 完全问题 LLL。 3.2 Cook 定理 定理 8-7（Cook 定理）：布尔表达式的可满足性问题 SAT 是 NP 完全的。 Cook 定理的重要性在于，它给出了第一个NP完全问题，使得对于任何问题 QQQ，只要能证明 Q∈NPQ \\in \\text{NP}Q∈NP 且 SAT∝pQ\\text{SAT} \\propto_p QSAT∝p​Q，就有 Q∈NPCQ\\in \\text{NPC}Q∈NPC. 4 一些典型的 NP 完全问题 4.1 合取范式的可满足性问题（CNF-SAT） 问题描述： 给定一个合取范式 α\\alphaα，判定它是否可满足。 如果一个布尔表达式是一些因子和之积，则称之为合取范式，简称 CNF (Conjunctive Normal Form)。这里的因子是变量 χ\\chiχ 或xxx。例如：(x1+x2)(x2+x3)(x1+x2+x3)(x_1+x_2)(x_2+x_3)(x_1+x_2+x_3)(x1​+x2​)(x2​+x3​)(x1​+x2​+x3​) 就是一个合取范式，而 x1x2+x3x_1x_2+x_3x1​x2​+x3​ 就不是合取范式。 要证明 CNF-SAT∈NPC\\text{CNF-SAT}\\in\\text{NPC}CNF-SAT∈NPC，只要证明在 Cook 定理中定义的布尔表达式 A,…,GA, \\ldots, GA,…,G 或者已是合取范式，或者有的虽然不是合取范式，但可以用布尔代数中的变换方法将它们化成合取范式，而且合取范式的长度与原表达式的长度只差一个常数因子。 4.2 三元合取范式的可满足性问题（3-SAT） 问题描述： 给定一个三元合取范式 α\\alphaα，判定它是否可满足。 证明思路： 3-SAT∈NP\\text{3-SAT}\\in\\text{NP}3-SAT∈NP 是显而易见的。为了证明 3-SAT∈NPC\\text{3-SAT}\\in\\text{NPC}3-SAT∈NPC，只要证明 CNF-SAT∝p3-SAT\\text{CNF-SAT}\\propto_p\\text{3-SAT}CNF-SAT∝p​3-SAT，即合取范式的可满足性问题可在多项式时间内变换为 3-SAT。 4.3 团问题（CLIQUE） 问题描述： 给定一个无向图 G=(V,E)G=(V, E)G=(V,E) 和一个正整数 kkk，判定图 GGG 是否包含一个 kkk 团，即是否存在，V′⊆V，∣V′∣=kV&#x27;\\subseteq V，\\vert V&#x27; \\vert=kV′⊆V，∣V′∣=k，且对任意 u,w∈V′u, w\\in V&#x27;u,w∈V′ 有 (u，w)∈E(u，w)\\in E(u，w)∈E。 证明思路： 已经知道 CLIQUE∈NP\\text{CLIQUE}\\in\\text{NP}CLIQUE∈NP。通过 3-SAT∝pCLIQUE\\text{3-SAT}\\propto_p\\text{CLIQUE}3-SAT∝p​CLIQUE 来证明 CLIQUE 是 NP 难的，从而证明团问题是 NP 完全的。 4.4 顶点覆盖问题（VERTEX-COVER） 问题描述： 给定一个无向图 G=(V,E)G=(V, E)G=(V,E) 和一个正整数 kkk，判定是否存在 V′⊆V,∣V′∣=kV&#x27; \\subseteq V, \\vert V&#x27;\\vert=kV′⊆V,∣V′∣=k，使得对于任意 (u,v)∈E(u, v)\\in E(u,v)∈E 有 u∈V′u\\in V&#x27;u∈V′ 或 v∈V′v\\in V&#x27;v∈V′。如果存在这样的 V′V&#x27;V′，就称 V′V&#x27;V′ 为图 GGG 的一个大小为 kkk 顶点覆盖。 证明思路： 首先，VERTEX-COVER∈NP\\text{VERTEX-COVER}\\in\\text{NP}VERTEX-COVER∈NP。因为对于给定的图 GGG 和正整数 kkk 以及一个“证书”V′V&#x27;V′，验证 ∣V′∣=k\\vert V&#x27;\\vert=k∣V′∣=k，然后对每条边 (u,v)∈E(u, v)\\in E(u,v)∈E，检查是否有 u∈V′u\\in V&#x27;u∈V′ 或 v∈V′v\\in V&#x27;v∈V′，显然可在多项式时间内完成。 其次，通过 CLIQUE∝pVERTEX-COVER\\text{CLIQUE}\\propto_p \\text{VERTEX-COVER}CLIQUE∝p​VERTEX-COVER 来证明顶点覆盖问题是 NP 难的。 4.5 子集和问题（SUBSET-SUM） 问题描述： 给定整数集合 SSS 和一个整数 ttt，判定是否存在 SSS 的一个子集S′⊆SS&#x27;\\subseteq SS′⊆S，使得 S′S&#x27;S′ 中整数的和为 ttt。例如，若 S={1,4,16,64,256,1040,1041,1093,1284,1344}S=\\lbrace 1, 4, 16, 64, 256, 1040, 1041, 1093, 1284, 1344\\rbraceS={1,4,16,64,256,1040,1041,1093,1284,1344} 且 t=3754t=3754t=3754，则子集 S′={1,16,64,256,1040,1093,1284}S&#x27;=\\lbrace 1, 16, 64, 256, 1040, 1093, 1284\\rbraceS′={1,16,64,256,1040,1093,1284} 是一个解。 证明思路： 首先，对于子集和问题的一个实例 ⟨S,t⟩\\langle S, t\\rangle⟨S,t⟩，给定一个“证书”S′S&#x27;S′，要验证 t=∑i∈S′it= \\sum_{i\\in S&#x27;}it=∑i∈S′​i 是否成立，显然可在多项式时间内完成。因此，SUBSET-SUM∈NP\\text{SUBSET-SUM}\\in \\text{NP}SUBSET-SUM∈NP； 其次，证明 VERTEX-COVER∝pSUBSET-SUM\\text{VERTEX-COVER}\\propto_p \\text{SUBSET-SUM}VERTEX-COVER∝p​SUBSET-SUM。 4.6 哈密顿回路问题（HAM-CYCLE） 问题描述： 给定无向图 G=(V,E)G=(V, E)G=(V,E)，判定其是否含有一哈密顿回路。 证明思路： 首先，已知哈密顿回路问题是一个 NP 类问题。 其次，通过证明 3-SAT∝pHAM-CYCLE\\text{3-SAT}\\propto_p\\text{HAM-CYCLE}3-SAT∝p​HAM-CYCLE。 得出：HAM-CYCLE∈NPC\\text{HAM-CYCLE}\\in\\text{NPC}HAM-CYCLE∈NPC。 4.7 旅行售货员问题（TSP） 问题描述： 给定一个无向完全图 G=(V,E)G=(V, E)G=(V,E) 及定义在 V×VV\\times VV×V 上的一个费用函数 ccc 和一个整数 kkk，判定 GGG 是否存在经过 VVV 中各顶点恰好一次的回路，使得该回路的费用不超过 kkk。 首先，给定 TSP 的一个实例 (G,c,k)(G, c, k)(G,c,k)，和一个由 nnn 个顶点组成的顶点序列。验证算法要验证这 nnn 个顶点组成的序列是图 GGG 的一条回路，且经过每个顶点一次。另外，将每条边的费用加起来，并验证所得的和不超过 kkk。这个过程显然可在多项式时间内完成，即 TSP∈NP\\text{TSP}\\in\\text{NP}TSP∈NP。 其次，旅行售货员问题与哈密顿回路问题有着密切的联系。哈密顿回路问题可在多项式时间内变换为旅行售货员问题。即 HAM-CYCLE∝pTSP\\text{HAM-CYCLE}\\propto_p\\text{TSP}HAM-CYCLE∝p​TSP。从而，旅行售货员问题是 NP 难的。 因此，TSP∈NPC\\text{TSP}\\in\\text{NPC}TSP∈NPC。 ","link":"https://faded.auspicious.space/post/np-completeness-theory/"},{"title":"算法复杂度比较","content":" 算法复杂度学习（上） 算法复杂度学习（下） 1 定义 时间复杂度一般采用 大 O 标记法, 即 T(n)=O(f(n))T(n)=O(f(n))T(n)=O(f(n)), 其中 T(n)T(n)T(n) 表示代码运行时间；nnn 表示数据规模大小；f(n)f(n)f(n) 表示每行代码执行次数总和，OOO 表示 T(n)T(n)T(n) 与 f(n)f(n)f(n) 的正比关系。大 OOO 时间复杂度实际上并不具体表示代码的真正运行时间，而是表示代码执行时间随数据规模增长的变化趋势。 在大 OOO 表示分析中，低阶项和常数项都可以省略，只保留最高阶项即可；如 f(n)=2n+2f(n)=2n+2f(n)=2n+2 在大 OOO 标记法中记为 T(n)=O(n)T(n)=O(n)T(n)=O(n)，而对于形如 f(n)=2n2+2n+3f(n)=2n^2+2n+3f(n)=2n2+2n+3 表示为 T(n)=O(n2)T(n)=O(n^2)T(n)=O(n2)。 2 时间复杂度分析 2.1 关注循环次数多的代码 public int accumulate(int n) { int sum = 0; int i = 1; for (; i &lt;= n; i++) { sum += i; } return sum; } 其中 for 循环内的代码执行 nnn 次，而其余代码执行 1 次，与 nnn 的大小无关，忽略常数项，该段代码的时间复杂度为 O(n)O(n)O(n)。 2.2 加法法则 总复杂度为量级最大的那段代码的复杂度，抽象为公式为： 若 T1(N)=O(f(n)),T2(N)=O(g(n))T_{1}(N)=O(f(n)), T_{2}(N)=O(g(n))T1​(N)=O(f(n)),T2​(N)=O(g(n))，那么 T(N)=T1(N)+T2(N)=O(f(n))+O(g(n))=max⁡(O(f(n)),O(g(n)))T(N)=T_{1}(N)+T_{2}(N)=O(f(n))+O(g(n)) = \\max(O(f(n)), O(g(n)))T(N)=T1​(N)+T2​(N)=O(f(n))+O(g(n))=max(O(f(n)),O(g(n))) public int accumulate(int n) { int sum1 = 0; for (int i = 1; i &lt;= 100; i++) { sum1 += i; } int sum2 = 0; for (int i = 1; i &lt;= n; i++) { sum2 += i; } int sum3 = 0; for (int i = 1; i &lt;= n; i++) { for (int j = 1; j &lt;= n; j++) { sum3 += i * j; } } return sum1 + sum2 + sum3; } 其中 sum1 段的代码循环执行了 100 次，与 nnn 无关。sum2 段代码的复杂度为 O(N)O(N)O(N)，sum3 段的代码复杂度为 O(N2)O(N^2)O(N2)；根据加法法则，我们只去其中最大量级的复杂度，所以该段代码的时间复杂度为 O(N2)O(N^2)O(N2)。 2.3 乘法法则 嵌套代码的复杂度等于嵌套内外代码复杂度的乘积，抽象为公式为： 若 T1(N)=O(f(n)),T2(N)=O(g(n))T_{1}(N)=O(f(n)), T_{2}(N)=O(g(n))T1​(N)=O(f(n)),T2​(N)=O(g(n))，那么 T(N)=T1(N)×T2(N)=O(f(n)×g(n))T(N)=T_{1}(N) \\times T_{2}(N)=O(f(n) \\times g(n))T(N)=T1​(N)×T2​(N)=O(f(n)×g(n)) 3 常见时间复杂度量级 度量级 大 O 表示 常量阶 O(1)O(1)O(1) 对数阶 O(log⁡N)O(\\log{N})O(logN) 线性阶 O(N)O(N)O(N) 线性对数阶 O(Nlog⁡N)O(N\\log{N})O(NlogN) 平方阶 O(N2)O(N^2)O(N2) 立方阶 O(N3)O(N^3)O(N3) 指数阶 O(2n)O(2^n)O(2n) 阶乘阶 O(N!)O(N!)O(N!) 3.1 常见的时间复杂度 常见的时间复杂度有常量阶、对数阶、线性阶、线性对数阶以及平方阶，常量阶、线性阶与平方阶在第二节中已经分析，不再赘述；而一些高效的排序算法的时间复杂度就是线性对数阶，如快速排序，归并排序以及堆排序等。 3.2 对数阶 我们所熟知的二分查找的复杂度就是 O(log⁡N)O(\\log{N})O(logN)，以下通过一段代码来分析对数阶复杂度： public int test(int n) { int res = 1; while (res &lt;= n) { res *= 2; } return res; } 该段代码是求 2x=n2^x=n2x=n 的解，更确切的说，是找出 2x2^x2x 在小于或等于 nnn 的范围内最接近 nnn 的xxx 的值；其中 x=log⁡2nx=\\log_{2}{n}x=log2​n，即 while 循环体内代码要执行 log⁡2n\\log_{2}{n}log2​n 次，即其时间复杂度为 O(log⁡2n)O(\\log_{2}{n})O(log2​n)。 若把循环体内代码 res *= 2 改为 res *= 3，不难分析出其时间复杂度就变为 O(log⁡3n)O(\\log_{3}{n})O(log3​n)；但是为什么所有对数阶的时间复杂度都统一表示为 O(log⁡N)O(\\log{N})O(logN)？ 首先我们先复习对数换底公式: log⁡AB=log⁡CBlog⁡CA\\log_{A}{B}=\\frac{\\log_{C}{B}}{\\log_{C}{A}} logA​B=logC​AlogC​B​ 则 log⁡3n=log⁡2nlog⁡23=log⁡22log⁡23⋅log⁡2n=log⁡32⋅log⁡2n\\log_{3}{n}=\\frac{\\log_{2}{n}}{\\log_{2}{3}}=\\frac{\\log_{2}{2}}{\\log_{2}{3}}\\cdot\\log_{2}{n}=\\log_{3}{2}\\cdot\\log_{2}{n} log3​n=log2​3log2​n​=log2​3log2​2​⋅log2​n=log3​2⋅log2​n 所以 O(log⁡3n)=O(log⁡32⋅log⁡2n)O(\\log_{3}{n})=O(\\log_{3}{2}\\cdot\\log_{2}{n})O(log3​n)=O(log3​2⋅log2​n)，因为 log⁡32\\log_{3}{2}log3​2 为常数项，所以该项可以忽略，因此 O(log⁡3n)=O(log⁡2n)O(\\log_{3}{n})=O(\\log_{2}{n})O(log3​n)=O(log2​n)；所以无论对数以哪个数为底，最后都可以转化为一个常数项与以 222 为底的对数相乘，因此在对数阶时间复杂度的表示方法里，就忽略对数的底，统一表示为 O(log⁡N)O(\\log{N})O(logN) 3.3 O(m+n)O(m+n)O(m+n) 与 O(m×n)O(m \\times n)O(m×n) 此种表示形式的时间复杂度是由两个数据规模来决定的 public int accumulate(int m, int n) { int sum1 = 0; for (int i = 1; i &lt;= m; i++) { sum1 += i; } int sum2 = 0; for (int i = 1; i &lt;= n; i++) { sum2 += i; } return sum1 + sum2; } 由于我们不能事先知晓 mmm 与 nnn 哪个量级大，所以就不能简单的利用加法规则取其最大量级，那么像这种代码的时间复杂度就为 O(m+n)O(m+n)O(m+n)。 public int accumulate(int m, int n) { int sum = 0; for (int i = 1; i &lt;= m; i++) { for (int j = 1; j &lt;= n; j++) { sum += i * j; } } return sum; } 而类似上述代码依然可以使用乘法法则，其时间复杂度为 O(m×n)O(m \\times n)O(m×n)。 4 最好、最坏、平均和均摊时间复杂度 以下将通过一段代码来讲述这几个时间复杂度： public class Test { private int[] array = new int[5]; private int N = 0; public void push(int item) { if (N == array.length) { resize(2 * array.length); } array[N++] = item; } private void resize(int size) { int[] temp = new int[size]; for (int i = 0; i &lt; N; i++) { temp[i] = array[i]; } array = temp; } } 上述代码是用数组模拟一个栈的部分代码，其中 push 表示压栈操作，resize 表示对数组进行扩容的操作；当压入栈中的元素数量达到数组的容量时，就定义一个容量为之前两倍的新数组 temp，将旧数组 array 中的元素复制到新数组中，然后将 array 指向 temp。 4.1 最好时间复杂度 最理想的情况下，当前栈中元素数量比数组的容量小，此时就直接执行代码块 array[N++] = item;，即此时的时间复杂度为 O(1)O(1)O(1)。 4.2 最坏时间复杂度 最糟糕的情况下，当前栈中元素数量与数组的容量相等，此时就要执行 resize 方法进行扩容了，进入循环体，执行 N 次复制操作，此时的时间复杂度为 O(N)O(N)O(N)。 4.3 平均时间复杂度 当栈中元素小于数组容量时，此时进行压栈就有 NNN 种情况，且每种情况的时间复杂度为 O(1)O(1)O(1)；当栈中元素与数组容量相等时，此时进行压栈就只有一种情况了，要进行扩容操作，这种情况的时间复杂度为 O(N)O(N)O(N)；则总共有 N+N+N+` 种情况，对其取平均值： 1+1+1+…+1+NN+1=2NN+1\\cfrac{1+1+1+\\ldots+1+N}{N+1}=\\cfrac{2N}{N+1} N+11+1+1+…+1+N​=N+12N​ 在大 O 标记法中，可以省略系数与低阶项，所以其平均时间复杂度为 O(1)O(1)O(1) 下面使用概率来分析，由于有 N+1N+1N+1 种情况，每种情况的发生概率为 1N+1\\frac{1}{N+1}N+11​，则其平均时间复杂度为： 1×1N+1+1×1N+1+…+1×1N+1+N×1N+1=O(1)1\\times\\frac{1}{N+1}+1\\times\\frac{1}{N+1}+\\ldots+1\\times\\frac{1}{N+1}+N\\times\\frac{1}{N+1}=O(1) 1×N+11​+1×N+11​+…+1×N+11​+N×N+11​=O(1) 4.4 均摊时间复杂度 是一种特殊的平均时间复杂度，根据上述代码，每出现一次扩容操作时，即此时压栈的时间复杂度为 O(N)O(N)O(N)，那么后面的 NNN 次压栈操作的时间复杂度均为 O(1)O(1)O(1)，前后是连贯的，因此将 O(N)O(N)O(N) 平摊到前 NNN 次上，得出均摊时间复杂度为 O(1)O(1)O(1)。 5 不同时间复杂度算法对比 这一节将以一个具体的算法题给出 4 种不同解法，分析各自的时间复杂度并比较其各自的运行性能。 给出两个求和公式，以下分析中会用到： ∑i=1Ni=N(N+1)2∑i=1Ni2=N(N+1)(2N+1)6\\begin{aligned} \\sum_{i=1}^{N}i &amp;=\\frac{N(N+1)}{2} \\\\ \\sum_{i=1}^{N}i^2 &amp;=\\frac{N(N+1)(2N+1)}{6} \\end{aligned} i=1∑N​ii=1∑N​i2​=2N(N+1)​=6N(N+1)(2N+1)​​ 最大子序列和问题 A1,A2,A3,…,ANA_1, A_2, A_3, \\ldots, A_NA1​,A2​,A3​,…,AN​，求 ∑k=ijAk\\sum_{k=i}^{j}A_k∑k=ij​Ak​ 的最大值。（为方便起见，若所有整数均为负数，则最大子序列和为 0）。 例如：输入 −2,11,−4,13,−5,−2-2, 11, -4, 13, -5, -2−2,11,−4,13,−5,−2，其最大子序列和为 11+(−4)+13=2011+(-4)+13=2011+(−4)+13=20。 5.1 时间复杂度为 O(N3)O(N^3)O(N3) 的解法 public static int maxSubSum1(int[] a) { int maxSum = 0; for (int i = 0; i &lt; a.length; i++) { for (int j = i; j &lt; a.length; j++) { int thisSum = 0; for (int k = i; k &lt;= j; k++) { thisSum += a[k]; } if (thisSum &gt; maxSum) { maxSum = thisSum; } } } return maxSum; } 该种解法最简单暴力，定义子序列的起始位置为 i，结束位置为 j，假设数组 a 的长度为 NNN，当 i=0i=0i=0 时，j=0,1,2,3,…,N−1j=0,1,2,3, \\ldots,N-1j=0,1,2,3,…,N−1，共 NNN 种情况，当 N=1N=1N=1 时，j=1,2,3,⋯ ,N−1j=1,2,3,\\cdots,N-1j=1,2,3,⋯,N−1，共 N−1N-1N−1 种情况，以此类推，当 i=N−1i=N-1i=N−1 时，j=N−1j=N-1j=N−1，仅此一种情况；将 i 与 j 之间的所有元素和记为 thisSum，一旦 thisSum 的值比 maxSum 大，就更新 maxSum 的值为 thisSum。 第一个循环大小为 NNN，第二个循环大小为 N−iN-iN−i，第三个循环大小为 j−i+1j-i+1j−i+1，则总运行次数和为: ∑i=0N−1∑j=iN−1∑k=ij1\\sum_{i=0}^{N-1}\\sum_{j=i}^{N-1}\\sum_{k=i}^{j}1 i=0∑N−1​j=i∑N−1​k=i∑j​1 首先有： ∑k=ij1=j−i+1\\sum_{k=i}^{j}1 =j-i+1 k=i∑j​1=j−i+1 接着： ∑j=iN−1(j−i+1)=(N−i+1)(N−i)2\\sum_{j=i}^{N-1}(j-i+1)=\\frac{(N-i+1)(N-i)}{2} j=i∑N−1​(j−i+1)=2(N−i+1)(N−i)​ 那么： ∑i=0N−1(N−i+1)(N−i)2=∑i=1N(N−i+1)(N−i+2)2=12∑i=1Ni2−(N+32)∑i=1Ni+12(N2+3N+2)∑i=1N1=12N(N+1)(2N+1)6−(N+32)N(N+1)2+N2+3N+22N=N3+3N2+2N6\\begin{aligned} \\sum_{i=0}^{N-1} \\frac{(N-i+1)(N-i)}{2} &amp;= \\sum_{i=1}^{N}\\frac{(N-i+1)(N-i+2)}{2}\\\\ &amp;=\\frac{1}{2}\\sum_{i=1}^{N}i^2-(N+\\frac{3}{2})\\sum_{i=1}^{N}i +\\frac{1}{2}(N^2+3N+2)\\sum_{i=1}^{N}1\\\\ &amp;=\\frac{1}{2}\\frac{N(N+1)(2N+1)}{6}-(N+\\frac{3}{2})\\frac{N(N+1)}{2}+\\frac{N^2+3N+2}{2}N\\\\ &amp;=\\frac{N^3+3N^2+2N}{6} \\end{aligned} i=0∑N−1​2(N−i+1)(N−i)​​=i=1∑N​2(N−i+1)(N−i+2)​=21​i=1∑N​i2−(N+23​)i=1∑N​i+21​(N2+3N+2)i=1∑N​1=21​6N(N+1)(2N+1)​−(N+23​)2N(N+1)​+2N2+3N+2​N=6N3+3N2+2N​​ 所以该种解法的时间复杂度为：O(N3+3N2+2N6)=O(N3)O(\\frac{N^3+3N^2+2N}{6})=O(N^3)O(6N3+3N2+2N​)=O(N3)。 5.2 时间复杂度为 O(N2)O(N^2)O(N2)的解法 public static int maxSubSum2(int[] a) { int maxSum = 0; for (int i = 0; i &lt; a.length; i++) { int thisSum = 0; for (int j = i; j &lt; a.length; j++) { thisSum += a[j]; if (thisSum &gt; maxSum) { maxSum = thisSum; } } } return maxSum; } 在第一种解法中，拿掉最里面的那层循环，并稍做改动，就是现在的解法 2。 其中第一层循环大小为 NNN，第二层循环为 N−iN-iN−i，则总运行次数为： ∑i=0N−1∑j=iN−11\\sum_{i=0}^{N-1} \\sum_{j=i}^{N-1}1 i=0∑N−1​j=i∑N−1​1 其中： ∑j=iN−11=N−1−i+1=N−i\\sum_{j=i}^{N-1}1 = N-1-i+1=N-i j=i∑N−1​1=N−1−i+1=N−i 那么： ∑i=0N−1(N−i)=N∑i=0N−11−∑i=0N−1i=N(N−1+1)−(N−1)N2=N2−N2\\begin{aligned} \\sum_{i=0}^{N-1}(N-i) &amp;= N\\sum_{i=0}^{N-1}1- \\sum_{i=0}^{N-1} i \\\\ &amp;= N(N-1+1) - \\frac{(N-1)N}{2} \\\\ &amp;= \\frac{N^2-N}{2} \\end{aligned} i=0∑N−1​(N−i)​=Ni=0∑N−1​1−i=0∑N−1​i=N(N−1+1)−2(N−1)N​=2N2−N​​ 所以第二种解法的时间复杂度为 O(N2−N2)=O(N2)O(\\frac{N^2-N}{2})=O(N^2)O(2N2−N​)=O(N2)。 5.3 时间复杂度为 O(Nlog⁡N)O(N\\log{N})O(NlogN) 的解法 如下图所示，可以将数组分为三部分，分别为前中后三部分。 最大子序列和就可能出现在这三个部分中，其中 mid=start+end2=0+52=2\\text{mid}=\\frac{\\text{start}+\\text{end}}{2}=\\frac{0+5}{2}=2mid=2start+end​=20+5​=2，前半部分是从 start\\text{start}start 到 mid\\text{mid}mid 这一部分的元素，即 −2,11,−4-2,11,-4−2,11,−4，所以该部分最大元素为 111111；后半部分是从 mid+1\\text{mid+1}mid+1 到 end\\text{end}end 这一部分的元素，即 13,−5,−213,-5,-213,−5,−2，所以该部分最大元素为 13\\text{13}13；而中间部分元素是以 mid\\text{mid}mid 起始，分别向左和向右进行累加计算，分别求出其向左和向右部分的最大值，从 mid\\text{mid}mid 向左得到其最大值：−4+11=7-4+11=7−4+11=7，而向右是从 mid+1\\text{mid+1}mid+1 开始算起得到其最大值：131313，最后将左右两部分和相加即为中间部分的最大值：7+13=207+13=207+13=20；比较前中后部分的最大值，发现中间部分的值 202020 最大，所以该数组最大子序列和为 202020。 那么在程序中如何实现呢？这就要采用分治策略，将数组 a 分为前后两半子数组 b, c，再将前半数组 b 分为前后两半子数组 d, e，后半数组 c 分为前后两半子数组 f, g, ……，直到数组不能再分为止，此时子数组中就只有一个元素，一个元素就好判断了，该元素为正就直接把该元素值返回给上一级子数组，为负就返回 0，然后回到上一级子数组，将之前返回的前后部分子数组的最大值与中间部分最大值进行比较，得出其最大值，接着将最大值返回其上一级子数组，直至回到原数组，这时原数组就得到了前后部分子数组的最大值，接着求出中间部分子数组的最大值并与前后部分进行比较即可得到整个数组的最大子序列和。 public static int maxSubSum3(int[] a) { return a.length &gt; 0 ? maxSumRec(a, 0, a.length - 1) : 0; } private static int maxSumRec(int[] a, int left, int right) { if (left == right) { if (a[left] &gt; 0) { return a[left]; } else { return 0; } } int center = (left + right) / 2; int maxLeftSum = maxSumRec(a, left, center); int maxRightSum = maxSumRec(a, center + 1, right); int maxLeftBorderSum = 0; int leftBorderSum = 0; for (int i = center; i &gt;= left; i--) { leftBorderSum += a[i]; if (leftBorderSum &gt; maxLeftBorderSum) { maxLeftBorderSum = leftBorderSum; } } int maxRightBorderSum = 0; int rightBorderSum = 0; for (int i = center + 1; i &lt;= right; i++) { rightBorderSum += a[i]; if (rightBorderSum &gt; maxRightBorderSum) { maxRightBorderSum = rightBorderSum; } } return max3(maxLeftSum, maxRightSum, maxLeftBorderSum + maxRightBorderSum); } private static int max3(int a, int b, int c) { return a &gt; b ? a &gt; c ? a : c : b &gt; c ? b : c; } 其中 center 为数组中间元素的下标，maxLeftSum 和 maxRightSum 分别为数组前后部分的最大值，maxLeftBorderSum 为中间部分向左计算的最大值，maxRightBorderSum 为中间部分向右计算最大值；maxLeftBorderSum + maxRightBorderSum 即为中间部分的最大值。 计算中间部分，即计算 maxLeftBorderSum 和 maxRightBorderSum 总花费时间为 NNN，而计算前后两半部分，即 maxLeftSum 和 maxRightSum 每个花费 T(N/2)T(N/2)T(N/2) 个时间单元，则总共花费时间： T(N)=2T(N/2)+NT(N)=2T(N/2)+N T(N)=2T(N/2)+N 其中 T(1)=1T(1)=1T(1)=1，则 T(2)=4=2×2T(2)=4=2 \\times 2T(2)=4=2×2，T(4)=12=4×3T(4)=12=4 \\times 3T(4)=12=4×3，T(8)=32=8×4T(8)=32=8 \\times 4T(8)=32=8×4，T(16)=80=16×5T(16)=80=16 \\times 5T(16)=80=16×5。 那么当 N=2kN=2^kN=2k，则 T(N)=N×(k+1)=N(log⁡N+1)T(N)=N \\times (k+1)=N(\\log{N}+1)T(N)=N×(k+1)=N(logN+1)，忽略低阶项，所以该方法的时间复杂度为：O(Nlog⁡N)O(N\\log{N})O(NlogN)。 5.4 时间复杂度为 O(N)O(N)O(N) 的解法 public static int maxSubSum4(int[] a) { int maxSum = 0; int thisSum = 0; for (int i = 0; i &lt; a.length; i++) { thisSum += a[i]; if (thisSum &gt; maxSum) { maxSum = thisSum; } else if (thisSum &lt; 0) { thisSum = 0; } } return maxSum; } 此种方法将时间复杂度优化到了 O(N)O(N)O(N)，只需一轮循环即可找到最大子序列；其思路为：若当前子序列的和 thisSum 为负数，则将 thisSum 置为 000，下一个数组元素作为新的子序列的起始位置，thisSum 从该元素开始累加，直至找到最大子序列的和。 5.5 对比分析 使用下面代码测试上述 4 中解法所消耗的时间： public static void getTimingInfo(int n, int alg) { int[] test = new int[n]; Random rand = new Random(); long startTime = System.currentTimeMillis(); long totalTime = 0; int i; for (i = 0; totalTime &lt; 4000; i++) { for (int j = 0; j &lt; test.length; j++) { test[j] = rand.nextInt(100) - 50; } switch (alg) { case 1: maxSubSum1(test); break; case 2: maxSubSum2(test); break; case 3: maxSubSum3(test); break; case 4: maxSubSum4(test); break; default: } totalTime = System.currentTimeMillis() - startTime; } System.out.print(String.format(&quot;\\t%12.6f&quot;, (totalTime * 1000 / i) / (double) 1000000)); } public static void main(String[] args) { for (int n = 100; n &lt;= 1000000; n *= 10) { System.out.print(String.format(&quot;N = %7d&quot;, n)); for (int alg = 1; alg &lt;= 4; alg++) { if ((alg == 1 &amp;&amp; n &gt; 50000) || (alg == 2 &amp;&amp; n &gt; 500000)) { System.out.print(&quot;\\t NA &quot;); continue; } getTimingInfo(n, alg); } System.out.println(); } } 运行结果如下图，当预测时间过长，将其设为 NA，从图中可以看出，不同时间复杂度的程序虽然得出的结果是一样的，但运行性能相差巨大，犹如波音与摩拜的差别。 输入大小 N O(N3)O(N^3)O(N3) O(N2)O(N^2)O(N2) O(Nlog⁡N)O(N\\log{N})O(NlogN) O(N)O(N)O(N) 100 0.000063 0.000005 0.000003 0.000001 1000 0.054986 0.000201 0.000036 0.000014 10000 55.234000 0.018058 0.000371 0.000125 100000 NA 1.790000 0.003937 0.001249 1000000 NA NA 0.041979 0.012479 6 总结 以后写代码之前要多思考，避免一上来就暴力求解，造成巨大的性能开销，应尽量将程序优化到线性阶或线性对数阶以内。 ","link":"https://faded.auspicious.space/post/algorithm-complexity-comparison/"},{"title":"笔算平方根","content":"我以计算 2016 的算术平方根作为例子。全程用竖式计算。 首先， 以小数点为基准，每两位数作为一组。像这样写： 注意，平方根的两倍那个位置要往左边写一点，不然后面不够位置。接下来估算第一节，是 202020，42&lt;20&lt;524^2 \\lt 20 \\lt 5^242&lt;20&lt;52，所以在第一节的上面写上 444，对应的在左边写上 888，42=164^2=1642=16 写到 202020 的下面，减一下，把下一节拉下来： 估计下一位的方法是： 8?×?=4168?\\times?=4168?×?=416。因为 83×3=24983 \\times 3=24983×3=249，84×4=33684 \\times 4=33684×4=336，85×5=42585 \\times 5=42585×5=425， 425425425 大于 416416416 了，所以下一位就是 444 了，更新算式，把下一节拉下来： 再估计下一位： 88?×?=800088? \\times ?=800088?×?=8000。 888×8=7104888 \\times 8=7104888×8=7104，889×9=8001889 \\times 9=8001889×9=8001， 哎呀，差一点点，还是 888 吧： 下一位：896?×?=89600896? \\times ?=89600896?×?=89600， 由于刚才就差一点点，这次应该是 999 了。8969×9=807218969 \\times 9=807218969×9=80721，没问题： 8978?×?=8879008978? \\times ?=8879008978?×?=887900，凭感觉应该还是 999，89789×9=80810189789 \\times 9=80810189789×9=808101，OK： 89798?×?=797990089798? \\times ?=797990089798?×?=7979900，这回是 888 了。 不过我还是不写下去了, 因为示范了那么多步，大家应该知道步骤了吧？ ","link":"https://faded.auspicious.space/post/written-calculation-of-square-root/"},{"title":"算法复杂度分析","content":" 算法复杂度分析 为什么要进行算法分析？ 预测算法所需的资源 计算时间（CPU 消耗） 内存空间（RAM 消耗） 通信时间（带宽消耗） 预测算法的运行时间 在给定输入规模时，所执行的基本操作数量。 或者称为算法复杂度（Algorithm Complexity） 如何衡量算法复杂度？ 内存（Memory） 时间（Time） 指令的数量（Number of Steps） 特定操作的数量 磁盘访问数量 网络包数量 渐进复杂度（Asymptotic Complexity） 算法的运行时间与什么相关？ 取决于输入的数据。（例如：如果数据已经是排好序的，时间消耗可能会减少。） 取决于输入数据的规模。（例如：6 和 6 * 109） 取决于运行时间的上限。（因为运行时间的上限是对使用者的承诺。） 算法分析的种类： 最坏情况（Worst Case）：任意输入规模的最大运行时间。（Usually） 平均情况（Average Case）：任意输入规模的期待运行时间。（Sometimes） 最佳情况（Best Case）：通常最佳情况不会出现。（Bogus） 例如，在一个长度为 n 的列表中顺序搜索指定的值，则 最坏情况：n 次比较 平均情况：n/2 次比较 最佳情况：1 次比较 而实际中，我们一般仅考量算法在最坏情况下的运行情况，也就是对于规模为 n 的任何输入，算法的最长运行时间。这样做的理由是： 一个算法的最坏情况运行时间是在任何输入下运行时间的一个上界（Upper Bound）。 对于某些算法，最坏情况出现的较为频繁。 大体上看，平均情况通常与最坏情况一样差。 算法分析要保持大局观（Big Idea），其基本思路： 忽略掉那些依赖于机器的常量。 关注运行时间的增长趋势。 比如：T(n)=73n3+29n3+8888T(n) = 73n^3 + 29n^3 + 8888T(n)=73n3+29n3+8888 的趋势就相当于 T(n)=Θ(n3)T(n) = \\Theta(n^3)T(n)=Θ(n3)。 渐近记号（Asymptotic Notation）通常有 OOO、Θ\\ThetaΘ 和 Ω\\OmegaΩ 记号法。Θ\\ThetaΘ 记号渐进地给出了一个函数的上界和下界，当只有渐近上界时使用 OOO 记号，当只有渐近下界时使用 Ω\\OmegaΩ 记号。尽管技术上 Θ\\ThetaΘ 记号较为准确，但通常仍然使用 OOO 记号表示。 使用 OOO 记号法（Big O Notation）表示最坏运行情况的上界。例如， 线性复杂度 O(n)O(n)O(n) 表示每个元素都要被处理一次。 平方复杂度 O(n2)O(n^2)O(n2) 表示每个元素都要被处理 nnn 次。 Notation Intuition Informal Definition f(n)∈O(g(n))f(n) \\in O(g(n))f(n)∈O(g(n)) fff is bounded above by ggg asymptotically ∣f(n)∣≤g(n)⋅k\\lvert f(n) \\rvert \\le g(n) \\cdot k∣f(n)∣≤g(n)⋅k f(n)∈Ω(g(n))f(n) \\in \\Omega(g(n))f(n)∈Ω(g(n)) Two definitions: Number theory: fff is not dominated by ggg asymptotically Complexity theory: fff is bounded below by ggg asymptotically f(n)≥g(n)⋅kf(n) \\ge g(n) \\cdot kf(n)≥g(n)⋅k f(n)∈Θ(g(n))f(n) \\in \\Theta(g(n))f(n)∈Θ(g(n)) fff is bounded both above and below by ggg asymptotically g(n)⋅k1≤f(n)≤g(n)⋅k2g(n) \\cdot k_1 \\le f(n) \\le g(n) \\cdot k_2g(n)⋅k1​≤f(n)≤g(n)⋅k2​ 例如： T(n)=O(n3)T(n) = O(n^3)T(n)=O(n3) 等同于 T(n)∈O(n3)T(n) \\in O(n^3)T(n)∈O(n3) T(n)=Θ(n3)T(n) = \\Theta(n^3)T(n)=Θ(n3) 等同于 T(n)∈Θ(n3)T(n) \\in \\Theta(n3)T(n)∈Θ(n3). 相当于: T(n)T(n)T(n) 的渐近增长不快于 n3n^3n3。 T(n)T(n)T(n) 的渐近增长与 n3n^3n3 一样快。 复杂度 标记符号 描述 常量（Constant） O(1)O(1)O(1) 操作的数量为常数，与输入的数据的规模无关。 n = 1,000,000 →\\to→ 1-2 operations 对数（Logarithmic） O(log⁡2n)O(\\log_{2}{n})O(log2​n) 操作的数量与输入数据的规模 nnn 的比例是 log⁡2n\\log_{2}{n}log2​n。 n = 1,000,000 →\\to→ 30 operations 线性（Linear） O(n)O(n)O(n) 操作的数量与输入数据的规模 nnn 成正比。 n = 10,000 →\\to→ 5000 operations 平方（Quadratic） O(n2)O(n^2)O(n2) 操作的数量与输入数据的规模 nnn 的比例为二次平方。 n = 500 →\\to→ 250,000 operations 立方（Cubic） O(n3)O(n^3)O(n3) 操作的数量与输入数据的规模 nnn 的比例为三次方。 n = 200 →\\to→ 8,000,000 operations 指数（Exponential） O(2n)O(2^n)O(2n) O(kn)O(k^n)O(kn) O(n!)O(n!)O(n!) 指数级的操作，快速的增长。 n = 20 →\\to→ 1048576 operations 注1：快速的数学回忆，log⁡ab=y\\log_{a}{b} = yloga​b=y 其实就是 ay=ba^y = bay=b。所以，log⁡24=2\\log_{2}{4} = 2log2​4=2，因为 22=42^2 = 422=4。同样 log⁡28=3\\log_{2}{8} = 3log2​8=3，因为 23=82^3 = 823=8。我们说，log⁡2n\\log_{2}{n}log2​n 的增长速度要慢于 nnn，因为当 n=8n = 8n=8 时，log⁡2n=3\\log_{2}{n} = 3log2​n=3。 注2：通常将以 10 为底的对数叫做常用对数。为了简便，NNN 的常用对数 log⁡10N\\log_{10}{N}log10​N 简写做 lg⁡N\\lg{N}lgN，例如 log⁡105\\log_{10}{5}log10​5 记做 lg⁡5\\lg{5}lg5。 注3：通常将以无理数 eee 为底的对数叫做自然对数。为了方便，NNN 的自然对数 log⁡eN\\log_{e}{N}loge​N 简写做 ln⁡N\\ln{N}lnN，例如 loge3log_{e}{3}loge​3 记做 ln⁡3\\ln{3}ln3。 注4：在算法导论中，采用记号 lg⁡n=log⁡2n\\lg{n} = \\log_{2}{n}lgn=log2​n ，也就是以 2 为底的对数。改变一个对数的底只是把对数的值改变了一个常数倍，所以当不在意这些常数因子时，我们将经常采用 &quot;lg n&quot;记号，就像使用 OOO 记号一样。计算机工作者常常认为对数的底取 2 最自然，因为很多算法和数据结构都涉及到对问题进行二分。 而通常时间复杂度与运行时间有一些常见的比例关系： 复杂度 10 20 50 100 1000 10000 100000 O(1)O(1)O(1) &lt;1s &lt;1s &lt;1s &lt;1s &lt;1s &lt;1s &lt;1s O(log⁡2(n))O(\\log_{2}{(n)})O(log2​(n)) &lt;1s &lt;1s &lt;1s &lt;1s &lt;1s &lt;1s &lt;1s O(n)O(n)O(n) &lt;1s &lt;1s &lt;1s &lt;1s &lt;1s &lt;1s &lt;1s O(n∗log⁡2(n))O(n*\\log_{2}{(n)})O(n∗log2​(n)) &lt;1s &lt;1s &lt;1s &lt;1s &lt;1s &lt;1s &lt;1s O(n2)O(n^2)O(n2) &lt;1s &lt;1s &lt;1s &lt;1s &lt;1s 2s 3-4 min O(n3)O(n^3)O(n3) &lt;1s &lt;1s &lt;1s &lt;1s 20s 5 hours 231 days O(2n)O(2^n)O(2n) &lt;1s &lt;1s 260 days hangs hangs hangs hangs O(n!)O(n!)O(n!) &lt;1s hangs hangs hangs hangs hangs hangs O(nn)O(n^n)O(nn) 3-4 min hangs hangs hangs hangs hangs hangs 计算代码块的渐进运行时间的方法有如下步骤： 确定决定算法运行时间的组成步骤。 找到执行该步骤的代码，标记为 1。 查看标记为 1 的代码的下一行代码。如果下一行代码是一个循环，则将标记 1 修改为 1 倍于循环的次数 1 * n。如果包含多个嵌套的循环，则将继续计算倍数，例如 1 * n * m。 找到标记到的最大的值，就是运行时间的最大值，即算法复杂度描述的上界。 示例代码（1）： decimal Factorial(int n) { if (n == 0) return 1; else return n * Factorial(n - 1); } 阶乘（factorial），给定规模 nnn，算法基本步骤执行的数量为 nnn，所以算法复杂度为 O(n)O(n)O(n)。 示例代码（2）： int FindMaxElement(int[] array) { int max = array[0]; for (int i = 0; i &lt; array.Length; i++) { if (array[i] &gt; max) { max = array[i]; } } return max; } 这里，nnn 为数组 array 的大小，则最坏情况下需要比较 nnn 次以得到最大值，所以算法复杂度为 O(n)O(n)O(n)。 示例代码（3）： long FindInversions(int[] array) { long inversions = 0; for (int i = 0; i &lt; array.Length; i++) for (int j = i + 1; j &lt; array.Length; j++) if (array[i] &gt; array[j]) inversions++; return inversions; } 这里，nnn 为数组 array 的大小，则基本步骤的执行数量约为 n×n−12n \\times \\frac{n - 1}{2}n×2n−1​，所以算法复杂度为 O(n2)O(n^2)O(n2)。 示例代码（4）： long SumMN(int n, int m) { long sum = 0; for (int x = 0; x &lt; n; x++) for (int y = 0; y &lt; m; y++) sum += x * y; return sum; } 给定规模 nnn 和 mmm，则基本步骤的执行数量为 n×mn \\times mn×m，所以算法复杂度为 O(n2)O(n^2)O(n2)。 示例代码（5）： decimal Sum3(int n) { decimal sum = 0; for (int a = 0; a &lt; n; a++) for (int b = 0; b &lt; n; b++) for (int c = 0; c &lt; n; c++) sum += a * b * c; return sum; } 这里，给定规模 nnn，则基本步骤的执行数量约为 n×n×nn \\times n \\times nn×n×n ，所以算法复杂度为 O(n3)O(n^3)O(n3)。 示例代码（6）： decimal Calculation(int n) { decimal result = 0; for (int i = 0; i &lt; (1 &lt;&lt; n); i++) result += i; return result; } 这里，给定规模 nnn，则基本步骤的执行数量为 2n2^n2n，所以算法复杂度为 O(2n)O(2^n)O(2n)。 示例代码（7）： 斐波那契数列： fib(0)=0fib(0) = 0fib(0)=0 fib(1)=1fib(1) = 1fib(1)=1 fib(n)=fib(n−1)+fib(n−2)fib(n) = fib(n-1) + fib(n-2)fib(n)=fib(n−1)+fib(n−2) fib()=0,1,1,2,3,5,8,13,21,34...fib() = 0, 1, 1, 2, 3, 5, 8, 13, 21, 34 ...fib()=0,1,1,2,3,5,8,13,21,34... int Fibonacci(int n) { if (n &lt;= 1) return n; else return Fibonacci(n - 1) + Fibonacci(n - 2); } 这里，给定规模 nnn，计算 fib(n)fib(n)fib(n) 所需的时间为计算 fib(n−1)fib(n-1)fib(n−1) 的时间和计算 fib(n−2)fib(n-2)fib(n−2) 的时间的和。 T(n≤1)=O(1)T(n \\le 1) = O(1)T(n≤1)=O(1) T(n)=T(n−1)+T(n−2)+O(1)T(n) = T(n-1) + T(n-2) + O(1)T(n)=T(n−1)+T(n−2)+O(1) fib(5) / \\ fib(4) fib(3) / \\ / \\ fib(3) fib(2) fib(2) fib(1) / \\ / \\ / \\ 通过使用递归树的结构描述可知算法复杂度为 O(2n)O(2^n)O(2n)。 示例代码（8）： int Fibonacci(int n) { if (n &lt;= 1) return n; else { int[] f = new int[n + 1]; f[0] = 0; f[1] = 1; for (int i = 2; i &lt;= n; i++) { f[i] = f[i - 1] + f[i - 2]; } return f[n]; } } 同样是斐波那契数列，我们使用数组 f 来存储计算结果，这样算法复杂度优化为 O(n)O(n)O(n)。 示例代码（9）： int Fibonacci(int n) { if (n &lt;= 1) return n; else { int iter1 = 0; int iter2 = 1; int f = 0; for (int i = 2; i &lt;= n; i++) { f = iter1 + iter2; iter1 = iter2; iter2 = f; } return f; } } 同样是斐波那契数列，由于实际只有前两个计算结果有用，我们可以使用中间变量来存储，这样就不用创建数组以节省空间。同样算法复杂度优化为 O(n)。 示例代码（10）： 通过使用矩阵乘方的算法来优化斐波那契数列算法。 static int Fibonacci(int n) { if (n &lt;= 1) return n; int[,] f = { { 1, 1 }, { 1, 0 } }; Power(f, n - 1); return f[0, 0]; } static void Power(int[,] f, int n) { if (n &lt;= 1) return; int[,] m = { { 1, 1 }, { 1, 0 } }; Power(f, n / 2); Multiply(f, f); if (n % 2 != 0) Multiply(f, m); } static void Multiply(int[,] f, int[,] m) { int x = f[0, 0] * m[0, 0] + f[0, 1] * m[1, 0]; int y = f[0, 0] * m[0, 1] + f[0, 1] * m[1, 1]; int z = f[1, 0] * m[0, 0] + f[1, 1] * m[1, 0]; int w = f[1, 0] * m[0, 1] + f[1, 1] * m[1, 1]; f[0, 0] = x; f[0, 1] = y; f[1, 0] = z; f[1, 1] = w; } 优化之后算法复杂度为 O(log⁡2n)O(\\log_{2}{n})O(log2​n)。 示例代码（11）： 在 C# 中更简洁的代码如下。 static double Fibonacci(int n) { double sqrt5 = Math.Sqrt(5); double phi = (1 + sqrt5) / 2.0; double fn = (Math.Pow(phi, n) - Math.Pow(1 - phi, n)) / sqrt5; return fn; } 示例代码（12）： 插入排序的基本操作就是将一个数据插入到已经排好序的有序数据中，从而得到一个新的有序数据。算法适用于少量数据的排序，时间复杂度为 O(n2)O(n^2)O(n2)。 private static void InsertionSortInPlace(int[] unsorted) { for (int i = 1; i &lt; unsorted.Length; i++) { if (unsorted[i - 1] &gt; unsorted[i]) { int key = unsorted[i]; int j = i; while (j &gt; 0 &amp;&amp; unsorted[j - 1] &gt; key) { unsorted[j] = unsorted[j - 1]; j--; } unsorted[j] = key; } } } ","link":"https://faded.auspicious.space/post/algorithm-complexity-analysis/"},{"title":"一个漂亮的大 O 速查表","content":" 每个程序员都应该收藏的算法复杂度速查表 图例 绝佳 不错 一般 不佳 糟糕 数据结构操作 数据结构时间复杂度空间复杂度 &nbsp;平均最差最差 &nbsp;访问搜索插入删除访问搜索插入删除&nbsp; Array O(1) O(n) O(n) O(n) O(1) O(n) O(n) O(n) O(n) Stack O(n) O(n) O(1) O(1) O(n) O(n) O(1) O(1) O(n) Singly-Linked List O(n) O(n) O(1) O(1) O(n) O(n) O(1) O(1) O(n) Doubly-Linked List O(n) O(n) O(1) O(1) O(n) O(n) O(1) O(1) O(n) Skip List O(log(n)) O(log(n)) O(log(n)) O(log(n)) O(n) O(n) O(n) O(n) O(n log(n)) Hash Table - O(1) O(1) O(1) - O(n) O(n) O(n) O(n) Binary Search Tree O(log(n)) O(log(n)) O(log(n)) O(log(n)) O(n) O(n) O(n) O(n) O(n) Cartesian Tree - O(log(n)) O(log(n)) O(log(n)) - O(n) O(n) O(n) O(n) B-Tree O(log(n)) O(log(n)) O(log(n)) O(log(n)) O(log(n)) O(log(n)) O(log(n)) O(log(n)) O(n) Red-Black Tree O(log(n)) O(log(n)) O(log(n)) O(log(n)) O(log(n)) O(log(n)) O(log(n)) O(log(n)) O(n) Splay Tree - O(log(n)) O(log(n)) O(log(n)) - O(log(n)) O(log(n)) O(log(n)) O(n) AVL Tree O(log(n)) O(log(n)) O(log(n)) O(log(n)) O(log(n)) O(log(n)) O(log(n)) O(log(n)) O(n) 数组排序算法 算法时间复杂度空间复杂度 &nbsp;最佳平均最差最差 Quicksort O(n log(n)) O(n log(n)) O(n^2) O(log(n)) Mergesort O(n log(n)) O(n log(n)) O(n log(n)) O(n) Timsort O(n) O(n log(n)) O(n log(n)) O(n) Heapsort O(n log(n)) O(n log(n)) O(n log(n)) O(1) Bubble Sort O(n) O(n^2) O(n^2) O(1) Insertion Sort O(n) O(n^2) O(n^2) O(1) Selection Sort O(n^2) O(n^2) O(n^2) O(1) Shell Sort O(n) O((nlog(n))^2) O((nlog(n))^2) O(1) Bucket Sort O(n+k) O(n+k) O(n^2) O(n) Radix Sort O(nk) O(nk) O(nk) O(n+k) 图操作 节点 / 边界管理存储增加顶点增加边界移除顶点移除边界查询 Adjacency list O(|V|+|E|) O(1) O(1) O(|V| + |E|) O(|E|) O(|V|) Incidence list O(|V|+|E|) O(1) O(1) O(|E|) O(|E|) O(|E|) Adjacency matrix O(|V|^2) O(|V|^2) O(1) O(|V|^2) O(1) O(1) Incidence matrix O(|V| ⋅ |E|) O(|V| ⋅ |E|) O(|V| ⋅ |E|) O(|V| ⋅ |E|) O(|V| ⋅ |E|) O(|E|) 堆操作 类型时间复杂度 &nbsp;Heapify查找最大值分离最大值提升键插入删除合并 Linked List (sorted) - O(1) O(1) O(n) O(n) O(1) O(m+n) Linked List (unsorted) - O(n) O(n) O(1) O(1) O(1) O(1) Binary Heap O(n) O(1) O(log(n)) O(log(n)) O(log(n)) O(log(n)) O(m+n) Binomial Heap - O(1) O(log(n)) O(log(n)) O(1) O(log(n)) O(log(n)) Fibonacci Heap - O(1) O(log(n)) O(1) O(1) O(log(n)) O(1) 大 O 复杂度图表 ","link":"https://faded.auspicious.space/post/a-beautiful-big-o-cheatsheet/"},{"title":"《雷神之锤 3》中的平方根算法","content":"《雷神之锤 3》的作者是约翰卡马克，早前，《雷神之锤 3》的源码公开。卡马克大神有一段代码，简直是吊炸天。 float Q_rsqrt(float number) { long i; float x2, y; const float threehalfs = 1.5F; x2 = number * 0.5F; y = number; i = *(long) &amp;y; // evil floating point bit level hacking i = 0x5f3759df - (i &gt;&gt; 1); // what the fuck? y= *(float*) &amp;i; y = y * (threehalfs - (x2 * y * y)); //1st iteration // y = y * (threehalfs - (x2 * y * y)); // 2nd iteration, this can be removed #ifndef Q_3VM #ifdef __linux__ assert(!isnan(y)); //bk010122 - FPE? #endif #endif return y; } 这段代码，据说主要用处是把一个数开平方并且取倒。经过测试之后，据说上面这段代码，尽然比 (float)(1.0/sqrt(x)) 更快，而且是快 4 倍。 具体的实现过程比较复杂，下面是实现过程。 函数返回 1 / sqr(x)，这个函数在图像处理中比 sqrt(x) 更有用。 注意到这个函数只用了一次迭代！（其实就是根本没用迭代，直接运算）。编译，实验，这个函数不仅工作的很好，而且比标准的 sqrt() 函数快 4 倍！要知道，编译器自带的函数，可是经过严格仔细的汇编优化的啊！ 这个简洁的函数，最核心，也是最让人费解的，就是标注了 &quot;what the fuck?&quot; 的一句 i = 0x5f3759df - (i &gt;&gt; 1); 再加上 y = y * ( threehalfs - (x2 * y * y)); 两句话就完成了开方运算!而且注意到，核心那句是定点移位运算，速度极快!特别在很多没有乘法指令的 RISC结构 CPU上，这样做是极其高效的。算法的原理其实不复杂就是牛顿迭代法用 x−f(x)f′(x)x-f(x)f&#x27;(x)x−f(x)f′(x) 不断的逼近 f(x)=af(x)=af(x)=a 的根。 简单来说比如求平方根，f(x) = x ^ 2 = a，f'(x) = 2 * x，f(x) / f'(x) = x / 2， 把 f(x) 代入 x - f(x) / f'(x) 后有 (x + a / x) / 2，现在我们选 a = 5，选一个猜测值比如 2， 那么我们可以这么算 5 / 2 = 2.5; (2.5 + 2) / 2 = 2.25; 5 / 2 .25 = xxx; (2.25 + xxx) / 2 = xxxx ... 这样反复迭代下去，结果必定收敛于 sqrt(5)，没错，一般的求平方根都是这么算的 但是卡马克（quake 3 作者）真正牛B的地方是他选择了一个神秘的常数 0x5f3759df 来计算那个猜测值 就是我们加注释的那一行，那一行算出的值非常接近 1 / sqrt(n)，这样我们只需要 2 次牛顿迭代就可以达到我们所需要的精度。 ","link":"https://faded.auspicious.space/post/square-root-algorithm-in-quake-3/"},{"title":"Vim——常用命令大全","content":" VIM快捷键大全 目录 1 关于VIM 1.1 VIM 的几种模式 正常模式：可以使用快捷键命令，或按:输入命令行。 插入模式：可以输入文本，在正常模式下，按 i、a、o 等都可以进入插入模式。 可视模式：正常模式下按v可以进入可视模式， 在可视模式下，移动光标可以选择文本。按 V 进入可视行模式， 总是整行整行的选中。ctrl+v 进入可视块模式。 替换模式：正常模式下，按 R 进入。 2 启动Vim vim -c cmd file: 在打开文件前，先执行指定的命令； vim -r file: 恢复上次异常退出的文件； vim -R file: 以只读的方式打开文件，但可以强制保存； vim -M file: 以只读的方式打开文件，不可以强制保存； vim -y num file: 将编辑窗口的大小设为 num 行； vim + file: 从文件的末尾开始； vim +num file: 从第num行开始； vim +/string file: 打开 file，并将光标停留在第一个找到的 string 上。 vim --remote file: 用已有的 VIM 进程打开指定的文件。 如果你不想启用多个 VIM 会话，这个很有用。但要注意， 如果你用 VIM，会寻找名叫 VIM 的服务器；如果你已经有一个 gvim 在运行了， 你可以用 gvim --remote file 在已有的 gvim p 中打开文件。 3 文档操作 :e file --关闭当前编辑的文件，并开启新的文件。 如果对当前文件的修改未保存，vi 会警告。 :e! file --放弃对当前文件的修改，编辑新的文件。 :e+file -- 开始新的文件，并从文件尾开始编辑。 :e+n file -- 开始新的文件，并从第 n 行开始编辑。 :enew --编译一个未命名的新文档。(CTRL-W n)。 :e -- 重新加载当前文档。 :e! -- 重新加载当前文档，并丢弃已做的改动。 :e#或ctrl+^ -- 回到刚才编辑的文件，很实用。 :f或ctrl+g -- 显示文档名，是否修改，和光标位置。 :f filename -- 改变编辑的文件名，这时再保存相当于另存为。 gf -- 打开以光标所在字符串为文件名的文件。 :w -- 保存修改。 :n1,n2w filename -- 选择性保存从某 n1 行到另 n2 行的内容。 :wq -- 保存并退出。 ZZ -- 保存并退出。 :x -- 保存并退出。 :q[uit] --退出当前窗口。(CTRL-W q 或 CTRL-W CTRL-Q) :saveas newfilename -- 另存为 :browse e -- 会打开一个文件浏览器让你选择要编辑的文件。 如果是终端中，则会打开 netrw 的文件浏览窗口； 如果是 gvim，则会打开一个图形界面的浏览窗口。 实际上 :browse 后可以跟任何编辑文档的命令，如 sp 等。 用 browse 打开的起始目录可以由 browsedir 来设置： :set browsedir=last -- 用上次访问过的目录（默认）； :set browsedir=buffer -- 用当前文件所在目录； :set browsedir=current -- 用当前工作目录； :Sex -- 水平分割一个窗口，浏览文件系统； :Vex -- 垂直分割一个窗口，浏览文件系统； 4 光标的移动 4.1 基本移动 以下移动都是在 normal 模式下。 h 或退格: 左移一个字符； l 或空格: 右移一个字符； j: 下移一行； k: 上移一行； gj: 移动到一段内的下一行； gk: 移动到一段内的上一行； + 或 Enter: 把光标移至下一行第一个非空白字符。 -: 把光标移至上一行第一个非空白字符。 w: 前移一个单词，光标停在下一个单词开头； W: 移动下一个单词开头，但忽略一些标点； e: 前移一个单词，光标停在下一个单词末尾； E: 移动到下一个单词末尾，如果词尾有标点，则移动到标点； b: 后移一个单词，光标停在上一个单词开头； B: 移动到上一个单词开头，忽略一些标点； ge: 后移一个单词，光标停在上一个单词末尾； gE: 同 ge ，不过‘单词’包含单词相邻的标点。 (: 前移 1 句。 ): 后移 1 句。 {: 前移 1 段。 }: 后移 1 段。 fc: 把光标移到同一行的下一个 c 字符处 Fc: 把光标移到同一行的上一个 c 字符处 tc: 把光标移到同一行的下一个 c 字符前 Tc: 把光标移到同一行的上一个 c 字符后 ;: 配合 f &amp; t 使用，重复一次 ,: 配合 f &amp; t 使用，反向重复一次 上面的操作都可以配合 n 使用，比如在正常模式(下面会讲到)下输入 3h， 则光标向左移动 3 个字符。 27. 0: 移动到行首。 28. g0: 移到光标所在屏幕行行首。 29. ^: 移动到本行第一个非空白字符。 30. g^: 同 ^ ，但是移动到当前屏幕行第一个非空字符处。 31. $: 移动到行尾。 32. g$: 移动光标所在屏幕行行尾。 33. n|: 把光标移到递 n 列上。 34. nG: 到文件第 n 行。 35. :n 移动到第 n 行。 36. :$ 移动到最后一行。 37. H: 把光标移到屏幕最顶端一行。 38. M: 把光标移到屏幕中间一行。 39. L: 把光标移到屏幕最底端一行。 40. gg: 到文件头部。 41. G: 到文件尾部。 4.2 翻屏 ctrl+f: 下翻一屏。 ctrl+b: 上翻一屏。 ctrl+d: 下翻半屏。 ctrl+u: 上翻半屏。 ctrl+e: 向下滚动一行。 ctrl+y: 向上滚动一行。 n%: 到文件 n% 的位置。 zz: 将当前行移动到屏幕中央。 zt: 将当前行移动到屏幕顶端。 zb: 将当前行移动到屏幕底端。 4.3 标记 使用标记可以快速移动。到达标记后，可以用 Ctrl+o 返回原来的位置。Ctrl+o 和 Ctrl+i 很像浏览器上的 后退 和 前进 。 m{a-z}: 标记光标所在位置，局部标记，只用于当前文件。 m{A-Z}: 标记光标所在位置，全局标记。标记之后，退出 VIM， 重新启动，标记仍然有效。 `{a-z}: 移动到标记位置。 '{a-z}: 移动到标记行的行首。 `{0-9}：回到上 [2-10] 次关闭 VIM 时最后离开的位置。 ``: 移动到上次编辑的位置。'' 也可以，不过 `` 精确到列，而 '' 精确到行 。如果想跳转到更老的位置，可以按 C-o，跳转到更新的位置用 C-i。 `&quot;: 移动到上次离开的地方。 `.: 移动到最后改动的地方。 :marks 显示所有标记。 :delmarks a b -- 删除标记 a 和 b。 :delmarks a-c -- 删除标记 a、b 和 c。 :delmarks a c-f -- 删除标记 a、c、d、e、f。 :delmarks! -- 删除当前缓冲区的所有标记。 :help mark-motions 查看更多关于 mark 的知识。 5 插入文本 5.1 基本插入 i: 在光标前插入；一个小技巧：按 8，再按 i，进入插入模式，输入 =， 按 esc 进入命令模式，就会出现 8 个 =。 这在插入分割线时非常有用，如 30i+ 就插入了 36 个 + 组成的分割线。 I: 在当前行第一个非空字符前插入； gI: 在当前行第一列插入； a: 在光标后插入； A: 在当前行最后插入； o: 在下面新建一行插入； O: 在上面新建一行插入； :r filename 在当前位置插入另一个文件的内容。 :[n]r filename 在第 n 行插入另一个文件的内容。 :r !date 在光标处插入当前日期与时间。同理，:r !command 可以将其它 shell 命令的输出插入当前文档。 5.2 改写插入 c[n]w: 改写光标后 1(n) 个词。 c[n]l: 改写光标后 n 个字母。 c[n]h: 改写光标前 n 个字母。 [n]cc: 修改当前 [n] 行。 [n]s: 以输入的文本替代光标之后 1(n) 个字符，相当于 c[n]l。 [n]S: 删除指定数目的行，并以所输入文本代替之。 注意，类似 cnw，dnw，ynw 的形式同样可以写为 ncw，ndw，nyw。 6 剪切复制和寄存器 6.1 剪切和复制、粘贴 [n]x: 剪切光标右边 n 个字符，相当于 d[n]l。 [n]X: 剪切光标左边 n 个字符，相当于 d[n]h。 y: 复制在可视模式下选中的文本。 yy or Y: 复制整行文本。 y[n]w: 复制一(n)个词。 y[n]l: 复制光标右边1(n)个字符。 y[n]h: 复制光标左边1(n)个字符。 y$: 从光标当前位置复制到行尾。 y0: 从光标当前位置复制到行首。 :m,ny 复制 m 行到 n 行的内容。 y1G 或 ygg: 复制光标以上的所有行。 yG: 复制光标以下的所有行。 yaw 和 yas：复制一个词和复制一个句子，即使光标不在词首和句首也没关系。 d: 删除（剪切）在可视模式下选中的文本。 d$ or D: 删除（剪切）当前位置到行尾的内容。 d[n]w: 删除（剪切）1(n) 个单词。 d[n]l: 删除（剪切）光标右边 1(n) 个字符。 d[n]h: 删除（剪切）光标左边 1(n) 个字符。 d0: 删除（剪切）当前位置到行首的内容。 [n] dd: 删除（剪切）1(n) 行。 :m,nd 剪切 m 行到 n 行的内容。 d1G 或 dgg: 剪切光标以上的所有行。 dG: 剪切光标以下的所有行。 daw 和 das：剪切一个词和剪切一个句子，即使光标不在词首和句首也没关系。 d/f：这是一个比较高级的组合命令，它将删除当前位置 到下一个 f 之间的内容。 p: 在光标之后粘贴。 P: 在光标之前粘贴。 6.2 文本对象 aw：一个词 as：一句。 ap：一段。 ab：一块（包含在圆括号中的）。 y，d，c，v 都可以跟文本对象。 6.3 寄存器 a-z：都可以用作寄存器名。&quot;ayy 把当前行的内容放入 a 寄存器。 A-Z：用大写字母索引寄存器，可以在寄存器中追加内容。 如 &quot;Ayy 把当前行的内容追加到 a 寄存器中。 :reg 显示所有寄存器的内容。 &quot;&quot;：不加寄存器索引时，默认使用的寄存器。 &quot;*：当前选择缓冲区，&quot;*yy 把当前行的内容放入当前选择缓冲区。 &quot;+：系统剪贴板。&quot;+yy 把当前行的内容放入系统剪贴板。 7 查找与替换 7.1 查找 /something: 在后面的文本中查找 something。 ?something: 在前面的文本中查找 something。 /pattern/+number: 将光标停在包含 pattern 的行后面第 number 行上。 /pattern/-number: 将光标停在包含 pattern 的行前面第 number 行上。 n: 向后查找下一个。 N: 向前查找下一个。 可以用 grep 或 vimgrep 查找一个模式都在哪些地方出现过，其中: grep 是调用外部的 grep 程序，而 :vimgrep 是 VIM 自己的查找算法。用法为：:vim[grep]/pattern/[g] [j] files g 的含义是如果一个模式在一行中多次出现，则这一行也在结果中多次出现。j 的含义是 grep 结束后，结果停在第 j 项，默认是停在第一项。vimgrep 前面可以加数字限定搜索结果的上限，如 :1vim/pattern/ % 只查找那个模式在本文件中的第一个出现。 其实 vimgrep 在读纯文本电子书时特别有用，可以生成导航的目录。比如电子书中每一节的标题形式为：n. xxxx。你就可以这样：:vim/^d{1,}./ % 然后用 :cw 或 :copen 查看结果，可以用 C-w H 把 quickfix 窗口移到左侧，就更像个目录了。 7.2 替换 :s/old/new - 用 new 替换当前行第一个 old。 :s/old/new/g - 用 new 替换当前行所有的 old。 :n1,n2s/old/new/g - 用 new 替换文件 n1 行到 n2 行所有的 old。 :%s/old/new/g - 用 new 替换文件中所有的 old。 :%s/^/xxx/g - 在每一行的行首插入 xxx，^ 表示行首。 :%s/$/xxx/g - 在每一行的行尾插入 xxx，$ 表示行尾。 所有替换命令末尾加上 c，每个替换都将需要用户确认。 如：%s/old/new/gc，加上 i 则忽略大小写(ignore)。 还有一种比替换更灵活的方式，它是匹配到某个模式后执行某种命令， 语法为 :[range]g/pattern/command 例如 :%g/^ xyz/normal dd。 表示对于以一个空格和 xyz 开头的行执行 normal 模式下的 dd 命令。 关于 range 的规定为： 如果不指定 range，则表示当前行。 m,n: 从 m 行到 n 行。 0: 最开始一行（可能是这样）。 $: 最后一行 .: 当前行 %: 所有行 7.3 正则表达式 高级的查找替换就要用到正则表达式。 \\d: 表示十进制数（我猜的） \\s: 表示空格 \\S: 非空字符 \\a: 英文字母 \\|: 表示 或 \\.: 表示. {m,n}: 表示 m 到 n 个字符。这要和 \\s 与 \\a 等连用，如 \\a\\{m,n} 表示 m 到 n 个英文字母。 {m,}: 表示 m 到无限多个字符。 **: 当前目录下的所有子目录。 :help pattern 得到更多帮助。 8 排版 8.1 基本排版 &lt;&lt; 向左缩进一个 shiftwidth &gt;&gt; 向右缩进一个 shiftwidth :ce(nter) 本行文字居中 :le(ft) 本行文字靠左 :ri(ght) 本行文字靠右 gq 对选中的文字重排，即对过长的文字进行断行 gqq 重排当前行 gqnq 重排 n 行 gqap 重排当前段 gqnap 重排 n 段 gqnj 重排当前行和下面 n 行 gqQ 重排当前段对文章末尾 J 拼接当前行和下一行 gJ 同 J，不过合并后不留空格。 8.2 拼写检查 :set spell－开启拼写检查功能 :set nospell－关闭拼写检查功能 ]s－移到下一个拼写错误的单词 [s－作用与上一命令类似，但它是从相反方向进行搜索 z=－显示一个有关拼写错误单词的列表，可从中选择 zg－告诉拼写检查器该单词是拼写正确的 zw－与上一命令相反，告诉拼写检查器该单词是拼写错误的 8.3 统计字数 g ^g 可以统计文档字符数，行数。 将光标放在最后一个字符上，用字符数减去行数可以粗略统计中文文档的字数。 以上对 Mac 或 Unix 的文件格式适用。 如果是 Windows 文件格式（即换行符有两个字节），字数的统计方法为：字符数 - 行数 * 2。 9 编辑多个文件 9.1 一次编辑多个文件 我们可以一次打开多个文件，如 vi a.txt b.txt c.txt 使用 :next(:n) 编辑下一个文件。 :2n 编辑下 2 个文件。 使用 :previous 或 :N 编辑上一个文件。 使用 :wnext，保存当前文件，并编辑下一个文件。 使用 :wprevious，保存当前文件，并编辑上一个文件。 使用 :args 显示文件列表。 :n filenames 或 :args filenames 指定新的文件列表。 vi -o filenames 在水平分割的多个窗口中编辑多个文件。 vi -O filenames 在垂直分割的多个窗口中编辑多个文件。 9.2 多标签编辑 vim -p files: 打开多个文件，每个文件占用一个标签页。 :tabe, tabnew -- 如果加文件名，就在新的标签中打开这个文件， 否则打开一个空缓冲区。 ^w gf -- 在新的标签页里打开光标下路径指定的文件。 :tabn -- 切换到下一个标签。Control + PageDown，也可以。 :tabp -- 切换到上一个标签。Control + PageUp，也可以。 [n] gt -- 切换到下一个标签。如果前面加了 n ， 就切换到第 n 个标签。第一个标签的序号就是 1。 :tab split -- 将当前缓冲区的内容在新页签中打开。 :tabc[lose] -- 关闭当前的标签页。 :tabo[nly] -- 关闭其它的标签页。 :tabs -- 列出所有的标签页和它们包含的窗口。 :tabm[ove] [N] -- 移动标签页，移动到第 N 个标签页之后。 如 tabm 0 当前标签页，就会变成第一个标签页。 9.3 缓冲区 :buffers 或 :ls 或 :files 显示缓冲区列表。 ctrl+^：在最近两个缓冲区间切换。 :bn -- 下一个缓冲区。 :bp -- 上一个缓冲区。 :bl -- 最后一个缓冲区。 :b[n] 或 :[n]b -- 切换到第 n 个缓冲区。 :nbw(ipeout) -- 彻底删除第 n 个缓冲区。 :nbd(elete) -- 删除第 n 个缓冲区，并未真正删除，还在 unlisted 列表中。 :ba[ll] -- 把所有的缓冲区在当前页中打开，每个缓冲区占一个窗口。 10 分屏编辑 vim -o file1 file2: 水平分割窗口，同时打开 file1 和 file2 vim -O file1 file2: 垂直分割窗口，同时打开 file1 和 file2 10.1 水平分割 :split(:sp) -- 把当前窗水平分割成两个窗口。(CTRL-W s 或 CTRL-W CTRL-S) 注意如果在终端下，CTRL-S 可能会冻结终端，请按 CTRL-Q 继续。 :split filename -- 水平分割窗口，并在新窗口中显示另一个文件。 :nsplit(:nsp) -- 水平分割出一个 n 行高的窗口 :[N]new -- 水平分割出一个 N 行高的窗口，并编辑一个新文件。 (CTRL-W n 或 CTRL-W CTRL-N) ctrl+w f --水平分割出一个窗口，并在新窗口打开名称为光标所在词的文件 。 C-w C-^ -- 水平分割一个窗口，打开刚才编辑的文件。 10.2 垂直分割 :vsplit(:vsp) -- 把当前窗口分割成水平分布的两个窗口。 (CTRL-W v 或 CTRL CTRL-V) :[N]vne[w] -- 垂直分割出一个新窗口。 :vertical 水平分割的命令： 相应的垂直分割。 10.3 关闭子窗口 :qall -- 关闭所有窗口，退出 VIM。 :wall -- 保存所有修改过的窗口。 :only -- 只保留当前窗口，关闭其它窗口。(CTRL-W o) :close -- 关闭当前窗口，CTRL-W c 能实现同样的功能。 (象 :q :x 同样工作 ) 10.4 调整窗口大小 ctrl+w + --当前窗口增高一行。也可以用 n 增高 n 行。 ctrl+w - --当前窗口减小一行。也可以用 n 减小 n 行。 ctrl+w _ --当前窗口扩展到尽可能的大。也可以用 n 设定行数。 :resize n -- 当前窗口 n 行高。 ctrl+w = -- 所有窗口同样高度。 n ctrl+w _ -- 当前窗口的高度设定为 n 行。 ctrl+w &lt; --当前窗口减少一列。也可以用 n 减少 n 列。 ctrl+w &gt; --当前窗口增宽一列。也可以用 n 增宽 n 列。 ctrl+w | --当前窗口尽可能的宽。也可以用 n 设定列数。 10.5 切换和移动窗口 如果支持鼠标，切换和调整子窗口的大小就简单了。 ctrl+w ctrl+w: 切换到下一个窗口。或者是 ctrl+w w。 ctrl+w p: 切换到前一个窗口。 ctrl+w h(l,j,k):切换到左（右，下，上）的窗口。 ctrl+w t(b):切换到最上（下）面的窗口。 ctrl+w H(L,K,J): 将当前窗口移动到最左（右、上、下）面。 ctrl+w r：旋转窗口的位置。 ctrl+w T: 将当前的窗口移动到新的标签页上。 11 快速编辑 11.1 改变大小写 ~: 反转光标所在字符的大小写。 可视模式下的 U 或 u：把选中的文本变为大写或小写。 gu(U) 接范围（如 $，或 G），可以把从光标当前位置到指定位置之间字母全部转换成小写或大写。如 ggguG，就是把开头到最后一行之间的字母全部变为小 写。再如 gu5j，把当前行和下面四行全部变成小写。 11.2 替换（normal模式） r: 替换光标处的字符，同样支持汉字。 R: 进入替换模式，按 esc 回到正常模式。 11.3 撤消与重做（normal模式） [n] u: 取消一 (n) 个改动。 :undo 5 -- 撤销 5 个改变。 :undolist -- 你的撤销历史。 ctrl + r: 重做最后的改动。 U: 取消当前行中所有的改动。 :earlier 4m -- 回到 4 分钟前 :later 55s -- 前进 55 秒 11.4 宏 . --重复上一个编辑动作 qa：开始录制宏 a（键盘操作记录） q：停止录制 @a：播放宏 a 12 编辑特殊文件 12.1 文件加解密 vim -x file: 开始编辑一个加密的文件。 :X -- 为当前文件设置密码。 :set key= -- 去除文件的密码。 12.2 文件的编码 :e ++enc=utf8 filename, 让 VIM 用 utf-8 的编码打开这个文件。 :w ++enc=gbk，不管当前文件什么编码，把它转存成 gbk 编码。 :set fenc 或 :set fileencoding，查看当前文件的编码。 在 vimrc 中添加 set fileencoding=ucs-bom,utf-8,cp936，VIM 会根据要打开的文件选择合适的编码。 注意：编码之间不要留空格。cp936 对应于 gbk 编码。ucs-bom 对应于 Windows 下的文件格式。 让 VIM 正确处理文件格式和文件编码，有赖于 ~/.vimrc 的正确配置 12.3 文件格式 大致有三种文件格式：unix，dos，mac。 三种格式的区别主要在于回车键的编码：dos 下是回车加换行，unix 下只有换行符，mac 下只有回车符。 :e ++ff=dos filename, 让 VIM 用 dos 格式打开这个文件。 :w ++ff=mac filename, 以 mac 格式存储这个文件。 :set ff，显示当前文件的格式。 在 vimrc 中添加 set fileformats=unix,dos,mac，让 VIM 自动识别文件格式。 13 编程辅助 13.1 一些按键 gd: 跳转到局部变量的定义处； gD: 跳转到全局变量的定义处，从当前文件开头开始搜索； g;: 上一个修改过的地方； g,: 下一个修改过的地方； [[: 跳转到上一个函数块开始，需要有单独一行的 {。 ]]: 跳转到下一个函数块开始，需要有单独一行的 {。 []: 跳转到上一个函数块结束，需要有单独一行的 }。 ][: 跳转到下一个函数块结束，需要有单独一行的 }。 [{: 跳转到当前块开始处； ]}: 跳转到当前块结束处； [/: 跳转到当前注释块开始处； ]/: 跳转到当前注释块结束处； %: 不仅能移动到匹配的 (),{} 或 [] 上，而且能在 #if，#else，#endif 之间跳跃。 下面的括号匹配对编程很实用的。 ci', di', yi'：修改、剪切或复制 ' 之间的内容。 ca', da', ya'：修改、剪切或复制'之间的内容，包含 '。 ci&quot;, di&quot;, yi&quot;：修改、剪切或复制 &quot; 之间的内容。 ca&quot;, da&quot;, ya&quot;：修改、剪切或复制 &quot; 之间的内容，包含 &quot;。 ci(, di(, yi(：修改、剪切或复制 () 之间的内容。 ca(, da(, ya(：修改、剪切或复制 () 之间的内容，包含()。 ci[, di[, yi[：修改、剪切或复制 [] 之间的内容。 ca[, da[, ya[：修改、剪切或复制 [] 之间的内容，包含 []。 ci{, di{, yi{：修改、剪切或复制 {} 之间的内容。 ca{, da{, ya{：修改、剪切或复制 {} 之间的内容，包含 {}。 ci&lt;, di&lt;, yi&lt;：修改、剪切或复制 &lt;&gt; 之间的内容。 ca&lt;, da&lt;, ya&lt;：修改、剪切或复制 &lt;&gt; 之间的内容，包含 &lt;&gt;。 13.2 ctags ctags -R: 生成 tag 文件，-R 表示也为子目录中的文件生成 tags。 :set tags=path/tags -- 告诉 ctags 使用哪个 tag 文件。 :tag xyz -- 跳到 xyz 的定义处，或者将光标放在 xyz 上按 C-]，返回用 C-t :stag xyz -- 用分割的窗口显示 xyz 的定义，或者 C-w ]， 如果用 C-w n ]，就会打开一个 n 行高的窗口 :ptag xyz -- 在预览窗口中打开 xyz 的定义，热键是 C-w }。 :pclose -- 关闭预览窗口。热键是 C-w z。 :pedit abc.h -- 在预览窗口中编辑 abc.h :psearch abc -- 搜索当前文件和当前文件 include 的文件，显示包含 abc 的行。 有时一个 tag 可能有多个匹配，如函数重载，一个函数名就会有多个匹配。 这种情况会先跳转到第一个匹配处。 :[n]tnext -- 下一 [n] 个匹配。 :[n]tprev -- 上一 [n] 个匹配。 :tfirst -- 第一个匹配 :tlast -- 最后一个匹配 :tselect tagname -- 打开选择列表 tab 键补齐 :tag xyz-- 补齐以 xyz 开头的 tag 名，继续按 tab 键，会显示其他的。 :tag /xyz-- 会用名字中含有 xyz 的 tag 名补全。 13.3 cscope cscope -Rbq: 生成 cscope.out 文件 :cs add /path/to/cscope.out /your/work/dir :cs find c func -- 查找 func 在哪些地方被调用 :cw -- 打开 quickfix 窗口查看结果 13.4 gtags Gtags 综合了 ctags 和 cscope 的功能。 使用 Gtags 之前，你需要安装 GNU Gtags。 然后在工程目录运行 gtags 。 :Gtags funcname 定位到 funcname 的定义处。 :Gtags -r funcname 查询 funcname 被引用的地方。 :Gtags -s symbol 定位 symbol 出现的地方。 :Gtags -g string Goto string 出现的地方。:Gtags -gi string 忽略大小写。 :Gtags -f filename 显示 filename 中的函数列表。 你可以用 :Gtags -f % 显示当前文件。 :Gtags -P pattern 显示路径中包含特定模式的文件。 如 :Gtags -P .h$ 显示所有头文件，:Gtags -P /vm/ 显示 vm 目录下的文件。 13.5 编译 VIM 提供了 :make 来编译程序，默认调用的是 make， 如果你当前目录下有 makefile，简单地 :make 即可。 如果你没有 make 程序，你可以通过配置 makeprg 选项来更改 make 调用的程序。 如果你只有一个abc.java文件，你可以这样设置： set makeprg=javac\\ abc.java 然后 :make 即可。如果程序有错，可以通过 quickfix 窗口查看错误。 不过如果要正确定位错误，需要设置好 errorformat，让 VIM 识别错误信息。 如： :setl efm=%A%f:%l:\\ %m,%-Z%p^,%-C%.%# %f 表示文件名，%l 表示行号，%m 表示错误信息，其它的还不能理解。 请参考 :help errorformat。 13.6 快速修改窗口 其实是 quickfix 插件提供的功能， 对编译调试程序非常有用 😃 :copen -- 打开快速修改窗口。 :cclose -- 关闭快速修改窗口。 快速修改窗口在 make 程序时非常有用，当 make 之后： :cl -- 在快速修改窗口中列出错误。 :cn -- 定位到下一个错误。 :cp -- 定位到上一个错误。 :cr -- 定位到第一个错误。 13.7 自动补全 C-x C-s -- 拼写建议。 C-x C-v -- 补全 VIM 选项和命令。 C-x C-l -- 整行补全。 C-x C-f -- 自动补全文件路径。弹出菜单后，按 C-f 循环选择，当然也可以按 C-n 和 C-p。 C-x C-p 和 C-x C-n -- 用文档中出现过的单词补全当前的词。 直接按 C-p 和 C-n 也可以。 C-x C-o -- 编程时可以补全关键字和函数名啊。 C-x C-i -- 根据头文件内关键字补全。 C-x C-d -- 补全宏定义。 C-x C-n -- 按缓冲区中出现过的关键字补全。 直接按 C-n 或 C-p 即可。 当弹出补全菜单后： C-p 向前切换成员； C-n 向后切换成员； C-e 退出下拉菜单，并退回到原来录入的文字； C-y 退出下拉菜单，并接受当前选项。 13.8 多行缩进缩出 正常模式下，按两下 &gt;; 光标所在行会缩进。 如果先按了 n，再按两下 &gt;;，光标以下的 n 行会缩进。 对应的，按两下 &lt;;，光标所在行会缩出。 如果在编辑代码文件，可以用 = 进行调整。 在可视模式下，选择要调整的代码块，按 =，代码会按书写规则缩排好。 或者 n =，调整 n 行代码的缩排。 13.9 折叠 zf -- 创建折叠的命令，可以在一个可视区域上使用该命令； zd -- 删除当前行的折叠； zD -- 删除当前行的折叠； zfap -- 折叠光标所在的段； zo -- 打开折叠的文本； zc -- 收起折叠； za -- 打开/关闭当前折叠； zr -- 打开嵌套的折行； zm -- 收起嵌套的折行； zR (zO) -- 打开所有折行； zM (zC) -- 收起所有折行； zj -- 跳到下一个折叠处； zk -- 跳到上一个折叠处； zi -- enable/disable fold; 14 命令行 normal 模式下按:进入命令行模式 14.1 命令行模式下的快捷键： 上下方向键：上一条或者下一条命令。如果已经输入了部分命令，则找上一 条或者下一条匹配的命令。 左右方向键：左 / 右移一个字符。 C-w： 向前删除一个单词。 C-h： 向前删除一个字符，等同于 Backspace。 C-u： 从当前位置移动到命令行开头。 C-b： 移动到命令行开头。 C-e： 移动到命令行末尾。 Shift-Left：左移一个单词。 Shift-Right：右移一个单词。 @： 重复上一次的冒号命令。 q： 正常模式下，q 然后按 ':'，打开命令行历史缓冲区， 可以像编辑文件一样编辑命令。 q/ 和 q? 可以打开查找历史记录。 14.2 执行外部命令 :! cmd 执行外部命令。 :!! 执行上一次的外部命令。 :sh 调用 shell，用 exit 返回 VIM。 :r !cmd 将命令的返回结果插入文件当前位置。 :m,nw !cmd 将文件的 m 行到 n 行之间的内容做为命令输入执行命令。 15 其它 15.1 工作目录 :pwd 显示 VIM 的工作目录。 :cd path 改变 VIM 的工作目录。 :set autochdir 可以让 VIM 根据编辑的文件自动切换工作目录。 15.2 一些快捷键（收集中） K: 打开光标所在词的 manpage。 *: 向下搜索光标所在词。 g*: 同上，但部分符合即可。 #: 向上搜索光标所在词。 g#: 同上，但部分符合即可。 g C-g: 统计全文或统计部分的字数。 15.3 在线帮助 :h(elp) 或 F1 打开总的帮助。 :help user-manual 打开用户手册。 命令帮助的格式为：第一行指明怎么使用那个命令； 然后是缩进的一段解释这个命令的作用，然后是进一步的信息。 :helptags somepath 为 somepath 中的文档生成索引。 :helpgrep 可以搜索整个帮助文档，匹配的列表显示在 quickfix 窗口中。 Ctrl+] 跳转到 tag 主题，Ctrl+t 跳回。 :ver 显示版本信息。 15.4 一些小功能 简单计算器: 在插入模式下，输入 C-r =，然后输入表达式，就能在 光标处得到计算结果。 ","link":"https://faded.auspicious.space/post/vim-common-commands/"},{"title":"Vim——配置入门","content":" Vim 配置入门 Vim 的配置不太容易，它有自己的语法，许许多多的命令。我总是记不清楚，所以就整理了下面这篇文章，列出主要配置项的含义。 1 基础知识 Vim 的全局配置一般在 /etc/vim/vimrc 或者 /etc/vimrc，对所有用户生效。用户个人的配置在 ~/.vimrc。 如果只对单次编辑启用某个配置项，可以在命令模式下，先输入一个冒号，再输入配置。举例来说，set number 这个配置可以写在 .vimrc 里面，也可以在命令模式输入。 :set number 配置项一般都有“打开”和“关闭”两个设置。“关闭”就是在“打开”前面加上前缀 &quot;no&quot;。 &quot; 打开 set number &quot; 关闭 set nonumber 上面代码中，双引号开始的行表示注释。 查询某个配置项是打开还是关闭，可以在命令模式下，输入该配置，并在后面加上问号。 :set number? 上面的命令会返回 number 或者 nonumber。 如果想查看帮助，可以使用 help 命令。 :help number 2 基本配置 （1） set nocompatible 不与 Vi 兼容（采用 Vim 自己的操作命令）。 （2） syntax on 打开语法高亮。自动识别代码，使用多种颜色显示。 （3） set showmode 在底部显示，当前处于命令模式还是插入模式。 （4） set showcmd 命令模式下，在底部显示，当前键入的指令。比如，键入的指令是 2y3d，那么底部就会显示 2y3，当键入 d 的时候，操作完成，显示消失。 （5） set mouse=a 支持使用鼠标。 （6） set encoding=utf-8 使用 utf-8 编码。 （7） set t_Co=256 启用256色。 （8） filetype indent on 开启文件类型检查，并且载入与该类型对应的缩进规则。比如，如果编辑的是 .py 文件，Vim 就是会找 Python 的缩进规则 ~/.vim/indent/python.vim。 3 缩进 9） set autoindent 按下回车键后，下一行的缩进会自动跟上一行的缩进保持一致。 （10） set tabstop=2 按下 Tab 键时，Vim 显示的空格数。 （11） set shiftwidth=4 在文本上按下 &gt;&gt;（增加一级缩进）、&lt;&lt;（取消一级缩进）或者 ==（取消全部缩进）时，每一级的字符数。 （12） set expandtab 由于 Tab 键在不同的编辑器缩进不一致，该设置自动将 Tab 转为空格。 （13） set softtabstop=2 Tab 转为多少个空格。 4 外观 14） set number 显示行号 （15） set relativenumber 显示光标所在的当前行的行号，其他行都为相对于该行的相对行号。 （16） set cursorline 光标所在的当前行高亮。 （17） set textwidth=80 设置行宽，即一行显示多少个字符。 （18） set wrap 自动折行，即太长的行分成几行显示。 set nowrap 关闭自动折行 （19） set linebreak 只有遇到指定的符号（比如空格、连词号和其他标点符号），才发生折行。也就是说，不会在单词内部折行。 （20） set wrapmargin=2 指定折行处与编辑窗口的右边缘之间空出的字符数。 （21） set scrolloff=5 垂直滚动时，光标距离顶部/底部的位置（单位：行）。 （22） set sidescrolloff=15 水平滚动时，光标距离行首或行尾的位置（单位：字符）。该配置在不折行时比较有用。 （23） set laststatus=2 是否显示状态栏。0 表示不显示，1 表示只在多窗口时显示，2 表示显示。 （24） set ruler 在状态栏显示光标的当前位置（位于哪一行哪一列）。 5 搜索 25） set showmatch 光标遇到圆括号、方括号、大括号时，自动高亮对应的另一个圆括号、方括号和大括号。 （26） set hlsearch 搜索时，高亮显示匹配结果。 （27） set incsearch 输入搜索模式时，每输入一个字符，就自动跳到第一个匹配的结果。 （28） set ignorecase 搜索时忽略大小写。 （29） set smartcase 如果同时打开了 ignorecase，那么对于只有一个大写字母的搜索词，将大小写敏感；其他情况都是大小写不敏感。比如，搜索 Test 时，将不匹配 test；搜索 test 时，将匹配 Test。 6 编辑 30） set spell spelllang=en_us 打开英语单词的拼写检查。 （31） set nobackup 不创建备份文件。默认情况下，文件保存时，会额外创建一个备份文件，它的文件名是在原文件名的末尾，再添加一个波浪号（〜）。 （32） set noswapfile 不创建交换文件。交换文件主要用于系统崩溃时恢复文件，文件名的开头是 .、结尾是 .swp。 （33） set undofile 保留撤销历史。 Vim 会在编辑时保存操作历史，用来供用户撤消更改。默认情况下，操作记录只在本次编辑时有效，一旦编辑结束、文件关闭，操作历史就消失了。 打开这个设置，可以在文件关闭后，操作记录保留在一个文件里面，继续存在。这意味着，重新打开一个文件，可以撤销上一次编辑时的操作。撤消文件是跟原文件保存在一起的隐藏文件，文件名以 .un~ 开头。 （34） set backupdir=~/.vim/.backup// set directory=~/.vim/.swp// set undodir=~/.vim/.undo// 设置备份文件、交换文件、操作历史文件的保存位置。 结尾的 //表示生成的文件名带有绝对路径，路径中用 % 替换目录分隔符，这样可以防止文件重名。 （35） set autochdir 自动切换工作目录。这主要用在一个 Vim 会话之中打开多个文件的情况，默认的工作目录是打开的第一个文件的目录。该配置可以将工作目录自动切换到，正在编辑的文件的目录。 （36） set noerrorbells 出错时，不要发出响声。 （37） set visualbell 出错时，发出视觉提示，通常是屏幕闪烁。 （38） set history=1000 Vim 需要记住多少次历史操作。 （39） set autoread 打开文件监视。如果在编辑过程中文件发生外部改变（比如被别的编辑器编辑了），就会发出提示。 （40） set listchars=tab:»■,trail:■ set list 如果行尾有多余的空格（包括 Tab 键），该配置将让这些空格显示成可见的小方块。 （41） set wildmenu set wildmode=longest:list,full 命令模式下，底部操作指令按下 Tab 键自动补全。第一次按下 Tab，会显示所有匹配的操作指令的清单；第二次按下 Tab，会依次选择各个指令。 ","link":"https://faded.auspicious.space/post/vim-configuration-introduction/"},{"title":"Vim——重要的命令","content":" 一些不起眼但非常有用的 Vim 命令 1 保存文件并退出 :x :wq 都是保存当前文件并退出。这两个命令实际上并不完全等价，当文件被修改时两个命令时相同的。但如果未被修改，使用 :x 不会更改文件的修改时间，而使用 :wq 会改变文件的修改时间。 2 基本计算器 在插入模式下，你可以使用 Ctrl+r 键然后输入 =，再输入一个简单的算式。按 Enter 键，计算结果就会插入到文件中。例如，尝试输入： Ctrl+r '=2+2' ENTER 然后计算结果“4 ”会被插入到文件中。 3 查找重复的连续的单词 当你很快地打字时，很有可能会连续输入同一个单词两次，就像 this this。这种错误可能骗过任何一个人，即使是你自己重新阅读一遍也不可避免。幸运的是，有一个简单的正则表达式可以用来预防这个错误。使用搜索命令（默认时 /）然后输入： \\(\\&lt;\\w\\+\\&gt;\\)\\_s*\\1 这会显示所有重复的单词。要达到最好的效果，不要忘记把下面的命令： set hlsearch 放到你的 .vimrc 文件中高亮所有的匹配。 4 缩写 一个很可能是最令人印象深刻的窍门是你可以在 Vim 中定义缩写，它可以实时地把你输入的东西替换为另外的东西。语法格式如下： :ab [缩写] [要替换的文字] 一个通用的例子是： :ab asap as soon as possible 会把你输入的 “asap” 替换为 “as soon as possible”。 5 在你忘记用 root 方式打开文件时的文件保存 这可能是一个在论坛中一直受欢迎的命令。每当你打开一个你没有写入权限的文件（比如系统配置文件）并做了一些修改，Vim 无法通过普通的 :w 命令来保存。 你不需要重新以 root 方式打开文件再进行修改，只需要运行： :w !sudo tee % 这会直接以 root 方式保存。 6 实时加密文本 如果你不想让别人看懂你的屏幕上的内容，你可以使用一个内置的选项，通过下面的命令使用 ROT13 来对文本进行编码： ggVGg? gg 把光标移动到 Vim 缓冲区的第一行，V 进入可视模式，G 把光标移动到缓冲区的最后一行。因此，ggVG 使可视模式覆盖这个当前缓冲区。最后 g? 使用 ROT13 对整个区域进行编码。 注意它可以被映射到一个最常使用的键。它对字母符号也可以很好地工作。要对它进行撤销，最好的方法就是使用撤销命令：u。 7 自动补全 这是另外一个令我感到惭愧的功能，但我发现周围很多人并不知道。Vim 默认有自动补全的功能。的确这个功能是很基本的，并且可以通过插件来增强，但它也很有帮助。方法很简单。Vim 尝试通过已经输入的单词来预测单词的结尾。比如当你在同一个文件中第二次输入 “compiler” 时，仅仅输入 “com” 然后保持在插入模式，按 Ctrl+n 键就可以看到 Vim 为你补全了单词。很简单，但也很有用。 8 比较两个文件的不同 你们中的大多数很可能都知道 vimdiff 命令，它可以使用分离模式打开 Vim 并比较两个文件的不同。语法如下： $ vimdiff [文件1] [文件2] 但同样的结果也可以通过下面的 Vim 命令来获得： :diffthis 首先在 Vim 中打开原始文件。然后使用分离模式带来第二个文件： :vsp [文件2] 最后在第一个缓冲区里输入： :diffthis 通过 Ctrl+w 来切换缓冲区并再次输入： :diffthis 这样两个文件中不同的部分就会被高亮。 （译者注：可以直接在一个缓冲区里使用命令 :windo diffthis，而不用输入 :diffthis 两次） 要停止比较，使用： :diffoff 9 按时间回退文件 Vim 会记录文件的更改，你很容易可以回退到之前某个时间。该命令是相当直观的。比如： :earlier 1m 会把文件回退到 1 分钟以前的状态。 注意，你可以使用下面的命令进行相反的转换： :later 10 删除标记内部的文字 当我开始使用 Vim 时，一件我总是想很方便做的事情是如何轻松的删除方括号或圆括号里的内容。转到开始的标记，然后使用下面的语法： di[标记] 比如，把光标放在开始的圆括号上，使用下面的命令来删除圆括号内的文字： di( 如果是方括号或者是引号，则使用： di{ 和： di&quot; 11 删除指定标记前的内容 和删除标记内部有些相似，但目的不同。命令如下： dt[标记] 会删除所有光标和标记之间的内容（保持标记不动），如果在同一行有这个标记的话。例如 dt. 会删除至句子的末尾，但保持 . 不动。 12 把 Vim 变为十六进制编辑器 这不是我最喜欢的窍门，但有时会很有趣。你可以把 Vim 和 xxd 功能连起来来把文件转换为十六进制模式。命令如下：:%!xxd 类似的，你可以通过下面的命令恢复原来的状态： :%!xxd -r 13 把光标下的文字置于屏幕中央 我们所要做的事情如标题所示。如果你想强制滚动屏幕来把光标下的文字置于屏幕的中央，在可视模式中使用命令（译者注：在普通模式中也可以）： zz 14 跳到上一个／下一个位置 当你编辑一个很大的文件时，经常要做的事是在某处进行修改，然后跳到另外一处。如果你想跳回之前修改的地方，使用命令： Ctrl+o 来回到之前修改的地方；类似的： Ctrl+i 会回退上面的跳动。 15 把当前文件转化为网页 这会生成一个 HTML 文件来显示文本，并在分开的窗口显示源代码： :%TOhtml ","link":"https://faded.auspicious.space/post/vim-key-instruct/"},{"title":"Vim——光标移动篇","content":" Vim常用文档动作命令总结 1 基本方向移动 h ： 向左移动一列 l ： 向右移动一列 j ： 向下移动一个实际行 k ： 向上移动一个实际行 所谓列可能指一个字节，也可能是一个字符，根据文件内容决定。 实际行指的是文本截止到一个换行符为止称为一个实际行。有时因为文本太长，一个实际行在窗口中会显示成好几行。可以通过 :set number 命令查看实际的行数。 2 基于单词的移动 Vim有一组基于单词的正向和反向移动的命令。 w ： 正向移动到下一单词的开头 e ： 正向移动到当前/下一单词的结尾 b ： 反向移动到当前/上一单词的开头 ge ： 反向移动到上一单词的结尾 基于单词的移动命令可以和其他命令结合使用。例如 :ea 可以跳转到单词的结尾并进入插入模式。 3 基于查找的移动 f 命令是最常用的查找命令，用于当前行进行指定字符的查找。如果找到则光标移动到目标字符，否则不移动。 Vim 会记录上一次执行的查找命令，再次查找时可以使用 ; 命令来完成相同查找。如果查询跳过头了，可以使用 , 命令返回光标之前的位置。 查询不止 f 命令，其他命令总结如下 f{char} : 正向移动到下一个{char}所在位置 F{char} : 反向移动到上一个{char}所在位置 t{char} : 正向移动到下一个{char}的前一个字符上 T{char} : 反向移动到上一个{char}的后一个字符上 除了上述查询方式， / 也是一种常用的查询方式，基于字符串的查询，/{str} 可以高亮目标字符串。可以通过 n 命令跳到下一个匹配处， N 返回前一匹配处。 同样的， / 也可以和其他命令结合使用，例如选择文本。点击 v 进入可视模式，然后输入 /{str} 也有例如 d/{str} 删除光标到目标字符串之间的所有内容的操作方式。 4 精确的文本对象选择 这个是一个很NB的功能，完全颠覆了对文本编辑器的认知。 现在有一个 JS 文件，内容如图： 这里认识 a 和 i 两个命令，不是普通的插入命令，需要和 v 命令配合使用，选中匹配的文本对象。例如在当前光标所在处输入 vi} 会达到以下效果。 如果光标的位置在 href 上呢？相同命令下： 如果换做是 a 命令呢？ i 命令可以理解为 inside，即选中匹配符号之间不包含匹配符号的内容。而 a 则选中包含匹配项的内容。 常见分隔符总结： 'a)' 或 'ab' : 一对() 'a}' 或 'aB' : 一对{} a] : 一对[] a&gt; : 一对&lt;&gt; a' : 一对'' a&quot; : 一对&quot;&quot; a` : 一对`` at : 一对xml标签 i 与 a 对应，只不过是针对分隔符内部的内容而已。 5 删除周边、修改内部 Vim 除了可以根据分隔符操作，也可以操作文本块，如单词，句子，段落等。 常见文本范围： iw : 当前单词 aw : 当前单词及一个空格 iW : 当前字符串 aW : 当前字符串及一个空格 is : 当前句子 as : 当前句子及一个空格 ip : 当前段落 ap : 当前段落及一个空行 上面的范围命令可以和 v 、 c 等操作一起使用。 6 快速回跳 这些命令用的相对少一些，常用一些的有 `` : 当前文件上次跳转操作的位置 `. : 上次修改操作的地方 `^ : 上次插入的地方 `[ : 上次修改或复制的起始位置 `] : 上次修改或复制的结束位置 `&lt; : 上次高亮选区的起始位置 `&gt; : 上次高亮选区的结束位置 7 匹配括号间跳转 Vim的 % 命令允许光标在一对闭括号间跳转。例如当前光标在 [ 上， % 命令可以跳转到对应的 ] 上，反过来也一样ok。例如将一对 {} 修改为一对 []。 当前光标在 { 上，输入 % 命令 替换当前光标下的字符，通过 r] 将 } 替换为 ]。 输入 `` 命令，跳转回上次跳转的位置。 之后再通过 'r[' 将 '{' 转为 '['。 ","link":"https://faded.auspicious.space/post/vim-cursor-movement/"},{"title":"Bash 移动光标快捷键","content":" 快捷键 行为 ctrl+a 移动到行首 ctrl+e 移动到行尾 ctrl+f 向右移动一个字符 ctrl+b 向左移动一个字符 alt+f 向右移动一个单词 alt+b 向左移动一个单词 ","link":"https://faded.auspicious.space/post/bash-key-for-cursor-movement/"},{"title":"Vim——CheatSheet","content":" Vim速查表-帮你提高N倍效率 进入 vim 命令 描述 vim filename 打开或新建文件,并将光标置于第一行首 vim +n filename 打开文件，并将光标置于第n行首 vim + filename 打开文件，并将光标置于最后一行首 vim +/pattern filename 打开文件，并将光标置于第一个与pattern匹配的串处 vim -r filename 在上次正用vim编辑时发生系统崩溃，恢复filename vim filename….filename 打开多个文件，依次编辑 vim 配置 命令 描述 all 列出所有选项设置情况 term 设置终端类型 ignorance 在搜索中忽略大小写 list 显示制表位(Ctrl+I)和行尾标志（$) number 显示行号 report 显示由面向行的命令修改过的数目 terse 显示简短的警告信息 warn 在转到别的文件时若没保存当前文件则显示NO write信息 nomagic 允许在搜索模式中，使用前面不带“\\”的特殊字符 nowrapscan 禁止vi在搜索到达文件两端时，又从另一端开始 mesg 允许vi显示其他用户用write写到自己终端上的信息 :set number / set nonumber 显示/不显示行号 :set ruler /set noruler 显示/不显示标尺 :set hlsearch 高亮显示查找到的单词 :set nohlsearch 关闭高亮显示 :syntax on 语法高亮 :set nu 显示行号 :set tabstop=8 设置tab大小,8为最常用最普遍的设置 :set softtabstop=8 4:4个空格,8:正常的制表符,12:一个制表符4个空格,16:两个制表符 :set autoindent 自动缩进 :set cindent C语言格式里面的自动缩进 移动光标 命令 描述 k nk 上 向上移动n行 j nj 下 向下移动n行 h nh 左 向左移动n行 l nl 右 向右移动n行 Space 光标右移一个字符 Backspace 光标左移一个字符 Enter 光标下移一行 w/W 光标右移一个字至字首 b/B 光标左移一个字至字首 e或E 光标右移一个字至字尾 ) 光标移至句尾 ( 光标移至句首 } 光标移至段落开头 { 光标移至段落结尾 n$ 光标移至第n行尾 H 光标移至屏幕顶行 M 光标移至屏幕中间行 L 光标移至屏幕最后行 0 （注意是数字零）光标移至当前行首 ^ 移动光标到行首第一个非空字符上去 $ 光标移至当前行尾 gg 移到第一行 G 移到最后一行 f 移动光标到当前行的字符a上 F 相反 % 移动到与制匹配的括号上去（），{}，[]，&lt;&gt;等 nG 移动到第n行上 G 到最后一行 屏幕滚动 命令 描述 Ctrl+u 向文件首翻半屏 Ctrl+d 向文件尾翻半屏 Ctrl+f 向文件尾翻一屏 Ctrl＋b 向文件首翻一屏 nz 将第n行滚至屏幕顶部，不指定n时将当前行滚至屏幕顶部 插入文本类 命令 描述 i 在光标前 I 在当前行首 a 光标后 A 在当前行尾 o 在当前行之下新开一行 O 在当前行之上新开一行 r 替换当前字符 R 替换当前字符及其后的字符，直至按ESC键 s 从当前光标位置处开始，以输入的文本替代指定数目的字符 S 删除指定数目的行，并以所输入文本代替之 ncw/nCW 修改指定数目的字 nCC 修改指定数目的行 删除命令 命令 描述 x/X 删除一个字符，x删除光标后的，而X删除光标前的 dw 删除一个单词(删除光标位置到下一个单词开始的位置) dnw 删除n个单词 dne 也可，只是删除到单词尾 do 删至行首 d$ 删至行尾 dd 删除一行 ndd 删除当前行及其后n-1行 dnl 向右删除n个字母 dnh 向左删除n个字母 dnj 向下删除n行,当前行+其上n行 dnk 向上删除n行,当期行+其下n行 cnw[word] 将n个word改变为word C$ 改变到行尾 cc 改变整行 shift+j 删除行尾的换行符，下一行接上来了 复制粘贴 命令 描述 p 粘贴用x或d删除的文本 ynw 复制n个单词 yy 复制一行 ynl 复制n个字符 y$ 复制当前光标至行尾处 nyy 拷贝n行 撤销 命令 描述 u 撤销前一次的操作 shif+u(U) 撤销对该行的所有操作 搜索及替换 命令 描述 /pattern 从光标开始处向文件尾搜索pattern ?pattern 从光标开始处向文件首搜索pattern n 在同一方向重复上一次搜索命令 N 在反方向上重复上一次搜索命令 cw newword 替换为newword n 继续查找 . 执行替换 :s/p1/p2/g 将当前行中所有p1均用p2替代,g表示执行 用c表示需要确认 :n1,n2 s/p1/p2/g 将第n1至n2行中所有p1均用p2替代 :g/p1/s//p2/g 将文件中所有p1均用p2替换 :1,$ s/string1/string2/g 在全文中将string1替换为string2 书签 命令 描述 m[a-z] 在文中做标记，标记号可为a-z的26个字母 `a 移动到标记a处 visual 模式 命令 描述 v 进入visual 模式 V 进入行的visual 模式 ctrl+v 进如块操作模式用o和O改变选择的边的大小 在所有行插入相同的内容如include&lt; 将光标移到开始插入的位置，按CTRL+V进入VISUAL模式，选择好模块后按I（shift+i)，后插入要插入的文本，按[ESC]完成 行方式命令 命令 描述 :n1,n2 co n3 将n1行到n2行之间的内容拷贝到第n3行下 :n1,n2 m n3 将n1行到n2行之间的内容移至到第n3行下 :n1,n2 d 将n1行到n2行之间的内容删除 :n1,n2 w!command 将文件中n1行至n2行的内容作为command的输入并执行之 若不指定n1，n2，则表示将整个文件内容作为command的输入 宏 命令 描述 q[a-z] 开始记录但前开始的操作为宏，名称可为【a-z】，然后用q终止录制宏 reg 显示当前定义的所有的宏，用@[a-z]来在当前光标处执行宏[a-z] 窗口操作 命令 描述 :split 分割一个窗口 :split file.c 为另一个文件file.c分隔窗口 :nsplit file.c 为另一个文件file.c分隔窗口，并指定其行数 ctrl＋w 在窗口中切换 :close 关闭当前窗口 文件及其他 命令 描述 :q 退出vi :q! 不保存文件并退出vi :e filename 打开文件filename进行编辑 :e! 放弃修改文件内容，重新载入该文件编辑 :w 保存当前文件 :wq 存盘退出 :ZZ 保存当前文档并退出VIM :!command 执行shell命令command :r!command 将命令command的输出结果放到当前行 :n1,n2 write temp.c :read file.c 将文件file.c的内容插入到当前光标所在的下面 几张图 vim 工作模式 vim 快捷键盘图 vim 快捷键思维导图 vimium 快捷键盘图 ","link":"https://faded.auspicious.space/post/vim-cheatsheet/"},{"title":"区块链——商用调查","content":" 区块链商用调查 1 信息共享——信息对齐、提高效率 这应该是区块链最简单的应用场景，就是信息互通有无。 1.1 传统的信息共享的痛点 要么是统一由一个中心进行信息发布和分发，要么是彼此之间定时批量对账（典型的每天一次），对于有时效性要求的信息共享，难以达到实时共享。 信息共享的双方缺少一种相互信任的通信方式，难以确定收到的信息是否是对方发送的。 1.2 区块链 + 信息共享 首先，区块链本身就是需要保持各个节点的数据一致性的，可以说是自带信息共享功能；其次，实时的问题通过区块链的 P2P 技术可以实现；最后，利用区块链的不可篡改和共识机制，可构建其一条安全可靠的信息共享通道。 也行你会有这样的疑问：解决上面的问题，不用区块链技术，我自己建个加密通道也可以搞定啊！但我想说，既然区块链技术能够解决这些问题，并且增加节点非常方便，在你没有已经建好一套安全可靠的信息共享系统之前，为什么不用区块链技术呢？ 2 版权保护——不可篡改、永久保存 2.1 传统鉴证证明的痛点 流程复杂：以版权保护为例，现有鉴证证明方式，登记时间长，且费用高。 公信力不足：以法务存证为例，个人或中心化的机构存在篡改数据的可能，公信力难以得到保证。 2.2 区块链 + 鉴证证明 流程简化：区块链应用到鉴证证明后，无论是登记还是查询都非常方便，无需再奔走于各个部门之间， 安全可靠：区块链的去中心化存储，保证没有一家机构可以任意篡改数据， 2.3 应用案例 区块链在鉴权证明领域的应用有版权保护、法务存证等，下面以版权保护为例，简单说下如何区块链如何实现版权登记和查询。 电子身份证：将“申请人+发布时间+发布内容”等版权信息加密后上传，版权信息用于唯一区块链 ID，相当拥有了一张电子身份证。 时间戳保护：版权信息存储时，是加上时间戳信息的，如右雷同，可用于证明先后。 可靠性保证：区块链的去中心化存储、私钥签名、不可篡改的特性提升了鉴权信息的可靠性。 2016 年 8 月，由 Onchain、微软（中国）、法大大等多个机构在北京成立了电子存证区块链联盟“法链”。 2017 年 12 月，微众银行、仲裁委（广州仲裁委）、杭州亦笔科技有限公司共同推出的仲裁联盟链，用于司法场景下的存证；2018 年 3 月，广州首个“仲裁链”判决书出炉。 3 物流链——溯源防伪 商品从生产商到消费者手中，需要经历多个环节（流程可能如上图所示），跨境购物则更加复杂；中间环节经常出问题，消费者很容易购买的假货。而假货问题正是困扰着各大商家和平台，至今无解。 3.1 传统是防伪溯源手段 以一直受假冒伪劣产品困扰的茅台酒的防伪技术为例，2000 年起，其酒盖里有一个唯一的 RFID 标签，可通过手机等设备以 NFC 方式读出，然后通过茅台的 APP 进行校验，以此防止伪造产品。 咋一看，这种防伪效果非常可靠。但 2016 年还是引爆了茅台酒防伪造假，虽然通过 NFC 方式验证 OK，但经茅台专业人士鉴定为假酒。后来，在“国酒茅台防伪溯源系统”数据库审计中发现 80 万条假的防伪标签记录，系防伪技术公司人员参与伪造；随后，茅台改用安全芯片防伪标签。 但这里暴露出来的痛点并没有解决，即防伪信息掌握在某个中心机构中，有权限的人可以任意修改。(备注：茅台的这种防伪方式，也衍生了旧瓶回收，旧瓶装假酒的产业，防伪道路任重而道远)。 2017 年 05 月贵阳数博会上，小马哥就建议茅台防伪使用区块链；那么区块链和物流链的结合有什么优势呢？ 3.2 区块链+物流链 区块链没有中心化节点，各节点是平等的，掌握单个节点无法实现修改数据；需要掌控足够多的节点，才可能伪造数据，大大提高伪造数据的成本。 区块链天生的开放、透明，使得任何人都可以公开查询，伪造数据被发现的概率大增。 区块链的数据不可篡改性，也保证了已销售出去的产品信息已永久记录，无法通过简单复制防伪信息蒙混过关，实现二次销售。 物流链的所有节点上区块链后，商品从生产商到消费者手里都有迹可循，形成完整链条；商品缺失的环节越多，将暴露出其是伪劣产品概率更大。 ##3.3 应用案例 目前，入局物流链的玩家较多，包括腾讯、阿里、京东、沃尔玛等。 据说，阿里的菜鸟在海淘进口应用区块链上，走在了前面，已经初步实现海外商品溯源，国际物流及进口申报溯源、境内物流溯源；下一步就是生产企业溯源了。下图是网上流传的关于阿里的菜鸟在海淘场景运用区块链的示意图。 4 供应链金融——解决中小微企业融资难 4.1 传统的供应链单点融资 在一般供应链贸易中，从原材料的采购、加工、组装到销售的各企业间都涉及到资金的支出和收入，而企业的资金支出和收入是有时间差的，这就形成了资金缺口，多数需要进行融资生产。我们先来看个简单的供应链，如下图： 我们再来看看图中各个角色的融资情况： 核心企业或大企业：规模大、信用好，议价能力强，通过先拿货后付款，延长账期将资金压力传导给后续供应商；此外，其融资能力也是最强的。 一级供应商：通过核心企业的债权转让，可以获得银行的融资。 其他供应商（多数是中小微企业）：规模小、发展不稳定、信用低，风险高，难以获得银行的贷款；也无法想核心企业一样有很长的账期；一般越小的企业其账期越短，微小企业还需要现金拿货。这样一出一入对比就像是：中小微企业无息借钱给大企业做生意。 4.2 区块链 + 供应链金融 面对，上述供应链里的中小微企业融资难问题，主要原因是银行和中小企业之间缺乏一个有效的信任机制。 假如供应链所有节点上链后，通过区块链的私钥签名技术，保证了核心企业等的数据可靠性；而合同、票据等上链，是对资产的数字化，便于流通，实现了价值传递。 如上图所示，在区块链解决了数据可靠性和价值流通后，银行等金融机构面对中小企业的融资，不再是对这个企业进行单独评估；而是站在整个供应链的顶端，通过信任核心企业的付款意愿，对链条上的票据、合同等交易信息进行全方位分析和评估。即借助核心企业的信用实力以及可靠的交易链条，为中小微企业融资背书，实现从单环节融资到全链条融资的跨越，从而缓解中小微企业融资难问题。 5 跨境支付——提高效率、降低费用 5.1 传统跨境支付 跨境支付涉及多种币种，存在汇率问题，传统跨境支付非常依赖于第三方机构，大致的简化模型如上图所示，存在着两个问题； 流程繁琐，结算周期长：传统跨境支付基本都是非实时的，银行日终进行交易的批量处理，通常一笔交易需要 24 小时以上才能完成；某些银行的跨境支付看起来是实时的，但实际上，是收款银行基于汇款银行的信用做了一定额度的垫付，在日终再进行资金清算和对账，业务处理速度慢。 手续费高：传统跨境支付模式存在大量人工对账操作，加之依赖第三方机构，导致手续费居高不下，麦肯锡《2016 全球支付》报告数据显示，通过代理行模式完成一笔跨境支付的平均成本在 25 美元到 35 美元之间。 5.2 区块链 + 跨境支付 这些问题的存在，很大原因还是信息不对称，没有建立有效的信任机制。 如上图所示，区块链的引入，解决了跨境支付信息不对称的问题，并建立起一定程度的信任机制；带来了两个好处。 效率提高，费用降低：接入区块链技术后，通过公私钥技术，保证数据的可靠性，再通过加密技术和去中心，达到数据不可篡改的目的，最后，通过 P2P 技术，实现点对点的结算；去除了传统中心转发，提高了效率，降低了成本(也展望了普及跨境小额支付的可能性)。 可追溯，符合监管需求：传统的点对点结算不能不规模应用，除了信任问题，还有就是存在监管漏洞（点对点私下交易，存在洗黑钱的风险），而区块链的交易透明，信息公开，交易记录永久保存实现了可追溯，符合监管的需求。 6 资产数字化——便于资产流通 6.1 实体资产存在的问题 实体资产往往难以分割，不便于流通 实体资产的流通难以监控，存在洗黑钱等风险 6.2 区块链实现资产数字化 资产数字化后，易于分割、流通方便，交易成本低 用区块链技术实现资产数字化后，所有资产交易记录公开、透明、永久存储、可追溯，完全符合监管需求 6.3 应用案例 还是以腾讯的微黄金应用为例，继续借用腾讯区块链官网（trustsql.qq.com）上的图片，可以看到，在资产数字化之后，流通更为方便了，不再依赖于发行机构；且购买 0.001g 黄金成为了可能，降低了参与门槛。 7 代币——去中介、去信任 本来不像把代币加进来的，但说到区块链，始终绕不开代币；因区块链脱胎于比特币，天生具有代币的属性，目前区块链最成功的应用也正是比特币。 7.1 传统货币存在的问题 传统的货币发行权掌握在国家手中，存在着货币滥发的风险 货币滥发案例 1：元朝自 1271 年建立后，依然四处征战，消耗大量的钱财和粮食，为了财政问题，长期滥发货币，造成严重通货膨胀，多数百姓生活在水生火热中，导致流民四起，国家大乱，1368 年，不可一世的元朝成了只有 97 年短命鬼，走向了灭亡。 货币滥发案例 2：1980 年津巴布韦独立，后因土改失败，经济崩溃，政府入不敷出，开始印钞；2001 年时 100 津巴布韦币可兑换约 1 美元；2009 年 1 月，津央行发行 100 万亿面值新津元（如下图）加速货币崩溃，最终津元被废弃，改用“美元化”货币政策。2017 年津巴布韦发生政变，总统穆加贝被赶下台。 传统的记账权掌握在一个中心化的中介机构手中，存在中介系统瘫痪、中介违约、中介欺瞒、甚至是中介耍赖等风险。 2013 年 3 月，塞浦路斯为获得救助，对银行储户进行一次性征税约 58 亿欧元, 向不低于 10 万欧元的存款一次性征税 9.9%，向低于 10 万欧元的一次性征税 6.75%。 2017 年 4 月，民生银行 30 亿假理财事件暴露，系一支行行长伪造保本保息理财产品所致，超过 150 名投资者被套。 7.2 区块链如何解决这些问题 比特币解决了货币在发行和记账环节的信任问题，我们来看下比特币是如何一一破解上面的两个问题。 7.2.1 滥发问题 比特币的获取只能通过挖矿获得，且比特币总量为 2100 万个，在发行环节解决了货币滥发的问题； 7.2.2 账本修改问题 比特币的交易记录通过链式存储和去中心化的全球节点构成网络来解决账本修改问题。 7.2.3 链式存储 可以简单理解为：存储记录的块是一块连着一块的，形成一个链条；除第一个块的所有区块都的记录包含了前一区块的校验信息，改变任一区块的信息，都将导致后续区块校验出错。因为这种关联性，中间也无法插入其他块，所以修改已有记录是困难的。 7.2.4 去中心化节点 可以简单理解为：全球的中心节点都是平等的，都拥有一模一样的账本，所以，任一节点出问题都不影响账本记录。而要修改账本，必须修改超过全球一半的节点才能完成；而这在目前看来几乎不可能。 既然账本无法修改，那要是记账的时候作弊呢？ 首先，比特币的每条交易记录是有私钥签名的，别人伪造不了这个记录。你能修改的仅仅自己发起的交易记录。 其次，是关于记账权问题：比特币的记账权，通过工作量证明获得，可以简单理解为：通过算法确定同一时刻，全球只有一个节点获得了记账权，基本规律是谁拥有的计算资源越多，谁获得记账权的概率越大，只有超过全网一半的算力，才可能实现双花。 7.2.5 备注 比特币的模式是不可复制的，比特币已经吸引了全球绝大多数的算力，从而降低 51% 攻击发生等问题；其他的复制品基本无法获得相应的算力保证。 目前，比特币还存在着 51% 和效率低等问题有待解决，另外，关于交易本身的信任问题是个社会问题，比特币是没有解决的，也解决不了的。 7.3 应用案例 最具代表性的当然是比特币，也不用多说了。 备注：代币这块真的不看好，比特币目前吸引了全球绝大部分的算力，有独一无二的算力资源作为支撑还稍好一点，其他的代币和传统的货币相比，其背后缺乏国家和武力为其做信用背书，且夺取了国家发币带来的各种好处（如宏观调控），仔细想想就知道有多不靠谱。 8 小结 区块链应用的场景肯定还有很多，但很多都还不大明朗，暂时就先梳理以上7种场景，顺便归纳一下。 ","link":"https://faded.auspicious.space/post/blockchain-commercial-survey/"},{"title":"区块链——行业名词","content":" 名称 中文 解释 2-Way Peg 双向锚定 一种跨链技术 ABI 智能合约的接口说明 Application Binary Interface，ABI 是以太坊的一种合约间调用的消息格式，类似于 WebService 的 SOAP 协议一样，也就是定义操作函数签名，参数编码，返回结果编码等的协议。 altcoin 山寨币 AML 反洗钱 Anti-Money Laundering ASIC 专用集成电路 Application Specific Integrated Circuit，通常，与 GPU 相比，ASIC 专门用于挖矿，可能会节省大量能源。 autonomous 自治 BAAS 区块链服务 Blockchain As A Service，区块链即服务。 BIP 比特币改进建议 Bitcoin Improvement Proposals Block 区块 用于记录区块链系统中数据的存储。 Block Explorer 区块资源管理器 区块资源管理器是一种用来来查看区块上的所有交易（过去和当前）在线工具。 它们提供有用的信息，如网络哈希率和交易增长率。 Block Height 区块高度 连接在区块链上的块数。 Block Reward 出块奖励 它是在采矿期间成功计算区块中的哈希的矿工的一种激励形式。 在区块链上的交易验证的过程中产生新的币，并且矿工被奖励其中的一部分。 Blockchain 区块链 分布式存储、加密算法、共识机制、P2P传输等计算机技术结合的新型应用模式。 Blockchain Wallet 区块链钱包 一个包含私钥的文件。 它通常包含一个软件客户端，允许访问查看和创建钱包所设计的特定块链的交易。 Bulletproofs Bulletproofs 由斯坦福大学提出的，把膨胀系数减少到普通交易的三倍（原来是 60 倍），可以大幅降低隐私交易的数据量大小的算法 CAP CAP 分布式异步网络模型中，不能同时保证**一致性**，**可用性**和**分区容错性**，只能三选二 Central Ledger 中央帐簿 由中央机构维持的分类帐。 Chain 链 区块头中通过引用哈希值链接。 Confirmation 确认 去中心化的一次交易，将其添加到 blockchain 的成功确认。 Consensus 共识机制 区块链中事务达成的分布式共识算法。 Consensus 共识 当所有网络参与者同意交易的有效性时，达成共识，确保分布式账本是彼此的精确副本。 Consortium Block Chains 联盟链 共识过程由预选节点控制，一般为各企业机构互联形成。 Corda Corda R3联盟推出的金融联盟“类区块链”技术架构，Corda 中同样是用交易组成账本，但并没有区块，交易仅在参与方和公证人间传播 Cryptocurrency 加密货币 也称为令牌，加密货币是数字资产的呈现方式。 Cryptographic Hash Function 加密哈希函数 密码哈希产生从可变大小交易输入固定大小和唯一哈希值。 SHA-256计算算法是加密散列的一个例子。 DAO 去中心化自治组织 Decentralized Autonomous Organizations，去中心化自治组织可以被认为是在没有任何人为干预的情况下运行的公司，并将一切形式的控制交给一套不可破坏的业务规则。 Dapp 去中心化应用 是一种开源的应用程序，自动运行，将其数据存储在区块链上，以密码令牌的形式激励，并以显示有价值证明的协议进行操作。 DD 尽职调查 Due Diligence Decentralized 分布式 不依赖中心服务器，分布的计算机资源进行计算处理的模式。 Difficulty 挖矿难度 这是指成功挖掘交易信息的数据块的容易程度。 Distributed Ledger 分布式账本 分布式账本，数据通过分布式节点网络进行存储。 分布式账本不是必须具有自己的货币，它可能会被许可和私有。 Distributed Network 分布式网络 处理能力和数据分布在节点上而不是拥有集中式数据中心的一种网络。 Double Spending 双重支付 当花费一笔钱多于一次支付限额时，就会发生双重支付。 DPoS 委托权益证明 Delegated Proof Of Stake，一种共识算法 EIP 以太坊改进建议 Ethereum Improvement Proposals EOA 外部账户 Externally Owned Accounts ERC 以太坊意见征求 Ethereum Requests for Comment，讨论项目时，一开始会用EIP提出建议，在讨论过程中有一些要征求更多人意见时，就会把细节放在ERC中，而且他们会用同一个号码，比如ERC-20 对应 EIP-20 Ethash Ethash 以前这个算法称为 Dagger Hashimoto，Ethash是最新版本的 Dagger-Hashimoto 改良算法，是 Hashimoto 算法结合 Dagger 算法产成的一个新变种。实现两个主要目的：抵抗 ASIC 矿机和轻客户端易验证 Ethereum 以太坊 Ethereum是一个基于blockchain的去中心化运行智能合约的平台，旨在解决与审查，欺诈和第三方干扰相关的问题。 EVM 以太坊虚拟机 Ethereum Virtual Machine，借助以太坊虚拟机将 Solidity 代码变成可以在区块链上执行的加密代码。以太坊虚拟机是设计运行在点对点网络中所有参与节点上的一个虚拟机，它可以读写一个区块链中可执行的代码和数据，校验数据签名，并以半图灵完备的方式来运行代码。每个Ethereum节点都运行在 EVM 上，以保持整个块链的一致性。 FLP FLP 在网络可靠并且存在节点失效的异步模型中，不存在一个可以解决一致性问题的确定性算法 Fork 分叉 分叉可以创建区块链的交叉版本，在网络不同的地方兼容的运行两个区块链。 Frontier 前沿 以太坊开发第一阶段 gas gas gas是在以太坊网络中用于衡量执行交易或智能合约工作量的计算单位 gas limit gas limit 某笔具体的交易能够消耗的 gas 最大值，一笔标准的以太坊交易需要 21,000 gas。当交易的 gas limit 不足时，会出现 out of gas 错误 gas price gas price 以另一种货币或 token（例如 Ether）计量交易花费的价格。为了稳定消耗 gas 的价值，gas price 是浮动的，根据货币或 token 价格浮动而相应变动以保持总价格稳定。gas price 由市场供需决定（用户愿意支出的价格和矿工节点愿意接受的价格的博弈） gas used gas used 有效支付用于计算或智能合约运行的 gas 数量（在成功的交易中 gas fee 小于 gas limit)，一笔以太坊交易的实际矿工费(Tx Fees) = gas used * gas price Genesis Block 创世区块 区块链的第一个区块。 Go Ethereum geth 实现了以太坊协议的 JavaScript运行时环境，可以以交互式或非交互式模式运行 Hard Fork 硬分叉 区块链发生永久性分歧，在新共识规则发布后，部分没有升级的节点无法验证已经升级的节点生产的区块，产生硬分叉。 Hash 哈希 对输出数据执行散列函数的行为。 这是用于确认货币交易。 Hash Rate 哈希率 采矿钻机的性能测量值以秒为单位表示。 Homestead 家园 以太坊开发第二阶段 Hybrid PoS/PoW 混合PoS / PoW 混合 PoS / PoW 可以将网络上的共享分发算法作为共享证明和工作证明。 在这种方法中，可以实现矿工和选民（持有者）之间的平衡，由内部人（持有人）和外部人（矿工）创建一个基于社区的治理体系。 I2P I2P Invisible Internet Project，建立在互联网上的隐匿网络层，用于为网络通讯提供隐私保护 Infura Infura 提供全球范围区块链集群和 API 端点等基础架构服务；可用于以太坊，IPFS 等其他新兴的分布式平台。致力于提供安全，稳定，高容错性金额可扩展的区块链访问接口 keccak keccak 一种SHA-3加密算法 Kovri Kovri I2P 网络的 C++ 实现版本，目前还在开发中尚未集成到门罗币中，可以提高交易的安全等级（可以隐藏 IP 地址） KYC 了解你的客户 Know Your Customer Metropolis 大都会 以太坊开发第三阶段 Mining 挖矿 挖矿是验证区块链交易的行为。 验证的必要性通常以货币的形式奖励给矿工。 在这个密码安全的繁荣期间，当正确完成计算，采矿可以是一个有利可图的业务。 通过选择最有效和最适合的硬件和采矿目标，采矿可以产生稳定的被动收入形式。 Multi-Signature 多重签名 多重签名地址需要一个以上的密钥来授权交易，从而增加了一层安全性。 Node 节点 由区块链网络的参与者操作的分类帐的副本。 Oracles 预言机 Oracle 通过向智能合约提供数据，它现实世界和区块链之间的桥梁。注意此 Oracle不是指数据库。预言机连接虚拟与现实，核心功能是提供数据上链服务，是实现智能合约的必要条件。智能合约是在区块链提供的沙盒环境中运行，沙盒是个封闭环境，使合约代码不能读取链外数据，但很多时候智能合约又必须依赖外部数据，Oracle 在这里就承担了提供外部数据的功能。 P2P Peer-to-Peer 对等互联网网络技术。 Paxos Paxos 一种用于传统分布式系统的共识协议 Payment Codes 可重用支付码 BIP47，支付码是一种用于创建永久性比特币地址的技术，这些地址可以重复使用，与现实生活中的身份公开相关，同时无损于财务隐私。它们类似于隐形地址。即使他人知道你的支付码也无法追踪你的交易历史，可以用于想要私密的接收BTC的场景 PBFT 实用拜占庭容错 Practical Byzantine Fault Tolerance pegged zone 锚定分区 一种锚定分区的桥接机制，出现于Cosmos项目 POA 权威证明 Proof Of Authority，一种共识算法 portfolio 投资组合 POS 权益证明 Proof of Stake，根据你持有货币的量和时间进行利息分配的制度，在 POS 模式下，你的“挖矿”收益正比于你的币龄，而与电脑的计算性能无关。 POW 工作量证明 Proof of Work，是指获得多少货币，取决于你挖矿贡献的工作量，电脑性能越好，分给你的矿就会越多。 Private Block Chains 私有链 私有区块链，数据记录在单一组织机构中，分权限对外开放，一般是单一企业机构构建。 Private Key 私钥 私钥是一串数据，它是允许您访问特定钱包中的令牌。 它们作为密码，除了地址的所有者之外，都被隐藏。 Public Address 公用地址 公共地址是公钥的密码哈希值。 它们作为可以在任何地方发布的电子邮件地址，与私钥不同。 Public Block Chains 公有链 公共网络中任何个人团体接入，任何节点均可参与共识过程。 Raft Raft Paxos协议的一种简单实现 Ring Signatures 环签名 用于隐匿发送发信息的技术，门罗币采用 RingCT 环加密交易 Ring Confidential Transactions，隐藏交易信息（包括交易双方信息和交易金额）的加密技术，门罗币采用 RLP RLP 编码 Recursive Length Prefix（递归长度前缀）是一种适用于任意二进制数据数组的编码。是以太坊中对象进行序列化/反序列化的主要编码方式。区块，交易等数据结构在持久化时会先经过RLP编码后再存储到持久层中。 RPCA 瑞波共识算法 Ripple Protocol Consensus Algorithm，类似PBFT的共识机制 Scrypt Scrypt 是一种由 Litecoin 使用加密算法。 与 SHA256 相比，它的速度更快，因为它不会占用很多处理时间。 Serenity 宁静 以太坊开发第四阶段（也是最后一个阶段） SHA-256 SHA-256 是比特币一些列数字货币使用的加密算法。 然而，它使用了大量的计算能力和处理时间，迫使矿工组建采矿池以获取收益。 Smart Contracts 智能合约 智能合约将可编程语言的业务规则编码到区块上，并由网络的参与者实施。部署在区块链系统中，一段合约代码，或一套以数字形式定义的承诺，包括合约参与方可以在其上执行承诺的协议。 Soft Fork 软分叉 软分叉与硬分叉不同之处在于，只有先前有效的交易才能使其无效。 由于旧节点将新的块识别为有效，所以软分叉基本上是向后兼容的。 这种分叉需要大多数矿工升级才能执行，而硬分叉需要所有节点就新版本达成一致。 Solidity Solidity 是 Ethereum 用于开发智能合约的编程语言。 SPV 简单支付验证 Simplified Payment Verification Stealth Address 隐匿地址 能够隐藏接收方信息 Swarm Swarm 去中心化的数据存储访问协议，以 ETH 作为激励。类似使用了 Filecoin 的 IPFS Sybil Attack 女巫攻击 P2P网络中的一种攻击形式：攻击者利用单个节点来伪造多个身份存在于 P2P 网络中，从而达到削弱网络的冗余性，降低网络健壮性，监视或干扰网络正常活动等目的 Testnet Testnet 开发商使用的测试区块链，它主要是用来防止改变在主链上的资产。 testrpc testrpc 以太坊节点客户端 Transaction Block 交易区块 聚集到一个块中的交易的集合，然后可以将其散列并添加到区块链中。 Transaction Fee 交易费 所有的加密货币交易都会涉及到一笔很小的手续费。这些手续费用加起来给矿工在成功处理区块时收到的区块奖励。 Truffle Truffle 一个基于以太坊技术的开发、测试和部署框架，旨在帮助以太坊开发者更容易开发去中心化应用（DApp） Turing Complete 图灵完备 图灵完备是指计算机中一切计算的问题都可以计算，这样的虚拟机或者编程语言称为图灵完备。一个例子是 Ethereum 虚拟机（EVM）。 Unlinkability 无关联性 whisper whisper 去中心化的通信协议 YC YC Y Combinator，成立于 2005 年是美国著名创业孵化器，扶持初创企业并为其提供创业指南（Airbnb，Dropbox，Stripe，Reddit, Docker, Coinbase 等），投资孵化过多个区块链项目 ZKRP 零知识范围证明 Zero Knownledge Range Proof，证明一个具体声明的真实性而不会泄露它试图证明的额外信息 ZK-SNARKs 零知识证明 ZK-Succint Non-interactive Arguments of Knownledge ","link":"https://faded.auspicious.space/post/blockchain-industry-terms/"},{"title":"区块链——行业名词","content":" 名称 中文 解释 2-Way Peg 双向锚定 一种跨链技术 ABI 智能合约的接口说明 Application Binary Interface，ABI 是以太坊的一种合约间调用的消息格式，类似于 WebService 的 SOAP 协议一样，也就是定义操作函数签名，参数编码，返回结果编码等的协议。 altcoin 山寨币 AML 反洗钱 Anti-Money Laundering ASIC 专用集成电路 Application Specific Integrated Circuit，通常，与 GPU 相比，ASIC 专门用于挖矿，可能会节省大量能源。 autonomous 自治 BAAS 区块链服务 Blockchain As A Service，区块链即服务。 BIP 比特币改进建议 Bitcoin Improvement Proposals Block 区块 用于记录区块链系统中数据的存储。 Block Explorer 区块资源管理器 区块资源管理器是一种用来来查看区块上的所有交易（过去和当前）在线工具。 它们提供有用的信息，如网络哈希率和交易增长率。 Block Height 区块高度 连接在区块链上的块数。 Block Reward 出块奖励 它是在采矿期间成功计算区块中的哈希的矿工的一种激励形式。 在区块链上的交易验证的过程中产生新的币，并且矿工被奖励其中的一部分。 Blockchain 区块链 分布式存储、加密算法、共识机制、P2P传输等计算机技术结合的新型应用模式。 Blockchain Wallet 区块链钱包 一个包含私钥的文件。 它通常包含一个软件客户端，允许访问查看和创建钱包所设计的特定块链的交易。 Bulletproofs Bulletproofs 由斯坦福大学提出的，把膨胀系数减少到普通交易的三倍（原来是 60 倍），可以大幅降低隐私交易的数据量大小的算法 CAP CAP 分布式异步网络模型中，不能同时保证**一致性**，**可用性**和**分区容错性**，只能三选二 Central Ledger 中央帐簿 由中央机构维持的分类帐。 Chain 链 区块头中通过引用哈希值链接。 Confirmation 确认 去中心化的一次交易，将其添加到 blockchain 的成功确认。 Consensus 共识机制 区块链中事务达成的分布式共识算法。 Consensus 共识 当所有网络参与者同意交易的有效性时，达成共识，确保分布式账本是彼此的精确副本。 Consortium Block Chains 联盟链 共识过程由预选节点控制，一般为各企业机构互联形成。 Corda Corda R3联盟推出的金融联盟“类区块链”技术架构，Corda 中同样是用交易组成账本，但并没有区块，交易仅在参与方和公证人间传播 Cryptocurrency 加密货币 也称为令牌，加密货币是数字资产的呈现方式。 Cryptographic Hash Function 加密哈希函数 密码哈希产生从可变大小交易输入固定大小和唯一哈希值。 SHA-256计算算法是加密散列的一个例子。 DAO 去中心化自治组织 Decentralized Autonomous Organizations，去中心化自治组织可以被认为是在没有任何人为干预的情况下运行的公司，并将一切形式的控制交给一套不可破坏的业务规则。 Dapp 去中心化应用 是一种开源的应用程序，自动运行，将其数据存储在区块链上，以密码令牌的形式激励，并以显示有价值证明的协议进行操作。 DD 尽职调查 Due Diligence Decentralized 分布式 不依赖中心服务器，分布的计算机资源进行计算处理的模式。 Difficulty 挖矿难度 这是指成功挖掘交易信息的数据块的容易程度。 Distributed Ledger 分布式账本 分布式账本，数据通过分布式节点网络进行存储。 分布式账本不是必须具有自己的货币，它可能会被许可和私有。 Distributed Network 分布式网络 处理能力和数据分布在节点上而不是拥有集中式数据中心的一种网络。 Double Spending 双重支付 当花费一笔钱多于一次支付限额时，就会发生双重支付。 DPoS 委托权益证明 Delegated Proof Of Stake，一种共识算法 EIP 以太坊改进建议 Ethereum Improvement Proposals EOA 外部账户 Externally Owned Accounts ERC 以太坊意见征求 Ethereum Requests for Comment，讨论项目时，一开始会用EIP提出建议，在讨论过程中有一些要征求更多人意见时，就会把细节放在ERC中，而且他们会用同一个号码，比如ERC-20 对应 EIP-20 Ethash Ethash 以前这个算法称为 Dagger Hashimoto，Ethash是最新版本的 Dagger-Hashimoto 改良算法，是 Hashimoto 算法结合 Dagger 算法产成的一个新变种。实现两个主要目的：抵抗 ASIC 矿机和轻客户端易验证 Ethereum 以太坊 Ethereum是一个基于blockchain的去中心化运行智能合约的平台，旨在解决与审查，欺诈和第三方干扰相关的问题。 EVM 以太坊虚拟机 Ethereum Virtual Machine，借助以太坊虚拟机将 Solidity 代码变成可以在区块链上执行的加密代码。以太坊虚拟机是设计运行在点对点网络中所有参与节点上的一个虚拟机，它可以读写一个区块链中可执行的代码和数据，校验数据签名，并以半图灵完备的方式来运行代码。每个Ethereum节点都运行在 EVM 上，以保持整个块链的一致性。 FLP FLP 在网络可靠并且存在节点失效的异步模型中，不存在一个可以解决一致性问题的确定性算法 Fork 分叉 分叉可以创建区块链的交叉版本，在网络不同的地方兼容的运行两个区块链。 Frontier 前沿 以太坊开发第一阶段 gas gas gas是在以太坊网络中用于衡量执行交易或智能合约工作量的计算单位 gas limit gas limit 某笔具体的交易能够消耗的 gas 最大值，一笔标准的以太坊交易需要 21,000 gas。当交易的 gas limit 不足时，会出现 out of gas 错误 gas price gas price 以另一种货币或 token（例如 Ether）计量交易花费的价格。为了稳定消耗 gas 的价值，gas price 是浮动的，根据货币或 token 价格浮动而相应变动以保持总价格稳定。gas price 由市场供需决定（用户愿意支出的价格和矿工节点愿意接受的价格的博弈） gas used gas used 有效支付用于计算或智能合约运行的 gas 数量（在成功的交易中 gas fee 小于 gas limit)，一笔以太坊交易的实际矿工费(Tx Fees) = gas used * gas price Genesis Block 创世区块 区块链的第一个区块。 Go Ethereum geth 实现了以太坊协议的 JavaScript运行时环境，可以以交互式或非交互式模式运行 Hard Fork 硬分叉 区块链发生永久性分歧，在新共识规则发布后，部分没有升级的节点无法验证已经升级的节点生产的区块，产生硬分叉。 Hash 哈希 对输出数据执行散列函数的行为。 这是用于确认货币交易。 Hash Rate 哈希率 采矿钻机的性能测量值以秒为单位表示。 Homestead 家园 以太坊开发第二阶段 Hybrid PoS/PoW 混合PoS / PoW 混合 PoS / PoW 可以将网络上的共享分发算法作为共享证明和工作证明。 在这种方法中，可以实现矿工和选民（持有者）之间的平衡，由内部人（持有人）和外部人（矿工）创建一个基于社区的治理体系。 I2P I2P Invisible Internet Project，建立在互联网上的隐匿网络层，用于为网络通讯提供隐私保护 Infura Infura 提供全球范围区块链集群和 API 端点等基础架构服务；可用于以太坊，IPFS 等其他新兴的分布式平台。致力于提供安全，稳定，高容错性金额可扩展的区块链访问接口 keccak keccak 一种SHA-3加密算法 Kovri Kovri I2P 网络的 C++ 实现版本，目前还在开发中尚未集成到门罗币中，可以提高交易的安全等级（可以隐藏 IP 地址） KYC 了解你的客户 Know Your Customer Metropolis 大都会 以太坊开发第三阶段 Mining 挖矿 挖矿是验证区块链交易的行为。 验证的必要性通常以货币的形式奖励给矿工。 在这个密码安全的繁荣期间，当正确完成计算，采矿可以是一个有利可图的业务。 通过选择最有效和最适合的硬件和采矿目标，采矿可以产生稳定的被动收入形式。 Multi-Signature 多重签名 多重签名地址需要一个以上的密钥来授权交易，从而增加了一层安全性。 Node 节点 由区块链网络的参与者操作的分类帐的副本。 Oracles 预言机 Oracle 通过向智能合约提供数据，它现实世界和区块链之间的桥梁。注意此 Oracle不是指数据库。预言机连接虚拟与现实，核心功能是提供数据上链服务，是实现智能合约的必要条件。智能合约是在区块链提供的沙盒环境中运行，沙盒是个封闭环境，使合约代码不能读取链外数据，但很多时候智能合约又必须依赖外部数据，Oracle 在这里就承担了提供外部数据的功能。 P2P Peer-to-Peer 对等互联网网络技术。 Paxos Paxos 一种用于传统分布式系统的共识协议 Payment Codes 可重用支付码 BIP47，支付码是一种用于创建永久性比特币地址的技术，这些地址可以重复使用，与现实生活中的身份公开相关，同时无损于财务隐私。它们类似于隐形地址。即使他人知道你的支付码也无法追踪你的交易历史，可以用于想要私密的接收BTC的场景 PBFT 实用拜占庭容错 Practical Byzantine Fault Tolerance pegged zone 锚定分区 一种锚定分区的桥接机制，出现于Cosmos项目 POA 权威证明 Proof Of Authority，一种共识算法 portfolio 投资组合 POS 权益证明 Proof of Stake，根据你持有货币的量和时间进行利息分配的制度，在 POS 模式下，你的“挖矿”收益正比于你的币龄，而与电脑的计算性能无关。 POW 工作量证明 Proof of Work，是指获得多少货币，取决于你挖矿贡献的工作量，电脑性能越好，分给你的矿就会越多。 Private Block Chains 私有链 私有区块链，数据记录在单一组织机构中，分权限对外开放，一般是单一企业机构构建。 Private Key 私钥 私钥是一串数据，它是允许您访问特定钱包中的令牌。 它们作为密码，除了地址的所有者之外，都被隐藏。 Public Address 公用地址 公共地址是公钥的密码哈希值。 它们作为可以在任何地方发布的电子邮件地址，与私钥不同。 Public Block Chains 公有链 公共网络中任何个人团体接入，任何节点均可参与共识过程。 Raft Raft Paxos协议的一种简单实现 Ring Signatures 环签名 用于隐匿发送发信息的技术，门罗币采用 RingCT 环加密交易 Ring Confidential Transactions，隐藏交易信息（包括交易双方信息和交易金额）的加密技术，门罗币采用 RLP RLP 编码 Recursive Length Prefix（递归长度前缀）是一种适用于任意二进制数据数组的编码。是以太坊中对象进行序列化/反序列化的主要编码方式。区块，交易等数据结构在持久化时会先经过RLP编码后再存储到持久层中。 RPCA 瑞波共识算法 Ripple Protocol Consensus Algorithm，类似PBFT的共识机制 Scrypt Scrypt 是一种由 Litecoin 使用加密算法。 与 SHA256 相比，它的速度更快，因为它不会占用很多处理时间。 Serenity 宁静 以太坊开发第四阶段（也是最后一个阶段） SHA-256 SHA-256 是比特币一些列数字货币使用的加密算法。 然而，它使用了大量的计算能力和处理时间，迫使矿工组建采矿池以获取收益。 Smart Contracts 智能合约 智能合约将可编程语言的业务规则编码到区块上，并由网络的参与者实施。部署在区块链系统中，一段合约代码，或一套以数字形式定义的承诺，包括合约参与方可以在其上执行承诺的协议。 Soft Fork 软分叉 软分叉与硬分叉不同之处在于，只有先前有效的交易才能使其无效。 由于旧节点将新的块识别为有效，所以软分叉基本上是向后兼容的。 这种分叉需要大多数矿工升级才能执行，而硬分叉需要所有节点就新版本达成一致。 Solidity Solidity 是 Ethereum 用于开发智能合约的编程语言。 SPV 简单支付验证 Simplified Payment Verification Stealth Address 隐匿地址 能够隐藏接收方信息 Swarm Swarm 去中心化的数据存储访问协议，以 ETH 作为激励。类似使用了 Filecoin 的 IPFS Sybil Attack 女巫攻击 P2P网络中的一种攻击形式：攻击者利用单个节点来伪造多个身份存在于 P2P 网络中，从而达到削弱网络的冗余性，降低网络健壮性，监视或干扰网络正常活动等目的 Testnet Testnet 开发商使用的测试区块链，它主要是用来防止改变在主链上的资产。 testrpc testrpc 以太坊节点客户端 Transaction Block 交易区块 聚集到一个块中的交易的集合，然后可以将其散列并添加到区块链中。 Transaction Fee 交易费 所有的加密货币交易都会涉及到一笔很小的手续费。这些手续费用加起来给矿工在成功处理区块时收到的区块奖励。 Truffle Truffle 一个基于以太坊技术的开发、测试和部署框架，旨在帮助以太坊开发者更容易开发去中心化应用（DApp） Turing Complete 图灵完备 图灵完备是指计算机中一切计算的问题都可以计算，这样的虚拟机或者编程语言称为图灵完备。一个例子是 Ethereum 虚拟机（EVM）。 Unlinkability 无关联性 whisper whisper 去中心化的通信协议 YC YC Y Combinator，成立于 2005 年是美国著名创业孵化器，扶持初创企业并为其提供创业指南（Airbnb，Dropbox，Stripe，Reddit, Docker, Coinbase 等），投资孵化过多个区块链项目 ZKRP 零知识范围证明 Zero Knownledge Range Proof，证明一个具体声明的真实性而不会泄露它试图证明的额外信息 ZK-SNARKs 零知识证明 ZK-Succint Non-interactive Arguments of Knownledge ","link":"https://faded.auspicious.space/post/blockchain-profession-words/"},{"title":"区块链——六大核心算法","content":" 区块链技术六大核心算法 拜占庭协定 拜占庭的故事大概是这么说的：拜占庭帝国拥有巨大的财富，周围 10 个邻邦垂诞已久，但拜占庭高墙耸立，固若金汤，没有一个单独的邻邦能够成功入侵。任何单个邻邦入侵的都会失败，同时也有可能自身被其他 9 个邻邦入侵。拜占庭帝国防御能力如此之强，至少要有十个邻邦中的一半以上同时进攻，才有可能攻破。然而，如果其中的一个或者几个邻邦本身答应好一起进攻，但实际过程出现背叛，那么入侵者可能都会被歼灭。于是每一方都小心行事，不敢轻易相信邻国。这就是拜占庭将军问题。 在这个分布式网络里：每个将军都有一份实时与其他将军同步的消息账本。账本里有每个将军的签名都是可以验证身份的。如果有哪些消息不一致，可以知道消息不一致的是哪些将军。尽管有消息不一致的，只要超过半数同意进攻，少数服从多数，共识达成。 由此，在一个分布式的系统中，尽管有坏人，坏人可以做任意事情（不受protocol限制），比如不响应、发送错误信息、对不同节点发送不同决定、不同错误节点联合起来干坏事等等。但是，只要大多数人是好人，就完全有可能去中心化地实现共识。 非对称加密技术 在上述拜占庭协定中，如果 10 个将军中的几个同时发起消息，势必会造成系统的混乱，造成各说各的攻击时间方案，行动难以一致。谁都可以发起进攻的信息，但由谁来发出呢？其实这只要加入一个成本就可以了，即：一段时间内只有一个节点可以传播信息。当某个节点发出统一进攻的消息后，各个节点收到发起者的消息必须签名盖章，确认各自的身份。 在如今看来，非对称加密技术完全可以解决这个签名问题。非对称加密算法的加密和解密使用不同的两个密钥.这两个密钥就是我们经常听到的“公钥”和“私钥”。公钥和私钥一般成对出现, 如果消息使用公钥加密,那么需要该公钥对应的私钥才能解密; 同样，如果消息使用私钥加密,那么需要该私钥对应的公钥才能解密。 容错问题 我们假设在此网络中，消息可能会丢失、损坏、延迟、重复发送，并且接受的顺序与发送的顺序不一致。此外，节点的行为可以是任意的：可以随时加入、退出网络，可以丢弃消息、伪造消息、停止工作等，还可能发生各种人为或非人为的故障。我们的算法对由共识节点组成的共识系统，提供的容错能力，这种容错能力同时包含安全性和可用性，并适用于任何网络环境。 Paxos 算法（一致性算法） Paxos算法解决的问题是一个分布式系统如何就某个值（决议）达成一致。一个典型的场景是，在一个分布式数据库系统中，如果各节点的初始状态一致，每个节点都执行相同的操作序列，那么他们最后能得到一个一致的状态。为保证每个节点执行相同的命令序列，需要在每一条指令上执行一个“一致性算法”以保证每个节点看到的指令一致。一个通用的一致性算法可以应用在许多场景中，是分布式计算中的重要问题。 节点通信存在两种模型：共享内存和消息传递。Paxos 算法就是一种基于消息传递模型的一致性算法。 共识机制 区块链共识算法主要是工作量证明和权益证明。拿比特币来说，其实从技术角度来看可以把 PoW 看做重复使用的 Hashcash，生成工作量证明在概率上来说是一个随机的过程。开采新的机密货币，生成区块时，必须得到所有参与者的同意，那矿工必须得到区块中所有数据的 PoW 工作证明。与此同时矿工还要时时观察调整这项工作的难度，因为对网络要求是平均每 10 分钟生成一个区块。 分布式存储 分布式存储是一种数据存储技术，通过网络使用每台机器上的磁盘空间，并将这些分散的存储资源构成一个虚拟的存储设备，数据分散的存储在网络中的各个角落。所以，分布式存储技术并不是每台电脑都存放完整的数据，而是把数据切割后存放在不同的电脑里。就像存放 100 个鸡蛋，不是放在同一个篮子里，而是分开放在不同的地方，加起来的总和是 100 个。 ","link":"https://faded.auspicious.space/post/blockchain-core-algorithms/"},{"title":"区块链——入门","content":"狭义来讲，区块链是一种按照时间顺序将数据区块以顺序相连的方式组合成的一种链式数据结构， 并以密码学方式保证的不可篡改和不可伪造的分布式账本。广义来讲，区块链技术是利用块链式数据结构来验证与存储数据、利用分布式节点共识算法来生成和更新数据、利用密码学的方式保证数据传输和访问的安全、利用由自动化脚本代码组成的智能合约来编程和操作数据的一种全新的分布式基础架构与计算范式。 1 涉及到的技术 密码学 分布式一致性协议 点对点网络通信技术 智能合约编程语言等。 2 区块链的分类 区块链严格定义上被划分为 3 种类型：公有链，私有链、和联盟链，但是在实际应用中单一的某种链常常无法满足用户需求，就出现了多种类型的结合，比如私有链 + 联盟链、联盟链 + 公有链等不同组合形式，最后产生了侧链和互联链。掌握了这 5 种区块链类型的各自特点，是理解和设计区块链网络系统架构的基础和核心，其重要性不言而喻。 2.1 公有链（Public blockchains） 公有链是对所有人公开，用户不需要注册和授权就能够匿名访问网络和区块，任何人都可以自由加入和退出网络，并参与记账和交易。 公有链是真正完全意义上的去中心化区块链，它通过密码学（非对称加密）算法保证了交易的安全性和不可篡改性，在陌生的网络（非安全）环境中，建立了互信和共识机制。在公有链中共识机制一般是工作量证明（POW）和权益证明（POS）。 公有链因为人人可参与，无需授权的特点又被称为非许可链，即不需要验证身份即可参与一切网络活动。目前比特币、以太坊、超级账本、大多数山寨币以及智能合约都是建立在公有链上，其中公有链的始祖是比特币区块链。 公有链适用于数字货币、电子商务、互联网金融、知识产权等应用场景。 2.2 联盟链（Consortium blockchains） 联盟链仅限于联盟成员，因其只针对成员开放全部或部分功能，所以联盟链上的读写权限、以及记账规则都按联盟规则来“私人定制”。联盟链上的共识过程由预先选好的节点控制，一般来说，他适用于机构间的交易、结算、或清算等 B2B 场景。比如人民银行开发一个基于联盟链的结算、清算系统，工建中农等银行作为联盟成员加入这个系统，获得相应的授权，就可以实时进行不同银行之间的实时结算、清算，与现有的中心化系统相比，这样不仅大大提升了结算、清算效率，几乎不需要人工参与，还能大大降低结算、清算成本。联盟链几乎不采用工作量证明共识机制而是采用权益证明或PBTF等共识算法。 联盟链由参与成员机构共同维护，并提供了对参与成员的管理、认证、授权、监控、审计等全套安全管理功能。2015 年成立的 R3 联盟，就是银行业的一个联盟链，目前已加入的成员多达 40 多个，包括世界著名的银行摩根大通、汇丰、高盛等。 联盟链适用于行业协会、高级别机构组织、大型连锁企业对下属单位和分管机构的交易和监管。 2.3 私有链（Private blockchain） 私有链对单独的个人或实体开放，仅在私有组织，比如公司内部使用，私有链上的读写权限，参与记账的权限都由私有组织来制定。比如企业内部的办公审批、财务审计；政府行业的预算和执行。私有链的主要价值在于提供安全、可塑源，不可篡改，自动执行，这是传统系统很难同时做到的。 因为私有链加入结点少，所以交易速度快。私有链的交易速度可以比任何其他的区块链都快，甚至接近了并不是一个区块链的常规数据库的速度。而且因为就算少量的节点，也都具有很高的信任度，所以并不需要每个节点来验证一个交易(无需挖矿)。 由于私有链和联盟链都需要授权加入和访问，私有链和联盟链也被称作许可链。 私有链适用于企业、组织内部。 2.4 侧链（Side Chains） 严格来说侧链不是区块链的一种类型，它只是在现实应用中，开发者对区块链的一种延伸（扩展），而特别取了个绰号。目前，市场上公开的虚拟货币系统，绝大多数都是基于比特币系统进行规则修改或扩展而来，因为比特币的设计规则已十分固定，难以做出较大修改和扩展，于是这些代币系统的开发者门干脆以比特币平台为基础，重构出一条区块链，然后使用新的规则，发布新的虚拟货币，这条重构出来的区块链就被称为侧链。普遍认为能和比特币区块链进行交互，并能与比特币挂钩的区块链就是侧链。 侧链目前主要适用于代币发行。 2.5 互联链（InteChains） 互联链就是各种不同的区块链之间的互联互通所形成的一个更大的生态区块链。比如电商平台公有链 + 物流公有链 + 物流联盟链 + 银行联盟链 +.....，它们之间的相互协作、通讯、共识、就是一个典型的互联链。 3 区块链的工作量证明机制 3.1 POW：proof of power, 工作量证明机制 PoW（工作量证明），也就是像比特币的挖矿机制，矿工通过把网络尚未记录的现有交易打包到一个区块，然后不断遍历尝试来寻找一个随机数，使得新区块加上随机数的哈希值满足一定的难度条件，例如前面 10 位是零。找到满足条件的随机数，就相当于确定了区块链最新的一个区块，也相当于获得了区块链的本轮记账权。矿工把满足挖矿难度条件的区块在网络中广播出去，全网其他节点在验证该区块满足挖矿难度条件，同时区块里的交易数据符合协议规范后，将各自把该区块链接到自己版本的区块链上，从而在全网形成对当前网络状态的共识。 优点：完全去中心化，节点自由进出，避免了建立和维护中心化信用机构的成本。只要网络破坏者的算力不超过网络总算力的 50%，网络的交易状态就能达成一致。 缺点：目前比特币挖矿造成大量的资源浪费；另外挖矿的激励机制也造成矿池算力的高度集中，背离了当初去中心化设计的初衷。更大的问题是 PoW 机制的共识达成的周期较长，每秒只能最多做 7 笔交易，不适合商业应用。 3.2 POS：proof of stake, 股权证明 PoS 权益证明，要求节点提供拥有一定数量的代币证明来获取竞争区块链记账权的一种分布式共识机制。如果单纯依靠代币余额来决定记账者必然使得富有者胜出，导致记账权的中心化，降低共识的公正性，因此不同的 PoS 机制在权益证明的基础上，采用不同方式来增加记账权的随机性来避免中心化。例如点点币（PeerCoin）PoS 机制中，拥有最多链龄长的比特币获得记账权的几率就越大。NXT 和 Blackcoin 则采用一个公式来预测下一个记账的节点。拥有多的代币被选为记账节点的概率就会大。未来以太坊也会从目前的 PoW 机制转换到 PoS 机制，从目前看到的资料看，以太坊的 PoS 机制将采用节点下赌注来赌下一个区块，赌中者有额外以太币奖，赌不中者会被扣以太币的方式来达成下一区块的共识。 优点：在一定程度上缩短了共识达成的时间，降低了 PoW 机制的资源浪费。 缺点：破坏者对网络攻击的成本低，网络的安全性有待验证。另外拥有代币数量大的节点获得记账权的几率更大，会使得网络的共识受少数富裕账户支配，从而失去公正性。 3.3 DPOS：delegated proof of stake, 共识机制，委托权以证明 DPoS（股份授权证明）机制，类似于董事会投票。比特股（bitshares）采用的 PoS 机制是持股者投票选出一定数量的见证人，每个见证人按序有两秒的权限时间生成区块，若见证人在给定的时间片不能生成区块，区块生成权限交给下一个时间片对应的见证人。持股人可以随时通过投票更换这些见证人。DPoS 的这种设计使得区块的生成更为快速，也更加节能。 优点：大幅缩小参与验证和记账节点的数量，可以达到秒级的共识验证。 缺点：选举固定数量的见证人作为记账候选人有可能不适合于完全去中心化的场景。另外在网络节点数少的场景，选举的见证人的代表性也不强。 4 分布式一致性算法 分布式一致性算法是基于传统的分布式一致性技术。其中有分为解决拜占庭将军问题的拜占庭容错算法，如 PBFT。另外解决非拜占庭问题的分布式一致性算法（Pasox、Raft）。该类算法目前是联盟链和私有链场景中常用的共识机制。 优点：实现秒级的快速共识机制，保证一致性。 缺点：去中心化程度不如公有链上的共识机制；更适合多方参与的多中心商业模式。 4.1 拜占庭将军问题/ Byzantine Generals Problem/ BGP 拜占庭将军问题由莱斯利·兰波特在其同名论文中提出的分布式对等网络通信容错问题。在分布式计算中，不同的计算机通过通讯交换信息达成共识而按照同一套协作策略行动。但有时候，系统中的成员计算机可能出错而发送错误的信息，用于传递信息的通讯网络也可能导致信息损坏，使得网络中不同的成员关于全体协作的策略得出不同结论，从而破坏系统一致性。拜占庭将军问题被认为是容错性问题中最难的问题类型之一。 4.2 改进型实用拜占庭容错/ Practical Byzantine Fault Tolerance/ PBFT PBET 共识机制是少数服从多数，根据信息在分布式网络中节点间互相交换后各节点列出所有得到的信息，一个节点代表一票，选择大多数的结果作为解决办法。PBET 将容错量控制在全部节点数的 1/3，即如只要有超过 2/3 的正常节点，整个系统便可正常运作。 4.3 授权拜占庭容错算法/ Delegated Byzantine Fault Tolerance /dBFT dBFT，是基于持有权益比例来选出专门的记账人（记账节点），然后记账人之间通过拜占庭容错算法（即少数服从多数的投票机制）来达成共识，决定动态参与节点。dBFT 可以容忍任何类型的错误，且专门的多个记账人使得每一个区块都有最终性、不会分叉。 4.4 联邦拜占庭协议/ Federated Byzantine Agreement / FBA 联邦拜占庭协议的主要特性是去中心化和任意行为容错，通过分布式的方法，达到法定人数或者节点足够的群体能达成共识，每一个节点不需要依赖相同的参与者就能决定信任的对象来完成共识。 5 图灵完备 一切可计算的问题都能计算，这样的虚拟机或者编程语言就叫图灵完备的。 5.1 图灵完备的系统和图灵完备的语言 一个能计算出每个图灵可计算函数（Turing-computable function）的计算系统被称为图灵完备的。一个语言是图灵完备的，意味着该语言的计算能力与一个通用图灵机 （Universal Turing Machine）相当，这也是现代计算机语言所能拥有的最高能力。 5.2 图灵完备深入解释 在可计算理论中，当一组数据操作的规则（一组指令集，编程语言，或者元胞自动机）满足任意数据按照一定的顺序可以计算出结果，被称为图灵完备（turing complete）。一个有图灵完备指令集的设备被定义为通用计算机。如果是图灵完备的，它（计算机设备）有能力执行条件跳转（“if” 和 “goto”语句）以及改变内存数据。 如果某个东西展现出了图灵完备，它就有能力表现出可以模拟原始计算机，而即使最简单的计算机也能模拟出最复杂的计算机。所有的通用编程语言和现代计算机的指令集都是图灵完备的（C++ template 就是图灵完备的），都能解决内存有限的问题。图灵完备的机器都被定义有无限内存，但是机器指令集却通常定义为只工作在特定的，有限数量的 RAM 上。 5.3 图灵完备优缺点 图灵完备意味着你的语言可以做到能够用图灵机能做到的所有事情，可以解决所有的可计算问题。 图灵不完备也不是没有意义，有些场景我们需要限制语言本身。 如限制循环和递归，可以保证该语言能写的程序一定是终止的。图灵不完备会更安全些，图灵完备会更智能些。 5.4比特币的图灵非完备性 比特币脚本语言包含许多操作，但都故意限定为一种重要的方式——没有循环或者复杂流控制功能以外的其他条件的流控制。这样就保证了脚本语言的图灵非完备性，这意味着脚本的复杂性有限，交易可执行的次数也可预见。脚本并不是一种通用语言，施加的这些限制确保该语言不被用于创造无限循环或其它类型的逻辑炸弹，这样的炸弹可以植入在一笔交易中，通过引起拒绝服务的方式攻击比特币网络。受限制的语言能防止交易激活机制被人当作薄弱环节而加以利用。 5.5 以太坊是一个图灵完备的区块链 以太坊的核心就是能够运行“无所不能”的智能合约，拥有图灵完备的编程语言，比如 Solidity，可以解决所有可计算问题。 6 零知识证明 “零知识证明”－zero-knowledge proof，是由S.Goldwasser、S.Micali及C.Rackoff在20世纪80年代初提出的。它指的是证明者能够在不向验证者提供任何有用的信息的情况下，使验证者相信某个论断是正确的。零知识证明实质上是一种涉及两方或更多方的协议，即两方或更多方完成一项任务所需采取的一系列步骤。证明者向验证者证明并使其相信自己知道或拥有某一消息，但证明过程不能向验证者泄漏任何关于被证明消息的信息。大量事实证明，零知识证明在密码学中非常有用。如果能够将零知识证明用于验证，将可以有效解决许多问题。 6.1 定义 零知识证明满足三个属性： 如果语句为真，诚实的验证者（即，正确遵循协议的验证者）将由诚实的证明者确信这一事实。 如果语句为假，不排除有概率欺骗者可以说服诚实的验证者它是真的。 如果语句为真，证明者的目的就是向验证者证明并使验证者相信自己知道或拥有某一消息，而在证明过程中不可向验证者泄漏任何有关被证明消息的内容。 零知识证明并不是数学意义上的证明，因为它存在小概率的误差，欺骗者有可能通过虚假陈述骗过证明者。换句话来说，零知识证明是概率证明而不是确定性证明。但是也存在有技术能将误差降低到可以忽略的值。 零知识的形式定义必须使用一些计算模型，最常见的是图灵机的计算模型。 6.2 证明举例 6.2.1 案例一 A 要向 B 证明自己拥有某个房间的钥匙，假设该房间只能用钥匙打开锁，而其他任何方法都打不开。这时有 2 个方法： A 把钥匙出示给 B，B 用这把钥匙打开该房间的锁，从而证明 A 拥有该房间的正确的钥匙。 B 确定该房间内有某一物体，A 用自己拥有的钥匙打开该房间的门，然后把物体拿出来出示给 B，从而证明自己确实拥有该房间的钥匙。 后面的方法 2 属于零知识证明。好处在于在整个证明的过程中，B 始终不能看到钥匙的样子，从而避免了钥匙的泄露。 6.2.2 案例二 A 拥有 B 的公钥，A 没有见过 B，而 B 见过 A 的照片，偶然一天 2 人见面了，B 认出了 A，但 A 不能确定面前的人是否是 B，这时 B 要向 A 证明自己是 B，也有 2 个方法。 B 把自己的私钥给 A，A 用这个私钥对某个数据加密，然后用 B 的公钥解密，如果正确，则证明对方确实是B。 A 给出一个随机值，B 用自己的私钥对其加密，然后把加密后的数据交给 A，A 用 B 的公钥解密，如果能够得到原来的随机值，则证明对方是 B。 后面方法 2 属于零知识证明。 ","link":"https://faded.auspicious.space/post/blockchain-introduction/"},{"title":"正则表达式——完整版教程","content":" JS正则表达式完整教程（略长） 引言 本文内容共有七章，用 JavaScript 语言完整地讨论了正则表达式的方方面面。 具体章节如下： 引言 第 1 章 正则表达式字符匹配攻略 第 2 章 正则表达式位置匹配攻略 第 3 章 正则表达式括号的作用 第 4 章 正则表达式回溯法原理 第 5 章 正则表达式的拆分 第 6 章 正则表达式的构建 第 7 章 正则表达式编程后记 下面简单地说说每一章都讨论了什么。 正则是匹配模式，要么匹配字符，要么匹配位置。 第 1 章和第 2 章以这个角度去讲解了正则的基础。 在正则中可以使用括号捕获数据，要么在 API 中进行分组引用，要么在正则里进行反向引用。 这是第 3 章的主题，讲解了正则中括号的作用。 学习正则表达式，是需要了解其匹配原理的。第 4 章，讲解了正则了正则表达式的回溯法原理。另外在第 6 章里，也讲解了正则的表达式的整体工作原理。 不仅能看懂别人的正则，还要自己会写正则。 第 5 章，是从读的角度，去拆分一个正则表达式，而第 6 章是从写的角度，去构建一个正则表达式。 学习正则，是为了在真实世界里应用的。 第 7 章讲解了正则的用法，和相关 API 需要注意的地方。 第 1 章 正则表达式字符匹配攻略 正则表达式是匹配模式，要么匹配字符，要么匹配位置。请记住这句话。 然而关于正则如何匹配字符的学习，大部分人都觉得这块比较杂乱。 毕竟元字符太多了，看起来没有系统性，不好记。本章就解决这个问题。 内容包括： 两种模糊匹配 字符组 量词 多选分支 案例分析 1.1 两种模糊匹配 如果正则只有精确匹配是没多大意义的，比如 /hello/，也只能匹配字符串中的 &quot;hello&quot; 这个子串。 const regex = /hello/; console.log(regex.test('hello')); // =&gt; true 正则表达式之所以强大，是因为其能实现模糊匹配。而模糊匹配，有两个方向上的“模糊”：横向模糊匹配和纵向模糊匹配。 1.1.1 横向模糊匹配 横向模糊指的是，一个正则可匹配的字符串的长度不是固定的，可以是多种情况的。其实现的方式是使用量词。譬如 {m,n}，表示连续出现最少 m 次，最多 n 次。比如 /ab{2,5}c/ 表示匹配这样一个字符串：第一个字符是 &quot;a&quot;，接下来是 2 到 5 个字符是 &quot;b&quot;，最后是字符 &quot;c&quot;。测试如下： const regex = /ab{2,5}c/g; const string = 'abc abbc abbbc abbbbc abbbbbc abbbbbbc'; console.log(string.match(regex)); // =&gt; [&quot;abbc&quot;, &quot;abbbc&quot;, &quot;abbbbc&quot;, &quot;abbbbbc&quot;] 注意：案例中用的正则是 /ab{2,5}c/g，后面多了 g，它是正则的一个修饰符。表示全局匹配，即在目标字符串中按顺序找到满足匹配模式的所有子串，强调的是“所有”，而不只是“第一个”。g 是单词 global 的首字母。 1.1.2 纵向模糊匹配 纵向模糊指的是，一个正则匹配的字符串，具体到某一位字符时，它可以不是某个确定的字符，可以有多种可能。其实现的方式是使用字符组。譬如 [abc]，表示该字符是可以字符 &quot;a&quot;、&quot;b&quot;、&quot;c&quot; 中的任何一个。比如 /a[123]b/ 可以匹配如下三种字符串：&quot;a1b&quot;、&quot;a2b&quot;、&quot;a3b&quot;。测试如下： const regex = /a[123]b/g; const string = 'a0b a1b a2b a3b a4b'; console.log(string.match(regex)); // =&gt; [&quot;a1b&quot;, &quot;a2b&quot;, &quot;a3b&quot;] 以上就是本章讲的主体内容，只要掌握横向和纵向模糊匹配，就能解决很大部分正则匹配问题。 接下来的内容就是展开说了，如果对此都比较熟悉的话，可以跳过，直接看本章案例那节。 1.2 字符组 需要强调的是，虽叫字符组（字符类），但只是其中一个字符。例如 [abc]，表示匹配一个字符，它可以是 &quot;a&quot;、&quot;b&quot;、&quot;c&quot; 之一。 1.2.1 范围表示法 如果字符组里的字符特别多的话，怎么办？可以使用范围表示法。 比如 [123456abcdefGHIJKLM]，可以写成 [1-6a-fG-M]。用连字符 - 来省略和简写。 因为连字符有特殊用途，那么要匹配 &quot;a&quot;、&quot;-&quot;、&quot;z&quot; 这三者中任意一个字符，该怎么做呢？ 不能写成 [a-z]，因为其表示小写字符中的任何一个字符。 可以写成如下的方式：[-az] 或[az-] 或 [a\\-z]。即要么放在开头，要么放在结尾，要么转义。总之不会让引擎认为是范围表示法就行了。 1.2.2 排除字符组 纵向模糊匹配，还有一种情形就是，某位字符可以是任何东西，但就不能是 &quot;a&quot;、&quot;b&quot;、&quot;c&quot;。 此时就是排除字符组（反义字符组）的概念。例如 [^abc]，表示是一个除 &quot;a&quot;、&quot;b&quot;、&quot;c&quot; 之外的任意一个字符。字符组的第一位放 ^（脱字符），表示求反的概念。 当然，也有相应的范围表示法。 1.2.3 常见的简写形式 有了字符组的概念后，一些常见的符号我们也就理解了。因为它们都是系统自带的简写形式。 \\d 就是 [0-9]。表示是一位数字。记忆方式：其英文是 digit（数字）； \\D 就是 [^0-9]。表示除数字外的任意字符； \\w 就是 [0-9a-zA-Z_]。表示数字、大小写字母和下划线。记忆方式：w 是 word 的简写，也称单词字符； \\W 是 [^0-9a-zA-Z_]。非单词字符； \\s是 [ \\t\\v\\n\\r\\f]。表示空白符，包括空格、水平制表符、垂直制表符、换行符、回车符、换页符。记忆方式：s 是 space character 的首字母； \\S 是 [^ \\t\\v\\n\\r\\f]。 非空白符。 . 就是 [^\\n\\r\\u2028\\u2029]。通配符，表示几乎任意字符。换行符、回车符、行分隔符和段分隔符除外。记忆方式：想想省略号...中的每个点，都可以理解成占位符，表示任何类似的东西。 如果要匹配任意字符怎么办？可以使用 [\\d\\D]、[\\w\\W]、[\\s\\S] 和 [^] 中任何的一个。 1.3 量词 量词也称重复。掌握 {m,n} 的准确含义后，只需要记住一些简写形式。 1.3.1 简写形式 {m,} 表示至少出现 m 次。 {m} 等价于 {m,m}，表示出现 m 次。 ? 等价于 {0,1}，表示出现或者不出现。记忆方式：问号的意思表示，有吗？ + 等价于 {1,}，表示出现至少一次。记忆方式：加号是追加的意思，得先有一个，然后才考虑追加。 * 等价于 {0,}，表示出现任意次，有可能不出现。记忆方式：看看天上的星星，可能一颗没有，可能零散有几颗，可能数也数不过来。 1.3.2 贪婪匹配和惰性匹配 看如下的例子： const regex = /\\d{2,5}/g; const string = '123 1234 12345 123456'; console.log(string.match(regex)); // =&gt; [&quot;123&quot;, &quot;1234&quot;, &quot;12345&quot;, &quot;12345&quot;] 其中正则 /\\d{2,5}/，表示数字连续出现 2 到 5 次。会匹配 2 位、3 位、4 位、5 位连续数字。 但是其是贪婪的，它会尽可能多的匹配。你能给我 6 个，我就要 5 个。你能给我3个，我就要 3 个。反正只要在能力范围内，越多越好。 我们知道有时贪婪不是一件好事（请看文章最后一个例子）。而惰性匹配，就是尽可能少的匹配： const regex = /\\d{2,5}?/g; const string = '123 1234 12345 123456'; console.log(string.match(regex)); // =&gt; [&quot;12&quot;, &quot;12&quot;, &quot;34&quot;, &quot;12&quot;, &quot;34&quot;, &quot;12&quot;, &quot;34&quot;, &quot;56&quot;] 其中 /\\d{2,5}?/ 表示，虽然 2 到 5 次都行，当 2 个就够的时候，就不在往下尝试了。 通过在量词后面加个问号就能实现惰性匹配，因此所有惰性匹配情形如下： {m,n}? {m,}? ?? +? *? 1.4 多选分支 一个模式可以实现横向和纵向模糊匹配。而多选分支可以支持多个子模式任选其一。 具体形式如下：(p1|p2|p3)，其中 p1、p2 和 p3 是子模式，用 |（管道符）分隔，表示其中任何之一。 例如要匹配 &quot;good&quot; 和 &quot;nice&quot; 可以使用 /good|nice/。测试如下： const regex = /good|nice/g; const string = 'good idea, nice try.'; console.log(string.match(regex)); // =&gt; [&quot;good&quot;, &quot;nice&quot;] 但有个事实我们应该注意，比如我用 /good|goodbye/，去匹配 &quot;goodbye&quot; 字符串时，结果是 &quot;good&quot;： const regex = /good|goodbye/g; const string = 'goodbye'; console.log(string.match(regex)); // =&gt; [&quot;good&quot;] 而把正则改成 /goodbye|good/，结果是： var regex = /goodbye|good/g; var string = 'goodbye'; console.log(string.match(regex)); // =&gt; [&quot;goodbye&quot;] 也就是说，分支结构也是惰性的，即当前面的匹配上了，后面的就不再尝试了。 1.5 案例分析 匹配字符，无非就是字符组、量词和分支结构的组合使用罢了。 下面找几个例子演练一下（其中，每个正则并不是只有唯一写法）： 1.5.1 匹配 16 进制颜色值 要求匹配： #ffbbad #Fc01DF #FFF #ffE 分析： 表示一个 16 进制字符，可以用字符组 [0-9a-fA-F]。 其中字符可以出现 3 或 6 次，需要是用量词和分支结构。 使用分支结构时，需要注意顺序。 正则如下： const regex = /#([0-9a-fA-F]{6}|[0-9a-fA-F]{3})/g; const string = '#ffbbad #Fc01DF #FFF #ffE'; console.log(string.match(regex)); // =&gt; [&quot;#ffbbad&quot;, &quot;#Fc01DF&quot;, &quot;#FFF&quot;, &quot;#ffE&quot;] 1.5.2 匹配时间 以 24 小时制为例。 要求匹配： 23:59 02:07 分析： 共 4 位数字，第一位数字可以为 [0-2]； 当第 1 位为 2 时，第2位可以为 [0-3]，其他情况时，第 2 位为 [0-9]； 第 3 位数字为 [0-5]，第 4 位为 [0-9]。 正则如下： const regex = /^([01][0-9]|[2][0-3]):[0-5][0-9]$/; console.log(regex.test('23:59')); console.log(regex.test('02:07')); // =&gt; true // =&gt; true 如果也要求匹配 7:9，也就是说时分前面的 0 可以省略。 此时正则变成： const regex = /^(0?[0-9]|1[0-9]|[2][0-3]):(0?[0-9]|[1-5][0-9])$/; console.log( regex.test('23:59') ); console.log( regex.test('02:07') ); console.log( regex.test('7:9') ); // =&gt; true // =&gt; true // =&gt; true 1.5.3 匹配日期 比如 yyyy-mm-dd 格式为例。 要求匹配： 2017-06-10 分析： 年，四位数字即可，可用 [0-9]{4}； 月，共 12 个月，分两种情况 01、02、……、09 和 10、11、12，可用 (0[1-9]|1[0-2])； 日，最大 31 天，可用 (0[1-9]|[12][0-9]|3[01])。 正则如下： const regex = /^[0-9]{4}-(0[1-9]|1[0-2])-(0[1-9]|[12][0-9]|3[01])$/; console.log(regex.test('2017-06-10')); // =&gt; true 1.5.4 Window 操作系统文件路径 要求匹配： F:\\study\\javascript\\regex\\regular expression.pdf F:\\study\\javascript\\regex\\ F:\\study\\javascript -F:\\ 分析： 整体模式是: 盘符:\\文件夹\\文件夹\\文件夹\\； 其中匹配 F:\\，需要使用[a-zA-Z]:\\\\，其中盘符不区分大小写，注意 \\ 字符需要转义； 文件名或者文件夹名，不能包含一些特殊字符，此时我们需要排除字符组 [^\\\\:*&lt;&gt;|&quot;?\\r\\n/] 来表示合法字符。另外不能为空名，至少有一个字符，也就是要使用量词 +。因此匹配 &quot;文件夹&quot;，可用[^\\\\:*&lt;&gt;|&quot;?\\r\\n/]+\\\\； 另外“文件夹\\”，可以出现任意次。也就是 ([^\\\\:*&lt;&gt;|&quot;?\\r\\n/]+\\\\)*。其中括号提供子表达式； 路径的最后一部分可以是“文件夹”，没有 \\，因此需要添加 ([^\\\\:*&lt;&gt;|&quot;?\\r\\n/]+)?。 最后拼接成了一个看起来比较复杂的正则： const regex = /^[a-zA-Z]:\\\\([^\\\\:*&lt;&gt;|&quot;?\\r\\n/]+\\\\)*([^\\\\:*&lt;&gt;|&quot;?\\r\\n/]+)?$/; console.log(regex.test('F:\\\\study\\\\javascript\\\\regex\\\\regular expression.pdf')); console.log(regex.test('F:\\\\study\\\\javascript\\\\regex\\\\')); console.log(regex.test('F:\\\\study\\\\javascript')); console.log(regex.test('F:\\\\')); // =&gt; true // =&gt; true // =&gt; true // =&gt; true 其中，JS 中字符串表示 \\ 时，也要转义。 1.5.5 匹配 id 要求从 &lt;div id=&quot;container&quot; class=&quot;main&quot;&gt;&lt;/div&gt; 提取出 id=&quot;container&quot;。 可能最开始想到的正则是： const regex = /id=&quot;.*&quot;/ const string = '&lt;div id=&quot;container&quot; class=&quot;main&quot;&gt;&lt;/div&gt;'; console.log(string.match(regex)[0]); // =&gt; id=&quot;container&quot; class=&quot;main&quot; 因为 . 是通配符，本身就匹配双引号的，而量词 * 又是贪婪的，当遇到 container 后面双引号时，不会停下来，会继续匹配，直到遇到最后一个双引号为止。 解决之道，可以使用惰性匹配： const regex = /id=&quot;.*?&quot;/ const string = '&lt;div id=&quot;container&quot; class=&quot;main&quot;&gt;&lt;/div&gt;'; console.log(string.match(regex)[0]); // =&gt; id=&quot;container&quot; 当然，这样也会有个问题。效率比较低，因为其匹配原理会涉及到“回溯”这个概念（这里也只是顺便提一下，第四章会详细说明）。可以优化如下： const regex = /id=&quot;[^&quot;]*&quot;/ const string = '&lt;div id=&quot;container&quot; class=&quot;main&quot;&gt;&lt;/div&gt;'; console.log(string.match(regex)[0]); // =&gt; id=&quot;container&quot; 第 1 章小结 字符匹配相关的案例，挺多的，不一而足。 掌握字符组和量词就能解决大部分常见的情形，也就是说，当你会了这二者，JS 正则算是入门了。 第 2 章 正则表达式位置匹配攻略 正则表达式是匹配模式，要么匹配字符，要么匹配位置。请记住这句话。 然而大部分人学习正则时，对于匹配位置的重视程度没有那么高。 本章讲讲正则匹配位置的总总。 内容包括： 什么是位置？ 如何匹配位置？ 位置的特性 几个应用实例分析 2.1 什么是位置呢？ 位置是相邻字符之间的位置。比如，下图中箭头所指的地方： 2.2. 如何匹配位置呢？ 在 ES5 中，共有 6 个锚字符：^ $ \\b \\B (?=p) (?!p)。 2.2.1 ^ 和 $ ^（脱字符）匹配开头，在多行匹配中匹配行开头； $（美元符号）匹配结尾，在多行匹配中匹配行结尾。 比如我们把字符串的开头和结尾用“#”替换（位置是可以替换成字符的！）： const result = 'hello'.replace(/^|$/g, '#'); console.log(result); // =&gt; &quot;#hello#&quot; 多行匹配模式时，二者是行的概念，这个需要我们的注意： const result = 'I\\nlove\\njavascript'.replace(/^|$/gm, '#'); console.log(result); /* #I# #love# #javascript# */ 2.2.2 \\b 和 \\B \\b 是单词边界，具体就是 \\w 和 \\W 之间的位置，也包括 \\w 和 ^ 之间的位置，也包括 \\w和 $ 之间的位置。 比如一个文件名是 &quot;[JS] Lesson_01.mp4&quot; 中的 \\b，如下： const result = '[JS] Lesson_01.mp4'.replace(/\\b/g, '#'); console.log(result); // =&gt; &quot;[#JS#] #Lesson_01#.#mp4#&quot; 为什么是这样呢？这需要仔细看看。 首先，我们知道，\\w 是字符组 [0-9a-zA-Z_] 的简写形式，即 \\w 是字母数字或者下划线的中任何一个字符。而 \\W 是排除字符组 [^0-9a-zA-Z_] 的简写形式，即 \\W 是 \\w 以外的任何一个字符。此时我们可以看看 &quot;[#JS#] #Lesson_01#.#mp4#&quot; 中的每一个 &quot;#&quot;，是怎么来的。 第一个&quot;#&quot;，两边是 &quot;[&quot;与 &quot;J&quot;，是 \\W 和 \\w 之间的位置； 第二个&quot;#&quot;，两边是 &quot;S&quot; 与 &quot;]&quot;，也就是 \\w 和 \\W 之间的位置； 第三个&quot;#&quot;，两边是空格与 &quot;L&quot;，也就是 \\W 和 \\w 之间的位置； 第四个&quot;#&quot;，两边是 &quot;1&quot; 与 &quot;.&quot;，也就是 \\w 和 \\W 之间的位置； 第五个&quot;#&quot;，两边是 &quot;.&quot;与 &quot;m&quot;，也就是 \\W 和 \\w 之间的位置； 第六个&quot;#&quot;，其对应的位置是结尾，但其前面的字符 &quot;4&quot; 是 \\w，即 \\w 和 $ 之间的位置。 知道了 \\b 的概念后，那么 \\B 也就相对好理解了。 \\B 就是 \\b 的反面的意思，非单词边界。例如在字符串中所有位置中，扣掉 \\b，剩下的都是 \\B 的。 具体说来就是 \\w 与 \\w、\\W 与 \\W、^ 与 \\W，\\W 与 $ 之间的位置。 比如上面的例子，把所有 \\B 替换成 &quot;#&quot;： const result = '[JS] Lesson_01.mp4'.replace(/\\B/g, '#'); console.log(result); // =&gt; &quot;#[J#S]# L#e#s#s#o#n#_#0#1.m#p#4&quot; 2.2.3 (?=p) 和 (?!p) (?=p)，其中 p 是一个子模式，即 p 前面的位置。 比如 (?=l)，表示 &quot;l&quot; 字符前面的位置，例如： const result = 'hello'.replace(/(?=l)/g, '#'); console.log(result); // =&gt; &quot;he#l#lo&quot; 而 (?!p) 就是 (?=p) 的反面意思，比如： const result = 'hello'.replace(/(?!l)/g, '#'); console.log(result); // =&gt; &quot;#h#ell#o#&quot; 二者的学名分别是 positive lookahead 和 negative lookahead。 中文翻译分别是正向先行断言和负向先行断言。 ES6中，还支持 positive lookbehind 和 negative lookbehind。 具体是 (?&lt;=p) 和 (?&lt;!p)。 也有书上把这四个东西，翻译成环视，即看看右边或看看左边。 但一般书上，没有很好强调这四者是个位置。 比如 (?=p)，一般都理解成：要求接下来的字符与 p 匹配，但不能包括 p 的那些字符。 而在本人看来 (?=p) 就与 ^ 一样好理解，就是 p 前面的那个位置。 2.3. 位置的特性 对于位置的理解，我们可以理解成空字符&quot;&quot;。 比如 &quot;hello&quot; 字符串等价于如下的形式： 'hello' == '' + 'h' + '' + 'e' + '' + 'l' + '' + 'l' + 'o' + ''; 也等价于： 'hello' == '' + '' + 'hello'; 因此，把 /^hello$/ 写成 /^^hello$$$/，是没有任何问题的： const result = /^^hello$$$/.test('hello'); console.log(result); // =&gt; true 甚至可以写成更复杂的： const result = /(?=he)^^he(?=\\w)llo$\\b\\b$/.test('hello'); console.log(result); // =&gt; true 也就是说字符之间的位置，可以写成多个。 把位置理解空字符，是对位置非常有效的理解方式。 2.4 相关案例 2.4.1 不匹配任何东西的正则 让你写个正则不匹配任何东西 easy，/.^/ 因为此正则要求只有一个字符，但该字符后面是开头。 2.4.2 数字的千位分隔符表示法 比如把 &quot;12345678&quot;，变成 &quot;12,345,678&quot;。 可见是需要把相应的位置替换成 &quot;,&quot;。 思路是什么呢？ 2.4.2.1 弄出最后一个逗号 使用 (?=\\d{3}$) 就可以做到： const result = '12345678'.replace(/(?=\\d{3}$)/g, ',') console.log(result); // =&gt; &quot;12345,678&quot; 2.4.2.2 弄出所有的逗号 因为逗号出现的位置，要求后面 3 个数字一组，也就是 \\d{3} 至少出现一次。 此时可以使用量词 +： const result = '12345678'.replace(/(?=(\\d{3})+$)/g, ',') console.log(result); // =&gt; &quot;12,345,678&quot; 2.4.2.3 匹配其余案例 写完正则后，要多验证几个案例，此时我们会发现问题： const result = '123456789'.replace(/(?=(\\d{3})+$)/g, ',') console.log(result); // =&gt; &quot;,123,456,789&quot; 因为上面的正则，仅仅表示把从结尾向前数，一但是 3 的倍数，就把其前面的位置替换成逗号。因此才会出现这个问题。 怎么解决呢？我们要求匹配的到这个位置不能是开头。 我们知道匹配开头可以使用 ^，但要求这个位置不是开头怎么办？easy，(?!^)，你想到了吗？测试如下： const string1 = '12345678'; const string2 = '123456789'; const reg = /(?!^)(?=(\\d{3})+$)/g; const result = string1.replace(reg, ',') console.log(result); // =&gt; &quot;12,345,678&quot; result = string2.replace(reg, ','); console.log(result); // =&gt; &quot;123,456,789&quot; 2.4.2.4 支持其他形式 如果要把 &quot;12345678 123456789&quot; 替换成 &quot;12,345,678 123,456,789&quot;。 此时我们需要修改正则，把里面的开头 ^ 和结尾 $，替换成 \\b： const string = '12345678 123456789'; const reg = /(?!\\b)(?=(\\d{3})+\\b)/g; const result = string.replace(reg, ','); console.log(result); // =&gt; &quot;12,345,678 123,456,789&quot; 其中 (?!\\b) 怎么理解呢？ 要求当前是一个位置，但不是 \\b 前面的位置，其实 (?!\\b) 说的就是 \\B。 因此最终正则变成了：/\\B(?=(\\d{3})+\\b)/g。 2.4.3 验证密码问题 密码长度 6-12 位，由数字、小写字符和大写字母组成，但必须至少包括 2 种字符。 此题，如果写成多个正则来判断，比较容易。但要写成一个正则就比较困难。 那么，我们就来挑战一下。看看我们对位置的理解是否深刻。 2.4.3.1 简化 不考虑“但必须至少包括 2 种字符”这一条件。我们可以容易写出： const reg = /^[0-9A-Za-z]{6,12}$/; 2.4.3.2 判断是否包含有某一种字符 假设，要求的必须包含数字，怎么办？此时我们可以使用 (?=.*[0-9]) 来做。 因此正则变成： const reg = /(?=.*[0-9])^[0-9A-Za-z]{6,12}$/; 2.4.3.3 同时包含具体两种字符 比如同时包含数字和小写字母，可以用 (?=.*[0-9])(?=.*[a-z]) 来做。 因此正则变成： const reg = /(?=.*[0-9])(?=.*[a-z])^[0-9A-Za-z]{6,12}$/; 2.4.3.4 解答 我们可以把原题变成下列几种情况之一： 同时包含数字和小写字母； 同时包含数字和大写字母； 同时包含小写字母和大写字母； 同时包含数字、小写字母和大写字母。 以上的 4 种情况是或的关系（实际上，可以不用第 4 条）。 最终答案是： const reg = /((?=.*[0-9])(?=.*[a-z])|(?=.*[0-9])(?=.*[A-Z])|(?=.*[a-z])(?=.*[A-Z]))^[0-9A-Za-z]{6,12}$/; console.log(reg.test('1234567')); // false 全是数字 console.log(reg.test('abcdef')); // false 全是小写字母 console.log(reg.test('ABCDEFGH')); // false 全是大写字母 console.log(reg.test('ab23C')); // false 不足 6 位 console.log(reg.test('ABCDEF234')); // true 大写字母和数字 console.log(reg.test('abcdEF234')); // true 三者都有 2.4.3.5 解惑 上面的正则看起来比较复杂，只要理解了第二步，其余就全部理解了。 /(?=.*[0-9])^[0-9A-Za-z]{6,12}$/ 对于这个正则，我们只需要弄明白 (?=.*[0-9])^即可。 分开来看就是 (?=.*[0-9]) 和 ^。 表示开头前面还有个位置（当然也是开头，即同一个位置，想想之前的空字符类比）。 (?=.*[0-9]) 表示该位置后面的字符匹配 .*[0-9]，即，有任何多个任意字符，后面再跟个数字。 翻译成大白话，就是接下来的字符，必须包含个数字。 2.4.3.6 另外一种解法 “至少包含两种字符”的意思就是说，不能全部都是数字，也不能全部都是小写字母，也不能全部都是大写字母。 那么要求“不能全部都是数字”，怎么做呢？(?!p) 出马！ 对应的正则是： const reg = /(?!^[0-9]{6,12}$)^[0-9A-Za-z]{6,12}$/; 三种“都不能”呢？ 最终答案是： const reg = /(?!^[0-9]{6,12}$)(?!^[a-z]{6,12}$)(?!^[A-Z]{6,12}$)^[0-9A-Za-z]{6,12}$/; console.log(reg.test('1234567')); // false 全是数字 console.log(reg.test('abcdef')); // false 全是小写字母 console.log(reg.test('ABCDEFGH')); // false 全是大写字母 console.log(reg.test('ab23C')); // false 不足 6 位 console.log(reg.test('ABCDEF234')); // true 大写字母和数字 console.log(reg.test('abcdEF234')); // true 三者都有 第 2 章小结 位置匹配相关的案例，挺多的，不一而足。 掌握匹配位置的这 6 个锚字符，给我们解决正则问题一个新工具。 第 3 章 正则表达式括号的作用 不管哪门语言中都有括号。正则表达式也是一门语言，而括号的存在使这门语言更为强大。 对括号的使用是否得心应手，是衡量对正则的掌握水平的一个侧面标准。 括号的作用，其实三言两语就能说明白，括号提供了分组，便于我们引用它。 引用某个分组，会有两种情形：在 JavaScript 里引用它，在正则表达式里引用它。 本章内容虽相对简单，但我也要写长点。内容包括： 分组和分支结构 引用分组 反向引用 非捕获分组 相关案例 3.1 分组和分支结构 这二者是括号最直觉的作用，也是最原始的功能。 3.1.1 分组 我们知道 /a+/ 匹配连续出现的 &quot;a&quot;，而要匹配连续出现的 &quot;ab&quot; 时，需要使用 /(ab)+/。 其中括号是提供分组功能，使量词 + 作用于 &quot;ab&quot; 这个整体，测试如下： const regex = /(ab)+/g; const string = 'ababa abbb ababab'; console.log(string.match(regex)); // =&gt; [&quot;abab&quot;, &quot;ab&quot;, &quot;ababab&quot;] 3.1.2 分支结构 而在多选分支结构 (p1|p2) 中，此处括号的作用也是不言而喻的，提供了子表达式的所有可能。 比如，要匹配如下的字符串： I love JavaScript I love Regular Expression 可以使用正则： const regex = /^I love (JavaScript|Regular Expression)$/; console.log(regex.test('I love JavaScript')); console.log(regex.test('I love Regular Expression')); // =&gt; true // =&gt; true 如果去掉正则中的括号，即 /^I love JavaScript|Regular Expression$/，匹配字符串是 &quot;I love JavaScript&quot; 和 &quot;Regular Expression&quot;，当然这不是我们想要的。 3.2 引用分组 这是括号一个重要的作用，有了它，我们就可以进行数据提取，以及更强大的替换操作。 而要使用它带来的好处，必须配合使用实现环境的 API。 以日期为例。假设格式是 yyyy-mm-dd 的，我们可以先写一个简单的正则： const regex = /\\d{4}-\\d{2}-\\d{2}/; 然后再修改成括号版的： const regex = /(\\d{4})-(\\d{2})-(\\d{2})/; 为什么要使用这个正则呢？ 3.2.1 提取数据 比如提取出年、月、日，可以这么做： const regex = /(\\d{4})-(\\d{2})-(\\d{2})/; const string = '2017-06-12'; console.log(string.match(regex)); // =&gt; [&quot;2017-06-12&quot;, &quot;2017&quot;, &quot;06&quot;, &quot;12&quot;, index: 0, input: &quot;2017-06-12&quot;] match 返回的一个数组，第一个元素是整体匹配结果，然后是各个分组（括号里）匹配的内容，然后是匹配下标，最后是输入的文本。（注意：如果正则是否有修饰符 g，match 返回的数组格式是不一样的）。 另外也可以使用正则对象的 exec 方法： const regex = /(\\d{4})-(\\d{2})-(\\d{2})/; const string = '2017-06-12'; console.log(regex.exec(string)); // =&gt; [&quot;2017-06-12&quot;, &quot;2017&quot;, &quot;06&quot;, &quot;12&quot;, index: 0, input: &quot;2017-06-12&quot;] 同时，也可以使用构造函数的全局属性 $1 至 $9 来获取： const regex = /(\\d{4})-(\\d{2})-(\\d{2})/; const string = '2017-06-12'; regex.test(string); // 正则操作即可，例如 //regex.exec(string); //string.match(regex); console.log(RegExp.$1); // &quot;2017&quot; console.log(RegExp.$2); // &quot;06&quot; console.log(RegExp.$3); // &quot;12&quot; 3.2.2 替换数据 比如，想把 yyyy-mm-dd 格式，替换成 mm/dd/yyyy 怎么做？ const regex = /(\\d{4})-(\\d{2})-(\\d{2})/; const string = '2017-06-12'; const result = string.replace(regex, '$2/$3/$1'); console.log(result); // =&gt; &quot;06/12/2017&quot; 其中 replace 中的，第二个参数里用 $1、$2、$3 指代相应的分组。等价于如下的形式： const regex = /(\\d{4})-(\\d{2})-(\\d{2})/; const string = '2017-06-12'; const result = string.replace(regex, function() { return RegExp.$2 + '/' + RegExp.$3 + '/' + RegExp.$1; }); console.log(result); // =&gt; &quot;06/12/2017&quot; 也等价于： const regex = /(\\d{4})-(\\d{2})-(\\d{2})/; const string = '2017-06-12'; const result = string.replace(regex, function(match, year, month, day) { return month + '/' + day + '/' + year; }); console.log(result); // =&gt; &quot;06/12/2017&quot; 3.3 反向引用 除了使用相应 API 来引用分组，也可以在正则本身里引用分组。但只能引用之前出现的分组，即反向引用。 还是以日期为例。 比如要写一个正则支持匹配如下三种格式： 2016-06-12 2016/06/12 2016.06.12 最先可能想到的正则是： const regex = /\\d{4}(-|\\/|\\.)\\d{2}(-|\\/|\\.)\\d{2}/; const string1 = '2017-06-12'; const string2 = '2017/06/12'; const string3 = '2017.06.12'; const string4 = '2016-06/12'; console.log(regex.test(string1)); // true console.log(regex.test(string2)); // true console.log(regex.test(string3)); // true console.log(regex.test(string4)); // true 其中 / 和 . 需要转义。虽然匹配了要求的情况，但也匹配 &quot;2016-06/12&quot; 这样的数据。 假设我们想要求分割符前后一致怎么办？此时需要使用反向引用： const regex = /\\d{4}(-|\\/|\\.)\\d{2}\\1\\d{2}/; const string1 = '2017-06-12'; const string2 = '2017/06/12'; const string3 = '2017.06.12'; const string4 = '2016-06/12'; console.log(regex.test(string1)); // true console.log(regex.test(string2)); // true console.log(regex.test(string3)); // true console.log(regex.test(string4)); // false 注意里面的 \\1，表示的引用之前的那个分组 (-|\\/|\\.)。不管它匹配到什么（比如 -），\\1 都匹配那个同样的具体某个字符。 我们知道了 \\1 的含义后，那么 \\2 和 \\3 的概念也就理解了，即分别指代第二个和第三个分组。 看到这里，此时，恐怕你会有三个问题。 3.3.1 括号嵌套怎么办？ 以左括号（开括号）为准。比如： const regex = /^((\\d)(\\d(\\d)))\\1\\2\\3\\4$/; const string = '1231231233'; console.log(regex.test(string)); // true console.log(RegExp.$1); // 123 console.log(RegExp.$2 ); // 1 console.log(RegExp.$3); // 23 console.log(RegExp.$4); // 3 我们可以看看这个正则匹配模式： 第一个字符是数字，比如说 1， 第二个字符是数字，比如说 2， 第三个字符是数字，比如说 3， 接下来的是 \\1，是第一个分组内容，那么看第一个开括号对应的分组是什么，是 123， 接下来的是 \\2，找到第 2 个开括号，对应的分组，匹配的内容是 1， 接下来的是 \\3，找到第 3 个开括号，对应的分组，匹配的内容是 23， 最后的是 \\4，找到第 3 个开括号，对应的分组，匹配的内容是 3。 这个问题，估计仔细看一下，就该明白了。 3.3.2 \\10 表示什么呢？ 另外一个疑问可能是，即 \\10 是表示第 10 个分组，还是 \\1 和 0 呢？ 答案是前者，虽然一个正则里出现 \\10 比较罕见。测试如下： const regex = /(1)(2)(3)(4)(5)(6)(7)(8)(9)(#) \\10+/; const string = '123456789# ######'; console.log(regex.test(string)); // =&gt; true 3.3.3 引用不存在的分组会怎样？ 因为反向引用，是引用前面的分组，但我们在正则里引用了不存在的分组时，此时正则不会报错，只是匹配反向引用的字符本身。例如 \\2，就匹配“\\2”。注意“\\2”表示对“2”进行了转意。 const regex = /\\1\\2\\3\\4\\5\\6\\7\\8\\9/; console.log(regex.test('\\1\\2\\3\\4\\5\\6\\7\\8\\9')); console.log('\\1\\2\\3\\4\\5\\6\\7\\8\\9'.split('')); Chrome 浏览器打印的结果： Node 打印的结果： 4. 非捕获分组 之前文中出现的分组，都会捕获它们匹配到的数据，以便后续引用，因此也称他们是捕获型分组。 如果只想要括号最原始的功能，但不会引用它，即，既不在 API 里引用，也不在正则里反向引用。此时可以使用非捕获分组 (?:p)，例如本文第一个例子可以修改为： const regex = /(?:ab)+/g; const string = 'ababa abbb ababab'; console.log(string.match(regex)); // =&gt; [&quot;abab&quot;, &quot;ab&quot;, &quot;ababab&quot;] 3.5 相关案例 至此括号的作用已经讲完了，总结一句话，就是提供了可供我们使用的分组，如何用就看我们的了。 3.5.1 字符串 trim 方法模拟 trim 方法是去掉字符串的开头和结尾的空白符。有两种思路去做。 第一种，匹配到开头和结尾的空白符，然后替换成空字符。如： function trim(str) { return str.replace(/^\\s+|\\s+$/g, ''); } console.log(trim(' foobar ')); // =&gt; &quot;foobar&quot; 第二种，匹配整个字符串，然后用引用来提取出相应的数据： function trim(str) { return str.replace(/^\\s*(.*?)\\s*$/g, '$1'); } console.log(trim(' foobar ')); // =&gt; &quot;foobar&quot; 这里使用了惰性匹配 *?，不然也会匹配最后一个空格之前的所有空格的。 当然，前者效率高。 3.5.2 将每个单词的首字母转换为大写 function titleize(str) { return str.toLowerCase().replace(/(?:^|\\s)\\w/g, function(c) { return c.toUpperCase(); }); } console.log(titleize('my name is epeli')); // =&gt; &quot;My Name Is Epeli&quot; 思路是找到每个单词的首字母，当然这里不使用非捕获匹配也是可以的。 3.5.3 驼峰化 function camelize(str) { return str.replace(/[-_\\s]+(.)?/g, function(match, c) { return c ? c.toUpperCase() : ''; }); } console.log(camelize('-moz-transform')); // =&gt; &quot;MozTransform&quot; 其中分组 (.) 表示首字母。单词的界定是，前面的字符可以是多个连字符、下划线以及空白符。正则后面的 ? 的目的，是为了应对 str 尾部的字符可能不是单词字符，比如 str 是'-moz-transform'。 3.5.4 中划线化 function dasherize(str) { return str.replace(/([A-Z])/g, '-$1').replace(/[-_\\s]+/g, '-').toLowerCase(); } console.log(dasherize('MozTransform')); // =&gt; &quot;-moz-transform&quot; 驼峰化的逆过程。 3.5.5 html 转义和反转义 // 将HTML特殊字符转换成等值的实体 function escapeHTML(str) { var escapeChars = { '¢' : 'cent', '£' : 'pound', '¥' : 'yen', '€': 'euro', '©' :'copy', '®' : 'reg', '&lt;' : 'lt', '&gt;' : 'gt', '&quot;' : 'quot', '&amp;' : 'amp', '\\'' : '#39' }; return str.replace(new RegExp('[' + Object.keys(escapeChars).join('') +']', 'g'), function(match) { return '&amp;' + escapeChars[match] + ';'; }); } console.log(escapeHTML('&lt;div&gt;Blah blah blah&lt;/div&gt;')); // =&gt; &quot;&amp;lt;div&amp;gt;Blah blah blah&amp;lt;/div&amp;gt&quot;; 其中使用了用构造函数生成的正则，然后替换相应的格式就行了，这个跟本章没多大关系。 倒是它的逆过程，使用了括号，以便提供引用，也很简单，如下： // 实体字符转换为等值的HTML。 function unescapeHTML(str) { var htmlEntities = { nbsp: ' ', cent: '¢', pound: '£', yen: '¥', euro: '€', copy: '©', reg: '®', lt: '&lt;', gt: '&gt;', quot: '&quot;', amp: '&amp;', apos: '\\'' }; return str.replace(/\\&amp;([^;]+);/g, function(match, key) { if (key in htmlEntities) { return htmlEntities[key]; } return match; }); } console.log(unescapeHTML('&amp;lt;div&amp;gt;Blah blah blah&amp;lt;/div&amp;gt;')); // =&gt; &quot;&lt;div&gt;Blah blah blah&lt;/div&gt;&quot; 通过 key 获取相应的分组引用，然后作为对象的键。 3.5.6 匹配成对标签 要求匹配： &lt;title&gt;regular expression&lt;/title&gt; &lt;p&gt;laoyao bye bye&lt;/p&gt; 不匹配： &lt;title&gt;wrong!&lt;/p&gt; 匹配一个开标签，可以使用正则 &lt;[^&gt;]+&gt;， 匹配一个闭标签，可以使用 &lt;\\/[^&gt;]+&gt;， 但是要求匹配成对标签，那就需要使用反向引用，如： const regex = /&lt;([^&gt;]+)&gt;[\\d\\D]*&lt;\\/\\1&gt;/; const string1 = '&lt;title&gt;regular expression&lt;/title&gt;'; const string2 = '&lt;p&gt;laoyao bye bye&lt;/p&gt;'; const string3 = '&lt;title&gt;wrong!&lt;/p&gt;'; console.log(regex.test(string1)); // true console.log(regex.test(string2)); // true console.log(regex.test(string3)); // false 其中开标签 &lt;[^&gt;]+&gt; 改成 &lt;([^&gt;]+)&gt;，使用括号的目的是为了后面使用反向引用，而提供分组。闭标签使用了反向引用，&lt;\\/\\1&gt;。 另外 [\\d\\D] 的意思是，这个字符是数字或者不是数字，因此，也就是匹配任意字符的意思。 第 3 章小结 正则中使用括号的例子那可是太多了，不一而足。 重点理解括号可以提供分组，我们可以提取数据，应该就可以了。 例子中的代码，基本没做多少分析，相信你都能看懂的。 第 4 章 正则表达式回溯法原理 学习正则表达式，是需要懂点儿匹配原理的。 而研究匹配原理时，有两个字出现的频率比较高：“回溯”。 听起来挺高大上，确实还有很多人对此不明不白的。 因此，本章就简单扼要地说清楚回溯到底是什么东西。 内容包括： 没有回溯的匹配 有回溯的匹配 常见的回溯形式 4.1 没有回溯的匹配 假设我们的正则是 /ab{1,3}c/，其可视化形式是： 而当目标字符串是 &quot;abbbc&quot; 时，就没有所谓的“回溯”。其匹配过程是： 其中子表达式 b{1,3} 表示 &quot;b&quot;字符连续出现 1 到 3 次。 4.2 有回溯的匹配 如果目标字符串是 &quot;abbc&quot;，中间就有回溯。 图中第 5 步有红颜色，表示匹配不成功。此时 b{1,3} 已经匹配到了 2 个字符 &quot;b&quot;，准备尝试第三个时，结果发现接下来的字符是 &quot;c&quot;。那么就认为 b{1,3} 就已经匹配完毕。然后状态又回到之前的状态（即第 6 步，与第 4 步一样），最后再用子表达式 c，去匹配字符 &quot;c&quot;。当然，此时整个表达式匹配成功了。 图中的第 6 步，就是“回溯”。 你可能对此没有感觉，这里我们再举一个例子。正则是： 目标字符串是 &quot;abbbc&quot;，匹配过程是： 其中第 7 步和第 10 步是回溯。第 7 步与第 4 步一样，此时 b{1,3} 匹配了两个 &quot;b&quot;，而第 10 步与第 3 步一样，此时 b{1,3} 只匹配了一个 &quot;b&quot;，这也是 b{1,3} 的最终匹配结果。 这里再看一个清晰的回溯，正则是： 目标字符串是：&quot;acd&quot;ef，匹配过程是： 图中省略了尝试匹配双引号失败的过程。可以看出 .* 是非常影响效率的。 为了减少一些不必要的回溯，可以把正则修改为 /&quot;[^&quot;]*&quot;/。 4.3 常见的回溯形式 正则表达式匹配字符串的这种方式，有个学名，叫回溯法。 回溯法也称试探法，它的基本思想是：从问题的某一种状态（初始状态）出发，搜索从这种状态出发所能达到的所有“状态”，当一条路走到“尽头”的时候（不能再前进），再后退一步或若干步，从另一种可能“状态”出发，继续搜索，直到所有的“路径”（状态）都试探过。这种不断“前进”、不断“回溯”寻找解的方法，就称作“回溯法”。（来自百度百科）。 本质上就是深度优先搜索算法。其中退到之前的某一步这一过程，我们称为“回溯”。从上面的描述过程中，可以看出，路走不通时，就会发生“回溯”。即，尝试匹配失败时，接下来的一步通常就是回溯。 道理，我们是懂了。那么 JS 中正则表达式会产生回溯的地方都有哪些呢？ 4.3.1 贪婪量词 之前的例子都是贪婪量词相关的。比如 b{1,3}，因为其是贪婪的，尝试可能的顺序是从多往少的方向去尝试。首先会尝试 &quot;bbb&quot;，然后再看整个正则是否能匹配。不能匹配时，吐出一个 &quot;b&quot;，即在 &quot;bb&quot; 的基础上，再继续尝试。如果还不行，再吐出一个，再试。如果还不行呢？只能说明匹配失败了。 虽然局部匹配是贪婪的，但也要满足整体能正确匹配。否则，皮之不存，毛将焉附？ 此时我们不禁会问，如果当多个贪婪量词挨着存在，并相互有冲突时，此时会是怎样？ 答案是，先下手为强！因为深度优先搜索。测试如下： const string = '12345'; const regex = /(\\d{1,3})(\\d{1,3})/; console.log(string.match(regex)); // =&gt; [&quot;12345&quot;, &quot;123&quot;, &quot;45&quot;, index: 0, input: &quot;12345&quot;] 其中，前面的 \\d{1,3} 匹配的是 &quot;123&quot;，后面的 \\d{1,3} 匹配的是 &quot;45&quot;。 4.3.2 惰性量词 惰性量词就是在贪婪量词后面加个问号。表示尽可能少的匹配，比如： const string = '12345'; const regex = /(\\d{1,3}?)(\\d{1,3})/; console.log(string.match(regex)); // =&gt; [&quot;1234&quot;, &quot;1&quot;, &quot;234&quot;, index: 0, input: &quot;12345&quot;] 其中 \\d{1,3}? 只匹配到一个字符 &quot;1&quot;，而后面的 \\d{1,3} 匹配了 &quot;234&quot;。 虽然惰性量词不贪，但也会有回溯的现象。比如正则是： 目标字符串是 &quot;12345&quot;，匹配过程是： 知道你不贪、很知足，但是为了整体匹配成，没办法，也只能给你多塞点了。因此最后 \\d{1,3}? 匹配的字符是 &quot;12&quot;，是两个数字，而不是一个。 4.3.3 分支结构 我们知道分支也是惰性的，比如/can|candy/，去匹配字符串&quot;candy&quot;，得到的结果是&quot;can&quot;，因为分支会一个一个尝试，如果前面的满足了，后面就不会再试验了。 分支结构，可能前面的子模式会形成了局部匹配，如果接下来表达式整体不匹配时，仍会继续尝试剩下的分支。这种尝试也可以看成一种回溯。 比如正则： 目标字符串是 &quot;candy&quot;，匹配过程： 上面第 5 步，虽然没有回到之前的状态，但仍然回到了分支结构，尝试下一种可能。所以，可以认为它是一种回溯的。 第 4 章小结 其实回溯法，很容易掌握的。 简单总结就是，正因为有多种可能，所以要一个一个试。直到，要么到某一步时，整体匹配成功了；要么最后都试完后，发现整体匹配不成功。 贪婪量词“试”的策略是：买衣服砍价。价钱太高了，便宜点，不行，再便宜点。 惰性量词“试”的策略是：卖东西加价。给少了，再多给点行不，还有点少啊，再给点。 分支结构“试”的策略是：货比三家。这家不行，换一家吧，还不行，再换。 既然有回溯的过程，那么匹配效率肯定低一些。相对谁呢？相对那些 DFA 引擎。 而 JS 的正则引擎是 NFA，NFA 是“非确定型有限自动机”的简写。 大部分语言中的正则都是 NFA，为啥它这么流行呢？ 答：你别看我匹配慢，但是我编译快啊，而且我还有趣哦。 第 5 章 正则表达式的拆分 对于一门语言的掌握程度怎么样，可以有两个角度来衡量：读和写。 不仅要求自己能解决问题，还要看懂别人的解决方案。代码是这样，正则表达式也是这样。 正则这门语言跟其他语言有一点不同，它通常就是一大堆字符，而没有所谓“语句”的概念。 如何能正确地把一大串正则拆分成一块一块的，成为了破解“天书”的关键。 本章就解决这一问题，内容包括： 结构和操作符 注意要点 案例分析 5.1 结构和操作符 编程语言一般都有操作符。只要有操作符，就会出现一个问题。当一大堆操作在一起时，先操作谁，又后操作谁呢？为了不产生歧义，就需要语言本身定义好操作顺序，即所谓的优先级。 而在正则表达式中，操作符都体现在结构中，即由特殊字符和普通字符所代表的一个个特殊整体。 JS 正则表达式中，都有哪些结构呢？ 字符字面量，匹配一个具体字符，包括不用转义的和需要转义的。比如 a 匹配字符 &quot;a&quot;，又比如 \\n 匹配换行符，又比如 \\. 匹配小数点。 字符组，匹配一个字符，可以是多种可能之一，比如 [0-9]，表示匹配一个数字。也有 \\d 的简写形式。另外还有反义字符组，表示可以是除了特定字符之外任何一个字符，比如 [^0-9]，表示一个非数字字符，也有 \\D的简写形式。 量词，表示一个字符连续出现，比如 a{1,3}表示 &quot;a&quot; 字符连续出现 3 次。另外还有常见的简写形式，比如 a+ 表示 &quot;a&quot; 字符连续出现至少一次。 锚字符，匹配一个位置，而不是字符。比如 ^ 匹配字符串的开头，又比如 \\b 匹配单词边界，又比如 (?=\\d) 表示数字前面的位置。 分组，用括号表示一个整体，比如 (ab)+，表示 &quot;ab&quot; 两个字符连续出现多次，也可以使用非捕获分组 (?:ab)+。 选择分支，多个子表达式多选一，比如 abc|bcd，表达式匹配 &quot;abc&quot; 或者 &quot;bcd&quot; 字符子串。 反向引用，比如 \\2，表示引用第 2 个分组。 其中涉及到的操作符有： 转义符 \\ 括号和方括号 (...)、(?:...)、(?=...)、(?!...)、[...] 量词限定符 {m}、{m,n}、{m,}、?、*、+ 位置和序列 ^、$、\\ 元字符、 一般字符 管道符（竖杠）| 上面操作符的优先级从上至下，由高到低。这里，我们来分析一个正则：/ab?(c|de*)+|fg/： 由于括号的存在，所以，(c|de*) 是一个整体结构。 在 (c|de*) 中，注意其中的量词 *，因此 e* 是一个整体结构。 又因为分支结构 | 优先级最低，因此 c 是一个整体、而 de* 是另一个整体。 同理，整个正则分成了 a、b?、(...)+、f、g。而由于分支的原因，又可以分成 ab?(c|de*)+ 和 fg 这两部分。 希望你没被我绕晕，上面的分析可用其可视化形式描述如下： 5.2 注意要点 关于结构和操作符，还是有几点需要强调： 2.1 匹配字符串整体问题 因为是要匹配整个字符串，我们经常会在正则前后中加上锚字符 ^ 和 $。 比如要匹配目标字符串 &quot;abc&quot; 或者 &quot;bcd&quot; 时，如果一不小心，就会写成 /^abc|bcd$/。 而位置字符和字符序列优先级要比竖杠高，故其匹配的结构是： 应该修改成： 5.2.2 量词连缀问题 假设，要匹配这样的字符串： 每个字符为a、b、c任选其一； 字符串的长度是 3 的倍数。 此时正则不能想当然地写成 /^[abc]{3}+$/，这样会报错，说 + 前面没什么可重复的： 此时要修改成： 5.2.3 元字符转义问题 所谓元字符，就是正则中有特殊含义的字符。 所有结构里，用到的元字符总结如下：^ $ . * + ? | \\ / ( ) [ ] { } = ! : - ,。 当匹配上面的字符本身时，可以一律转义： const string = '^$.*+?|\\\\/[]{}=!:-,'; const regex = /\\^\\$\\.\\*\\+\\?\\|\\\\\\/\\[\\]\\{\\}\\=\\!\\:\\-\\,/; console.log(regex.test(string)); // =&gt; true 其中 string 中的 \\ 字符也要转义的。 另外，在 string 中，也可以把每个字符转义，当然，转义后的结果仍是本身： const string = '^$.*+?|\\\\/[]{}=!:-,'; const string2 = '\\^\\$\\.\\*\\+\\?\\|\\\\\\/\\[\\]\\{\\}\\=\\!\\:\\-\\,'; console.log(string == string2); // =&gt; true 现在的问题是，是不是每个字符都需要转义呢？否，看情况。 5.2.3.1 字符组中的元字符 跟字符组相关的元字符有 []、^、-。因此在会引起歧义的地方进行转义。例如开头的 ^ 必须转义，不然会把整个字符组，看成反义字符组。 const string = '^$.*+?|\\\\/[]{}=!:-,'; const regex = /[\\^$.*+?|\\\\/\\[\\]{}=!:\\-,]/g; console.log(string.match(regex)); // =&gt; [&quot;^&quot;, &quot;$&quot;, &quot;.&quot;, &quot;*&quot;, &quot;+&quot;, &quot;?&quot;, &quot;|&quot;, &quot;\\&quot;, &quot;/&quot;, &quot;[&quot;, &quot;]&quot;, &quot;{&quot;, &quot;}&quot;, &quot;=&quot;, &quot;!&quot;, &quot;:&quot;, &quot;-&quot;, &quot;,&quot;] 5.2.3.2 匹配 &quot;[abc]&quot; 和 &quot;{3,5}&quot; 我们知道 [abc]，是个字符组。如果要匹配字符串 &quot;[abc]&quot; 时，该怎么办？ 可以写成 /\\[abc\\]/，也可以写成 /\\[abc]/，测试如下： const string = '[abc]'; const regex = /\\[abc]/g; console.log(string.match(regex)[0]); // =&gt; &quot;[abc]&quot; 只需要在第一个方括号转义即可，因为后面的方括号构不成字符组，正则不会引发歧义，自然不需要转义。 同理，要匹配字符串 &quot;{3,5}&quot;，只需要把正则写成 /\\{3,5}/ 即可。 另外，我们知道量词有简写形式 {m,}，却没有 {,n} 的情况。虽然后者不构成量词的形式，但此时并不会报错。当然，匹配的字符串也是 &quot;{,n}&quot;，测试如下： const string = '{,3}'; const regex = /{,3}/g; console.log(string.match(regex)[0]); // =&gt; &quot;{,3}&quot; 5.2.3.3 其余情况 比如 = ! : - , 等符号，只要不在特殊结构中，也不需要转义。 但是，括号需要前后都转义的，如 /\\(123\\)/。 至于剩下的 ^ $ . * + ? | \\ / 等字符，只要不在字符组内，都需要转义的。 5.3 案例分析 接下来分析两个例子，一个简单的，一个复杂的。 5.3.1 身份证 正则表达式是：/^(\\d{15}|\\d{17}[\\dxX])$/，因为竖杠 | 的优先级最低，所以正则分成了两部分 \\d{15} 和 \\d{17}[\\dxX]。 \\d{15} 表示 15 位连续数字。 \\d{17}[\\dxX] 表示 17 位连续数字，最后一位可以是数字可以大小写字母 &quot;x&quot;。 可视化如下： 5.3.2 IPv4 地址 正则表达式是： /^((0{0,2}\\d|0?\\d{2}|1\\d{2}|2[0-4]\\d|25[0-5])\\.){3}(0{0,2}\\d|0?\\d{2}|1\\d{2}|2[0-4]\\d|25[0-5])$/ 这个正则，看起来非常吓人。但是熟悉优先级后，会立马得出如下的结构： ((...)\\.){3}(...) 上面的两个 (...) 是一样的结构。表示匹配的是 3 位数字。因此整个结构是： 3位数.3位数.3位数.3位数 然后再来分析 (...)： (0{0,2}\\d|0?\\d{2}|1\\d{2}|2[0-4]\\d|25[0-5])(0{0,2}\\d|0?\\d{2}|1\\d{2}|2[0-4]\\d|25[0-5]) 它是一个多选结构，分成 5 个部分： 0{0,2}\\d，匹配一位数，包括 0 补齐的。比如 9、09、009； 0?\\d{2}，匹配两位数，包括 0 补齐的，也包括一位数； 1\\d{2}，匹配 100 到 199; 2[0-4]\\d，匹配 200-249； 25[0-5]，匹配 250-255。 最后来看一下其可视化形式： 第 5 章小结 掌握正则表达式中的优先级后，再看任何正则应该都有信心分析下去了。 至于例子，不一而足，没有写太多。 这里稍微总结一下，竖杠的优先级最低，即最后运算。 只要知道这一点，就能读懂大部分正则。 另外关于元字符转义问题，当自己不确定与否时，尽管去转义，总之是不会错的。 第 6 章 正则表达式的构建 对于一门语言的掌握程度怎么样，可以有两个角度来衡量：读和写。 不仅要看懂别人的解决方案，也要能独立地解决问题。代码是这样，正则表达式也是这样。 与“读”相比，“写”往往更为重要，这个道理是不言而喻的。 对正则的运用，首重就是：如何针对问题，构建一个合适的正则表达式？ 本章就解决该问题，内容包括： 平衡法则 构建正则前提 准确性 效率 6.1 平衡法则 构建正则有一点非常重要，需要做到下面几点的平衡： 匹配预期的字符串 不匹配非预期的字符串 可读性和可维护性 效率 6.2 构建正则前提 6.2.1 是否能使用正则 正则太强大了，以至于我们随便遇到一个操作字符串问题时，都会下意识地去想，用正则该怎么做。但我们始终要提醒自己，正则虽然强大，但不是万能的，很多看似很简单的事情，还是做不到的。 比如匹配这样的字符串：1010010001.... 虽然很有规律，但是只靠正则就是无能为力。 6.2.2 是否有必要使用正则 要认识到正则的局限，不要去研究根本无法完成的任务。同时，也不能走入另一个极端：无所不用正则。能用字符串API解决的简单问题，就不该正则出马。 比如，从日期中提取出年月日，虽然可以使用正则： const string = '2017-07-01'; const regex = /^(\\d{4})-(\\d{2})-(\\d{2})/; console.log(string.match(regex)); // =&gt; [&quot;2017-07-01&quot;, &quot;2017&quot;, &quot;07&quot;, &quot;01&quot;, index: 0, input: &quot;2017-07-01&quot;] 其实，可以使用字符串的 split 方法来做，即可： const string = '2017-07-01'; const result = string.split('-'); console.log(result); // =&gt; [&quot;2017&quot;, &quot;07&quot;, &quot;01&quot;] 比如，判断是否有问号，虽然可以使用： const string = '?id=xx&amp;act=search'; console.log(string.search(/\\?/)); // =&gt; 0 其实，可以使用字符串的 indexOf 方法： const string = '?id=xx&amp;act=search'; console.log(string.indexOf('?')); // =&gt; 0 比如获取子串，虽然可以使用正则： const string = 'JavaScript'; console.log(string.match(/.{4}(.+)/)[1]); // =&gt; Script 其实，可以直接使用字符串的 substring 或 substr 方法来做： const string = 'JavaScript'; console.log(string.substring(4)); // =&gt; Script 6.2.3 是否有必要构建一个复杂的正则 比如密码匹配问题，要求密码长度 6-12 位，由数字、小写字符和大写字母组成，但必须至少包括 2 种字符。 在第 2 章里，我们写出了正则是：/(?!^[0-9]{6,12}$)(?!^[a-z]{6,12}$)(?!^[A-Z]{6,12}$)^[0-9A-Za-z]{6,12}$/ 其实可以使用多个小正则来做： const regex1 = /^[0-9A-Za-z]{6,12}$/; const regex2 = /^[0-9]{6,12}$/; const regex3 = /^[A-Z]{6,12}$/; const regex4 = /^[a-z]{6,12}$/; function checkPassword(string) { if (!regex1.test(string)) return false; if ( regex2.test(string)) return false; if ( regex3.test(string)) return false; if ( regex4.test(string)) return false; return true; } 6.3. 准确性 所谓准确性，就是能匹配预期的目标，并且不匹配非预期的目标。 这里提到了“预期”二字，那么我们就需要知道目标的组成规则。 不然没法界定什么样的目标字符串是符合预期的，什么样的又不是符合预期的。 下面将举例说明，当目标字符串构成比较复杂时，该如何构建正则，并考虑到哪些平衡。 6.3.1 匹配固定电话 比如要匹配如下格式的固定电话号码： 055188888888 0551-88888888 (0551)88888888 第一步，了解各部分的模式规则。 上面的电话，总体上分为区号和号码两部分（不考虑分机号和 +86 的情形）。 区号是 0 开头的 3 到 4 位数字，对应的正则是：0\\d{2,3} 号码是非 0 开头的 7 到 8 位数字，对应的正则是：[1-9]\\d{6,7} 因此，匹配 055188888888 的正则是：/^0\\d{2,3}[1-9]\\d{6,7}$/ 匹配 0551-88888888 的正则是：/^0\\d{2,3}-[1-9]\\d{6,7}$/ 匹配 (0551)88888888 的正则是：/^\\(0\\d{2,3}\\)[1-9]\\d{6,7}$/ 第二步，明确形式关系。 这三者情形是或的关系，可以构建分支： /^0\\d{2,3}[1-9]\\d{6,7}$|^0\\d{2,3}-[1-9]\\d{6,7}$|^\\(0\\d{2,3}\\)[1-9]\\d{6,7}$/ 提取公共部分： /^(0\\d{2,3}|0\\d{2,3}-|\\(0\\d{2,3}\\))[1-9]\\d{6,7}$/ 进一步简写： /^(0\\d{2,3}-?|\\(0\\d{2,3}\\))[1-9]\\d{6,7}$/ 其可视化形式： 上面的正则构建过程略显罗嗦，但是这样做，能保证正则是准确的。 上述三种情形是或的关系，这一点很重要，不然很容易按字符是否出现的情形把正则写成： /^\\(?0\\d{2,3}\\)?-?[1-9]\\d{6,7}$/ 虽然也能匹配上述目标字符串，但也会匹配 (0551-88888888 这样的字符串。当然，这不是我们想要的。 其实这个正则也不是完美的，因为现实中，并不是每个 3 位数和 4 位数都是一个真实的区号。 这就是一个平衡取舍问题，一般够用就行。 6.3.2 匹配浮点数 要求匹配如下的格式： 1.23、+1.23、-1.23 10、+10、-10 .2、+.2、-.2 可以看出正则分为三部分。 符号部分：[+-] 整数部分：\\d+ 小数部分：\\.\\d+ 上述三个部分，并不是全部都出现。如果此时很容易写出如下的正则： /^[+-]?(\\d+)?(\\.\\d+)?$/ 此正则看似没问题，但这个正则也会匹配空字符&quot;&quot;。 因为目标字符串的形式关系不是要求每部分都是可选的。 要匹配 1.23、+1.23、-1.23，可以用 /^[+-]?\\d+\\.\\d+$/ 要匹配 10、+10、-10，可以用 /^[+-]?\\d+$/ 要匹配.2、+.2、-.2，可以用 /^[+-]?\\.\\d+$/ 因此整个正则是这三者的或的关系，提取公众部分后是： /^[+-]?(\\d+\\.\\d+|\\d+|\\.\\d+)$/ 其可视化形式是： 如果要求不匹配 +.2 和 -.2，此时正则变成： /^([+-]?(\\d+\\.\\d+|\\d+)|\\.\\d+)$/ 其可视化形式是： 当然，/^[+-]?(\\d+\\.\\d+|\\d+|\\.\\d+)$/ 也不是完美的，我们也是做了些取舍，比如： 它也会匹配 012 这样以 0 开头的整数。如果要求不匹配的话，需要修改整数部分的正则。 一般进行验证操作之前，都要经过 trim 和判空。那样的话，也许那个错误正则也就够用了。 也可以进一步改写成：/^[+-]?(\\d+)?(\\.)?\\d+$/，这样我们就需要考虑可读性和可维护性了。 6.4 效率 保证了准确性后，才需要是否要考虑要优化。大多数情形是不需要优化的，除非运行的非常慢。什么情形正则表达式运行才慢呢？我们需要考察正则表达式的运行过程（原理）。 正则表达式的运行分为如下的阶段： 编译； 设定起始位置； 尝试匹配； 匹配失败的话，从下一位开始继续第 3 步； 最终结果：匹配成功或失败。 下面以代码为例，来看看这几个阶段都做了什么： const regex = /\\d+/g; console.log(regex.lastIndex, regex.exec('123abc34def')); console.log(regex.lastIndex, regex.exec('123abc34def')); console.log(regex.lastIndex, regex.exec('123abc34def')); console.log(regex.lastIndex, regex.exec('123abc34def')); // =&gt; 0 [&quot;123&quot;, index: 0, input: &quot;123abc34def&quot;] // =&gt; 3 [&quot;34&quot;, index: 6, input: &quot;123abc34def&quot;] // =&gt; 8 null // =&gt; 0 [&quot;123&quot;, index: 0, input: &quot;123abc34def&quot;] 具体分析如下： const regex = /\\d+/g; 当生成一个正则时，引擎会对其进行编译。报错与否出现这这个阶段。 regex.exec('123abc34def') 当尝试匹配时，需要确定从哪一位置开始匹配。一般情形都是字符串的开头，即第 0 位。 但当使用 test 和 exec 方法，且正则有 g 时，起始位置是从正则对象的 lastIndex 属性开始。 因此第一次 exec 是从第 0 位开始，而第二次是从 3 开始的。 设定好起始位置后，就开始尝试匹配了。 比如第一次 exec，从 0 开始，去尝试匹配，并且成功地匹配到 3 个数字。此时结束时的下标是 2，因此下一次的起始位置是 3。 而第二次，起始下标是 3，但第 3 个字符是 &quot;a&quot;，并不是数字。但此时并不会直接报匹配失败，而是移动到下一位置，即从第 4 位开始继续尝试匹配，但该字符是 b，也不是数字。再移动到下一位，是 c 仍不行，再移动一位是数字 3，此时匹配到了两位数字 34。此时，下一次匹配的位置是 d 的位置，即第 8 位。 第三次，是从第 8 位开始匹配，直到试到最后一位，也没发现匹配的，因此匹配失败，返回 null。同时设置 lastIndex 为 0，即，如要再尝试匹配的话，需从头开始。 从上面可以看出，匹配会出现效率问题，主要出现在上面的第 3 阶段和第 4 阶段。 因此，主要优化手法也是针对这两阶段的。 6.4.1 使用具体型字符组来代替通配符，来消除回溯 而在第三阶段，最大的问题就是回溯。 例如，匹配双引用号之间的字符。如，匹配字符串 123&quot;abc&quot;456 中的 &quot;abc&quot;。 如果正则用的是：/&quot;.*&quot;/，会在第 3 阶段产生 4 次回溯（粉色表示 .* 匹配的内容）： 如果正则用的是：/&quot;.*?&quot;/，会产生 2 次回溯（粉色表示 .*? 匹配的内容）： 因为回溯的存在，需要引擎保存多种可能中未尝试过的状态，以便后续回溯时使用。注定要占用一定的内存。 此时要使用具体化的字符组，来代替通配符.，以便消除不必要的字符，此时使用正则 /&quot;[^&quot;]*&quot;/，即可。 6.4.2 使用非捕获型分组 因为括号的作用之一是，可以捕获分组和分支里的数据。那么就需要内存来保存它们。 当我们不需要使用分组引用和反向引用时，此时可以使用非捕获分组。例如： /^[+-]?(\\d+\\.\\d+|\\d+|\\.\\d+)$/ 可以修改成：/^[+-]?(?:\\d+\\.\\d+|\\d+|\\.\\d+)$/ 6.4.3 独立出确定字符 例如 /a+/，可以修改成 /aa*/。 因为后者能比前者多确定了字符 a。这样会在第四步中，加快判断是否匹配失败，进而加快移位的速度。 6.4.4 提取分支公共部分 比如 /^abc|^def/，修改成 /^(?:abc|def)/。 又比如 /this|that/，修改成 /th(?:is|at)/。 这样做，可以减少匹配过程中可消除的重复。 6.4.5 减少分支的数量，缩小它们的范围 /red|read/，可以修改成 /rea?d/。此时分支和量词产生的回溯的成本是不一样的。但这样优化后，可读性会降低的。 第 6 章小结 本章涉及的内容并不多。一般情况下，针对某问题能写出一个满足需求的正则，基本上就可以了。至于准确性和效率方面的追求，纯属看个人要求了。我觉得够用就行了。关于准确性，本章关心的是最常用的解决思路：针对每种情形，分别写出正则，然用分支把他们合并在一起，再提取分支公共部分，就能得到准确的正则。至于优化，本章没有为了凑数，去写一大堆。了解了匹配原理，常见的优化手法也就这么几种。 第 7 章 正则表达式编程 什么叫知识，能指导我们实践的东西才叫知识。学习一样东西，如果不能使用，最多只能算作纸上谈兵。 正则表达式的学习，也不例外。掌握了正则表达式的语法后，下一步，也是关键的一步，就是在真实世界中使用它。那么如何使用正则表达式呢？有哪些关键的点呢？本章就解决这个问题。内容包括： 正则表达式的四种操作 相关API注意要点 真实案例 7.1 正则表达式的四种操作 正则表达式是匹配模式，不管如何使用正则表达式，万变不离其宗，都需要先“匹配”。 有了匹配这一基本操作后，才有其他的操作：验证、切分、提取、替换。 进行任何相关操作，也需要宿主引擎相关 API 的配合使用。当然，在 JS 中，相关 API 也不多。 7.1.1 验证 验证是正则表达式最直接的应用，比如表单验证。 在说验证之前，先要说清楚匹配是什么概念。 所谓匹配，就是看目标字符串里是否有满足匹配的子串。因此，“匹配”的本质就是“查找”。 有没有匹配，是不是匹配上，判断是否的操作，即称为“验证”。 这里举一个例子，来看看如何使用相关API进行验证操作的。 比如，判断一个字符串中是否有数字。 使用 search const regex = /\\d/; const string = 'abc123'; console.log(!!~string.search(regex)); // =&gt; true 使用 test const regex = /\\d/; const string = 'abc123'; console.log(regex.test(string)); // =&gt; true 使用 match const regex = /\\d/; const string = 'abc123'; console.log(!!string.match(regex)); // =&gt; true 使用 exec const regex = /\\d/; const string = 'abc123'; console.log(!!regex.exec(string)); // =&gt; true 其中，最常用的是 test。 7.1.2 切分 匹配上了，我们就可以进行一些操作，比如切分。 所谓“切分”，就是把目标字符串，切成一段一段的。在 JS 中使用的是 split。 比如，目标字符串是 &quot;html,css,javascript&quot;，按逗号来切分： const regex = /,/; const string = 'html,css,javascript'; console.log(string.split(regex)); // =&gt; [&quot;html&quot;, &quot;css&quot;, &quot;javascript&quot;] 又比如，如下的日期格式： 2017/06/26 2017.06.26 2017-06-26 可以使用 split“切出”年月日： const regex = /\\D/; console.log('2017/06/26'.split(regex)); console.log('2017.06.26'.split(regex)); console.log('2017-06-26'.split(regex)); // =&gt; [&quot;2017&quot;, &quot;06&quot;, &quot;26&quot;] // =&gt; [&quot;2017&quot;, &quot;06&quot;, &quot;26&quot;] // =&gt; [&quot;2017&quot;, &quot;06&quot;, &quot;26&quot;] 7.1.3 提取 虽然整体匹配上了，但有时需要提取部分匹配的数据。 此时正则通常要使用分组引用（分组捕获）功能，还需要配合使用相关 API。 这里，还是以日期为例，提取出年月日。注意下面正则中的括号： match const regex = /^(\\d{4})\\D(\\d{2})\\D(\\d{2})$/; const string = '2017-06-26'; console.log(string.match(regex)); // =&gt;[&quot;2017-06-26&quot;, &quot;2017&quot;, &quot;06&quot;, &quot;26&quot;, index: 0, input: &quot;2017-06-26&quot;] exec const regex = /^(\\d{4})\\D(\\d{2})\\D(\\d{2})$/; const string = '2017-06-26'; console.log(regex.exec(string)); // =&gt;[&quot;2017-06-26&quot;, &quot;2017&quot;, &quot;06&quot;, &quot;26&quot;, index: 0, input: &quot;2017-06-26&quot;] test const regex = /^(\\d{4})\\D(\\d{2})\\D(\\d{2})$/; const string = '2017-06-26'; regex.test(string); console.log(RegExp.$1, RegExp.$2, RegExp.$3); // =&gt; &quot;2017&quot; &quot;06&quot; &quot;26&quot; search const regex = /^(\\d{4})\\D(\\d{2})\\D(\\d{2})$/; const string = '2017-06-26'; string.search(regex); console.log(RegExp.$1, RegExp.$2, RegExp.$3); // =&gt; &quot;2017&quot; &quot;06&quot; &quot;26&quot; replace const regex = /^(\\d{4})\\D(\\d{2})\\D(\\d{2})$/; const string = '2017-06-26'; const date = []; string.replace(regex, function(match, year, month, day) { date.push(year, month, day); }); console.log(date); // =&gt; [&quot;2017&quot;, &quot;06&quot;, &quot;26&quot;] 其中，最常用的是 match。 7.1.4 替换 找，往往不是目的，通常下一步是为了替换。在 JS 中，使用 replace 进行替换。 比如把日期格式，从 yyyy-mm-dd 替换成 yyyy/mm/dd： const string = '2017-06-26'; const today = new Date(string.replace(/-/g, '/')); console.log(today); // =&gt; Mon Jun 26 2017 00:00:00 GMT+0800 (中国标准时间) 这里只是简单地应用了一下 replace。但，replace 方法是强大的，是需要重点掌握的。 7.2. 相关API注意要点 从上面可以看出用于正则操作的方法，共有 6 个，字符串实例 4 个，正则实例 2 个： String#search String#split String#match String#replace RegExp#test RegExp#exec 本文不打算详细地讲解它们的方方面面细节，具体可以参考《JavaScript权威指南》的第三部分。本文重点列出一些容易忽视的地方，以飨读者。 7.2.1 search 和 match 的参数问题 我们知道字符串实例的那 4 个方法参数都支持正则和字符串。 但 search 和 match，会把字符串转换为正则的。 const string = '2017.06.27'; console.log(string.search('.')); // =&gt; 0 //需要修改成下列形式之一 console.log(string.search('\\\\.')); console.log(string.search(/\\./)); // =&gt; 4 // =&gt; 4 console.log(string.match('.')); // =&gt; [&quot;2&quot;, index: 0, input: &quot;2017.06.27&quot;] //需要修改成下列形式之一 console.log(string.match('\\\\.')); console.log(string.match(/\\./)); // =&gt; [&quot;.&quot;, index: 4, input: &quot;2017.06.27&quot;] // =&gt; [&quot;.&quot;, index: 4, input: &quot;2017.06.27&quot;] console.log(string.split('.')); // =&gt; [&quot;2017&quot;, &quot;06&quot;, &quot;27&quot;] console.log(string.replace('.', '/')); // =&gt; &quot;2017/06.27&quot; 7.2.2 match 返回结果的格式问题 match 返回结果的格式，与正则对象是否有修饰符 g 有关。 const string = '2017.06.27'; const regex1 = /\\b(\\d+)\\b/; const regex2 = /\\b(\\d+)\\b/g; console.log(string.match(regex1)); console.log(string.match(regex2)); // =&gt; [&quot;2017&quot;, &quot;2017&quot;, index: 0, input: &quot;2017.06.27&quot;] // =&gt; [&quot;2017&quot;, &quot;06&quot;, &quot;27&quot;] 没有 g，返回的是标准匹配格式，即，数组的第一个元素是整体匹配的内容，接下来是分组捕获的内容，然后是整体匹配的第一个下标，最后是输入的目标字符串。 有 g，返回的是所有匹配的内容。 当没有匹配时，不管有无 g，都返回 null。 7.2.3 exec比match更强大 当正则没有 g 时，使用 match 返回的信息比较多。但是有 g 后，就没有关键的信息 index 了。 而 exec 方法就能解决这个问题，它能接着上一次匹配后继续匹配： const string = '2017.06.27'; const regex2 = /\\b(\\d+)\\b/g; console.log(regex2.exec(string)); console.log(regex2.lastIndex); console.log(regex2.exec(string)); console.log(regex2.lastIndex); console.log(regex2.exec(string)); console.log(regex2.lastIndex); console.log(regex2.exec(string)); console.log(regex2.lastIndex); // =&gt; [&quot;2017&quot;, &quot;2017&quot;, index: 0, input: &quot;2017.06.27&quot;] // =&gt; 4 // =&gt; [&quot;06&quot;, &quot;06&quot;, index: 5, input: &quot;2017.06.27&quot;] // =&gt; 7 // =&gt; [&quot;27&quot;, &quot;27&quot;, index: 8, input: &quot;2017.06.27&quot;] // =&gt; 10 // =&gt; null // =&gt; 0 其中正则实例 lastIndex 属性，表示下一次匹配开始的位置。 比如第一次匹配了 &quot;2017&quot;，开始下标是 0，共 4 个字符，因此这次匹配结束的位置是 3，下一次开始匹配的位置是 4。 从上述代码看出，在使用 exec 时，经常需要配合使用 while 循环： const string = '2017.06.27'; const regex2 = /\\b(\\d+)\\b/g; const result; while (result = regex2.exec(string)) { console.log(result, regex2.lastIndex); } // =&gt; [&quot;2017&quot;, &quot;2017&quot;, index: 0, input: &quot;2017.06.27&quot;] 4 // =&gt; [&quot;06&quot;, &quot;06&quot;, index: 5, input: &quot;2017.06.27&quot;] 7 // =&gt; [&quot;27&quot;, &quot;27&quot;, index: 8, input: &quot;2017.06.27&quot;] 10 7.2.4 修饰符g，对exex和test的影响 上面提到了正则实例的 lastIndex 属性，表示尝试匹配时，从字符串的 lastIndex 位开始去匹配。 字符串的四个方法，每次匹配时，都是从 0 开始的，即 lastIndex 属性始终不变。 而正则实例的两个方法 exec、test，当正则是全局匹配时，每一次匹配完成后，都会修改 lastIndex。下面让我们以 test 为例，看看你是否会迷糊： const regex = /a/g; console.log(regex.test('a'), regex.lastIndex); console.log(regex.test('aba'), regex.lastIndex); console.log(regex.test('ababc'), regex.lastIndex); // =&gt; true 1 // =&gt; true 3 // =&gt; false 0 注意上面代码中的第三次调用 test，因为这一次尝试匹配，开始从下标 lastIndex 即 3 位置处开始查找，自然就找不到了。 如果没有 g，自然都是从字符串第 0 个字符处开始尝试匹配： const regex = /a/; console.log(regex.test('a'), regex.lastIndex); console.log(regex.test('aba'), regex.lastIndex); console.log(regex.test('ababc'), regex.lastIndex); // =&gt; true 0 // =&gt; true 0 // =&gt; true 0 7.2.5 test 整体匹配时需要使用 ^ 和 $ 这个相对容易理解，因为 test 是看目标字符串中是否有子串匹配正则，即有部分匹配即可。 如果，要整体匹配，正则前后需要添加开头和结尾： console.log(/123/.test('a123b')); // =&gt; true console.log(/^123$/.test('a123b')); // =&gt; false console.log(/^123$/.test('123')); // =&gt; true 7.2.6 split 相关注意事项 split 方法看起来不起眼，但要注意的地方有两个的。 第一，它可以有第二个参数，表示结果数组的最大长度： const string = 'html,css,javascript'; console.log(string.split(/,/, 2)); // =&gt;[&quot;html&quot;, &quot;css&quot;] 第二，正则使用分组时，结果数组中是包含分隔符的： const string = 'html,css,javascript'; console.log(string.split(/(,)/)); // =&gt;[&quot;html&quot;, &quot;,&quot;, &quot;css&quot;, &quot;,&quot;, &quot;javascript&quot;] 7.2.7 replace 是很强大的 《JavaScript权威指南》认为 exec 是这 6 个 API 中最强大的，而我始终认为 replace 才是最强大的。因为它也能拿到该拿到的信息，然后可以假借替换之名，做些其他事情。 总体来说 replace 有两种使用形式，这是因为它的第二个参数，可以是字符串，也可以是函数。 当第二个参数是字符串时，如下的字符有特殊的含义： $1,$2,...,$99 匹配第 1~99 个分组里捕获的文本 $&amp; 匹配到的子串文本 $` 匹配到的子串的左边文本 $' 匹配到的子串的右边文本 $$ 美元符号 例如，把 &quot;2,3,5&quot;，变成 &quot;5=2+3&quot;： const result = '2,3,5'.replace(/(\\d+),(\\d+),(\\d+)/, '$3=$1+$2'); console.log(result); // =&gt; &quot;5=2+3&quot; 又例如，把 &quot;2,3,5&quot;，变成 &quot;222,333,555&quot;: const result = '2,3,5'.replace(/(\\d+)/g, '$&amp;$&amp;$&amp;'); console.log(result); // =&gt; &quot;222,333,555&quot; 当第二个参数是函数时，我们需要注意该回调函数的参数具体是什么： '1234 2345 3456'.replace(/(\\d)\\d{2}(\\d)/g, function(match, $1, $2, index, input) { console.log([match, $1, $2, index, input]); }); // =&gt; [&quot;1234&quot;, &quot;1&quot;, &quot;4&quot;, 0, &quot;1234 2345 3456&quot;] // =&gt; [&quot;2345&quot;, &quot;2&quot;, &quot;5&quot;, 5, &quot;1234 2345 3456&quot;] // =&gt; [&quot;3456&quot;, &quot;3&quot;, &quot;6&quot;, 10, &quot;1234 2345 3456&quot;] 此时我们可以看到 replace 拿到的信息，并不比 exec 少。 7.2.8 使用构造函数需要注意的问题 一般不推荐使用构造函数生成正则，而应该优先使用字面量。因为用构造函数会多写很多 \\。 const string = '2017-06-27 2017.06.27 2017/06/27'; const regex = /\\d{4}(-|\\.|\\/)\\d{2}\\1\\d{2}/g; console.log(string.match(regex)); // =&gt; [&quot;2017-06-27&quot;, &quot;2017.06.27&quot;, &quot;2017/06/27&quot;] regex = new RegExp('\\\\d{4}(-|\\\\.|\\\\/)\\\\d{2}\\\\1\\\\d{2}', 'g'); console.log(string.match(regex)); // =&gt; [&quot;2017-06-27&quot;, &quot;2017.06.27&quot;, &quot;2017/06/27&quot;] 7.2.9 修饰符 ES5中修饰符，共3个： g 全局匹配，即找到所有匹配的，单词是 global i 忽略字母大小写，单词 ingoreCase m 多行匹配，只影响 ^ 和 $，二者变成行的概念，即行开头和行结尾。单词是 multiline 当然正则对象也有相应的只读属性： const regex = /\\w/img; console.log(regex.global); console.log(regex.ignoreCase); console.log(regex.multiline); // =&gt; true // =&gt; true // =&gt; true 7.2.10 source 属性 正则实例对象属性，除了 global、ingnoreCase、multiline、lastIndex 属性之外，还有一个 source 属性。 它什么时候有用呢？ 比如，在构建动态的正则表达式时，可以通过查看该属性，来确认构建出的正则到底是什么： const className = 'high'; const regex = new RegExp('(^|\\\\s)' + className + '(\\\\s|$)'); console.log(regex.source) // =&gt; (^|\\s)high(\\s|$) 即字符串&quot;(^|\\\\s)high(\\\\s|$)&quot; 7.2.11 构造函数属性 构造函数的静态属性基于所执行的最近一次正则操作而变化。除了是 $1,...,$9 之外，还有几个不太常用的属性（有兼容性问题）： RegExp.input 最近一次目标字符串，简写成 RegExp[&quot;$_&quot;] RegExp.lastMatch 最近一次匹配的文本，简写成 RegExp[&quot;$&amp;&quot;] RegExp.lastParen 最近一次捕获的文本，简写成 RegExp[&quot;$+&quot;] RegExp.leftContext 目标字符串中 lastMatch 之前的文本，简写成 RegExp[&quot;$`&quot;] RegExp.rightContext 目标字符串中 lastMatch 之后的文本，简写成 RegExp[&quot;$'&quot;] 测试代码如下： const regex = /([abc])(\\d)/g; const string = 'a1b2c3d4e5'; string.match(regex); console.log(RegExp.input); console.log(RegExp['$_']); // =&gt; &quot;a1b2c3d4e5&quot; console.log(RegExp.lastMatch); console.log(RegExp['$&amp;']); // =&gt; &quot;c3&quot; console.log(RegExp.lastParen); console.log(RegExp['$+']); // =&gt; &quot;3&quot; console.log(RegExp.leftContext); console.log(RegExp[&quot;$`&quot;]); // =&gt; &quot;a1b2&quot; console.log(RegExp.rightContext); console.log(RegExp[&quot;$'&quot;]); // =&gt; &quot;d4e5&quot; 7.3 真实案例 7.3.1 使用构造函数生成正则表达式 我们知道要优先使用字面量来创建正则，但有时正则表达式的主体是不确定的，此时可以使用构造函数来创建。模拟 getElementsByClassName 方法，就是很能说明该问题的一个例子。 这里 getElementsByClassName 函数的实现思路是： 比如要获取 className 为 &quot;high&quot; 的 dom 元素； 首先生成一个正则：/(^|\\s)high(\\s|$)/； 然后再用其逐一验证页面上的所有 dom 元素的类名，拿到满足匹配的元素即可。 代码如下(可以直接复制到本地查看运行效果)： &lt;p class=&quot;high&quot;&gt;1111&lt;/p&gt; &lt;p class=&quot;high&quot;&gt;2222&lt;/p&gt; &lt;p&gt;3333&lt;/p&gt; &lt;script&gt; function getElementsByClassName(className) { var elements = document.getElementsByTagName(&quot;*&quot;); var regex = new RegExp(&quot;(^|\\\\s)&quot; + className + &quot;(\\\\s|$)&quot;); var result = []; for (var i = 0; i &lt; elements.length; i++) { var element = elements[i]; if (regex.test(element.className)) { result.push(element) } } return result; } var highs = getElementsByClassName('high'); highs.forEach(function(item) { item.style.color = 'red'; }); &lt;/script&gt; 7.3.2 使用字符串保存数据 一般情况下，我们都愿意使用数组来保存数据。但我看到有的框架中，使用的却是字符串。 使用时，仍需要把字符串切分成数组。虽然不一定用到正则，但总感觉酷酷的，这里分享如下： const utils = {}; 'Boolean|Number|String|Function|Array|Date|RegExp|Object|Error'.split('|').forEach(function(item) { utils['is' + item] = function(obj) { return {}.toString.call(obj) == '[object ' + item + ']'; }; }); console.log( utils.isArray([1, 2, 3]) ); // =&gt; true 7.3.3 if 语句中使用正则替代 &amp;&amp; 比如，模拟 ready 函数，即加载完毕后再执行回调（不兼容 IE 的）： const readyRE = /complete|loaded|interactive/; function ready(callback) { if (readyRE.test(document.readyState) &amp;&amp; document.body) { callback() } else { document.addEventListener( 'DOMContentLoaded', function () { callback() }, false ); } }; ready(function() { alert('加载完毕！') }); 7.3.4 使用强大的 replace 因为 replace 方法比较强大，有时用它根本不是为了替换，只是拿其匹配到的信息来做文章。 这里以查询字符串（querystring）压缩技术为例，注意下面 replace 方法中，回调函数根本没有返回任何东西。 function compress(source) { var keys = {}; source.replace(/([^=&amp;]+)=([^&amp;]*)/g, function(full, key, value) { keys[key] = (keys[key] ? keys[key] + ',' : '') + value; }); var result = []; for (var key in keys) { result.push(key + '=' + keys[key]); } return result.join('&amp;'); } console.log(compress(&quot;a=1&amp;b=2&amp;a=3&amp;b=4&quot;)); // =&gt; &quot;a=1,3&amp;b=2,4&quot; 7.3.5 综合运用 最后这里再做个简单实用的正则测试器。 具体效果如下： 代码，直接贴了，相信你能看得懂： &lt;section&gt; &lt;div id=&quot;err&quot;&gt;&lt;/div&gt; &lt;input id=&quot;regex&quot; placeholder=&quot;请输入正则表达式&quot;&gt; &lt;input id=&quot;text&quot; placeholder=&quot;请输入测试文本&quot;&gt; &lt;button id=&quot;run&quot;&gt;测试一下&lt;/button&gt; &lt;div id=&quot;result&quot;&gt;&lt;/div&gt; &lt;/section&gt; &lt;style&gt; section{ display:flex; flex-direction:column; justify-content:space-around; height:300px; padding:0 200px; } section *{ min-height:30px; } #err { color:red; } #result{ line-height:30px; } .info { background:#00c5ff; padding:2px; margin:2px; display:inline-block; } &lt;/style&gt; &lt;script&gt; (function() { // 获取相应dom元素 var regexInput = document.getElementById(&quot;regex&quot;); var textInput = document.getElementById(&quot;text&quot;); var runBtn = document.getElementById(&quot;run&quot;); var errBox = document.getElementById(&quot;err&quot;); var resultBox = document.getElementById(&quot;result&quot;); // 绑定点击事件 runBtn.onclick = function() { // 清除错误和结果 errBox.innerHTML = &quot;&quot;; resultBox.innerHTML = &quot;&quot;; // 获取正则和文本 var text = textInput.value; var regex = regexInput.value; if (regex == &quot;&quot;) { errBox.innerHTML = &quot;请输入正则表达式&quot;; } else if (text == &quot;&quot;) { errBox.innerHTML = &quot;请输入测试文本&quot;; } else { regex = createRegex(regex); if (!regex) return; var result, results = []; // 没有修饰符g的话，会死循环 if (regex.global) { while(result = regex.exec(text)) { results.push(result); } } else { results.push(regex.exec(text)); } if (results[0] == null) { resultBox.innerHTML = &quot;匹配到0个结果&quot;; return; } // 倒序是有必要的 for (var i = results.length - 1; i &gt;= 0; i--) { var result = results[i]; var match = result[0]; var prefix = text.substr(0, result.index); var suffix = text.substr(result.index + match.length); text = prefix + '&lt;span class=&quot;info&quot;&gt;' + match + '&lt;/span&gt;' + suffix; } resultBox.innerHTML = &quot;匹配到&quot; + results.length + &quot;个结果:&lt;br&gt;&quot; + text; } }; // 生成正则表达式，核心函数 function createRegex(regex) { try { if (regex[0] == &quot;/&quot;) { regex = regex.split(&quot;/&quot;); regex.shift(); var flags = regex.pop(); regex = regex.join(&quot;/&quot;); regex = new RegExp(regex, flags); } else { regex = new RegExp(regex, &quot;g&quot;); } return regex; } catch(e) { errBox.innerHTML = &quot;无效的正则表达式&quot;; return false; } } })(); &lt;/script&gt; 第 7 章小结 相关API的注意点，本章基本上算是一网打尽了。 至于文中的例子，都是点睛之笔，没有详细解析。如有理解不透的，建议自己敲一敲。 后记 文章要结束了，最后还要有几点说明。 1. 需要注意的地方 本文主要讨论的是 JavaScript 的正则表达式，更精确地说是 ES5 的正则表达式。 JavaScript 的正则表达式引擎是传统型 NFA 的，因此本系列的讨论是适合任何一门正则引擎是传统型 NFA 的编程语言。当然，市面上大部分语言的正则引擎都是这种的。而 JS 里正则涉及到的所有语法要点，是这种引擎支持的核心子集。也就是说，要学正则表达式，不妨以 JS 正则为出发点。 2. 参考资料 当然本文不是无本之末。主要参考的是几本书籍。 以下书籍中核心章节都认真阅读过，甚至阅读多遍。 《JavaScript权威指南》，看完本系列，再去看书中的第 10 章，你就知道了什么叫字字珠玑。 《精通正则表达式》，权威且比较杂乱，我阅读的第一本正则表达式书籍。 《正则表达式必知必会》，这是我看的第二本正则，看完后，确定自己算是入门了。 《正则指引》，《精通正则表达式》的译者写的，相对清晰。 《正则表达式入门》，我看的是英文版的，对于已经入门的我，基本没多少收获了。 《正则表达式经典实例》，除了第 3 章，比较杂外，也有收获，以实例为主导的一本书。 《JavaScript Regular Expressions》，为数不多转讲 JS 正则的。页数不多，也有收获。 《高性能JavaScript 》第 5 章，我看的是英文版的。第 5 章，讲了回溯和优化。 《JavaScript忍者秘籍》第 7 章，大概讲了一下正则的用法，几个例子还不错。 《JavaScript高级程序设计》第 5.4 节，比较简短的介绍。 使用的工具： Regulex，一款可视化工具 ProcessOn - 免费在线作图，实时协作 LICEcap – 灵活好用，GIF 屏幕录制工具 ","link":"https://faded.auspicious.space/post/regular-expression-full-tutorial/"},{"title":"正则表达式——断言人话版","content":" 这次不会说我的正则教程没写全了吧？？ 零宽断言 断言：俗话的断言就是“我断定什么什么”，而正则中的断言，就是说正则可以指明在指定内容的前面或后面会出现满足指定规则的内容，意思正则也可以像人类那样断定什么什么，比如“ss1aa2bb3”，正则可以用断言找出 aa2 前面有 bb3，也可以找出 aa2 后面有 ss1。 零宽：就是没有宽度，在正则中，断言只是匹配位置，不占字符，也就是说，匹配结果里是不会返回断言本身。 假设我们要用爬虫抓取 csdn 里的文章阅读量。通过查看源代码可以看到文章阅读量这个内容是这样的结构： &lt;span class=&quot;read-count&quot;&gt;阅读数：641&lt;/span&gt; 需要获得这里边的‘641’有很多种办法，但如果使用正则应该怎么匹配呢？下面先讲一下几种类型的断言： 💡正向先行断言（正前瞻） 语法：(?=pattern)； 作用：匹配 pattern 表达式的前面内容，不返回本身。 要取到阅读量，在正则表达式中就意味着要能匹配到‘&lt;/span&gt;’前面的数字内容，按照上所说的正向先行断言可以匹配表达式前面的内容，那意思就是：(?=&lt;/span&gt;) 就可以匹配到前面的内容了。 const regExp = /.+(?=&lt;\\/span&gt;)/; const str = &quot;&lt;span class=\\&quot;read-count\\&quot;&gt;阅读数：641&lt;/span&gt;&quot; console.log(regExp.exec(str)); // 匹配结果： [ '&lt;span class=&quot;read-count&quot;&gt;阅读数：641', index: 0, input: '&lt;span class=&quot;read-count&quot;&gt;阅读数：641&lt;/span&gt;', groups: undefined ] 仅匹配前面的数字： const regExp = /\\d+(?=&lt;\\/span&gt;)/; const str = &quot;&lt;span class=\\&quot;read-count\\&quot;&gt;阅读数：641&lt;/span&gt;&quot; console.log(regExp.exec(str)); // 匹配结果： [ '641', index: 29, input: '&lt;span class=&quot;read-count&quot;&gt;阅读数：641&lt;/span&gt;', groups: undefined ] 💡正向后行断言（正后顾）: 语法：(?&lt;=pattern)； 作用：匹配 pattern 表达式的后面的内容，不返回本身。 有先行就有后行，先行是匹配前面的内容，那后行就是匹配后面的内容啦。上面的栗子，我们也可以用后行断言来处理。 const regExp= /(?&lt;=&lt;span class=\\&quot;read-count\\&quot;&gt;阅读数：)\\d+/; const str = &quot;&lt;span class=\\&quot;read-count\\&quot;&gt;阅读数：641&lt;/span&gt;&quot; console.log(regExp.exec(str)); // 匹配结果 [ '641', index: 29, input: '&lt;span class=&quot;read-count&quot;&gt;阅读数：641&lt;/span&gt;', groups: undefined ] 💡负向先行断言（负前瞻） 语法：(?!pattern)； 作用：匹配非 pattern 表达式的前面内容，不返回本身。 有正向也有负向，负向在这里其实就是非的意思。举个栗子：比如有一句 “我爱祖国，我是祖国的花朵”，现在要找到不是 “的花朵”前面的“祖国”，用正则就可以这样写：祖国(?!的花朵)。 💡负向后行断言（负后顾） 语法：(?&lt;!pattern)； 作用：匹配非 pattern 表达式的后面内容，不返回本身。 捕获和非捕获 单纯说到捕获，他的意思是匹配表达式，但捕获通常和分组联系在一起，也就是“捕获组”。捕获组：匹配子表达式的内容，把匹配结果保存到内存中数字编号或显示命名的组里，以深度优先进行编号，之后可以通过序号或名称来使用这些匹配结果。 而根据命名方式的不同，又可以分为两种组： 💡数字编号捕获组 语法：(exp)； 解释：从表达式左侧开始，每出现一个左括号和它对应的右括号之间的内容为一个分组，在分组中，第 0 组为整个表达式，第一组开始为分组。 比如固定电话的：020-85653333，它的正则表达式为：(0\\d{2})-(\\d{8})，按照左括号的顺序，这个表达式有如下分组： 序号 编号 分组 内容 0 0 (0\\d{2})-(\\d{8}) 020-85653333 1 1 (0\\d{2}) 020 2 2 (\\d{8}) 85653333 下面来验证一下： const str = '020-85653333'; const regExp=/(0\\d{2})-(\\d{8})/; console.log(regExp.exec(str)); // 输出结果： [ '020-85653333', '020', '85653333', index: 0, input: '020-85653333', groups: undefined ] 可见，分组个数是2，但是因为第0个为整个表达式本身，因此也一起输出了。 💡命名编号捕获组： 语法：(?&lt;name&gt;exp)； 解释：分组的命名由表达式中的name指定。 比如区号也可以这样写: (?&lt;quhao&gt;\\0\\d{2})-(?&lt;haoma&gt;\\d{8})，按照左括号的顺序，这个表达式有如下分组： 序号 名称 分组 内容 0 0 (0\\d{2})-(\\d{8}) 020-85653333 1 quhao (0\\d{2}) 020 2 haoma (\\d{8}) 85653333 const str = '020-85653333'; const regExp=/(?&lt;quhao&gt;0\\d{2})-(?&lt;haoma&gt;\\d{8})/; console.log(regExp.exec(str)); // 输出结果： [ '020-85653333', '020', '85653333', index: 0, input: '020-85653333', groups: [Object: null prototype] { quhao: '020', haoma: '85653333' } ] 💡非捕获组： 语法：(?:exp)； 解释：和捕获组刚好相反，它用来标识那些不需要捕获的分组，说的通俗一点，就是你可以根据需要去保存你的分组。 比如上面的正则表达式，程序不需要用到第一个分组，那就可以这样写：1(?:\\0\\d{2})-(\\d{8})。 序号 编号 分组 内容 0 0 (0\\d{2})-(\\d{8}) 020-85653333 1 1 (\\d{8}) 85653333 const str = '020-85653333'; const regExp=/(?:0\\d{2})-(\\d{8})/; console.log(regExp.exec(str)); // 运行结果： [ '020-85653333', '85653333', index: 0, input: '020-85653333', groups: undefined ] 反向引用 上面讲到捕获，我们知道：捕获会返回一个捕获组，这个分组是保存在内存中，不仅可以在正则表达式外部通过程序进行引用，也可以在正则表达式内部进行引用，这种引用方式就是反向引用。 根据捕获组的命名规则，反向引用可分为： 数字编号组反向引用：\\k 或 \\number； 命名编号组反向引用：\\k或者 \\'name'。 捕获组通常是和反向引用一起使用的。上面说到捕获组是匹配子表达式的内容按序号或者命名保存起来以便使用。注意两个字眼：“内容” 和 “使用”，这里所说的“内容”，是匹配结果，而不是子表达式本身。这里所说的“使用”的作用主要是用来查找一些重复的内容或者做替换指定字符。 还是举栗子吧：比如要查找一串字母 &quot;aabbbbgbddesddfiid&quot; 里成对的字母，如果按照我们之前学到的正则，什么区间啊限定啊断言啊可能是办不到的，现在我们先用程序思维理一下思路： 匹配到一个字母； 匹配第下一个字母，检查是否和上一个字母是否一样； 如果一样，则匹配成功，否则失败； 这里的思路 2 中匹配下一个字母时，需要用到上一个字母，那怎么记住上一个字母呢？这下子捕获就有用处啦，我们可以利用捕获把上一个匹配成功的内容用来作为本次匹配的条件。好了，有思路就要实践，首先匹配一个字母：\\w，我们需要做成分组才能捕获，因此写成这样：(\\w)，那这个表达式就有一个捕获组：(\\w)，然后我们要用这个捕获组作为条件，那就可以：(\\w)\\1，这样就大功告成了，可能有人不明白了，\\1 是什么意思呢？ 还记得捕获组有两种命名方式吗，一种是是根据捕获分组顺序命名，一种是自定义命名来作为捕获组的命名。在默认情况下都是以数字来命名，而且数字命名的顺序是从 1 开始的。因此要引用第一个捕获组，根据反向引用的数字命名规则 就需要 \\k&lt;1&gt; 或者 \\1 当然，通常都是是后者。 我们来测试一下： const str = 'aabbbbgbddesddfiid'; const regExp=/(\\w)\\1/g; console.log(str.match(regExp)); 运行结果： [ 'aa', 'bb', 'bb', 'dd', 'dd', 'ii' ] 再举个替换的例子，假如想要把字符串中 abc 换成 a： const str = 'abcbbabcbcgbddesddfiid'; const regExp=/(a)(b)c/g; console.log(str.replace(regExp, '$1')); // 输出结果： abcbbabcbcgbddesddfiid 贪婪和非贪婪 💡贪婪 贪婪匹配：当正则表达式中包含能接受重复的限定符时，通常的行为是（在使整个表达式能得到匹配的前提下）匹配尽可能多的字符，这匹配方式叫做贪婪匹配。 特性：一次性读入整个字符串进行匹配，每当不匹配就舍弃最右边一个字符，继续匹配，依次匹配和舍弃（这种匹配-舍弃的方式也叫做回溯），直到匹配成功或者把整个字符串舍弃完为止，因此它是一种最大化的数据返回，能多不会少。 前面我们讲过重复限定符，其实这些限定符就是贪婪量词，比如表达式：\\d{3,6} 用来匹配 3 到 6 位数字，在这种情况下，它是一种贪婪模式的匹配，也就是假如字符串里有 6 个数字可以匹配，那它就是全部匹配到。例如： const str = &quot;61762828 176 2991 871&quot;; const regExp=/\\d{3,6}/g; console.log(str.match(regExp)); // 输出结果： [ '617628', '176', '2991', '871' ] 由结果可见：本来字符串中的“61762828”这一段，其实只需要出现3个（617）就已经匹配成功了的，但是他并不满足，而是匹配到了最大能匹配的字符，也就是6个。 一个量词就如此贪婪了，那有人会问，如果多个贪婪量词凑在一起，那他们是如何支配自己的匹配权的呢？是这样的，多个贪婪在一起时，如果字符串能满足他们各自最大程度的匹配时，就互不干扰，但如果不能满足时，会根据深度优先原则，也就是从左到右的每一个贪婪量词，优先最大数量的满足，剩余再分配下一个量词匹配。 const str = &quot;61762828 176 2991 87321&quot;; const regExp=/(\\d{1,2})(\\d{3,4})/g; console.log(str.match(regExp)); // 输出结果： [ '617628', '2991', '87321' ] 解答： “617628” 是前面的 \\d{1,2} 匹配出了 61，后面的匹配出了 7628； “2991”是前面的 \\d{1,2} 匹配出了 2 ，后面的匹配出了 991(满足匹配优先，再最大程度的贪婪)； “87321”是前面的 \\d{1,2} 匹配出了 87，后面的匹配出了 321。 💡懒惰（非贪婪） 懒惰匹配：当正则表达式中包含能接受重复的限定符时，通常的行为是（在使整个表达式能得到匹配的前提下）匹配尽可能少的字符，这匹配方式叫做懒惰匹配。 特性：从左到右，从字符串的最左边开始匹配，每次试图不读入字符匹配，匹配成功，则完成匹配，否则读入一个字符再匹配，依此循环（读入字符、匹配）直到匹配成功或者把字符串的字符匹配完为止。 懒惰量词是在贪婪量词后面加个“?”。 代码 说明 *? 重复任意次，但尽可能少重复 +? 重复 1 次或更多次，但尽可能少重复 ?? 重复 0 次或 1 次，但尽可能少重复 {n,m}? 重复 n 到 m 次，但尽可能少重复 {n,}? 重复 n 次以上，但尽可能少重复 const str = &quot;61762828 176 2991 87321&quot;; const regExp=/(\\d{1,2}?)(\\d{3,4})/g; console.log(str.match(regExp)); // 输出结果： [ '61762', '2991', '87321' ] 解答： “61762”是左边的懒惰匹配出 6，右边的贪婪匹配出 1762； “2991”是左边的懒惰匹配出 2，右边的贪婪匹配出 991； “87321”左边的懒惰匹配出 8，右边的贪婪匹配出 7321。 反义 前面说到元字符的都是要匹配什么什么，当然如果你想反着来，不想匹配某些字符，正则也提供了一些常用的反义元字符： 元字符 解释 \\W 匹配任意不是字母，数字，下划线，汉字的字符 \\S 匹配任意不是空白符的字符 \\D 匹配任意非数字的字符 \\B 匹配不是单词开头或结束的位置 [^x] 匹配除了 x 以外的任意字符 [^aeiou] 匹配除了 aeiou 这几个字母以外的任意字符 ","link":"https://faded.auspicious.space/post/regular-expression-assert-mandarin/"},{"title":"正则表达式——NFA","content":" 正则表达式和NFA NFA NFA 是指 Nondeterministic Finite Automaton，非确定有限状态自动机。 目前正则表达式引擎主要有两种：NFA 和 DFA； JavaScript 采用的是 NFA 引擎。 状态机中有这样一些要素，对照上图分别说下： 开始状态：圆圈表示状态，被一个“没有起点的箭头”指向的状态，是开始状态，上例中是 S1； 最终状态：也叫接受状态，图中用双圆圈表示，这个例子中也是 S1； 输入：在一个状态下，向状态机输入的符号/信号，不同输入导致状态机产生不同的状态改变； 转换：在一个状态下，根据特定输入，改变到特定状态的过程，就是转换。 所以有限状态机的工作过程，就是从开始状态，根据不同的输入，自动进行状态转换的过程。 上图中的状态机的功能，是检测二进制数是否含有偶数个 0。从图上可以看出，输入只有 1 和 0 两种。从 S1 状态开始，只有输入 0 才会转换到 S2 状态，同样 S2 状态下只有输入 0 才会转换到 S1。所以，二进制数输入完毕，如果满足最终状态，也就是最后停在 S1 状态，那么输入的二进制数就含有偶数个 0。 正则表达式，可以认为是对一组字符串集合的描述。例如 (a+|b)c 对应的字符串集合是： ac bc aac aaac aaaac ... 有限状态机也可以用来描述字符串集合，同样是正则表达式所描述的集合，用有限状态机来表示，可以是这样的： 并且，有限状态机是可以“执行”的，给出如上的状态机之后，就可以用来对输入的字符串进行检测。如果最终匹配，也就意味着输入的字符串和正则表达式 (a+|b)c 匹配。 所以，编程语言中的正则表达式，一般是通过有限状态机来实现。正则表达式匹配字符串的过程，可以分解为： 正则表达式转换为等价的有限状态机； 有限状态机输入字符串执行。 这里再讲一下 NFA 和 DFA 的区别。DFA 是 Deterministic Finite Automaton，确定有限状态机。DFA 可以认为是一种特殊的 NFA，它最大的特点，就是确定性。它的确定性在于，在一个状态下，输入一个符号，一定是转换到确定的状态，没有其他的可能性。 举个例子，对于正则表达式 ab|ac，对应 NFA 可以是这样的： 可以看到，在状态 1 这里，如果输入 a，其实有两种可能，如果后面的符号是 b，那么可以匹配成功，后面符号是 c 也能匹配成功。所以状态机在执行过程中，可能要尝试所有的可能性。在尝试一种可能路径匹配失败后，还要回到之前的状态再尝试其他的路径，这就是“回溯”。 但是 DFA 消除了这种不确定性，所以可以想见，其执行性能应该要比 NFA 更好，因为不需要回溯。 NFA 是可以转换为等价的 DFA 的，也就是说，理论上讲，正则表达式可以用 DFA 来实现，从而获得优于 NFA 的执行性能。但是 NFA 转换 DFA 的过程，会消耗更多资源，甚至最终得到的 DFA 要占用大量存储空间（据有的资料的说法，可能会产生指数级增长）。而且，DFA 相比 NFA，在实现一些正则表达式的特性时会更复杂，成本更高。所以当前的许多编程语言，其正则表达式引擎为 NFA 模式。 /nfa|nfa not/.test('nfa not'); 用上面的正则表达式来测试字符串 nfa not，NFA 引擎在检测满足 nfa 就返回匹配成功的结果了，而 DFA 则会尝试继续查找，也就是说会得到“最长的匹配结果”。 从正则表达式到 NFA 🏈Thompson 算法 Thompson 算法用于转换正则表达式为 NFA，它并非最高效的算法，但是实用，易于理解。 Thompson 算法中使用最基本的两种转换： 普通转换就是在一个状态下，输入字符 a 后转换至另一个状态；epsilon转换则不需要有输入，就从一个状态转换至另一个状态。 正则表达式中的各种运算，可以通过组合上述两种转换实现： 组合转换 RS： 替换转换 R|S： 重复转换 R*： 上面图中的 R、S 是有开始状态和结束状态的 NFA。 以正则表达式 ab|c 为例，包括两个运算： ab 组合 ab 的结果，与 c 替换 这样我们把正则表达式视为一系列输入和运算，进行分解、组合，就可以得到最终的 NFA。 首先，我们要把正则表达式转换为方便记录输入、运算的方式。 🏈正则表达式 → 后缀表达式 后缀表达式是一种方便记录输入、运算的表达式，本身已包含了运算符的优先级，也称为逆波兰表示法（Reverse Polish Notation，简写为 RPN）。 为方便记录运算，我们为正则表达式中的组合运算也创建一个运算符“.”（本文只涉及最简单的正则表达式形式，这里的“.”不是用于匹配任意字符的特殊符号）。 正则表达式 ab|c对应的后缀表达式为 ab.c|。 这样，通过逐个扫描后缀表达式，并识别其中的运算符来执行，就可以对后缀表达式进行求解。对于正则表达式来说，则是在将其变为后缀表达式后，通过“求值”的过程来进一步构建并得到最终的 NFA。 用于创建后缀表达式的是调度场算法。 对于这里的正则表达式处理的场景，算法的大致描述如下： 创建输出队列 output 和运算符栈 ops； 依次读取输入字符串中每一个字符 ch； 如果 ch 是普通字符，追加到 output； 如果 ch 是运算符，只要 ops 栈顶的运算符优先级不低于 ch，依次出栈并追加到 output，最后将 ch 入栈 ops； 如果 ch 是“(”，入栈 ops； 如果 ch 是“)”，只要 ops 栈顶不是“(”，依次出栈并追加到 output； 将 ops 中运算符依次出栈追加到 output； 返回 output。 具体处理过程中，由于原始正则表达式中并没有组合运算符，所以需要自行判断合理的插入位置。 运算符优先级如下（由高到低）： * ? + . | ( 🏈后缀表达式 → NFA 基于后缀表达式创建 NFA，是一个由简单的 NFA 进行不断组合得到复杂 NFA 的过程。 用于表示状态 State 的数据结构为： // State { id: String, type: String, // 'n' - normal, 'e' - epsilon, 'end' symbol: String, // 普通状态对应的输入字符 out: State, // 允许的下一个状态 out1: State // 允许的下一个状态 } 每个状态可以对应最多两个 out 状态，像 a|b|c 的表达式，会被分解为 (a|b)|c，每次运算符“|”都只处理两个（子）表达式。 在构造最终 NFA 过程中，每次会创建 NFA 的片段 Fragment： // Fragment { start: State, out: State } 不管 NFA 片段内部是怎样复杂，它都只有一个入口（开始状态），一个出口（最终状态）。 处理的过程大致为： 创建用于记录 NFA 片段的栈 stack； 依次读取输入的后缀表达式的每个字符 ch； 如果 ch 是运算符，从 stack 出栈所需数目的 NFA 片段，构建新的 NFA 片段后入栈 stack； 如果 ch 是普通字符，创建新的状态，并构建只包含此状态的 NFA 片段入栈 stack； 返回 stack 栈顶的 NFA 片段，即最终结果。 以对组合运算的处理为例： const e2 = stack.pop(); const e1 = stack.pop(); e1.out.out = e2.start; stack.push(new Fragment(e1.start, e2.out)); 从 stack 出栈两个 NFA 片段，然后将其首尾相连后构建新的 NFA 片段再入栈。 NFA 的执行 NFA 的执行过程就是用当前状态来比对字符串的当前字符，如果匹配就继续比对下一个状态和下一个字符，否则匹配失败。 不过由于 NFA 的不确定性，所以可能会同时有多个匹配的状态。 总结 综上，正则表达式的执行，可以通过构建等价的 NFA，然后执行 NFA 来匹配输入的字符串。真实的 JavaScript 中的正则表达式拥有更多的特性，其正则表达式引擎也更加复杂。 简单正则表达式引擎的实现 简单的正则表达式引擎实现 🏈基本的数据结构定义 核心思路是读取正则表达式以后生成对应的NFA，NFA中有边和状态两个结构。边的结构记录了它的起点和终点，同时通过枚举类型记录匹配的其他需求。 //用于处理‘^’字符 enum { NEXCLUDED = false, EXCLUDED = true }; //用于处理预处理类型，0-128以内ASCII字符直接匹配 enum { LCASES=256, UCASES=257, NUM=258, EPSILON=259, ANY=260, WS=261 }; class Edge { public: State *start; State *end; int type; int exclude; Edge(State *s, State *e, int t, bool ex = NEXCLUDED) :start(s), end(e), type(t), exclude(ex) {}; } 状态有预备，成功和失败三种，同时每个状态维护两个向量，向量存储了出边和入边的指针。 enum { READY = -1, SUCCESS = 1, FAIL = 0}; class State { public: int status; std::list&lt;Edge *&gt; InEdges; std::list&lt;Edge *&gt; OutEdges; } NFA 类会存储一个正则表达式，同时存储 NFA 的起点和终点，并使用了两个链表来维护 NFA 的边和状态，同时用一个链表来存储匹配成功的字符串。两个静态的字符串指针用于记录文件和正则表达式字符串的读取状态，静态常量，使得最终函数只会对文件内容和正则表达式扫描一次，避免在匹配成功的字符串中再匹配子串。 char *regex; State *Start; State *End; std::list&lt;Edge *&gt; edgeList; std::list&lt;State *&gt; stateList; std::list&lt;char&gt; matchedChar; static char *regRead; static char *fileRead; } 生成NFA的过程中，通过 currentEnd 和 currentStart 两个指针分别指向当前字符读取完成后生成的最后一个状态和当前字符读取之前的开始状态，维护这两个指针的目的是为了记录 NFA 的生成过程，在处理‘*’、‘+’、‘？’等字符的时候起到了重要的作用。同时我们利用list内置的迭代器对链表进行遍历，这个方式在匹配过程中也用到了。 State *currentEnd, *currentStart; State *alternate; list&lt;Edge *&gt;::iterator itor; 🏈NFA的生成 关键的部分在于匹配字符串时采取的思路，尤其是特殊字符的生成 NFA 的方式，这个不同于课本上最开始的 NFA 生成算法，而是基于读取字符串的过程，同时避免了字符串的回退等，读取一个字符就生成一个对应的边并压入链表中，对‘*’、‘+’，‘？’和特殊符号也是如此，使得处理更加简单的同时避免生成过于冗余的状态，兼顾了时间和空间效率。以下举例说明。 🏈边和状态的生成 边的生成使用 newEdge 函数,需要记录起点和终点，以及类型，同时在生成边以后要用重载的两个 patch函数将状态和边完全连接起来。 void Nfa::newEdge(State * start, State * end, int type, int exclude = NEXCLUDED) { Edge *out = new Edge(start, end, type, exclude); end-&gt;patch(out, end); start-&gt;patch(start, out); edgeList.push_back(out); } 以普通字符的生成和‘.’字符的产生方式为例，他们都是生成一条边和一个新的状态。 case '.': /* any */ currentStart = currentEnd; currentEnd = new State(); newEdge(currentStart, currentEnd, ANY, NEXCLUDED); stateList.push_back(currentEnd); default: currentStart = currentEnd; currentEnd = new State(); newEdge(currentStart, currentEnd, *regRead, NEXCLUDED); stateList.push_back(currentEnd); break; 如下图所示： 接下来的符号处理都假定初始状态如下图所示： 🏈'|'的处理 以 currentStart 指向的状态作为子 NFA 的起点，同时将子 NFA 的终点状态和原 NFA 的终点进行合并。 case '|': // alternate regRead++; currentStart = start; alternate= regex2nfa(regRead, start); currentEnd-&gt;merge(alternate); stateList.remove(alternate); regRead--; 如下图所示： 🏈'?' &amp; '*' &amp; '+'的处理 读取到‘?’只需要在上一条边的基础上继续连接原有的边即可： case '?': // zero or one newEdge(currentStart, currentEnd, EPSILON, NEXCLUDED); break; 读取到‘\\*’后，直接将 currentStart 和 currentEnd 进行合并成环： case '*': // zero or more alternate = currentEnd; currentStart-&gt;merge(alternate); stateList.remove(alternate); currentEnd = currentStart; break; 读取到‘+’后，只需添加若干条边从 currentEnd 状态指向 currentStart 状态的下一个状态即可： case '+': /* one or more */ itor = currentStart-&gt;OutEdges.begin(); for (;itor != currentStart-&gt;OutEdges.end();itor++) newEdge(currentEnd, (*itor)-&gt;end, (*itor)-&gt;type, (*itor)-&gt;exclude); break; 如下图所示： 🏈简单的分组支持 对于中括号和括号进行了一定的支持，括号直接递归调用 NFA 的生成函数，中括号和预定义字符都有其对应的函数进行支持。 🏈NFA匹配 匹配过程采用了递归的方式，step函数调用match函数匹配边和文件字符，匹配成功后即递归调用进入下一个状态。 if (End-&gt;status == SUCCESS) return SUCCESS; for(;itor != current-&gt;OutEdges.end();itor++) { if ((*itor)-&gt;match(fileRead)) { (*itor)-&gt;end-&gt;status = SUCCESS; matchedChar.push_back(*fileRead); ++fileRead; if (step((*itor)-&gt;end)) return SUCCESS; --fileRead; matchedChar.pop_back(); } if ((*itor)-&gt;type == EPSILON &amp;&amp; step((*itor)-&gt;end)) return SUCCESS; } return FAIL; ","link":"https://faded.auspicious.space/post/regular-expression-nfa/"},{"title":"正则表达式——断言","content":" 正则表达式断言 正则表达式大多数结构匹配的文本会出现在最终的匹配结果中，但也有些结构并不真正匹配文本，而只是负责判断某个位置左/右侧是否符合要求，这种结构被称为断言（assertion）。常见的断言有三类： 单词边界、行起始/结束位置、环视。本文主要简单阐述对三类断言的理解。 单词边界 单词边界顾名思义，是指单词字符 (\\w) 能匹配的字符串的左右位置。在 JavaScript、php、Python 2、Ruby 中，单词字符 (\\w) 等同于 [0-9a-zA-Z]，所以在这些语言中，给定一段文本可以用 \\b\\w+\\b 把所有单词提取出来。 例如： ('Love is composed of a single soul inhabiting two bodies.').match(/\\b\\w+\\b/g) return [&quot;Love&quot;, &quot;is&quot;, &quot;composed&quot;, &quot;of&quot;, &quot;a&quot;, &quot;single&quot;, &quot;soul&quot;, &quot;inhabiting&quot;, &quot;two&quot;, &quot;bodies&quot;] 这里值得注意的是，有些单词例如 E-mail 和组合词 I'm 这样的，\\b\\w+\\b 是无法匹配的。如要匹配，可根据需求修改为 \\b['-\\w]\\b。 单词边界记为 \\b，它能匹配的位置：一边是单词字符 \\w，一边是非单词字符 \\W。 与单词边界对应的是非单词边界 \\B，两者关系类似 \\w 与 \\W、\\d 与 \\D。 这里注意，非单词边界（\\B）和单词字符（\\w）是不一样的，因为前者是断言，而后者是普通匹配。 例如： // 式一 String(1234567890).replace(/(?=(\\B)(\\d{3})+$)/g, ',') =&gt; 1,234,567,890 // 式二 String(1234567890).replace(/(?=(\\w)(\\d{3})+$)/g, ',') =&gt; ,123,456,7890 // 附加常用例子，20180911格式化为2018-09-11 '20180911'.replace(/(?=\\B(\\d{2})+$)/g, '-').replace(/-/, '') =&gt;2018-09-11 造成差异的原因就是: 式一中的 \\B 匹配边界（是断言）。第一次匹配时，在 1234567890 中数字 1 的前方时，会环视后方进行肯定断言(?=)：后方必须是满足两个 pattern 才通过。第一个 pattern (\\B)在数字 1 的前方匹配成功；故继续在此位置匹配第二个 pattern (\\d{3})+$，发现 123456789 之后并不是结束符（结束符和开始符也是断言，下文讲述），故匹配失败。开始第二次匹配，从数字 1 和数字 2 的中间开始...最后会匹配成功三个位置：1 和 2 之间、4 和 5 之间、7 和 8 之间，再被,替换，故得到结果。 同理，式二在第一次匹配时，在数字 1 的前方环视后方进行肯定断言：后方必须是满足两个 pattern 才通过。第一个 pattern (\\w) 在数字 1 的前方匹配成功，并将匹配位置移动到 1 和 2 之间；然后继续匹配第二个pattern (\\d{3})+$...第一次匹配成功，故数字 1 前方的断言是成功的，标记该位置...最后得到三个位置：1 前方、3 和 4 之间、6 和 7 之间，再被,替换，故得到结果。 所以 \\B 只是去判断该位置左右是否只有一边有单词字符，另一边不是单词字符，且在匹配成功时，不会导致匹配位置发生改变。说起来算是一种判断吧~ 这种只是匹配某个位置而不是文本的元字符，在正则中也被称为锚点。下文继续介绍常见锚点之二：行起始/结束位置。 行起始/结束位置 ^ 与 $ 分别表示（行）起始位置和（行）结束位置，比如正则表达式 /^lu.*r$/ 只能匹配的 lu 开始并以 r 结束的字符串，例如：luwuer、lu fd --r，不能匹配 nb luwuer、lu fd --rb等。 其实行起始/结束位置断言，常用在正则表达式开启多行模式（Multiline Mode）的情况下。 例如： ('first line\\nsecond line\\nlast line').match(/^\\w+/gm) return [&quot;first&quot;, &quot;second&quot;, &quot;last&quot;] 既然是多行匹配，这里说说如何划分行。 在编辑文本时，敲回车键就向文本输入了行终止符（line terminal），表示结束当前行。这里只需注意，敲入回车时向文本中输入的行终止符在主流平台上是有差别的： Windows 的行终止符是 \\r\\n。 UNIX/Linux/Mac OS 的行终止符是 \\n。 不过正则的行起始/结束位置断言都是可以识别的哈~ 环视 环视是指在某个位置向左/向右看，保证其左/右位置必须出现某类字符（包括单词字符 \\w 和非单词字符\\W），且环视也同上两个断言，只是做一个判断（匹配一个位置，本身不匹配任何字符，但又比上两个断言灵活）。也有人称环视为零宽断言。 环视分为四种： 肯定顺序环视（正向肯定断言）positive-lookahead: ?=pattern； 否定顺序环视（正向否定断言）negative-lookahead: ?!pattern； 肯定逆序环视（反向肯定断言）positive-lookahead: ?&lt;=pattern，js不支持； 否定逆序环视（反向否定断言）negative-lookahead: ?&lt;=pattern，js不支持。 比如我们要匹配一串文字中包含在书名号《》中的书名，如不考虑环视可能需要如下实现： ('三体是刘慈欣创作的系列长篇科幻小说，由《三体》、《三体Ⅱ·黑暗森林》、《三体Ⅲ·死神永生》组成。').match(/《.*?》/g).join(',').replace(/[《》]/g, '').split(',') return [&quot;三体&quot;, &quot;三体Ⅱ·黑暗森林&quot;, &quot;三体Ⅲ·死神永生&quot;] 正则默认是贪婪模式（在整个表达式匹配成功的前提下，尽可能多的匹配），开启非贪婪模式（在整个表达式匹配成功的前提下，尽可能少的匹配）的方法：在贪婪量词 {m,n}、{m,}、?、*、+ 后加上一个 ? 号，例如 +?。 而在使用环视时会更简单： ('三体是刘慈欣创作的系列长篇科幻小说，由《三体》、《三体Ⅱ·黑暗森林》、《三体Ⅲ·死神永生》组成。').replace(/《/g,'\\n').match(/^.*?(?=》)/gm) return [&quot;三体&quot;, &quot;三体Ⅱ·黑暗森林&quot;, &quot;三体Ⅲ·死神永生&quot;] 似乎也没简单多少...当然最主要的原因是js不支持逆序环视啦啦啦 再举例，匹配6位数字构成的字符串： // 无环视 'http://luwuer.com/629212/1234567890'.match(/[^\\d]\\d{6}[^\\d]/g).join('').match(/\\d{6}/g) return [&quot;629212&quot;] // 环视 'http://luwuer.com/629212/1234567890'.match(/(?!\\d).\\d{6}(?!\\d)/g).join('').match(/\\d{6}/g) return [&quot;629212&quot;] 其实环视在js中更多的是与replace函数组合，就像在单词边界一节中最后的例子。 ","link":"https://faded.auspicious.space/post/regular-expression-assert/"},{"title":"jQuery——拓展","content":"😲 extend函数 $.extend(target,[object1],[onjectN]) $.extend([deep],target,object1,[objectN]) var obj1 = { height: 100, width: 100, length: 100, div: { x: 100, y: 100 } }; var obj2 = { height: 200, width: 200, div: { x: 200 } }; $.extend(obj1, obj2); console.log(obj1.height); console.log(obj1.div.y); //result:200,undefined 当使用true参数时， var obj1 = { height: 100, width: 100, length: 100, div: { x: 100, y: 100 } }; var obj2 = { height: 200, width: 200, div: { x: 200 } }; $.extend(true, obj1, obj2); console.log(obj1.height); console.log(obj1.div.y); //result:200,100 拓展jQuery的公共函数 $.extend({ minValue: function(a, b) { return a &gt; b ? a: b } }); var a = prompt(&quot;input a&quot;); var b = prompt(&quot;input b&quot;); console.log($.minValue(a, b)); $.fn.extend() 方法可以创建 jQuery 对象方法 $.fn.extend({ test: function() { alert(&quot;click &quot; + $(this).html() + &quot; this is test function&quot;); } }); $(&quot;#fnExtend&quot;).click(function() { $(this).test(); }); 😲 自定义jQuery函数 🤗 添加新的全局函数 $.clickDiv = function(node) { console.log(node.text() + &quot; click&quot;); }; $(&quot;div&quot;).click(function() { $.clickDiv($(this)); }); 🤗 通过 extend 函数添加全局函数 $.extend({ foo: function() { alert(&quot;this is a new function 'foo()'&quot;); } }); $.foo(); 🤗 使用命名空间 $.myPluin = { ale: function() { alert(&quot;function from myPluin&quot;); } }; $.nextPluin = { ale: function() { alert(&quot;function from nextPluin&quot;); } } $.myPluin.ale(); $.nextPluin.ale(); 😲 自定义选择器 $.myPluin = { ale: function() { alert(&quot;function from myPluin&quot;); } }; $.nextPluin = { ale: function() { alert(&quot;function from nextPluin&quot;); } }; index = -1; //定义全局变量 index jQuery.expr[&quot;:&quot;].le = function(elem, i, match) { // return i&gt;match[3]-0||i==match[3] console.log(index); index++; return index &gt; match[3] - 0; // 返回索引大于 3 的元素 }; $(&quot;p:le(2)&quot;).css(&quot;color&quot;, &quot;red&quot;); // 返回元素索引值大于等于 2 的元素 $.myPluin.ale(); $.nextPluin.ale(); ","link":"https://faded.auspicious.space/post/jquery-extension/"},{"title":"jQuery——选择器","content":"基本选择器 🎼 ID 选择器： // 选中 id 为 myDiv 的元素，速度最快 $(&quot;#myDiv&quot;) 🎼 类选择器： // 选中 class 属性为 red 的所有元素 $(&quot;.red&quot;) 🎼 元素选择器： // 选中所有 div 元素 $(&quot;div&quot;) 🎼 通配符选择器： // 选中所有元素 $(&quot;*&quot;) 🎼 复合选择器： // 选中所有 span 元素和所有 id 为 myDiv 的元素 $(&quot;span,#myDiv&quot;) 层次选择器 🎼 选择器1 选择器2： // 选中 body 内的所有 div 元素 $(&quot;body div&quot;) 🎼 选择器1 &gt; 选择器2： // 选中 body 内的所有直接 div 元素，不查找间接元素 $(&quot;body &gt; div&quot;) 🎼 选择器1 + 选择器2： // 选中 body 内的所有 div 元素 $(&quot;body div&quot;) 🎼 选择器1 ~ 选择器2： // 选中 body 内的所有 div 元素 $(&quot;body div&quot;) 基本过滤选择器 🎼 第一个元素选择器 // 选中第一个 div 元素 $(&quot;div:first&quot;) 🎼 最后一个元素选择器 // 选中最后一个 div 元素 $(&quot;div:last&quot;) 🎼 排除选择器 // 选中 class 不为 red 的所有 div 元素 $(&quot;div:not(.red)&quot;) 🎼 偶数选择器 // 选中索引值为偶数的 div 元素 $(&quot;div:even&quot;) 🎼 奇数选择器 // 选中索引值为奇数的 div 元素 $(&quot;div:odd&quot;) 🎼 索引值选择器 // 选中索引值为 2 的 div 元素 $(&quot;div:eq(2)&quot;) // 选中索引值大于 2 的 div 元素 $(&quot;div:gt(2)&quot;) // 选中索引值小于2的 div 元素 $(&quot;div:lt(2)&quot;) 内容过滤选择器 // 选中所有包含文本 ok 的 div 元素 $(&quot;div:contains(ok)&quot;) // 选中所有为空的 div 元素 $(&quot;div:empty&quot;) // 选中所有包含 class 为 red 的 div 元素 $(&quot;div:has(.red)&quot;) // 选中所有不为空的 div 元素 $(&quot;div:parent&quot;) 可见性过滤选择器 // 选中所有不可见的 div 元素 $(&quot;div:hidden&quot;) // 选中所有可见的 div 元素 $(&quot;div:visible&quot;) 属性过滤选择器 // 选中所有包含属性 title 的 div 元素 $(&quot;div[title]&quot;) // 选中所有属性 title 等于 ok 的 div 元素 $(&quot;div[title=ok]&quot;) // 选中所有属性 title 不等于 ok 的 div 元素 $(&quot;div[title!=ok]&quot;) // 选中所有属性 title 值以 ok 开头的 div 元素 $(&quot;div[title^=ok]&quot;) // 选中所有属性 title 值含有 ok 的 div 元素 $(&quot;div[title*=ok]&quot;) // 选中所有包含属性 id，并且属性 title 值以 ok 开头的 div 元素 $(&quot;div[id][title^=ok]&quot;) 子元素过滤选择器 // 选中所有是第二个子结点的 div 元素 $(&quot;div:nth-child(2)&quot;) // 选中所有是第一个子结点的 div 元素 $(&quot;div:first-child&quot;) // 选中所有是最后一个子结点的 div 元素 $(&quot;div:last-child&quot;) // 选中所有是唯一子结点的 div 元素 $(&quot;div:only-child&quot;) 表单属性过滤选择器 // 选中表单内可用 input $(&quot;#form1 input:enabled&quot;) // 选中表单内不可用 input $(&quot;#form1 input:disabled&quot;) // 选中表单内所有选中的元素 $(&quot;#form1 input:checked&quot;) // 选中下拉列表中选中的元素 $(&quot;select &gt; option:selected&quot;) ","link":"https://faded.auspicious.space/post/jquery-selectors/"},{"title":"正则表达式——匹配","content":"💊(?:pattern) 非获取匹配，匹配 pattern 但不获取匹配结果，不进行存储供以后使用。这在使用或字符“(|)”来组合一个模式的各个部分是很有用。例如“industr(?:y|ies)”就是一个比“industry|industries”更简略的表达式。 💊 (?=pattern) 非获取匹配，正向肯定预查，在任何匹配 pattern 的字符串开始处匹配查找字符串，该匹配不需要获取供以后使用。例如，“Windows(?=95|98|NT|2000)”能匹配“Windows2000”中的“Windows”，但不能匹配“Windows3.1”中的“Windows”。预查不消耗字符，也就是说，在一个匹配发生后，在最后一次匹配之后立即开始下一次匹配的搜索，而不是从包含预查的字符之后开始。 💊 (?!pattern) 非获取匹配，正向否定预查，在任何不匹配 pattern 的字符串开始处匹配查找字符串，该匹配不需要获取供以后使用。例如“Windows(?!95|98|NT|2000)”能匹配“Windows3.1”中的“Windows”，但不能匹配“Windows2000”中的“Windows”。 💊 (?&lt;=pattern) 非获取匹配，反向肯定预查，与正向肯定预查类似，只是方向相反。例如，“(?&lt;=95|98|NT|2000)Windows”能匹配“2000Windows”中的“Windows”，但不能匹配“3.1Windows”中的“Windows”。 💊 (?&lt;!pattern) 非获取匹配，反向否定预查，与正向否定预查类似，只是方向相反。例如“(?&lt;!95|98|NT|2000)Windows”能匹配“3.1Windows”中的“Windows”，但不能匹配“2000Windows”中的“Windows”。这个地方不正确，有问题 ","link":"https://faded.auspicious.space/post/regular-expression-pattern-matching/"},{"title":"设计师网站","content":"设计师导航 网站 网址 全球100+知名设计网站 http://www.bigbigwork.com/nav/6.html CND设计网址导航 - 优秀设计网站排名大全 http://wz.cndesign.com/ 拍信 https://www.paixin.com/ 我图网 https://www.ooopic.com/ 包图网 https://ibaotu.com/ 素材天下 http://www.sucaitianxia.net/ 素材中国 http://www.sccnn.com/ 站长素材 http://sc.chinaz.com/ 红动中国 https://www.redocn.com/ 千库网 https://588ku.com/ 觅元素 http://www.51yuansu.com/ unDraw https://undraw.co/ DrawKit https://www.drawkit.io/ pngtree https://pngtree.com/ VCG https://www.vcg.com/ Textures for 3D, graphic design and Photoshop! https://www.textures.com/ 無料DTP素材 【素材ページ 】食材・料理の著作権フリー写真 http://www.sozai-page.com/index.html 免费模板网 http://www.wangjie.org/ Landing page templates for startups https://cruip.com/ Avataaars Generator https://getavataaars.com/?avatarStyle=Circle 中国色 http://zhongguose.com/ 完美对称无缝平铺背景图底纹素材 - 图鱼 https://www.hituyu.com/ 花瓣网_陪你做生活的设计师（创意灵感天堂，搜索、发现设计灵感、设计素材） https://huaban.com/ 站酷 (ZCOOL) - 设计师互动平台 - 打开站酷，发现更好的设计！ https://www.zcool.com.cn/ UI中国用户体验设计平台 https://www.ui.cn/ 68Design - 找兼职设计师就上68Design - 【设计师接单平台】 https://www.68design.net/ Flat Design Inspiration - Flat UI https://flatui.com/ UI Movement - The best UI design inspiration, every day https://uimovement.com/ Collect UI - Daily inspiration collected from daily ui archive and beyond. Based on Dribbble shots, hand picked, updating daily. http://www.collectui.com/ siteInspire - Web Design Inspiration https://www.siteinspire.com/ Dribbble - Discover the World’s Top Designers &amp; Creative Professionals https://dribbble.com/ Blocs - Fast, easy to use and powerful visual web design tool, that lets you create responsive websites without writing code. https://blocsapp.com/ UI Design Resources, UI Kits, Wireframes, Icons and More - UI8 https://ui8.net/ UI-Patterns.com https://ui-patterns.com/ 学UI网-UI设计师导航网，最专业的UI设计网站 http://hao.xueui.cn/ 优设导航 - 学设计从这里开始！ http://hao.uisdc.com/ 饭团导航 精选设计师实用工具导航 hao.psefan.com http://hao.psefan.com/ 设计导航 - 精选最好的设计网站大全 https://hao.shejidaren.com/ 优波设计 - 设计师必备网址导航 ubuuk.com https://www.ubuuk.com/ 设计订阅 - 腾讯设计导航 https://idesign.qq.com/#!index/feed http://www.bigbigwork.com/nav/6.html http://www.foolo.cn/ 工业设计网站导航 | 设计癖 http://hao.shejipi.com/ 46设计导航_设计网站大全_46design.com http://www.46design.com/2019/ UI设计师导航网 - 优阁 http://so.uigreat.com/ Canva在线平面设计软件_免费设计模板素材和海量正版图片 - Canva中文官网 https://www.canva.cn/ 创客贴_平面设计作图神器_免费设计模板_在线稿定设计印刷 https://www.chuangkit.com/ 轻量级在线平面设计工具 - 图帮主 https://www.tubangzhu.com/ 图怪兽作图神器-在线图片编辑器-PS图片制作-搞定平面设计不求人 https://818ps.com/ Fotor在线设计工具_免费设计素材和模板_在线平面设计网站 https://www.fotor.com.cn/ Presentation Software Online Presentation Tools 设计癖 | 关注设计癖 提升幸福感 http://www.shejipi.com/ xiaopiu-产品原型设计工具与团队实时协作平台 https://www.xiaopiu.com/ 燃设计-共享全球好设计_软装素材分享_软装设计灵感图库 http://www.ransheji.com/ Themes - macOS - Human Interface Guidelines - Apple Developer https://developer.apple.com/design/human-interface-guidelines/macos/overview/themes/ Overview - Atlassian Design https://atlassian.design/guidelines/product/overview 介绍 - Ant Design https://ant.design/docs/spec/introduce-cn WeUI https://weui.io/ Documentation - Materialize https://materializecss.com/ Styleguide https://www.yelp.com/styleguide/mobile 优优灵感-设计师灵感展现与启发-优优教程网 https://uiiiuiii.com/inspiration Crello — Free Graphic Design Software Create Images Online Tool 优设导航 - 学设计从这里开始！ https://hao.uisdc.com/ Creative Mass https://creativemass.cn/#/ 设计师之家 https://www.51sjsj.com/ Design Seeds for all who ♥ color The Nordnet Brand - Nordnet Brand https://brand.nordnet.se/ STUDIO Design to live website in one click. KOPPT，一个做PPT的神器！ http://koppt.cn/ 数据可视化工具目录 https://datavizcatalogue.com/ZH/index.html 图片素材 网站 网址 Unsplash https://unsplash.com/ Pexels https://www.pexels.com/zh-cn/ Gratisography https://gratisography.com/ Beautiful free stock photos https://stocksnap.io/ Foodiesfeed https://www.foodiesfeed.com/ Freephotos https://freephotos.cc/zh Uniquely free photos. https://www.reshot.com/ Free images for creatives, by creatives https://morguefile.com/quest 沙沙野 https://www.ssyer.com/ 图虫 https://tuchong.com/ 摄图网 https://699pic.com/ 7MX——Home Business Advertising Ideas https://7mx.com/ 图品汇 https://www.88tph.com/ Free Photos for bloggers and creatives! http://photopin.com/ 花瓣美素 http://www.meisupic.com/ PAKUTASO https://www.pakutaso.com/ 懒人图库 http://www.lanrentuku.com/ SEARCH FOR CONTENT TO REUSE https://search.creativecommons.org/ Free Stock Photos by Canva https://www.canva.com/photos/free/ Creative Briefs. Request for photos https://morguefile.com/quest ImageFinder https://imagefinder.co/ 泼辣有图 http://www.polayoutu.com/collections visualhunt https://visualhunt.com/ foter https://foter.com/ Free high resolution photography - Life of Pix - Home https://www.lifeofpix.com/ New Old Stock https://nos.twnsnd.co/ 千图网 https://www.58pic.com/ Hand-picked free photos for your inspiration - Magdeleine https://magdeleine.co/ 昵图网 http://www.nipic.com/ photock https://www.photock.jp/ 免费正版高清图片素材库 https://pixabay.com/zh/ piqsels https://www.piqsels.com/zh DesignersPics http://www.designerspics.com/ freeimages https://cn.freeimages.com/ StreetWill http://www.streetwill.co/ Discover and share the world's best photos https://web.500px.com/ FREE TRAVEL PHOTOS https://www.bucketlistly.blog/photos Free Stock Photos For Commercial Use. https://www.splitshire.com/splitshire-free-stock-photos/ BURST https://burst.shopify.com/ FOCA https://focastock.com/ jay mantri https://jaymantri.com/#= LET'S FIND THE PERFECT PHOTO FOR YOU https://kaboompics.com/ A curated collection of free web design resources, all for commercial use. http://imcreator.com/free Zoommy https://zoommyapp.com/ STOKPIC - Free Stock Photos For Commercial Use https://stokpic.com/ Cupcake http://cupcake.nilssonlee.se/ Folkert Gorter Superfamous Images https://images.superfamous.com/ PICGRAPHY https://picography.co/ Free stock illustrations, Beautiful Free Art - Mixkit https://mixkit.co/free-stock-art/ Free Stock Photos https://photo-ac.com/ scrolller https://scrolller.com/art JOHN KRAUS PHOTOS https://www.johnkrausphotos.com/Portfolio/ Picrew https://picrew.me/ GENERATED FACES https://generated.photos/faces 用大作，不用翻墙和VPN秒看pixabay上的设计 http://www.bigbigwork.com/pixabay.html Awesome Wallpapers - wallhaven.cc https://wallhaven.cc/ 摄图网-正版高清图片免费下载_商用设计素材图库http://699pic.com/ 纹理 网站 网址 Subtle Patterns Free textures for your next web project 完美对称无缝平铺背景图底纹素材 - 图鱼 https://www.hituyu.com/ The Pattern Library http://thepatternlibrary.com 渐变 网站 网址 Fresh Background Gradients WebGradients.com 💎 uiGradients - Beautiful colored gradients https://uigradients.com/#TalkingToMiceElf LowPoly背景下载网站 网站 网址 uiGradients - Beautiful colored gradients https://uigradients.com/#Blu 地图生成网站 网站 网址 Pixel Map Generator amCharts 样机生成网站 网站 网址 Smartmockups - Free product mockup generator https://smartmockups.com/ Sketchsheets - Ready to print sketch sheet templates for UX designers https://sketchsheets.com/ Make Mockups, Logos, Videos and Designs in Seconds https://placeit.net/ GIF 网站 网址 With Stock Animated GIFs Crafted for Commercial Use https://cliply.co/ SOOGIF，找动图做动图.gif https://www.soogif.com/ Search all the GIFs and Stickers https://giphy.com/ LOGO 网站 网址 Instant Logo Search http://instantlogosearch.com/ Logo Maker - Create Your Own Logo, It's Free! - FreeLogoDesign https://www.freelogodesign.org/ PNG 网站 网址 free PNGs https://www.freepngs.com/search-pngs CLEAN PNG https://www.cleanpng.com/ 365psd https://cn.365psd.com/free-psd Download Free Vectors, Clipart Graphics, Vector Art &amp; Design Templates https://www.vecteezy.com/ freepik https://www.freepik.com/ humaaans https://www.humaaans.com/ illustrations 网站 网址 Free Vector Illustrations to Class up Your Project https://icons8.com/ouch IRA Design - Build your own amazing illustrations @ Creative Tim https://iradesign.io/ absurd illustrations that make sense https://absurd.design/ Illustration Gallery by ManyPixels Open-Source Editable Illustrations Free Vectors, Stock Photos &amp; PSD Downloads Freepik Illustration Gallery https://www.manypixels.co/gallery/ Illustration Gallery https://www.manypixels.co/gallery/ FREE ILLUSTRATIONS https://lukaszadam.com/illustrations 视频素材 网站 网址 Thousands of Free High-Resolution CC0 Photos and Videos https://isorepublic.com/ NASA Image and Video Library https://images.nasa.gov/ COVERR - Beautiful Free Stock Video Footage https://coverr.co/ Golden Wolf https://goldenwolf.tv/ 场库 https://www.vmovier.com/ Free stock videos · Pexels Videos https://www.pexels.com/videos/ 天空之城 https://www.skypixel.com/ Distill: Free HD Stock Video &amp; HD Video Clips https://wedistill.io/ Free Video Footage - Best Free Backgrounds Stock Video Footage https://www.free-video-footage.com/ Free Motion Backgrounds MP4, MOV video backgrounds for FREE! Free Stock Video Footage HD 4K Download Motion Graphics https://www.videvo.net/ Free Stock Footage Videos, 4k After Effects Templates and More! https://www.videezy.com/ Free 4K Stock Video | Stock Footage for Free – {Dareful} Completely Free 4K Stock Video https://www.dareful.com/ Free Stock Video Footage HD Royalty-Free Videos Download https://mazwai.com/#/ iTunes Movie Trailers https://trailers.apple.com/ Mixkit - Awesome free assets for your next video project https://mixkit.co/ XStockvideo http://www.xstockvideo.com/ ICON 网站 网址 IconMoon https://icomoon.io/ iconmonstr https://iconmonstr.com/ Zwicon – Icon set https://www.zwicon.com/cheatsheet.html Find Similar Icons http://compute.vision/nouns/index.html 图标下载，ICON(SVG/PNG/ICO/ICNS)图标搜索下载 - Easyicon http://www.easyicon.net Icon Ninja - 33350 vector icons and 700081 png icons for free download https://www.iconninja.com/ iconSweets — DesignBombs https://designbombs.com/iconsweets/ easyicon https://www.easyicon.net/ Icons for everything https://thenounproject.com/ SVG Icons Library - Vivid.js https://webkul.github.io/vivid/ 字体 网站 网址 iconfont https://www.iconfont.cn/ 100font.com - 免版权字体下载、免费商用字体下载网站 https://www.100font.com/ 造字工房 https://www.makefont.com/ 方正字库 http://www.foundertype.com/ 汉仪字库-用心绽放文字之美 http://www.hanyi.com.cn/ Font-To-Width http://font-to-width.com/ 字体下载-求字体网提供中文和英文字体库下载、识别与预览服务，找字体的好帮手 http://www.qiuziti.com/ ","link":"https://faded.auspicious.space/post/su-cai-wang-zhan/"},{"title":"资源网站","content":"PPT 网站 网址 51PPT模板 http://www.51pptmoban.com/ 优品PPT http://www.ypppt.com/ Office PLUS http://www.officeplus.cn/Template/Home.shtml 办公资源 https://www.bangongziyuan.com/ PPT Boss https://www.pptboss.com/template-center PPT之家 https://www.52ppt.com/moban/ 第一PPT http://www.1ppt.com/ 比格PPT http://www.tretars.com/ppt-templates 叮当设计PPT &lt;http://www.dingdangsheji.com/category/ppt/) 我图网精选PPT https://www.ooopic.com/intro/kidHome/ppt/ 设计师导航 网站 网址 叮当设计 http://www.dingdangsheji.com/ &lt;&gt; ","link":"https://faded.auspicious.space/post/zi-yuan-wang-zhan/"},{"title":"特色搜索","content":"网盘搜索 网站 网址 盘搜搜 https://www.pansoso.com/ 如风搜 http://www.rufengso.net/ 6miu百度云搜索 http://baiduyun.6miu.com/ 57分享百度云 https://www.57fx.com/user-drnew-daren/ 小不点搜索 https://www.xiaoso.net/ 网盘搜索，就用大圣盘 - 最好用的百度网盘搜索引擎 https://www.dashengpan.com/ 特色搜索 网站 网址 龙轩搜索 http://ilxdh.com/ 虫部落搜索 https://www.chongbuluo.com/ neets搜索站 https://neets.cc/ 西林街搜索 https://xilinjie.cc/ 茶杯狐 https://www.cupfox.com/ 疯狂影视搜索 http://ifkdy.com/ AnywhereAnything http://lackar.com/aa/ 源代码搜索 https://publicwww.com/ 变量名搜索 https://unbug.github.io/codelf/ 比菲尔德学术搜索 https://www.base-search.net/ 吉他尤克里里谱搜索 https://sopu.52cmajor.com/ Classcentral在线课程搜索 https://www.classcentral.com/ Coursade在线课程搜索 http://www.coursade.com/ Chinese Etymology 字源 https://hanziyuan.net/ 汉典 https://www.zdic.net/ 新华字典 https://zidian.911cha.com/ 导航站 网站 网址 创造狮导航 http://chuangzaoshi.com/ 导航湾 https://www.daohangwan.com/ 好用好玩导航 http://www.haoyonghaowan.com/ 比格张 https://bigezhang.com/ Web前端导航 http://nav.web-hub.cn/ 阿猫阿狗导航 https://dh.woshipm.com/ BTMoo导航 https://www.btmoo.net/ ","link":"https://faded.auspicious.space/post/te-se-sou-suo/"},{"title":"电子书下载","content":"电子书下载 网站 网址 Baen free library https://www.baen.com/allbooks/category/index/id/2012 智奇搜书 https://www.zqbook.top/ 云海电子图书馆 http://www.pdfbook.cn/ 必看网 https://www.biikan.com/ bookboon https://bookboon.com/en Free-Ebooks https://www.free-ebooks.net/ ZLibrary https://b-ok.org/ 书格 https://new.shuge.org/ Academia https://www.academia.edu/ 图灵社区 https://www.ituring.com.cn/ 书伴 https://bookfere.com/ 好读 http://haodoo.net/ ePUBee http://cn.epubee.com/books/ 三秋书屋 https://www.d4j.cn/ SoBooks https://sobooks.cc/ i-Book.in https://book.tstrs.me/ WOW! eBook https://www.wowebook.org/ 计算机书籍控 http://bestcbooks.com/ 鸠摩搜索 https://www.jiumodiary.com/ Library Genesis http://gen.lib.rus.ec/ Library Genesis 2M http://libgen.io/ 免费的计算机编程类中文书籍 https://github.com/justjavac/free-programming-books-zh_CN 国立国会图书馆 https://dl.ndl.go.jp/ ZLibrary https://b-ok.cc/s/ Free ebooks for .NET &amp; JavaScript Developers | Syncfusion https://www.syncfusion.com/ebooks PDFDrive https://www.pdfdrive.com/ BookSC https://booksc.org/ epubw https://epubw.com/ Manybooks https://manybooks.net/ 电子书转换 网站 网址 在线电子书转换器 http://cn.epubee.com/ 文本或 eBook 转换为 Mobi 格式 https://ebook.online-convert.com/convert-to-mobi ","link":"https://faded.auspicious.space/post/ebooks-download/"},{"title":"正则表达式——银行卡号","content":"记录中国主要银行发行的银行卡号的正则表达式 多种银行卡正则 var bankcardList = [{ bankName: &quot;中国邮政储蓄银行&quot;, bankCode: &quot;PSBC&quot;, patterns: [{ reg: /^(621096|621098|622150|622151|622181|622188|622199|955100|621095|620062|621285|621798|621799|621797|620529|621622|621599|621674|623218|623219)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(62215049|62215050|62215051|62218850|62218851|62218849)\\d{11}$/g, cardType: &quot;DC&quot; }, { reg: /^(622812|622810|622811|628310|625919)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;中国工商银行&quot;, bankCode: &quot;ICBC&quot;, patterns: [{ reg: /^(620200|620302|620402|620403|620404|620406|620407|620409|620410|620411|620412|620502|620503|620405|620408|620512|620602|620604|620607|620611|620612|620704|620706|620707|620708|620709|620710|620609|620712|620713|620714|620802|620711|620904|620905|621001|620902|621103|621105|621106|621107|621102|621203|621204|621205|621206|621207|621208|621209|621210|621302|621303|621202|621305|621306|621307|621309|621311|621313|621211|621315|621304|621402|621404|621405|621406|621407|621408|621409|621410|621502|621317|621511|621602|621603|621604|621605|621608|621609|621610|621611|621612|621613|621614|621615|621616|621617|621607|621606|621804|621807|621813|621814|621817|621901|621904|621905|621906|621907|621908|621909|621910|621911|621912|621913|621915|622002|621903|622004|622005|622006|622007|622008|622010|622011|622012|621914|622015|622016|622003|622018|622019|622020|622102|622103|622104|622105|622013|622111|622114|622017|622110|622303|622304|622305|622306|622307|622308|622309|622314|622315|622317|622302|622402|622403|622404|622313|622504|622505|622509|622513|622517|622502|622604|622605|622606|622510|622703|622715|622806|622902|622903|622706|623002|623006|623008|623011|623012|622904|623015|623100|623202|623301|623400|623500|623602|623803|623901|623014|624100|624200|624301|624402|623700|624000)\\d{12}$/g, cardType: &quot;DC&quot; }, { reg: /^(622200|622202|622203|622208|621225|620058|621281|900000|621558|621559|621722|621723|620086|621226|621618|620516|621227|621288|621721|900010|623062|621670|621720|621379|621240|621724|621762|621414|621375|622926|622927|622928|622929|622930|622931|621733|621732|621372|621369|621763)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(402791|427028|427038|548259|621376|621423|621428|621434|621761|621749|621300|621378|622944|622949|621371|621730|621734|621433|621370|621764|621464|621765|621750|621377|621367|621374|621731|621781)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(9558)\\d{15}$/g, cardType: &quot;DC&quot; }, { reg: /^(370246|370248|370249|370247|370267|374738|374739)\\d{9}$/g, cardType: &quot;CC&quot; }, { reg: /^(427010|427018|427019|427020|427029|427030|427039|438125|438126|451804|451810|451811|458071|489734|489735|489736|510529|427062|524091|427064|530970|530990|558360|524047|525498|622230|622231|622232|622233|622234|622235|622237|622239|622240|622245|622238|451804|451810|451811|458071|628288|628286|622206|526836|513685|543098|458441|622246|544210|548943|356879|356880|356881|356882|528856|625330|625331|625332|622236|524374|550213|625929|625927|625939|625987|625930|625114|622159|625021|625022|625932|622889|625900|625915|625916|622171|625931|625113|625928|625914|625986|625925|625921|625926|625942|622158|625917|625922|625934|625933|625920|625924|625017|625018|625019)\\d{10}$/g, cardType: &quot;CC&quot; }, { reg: /^(45806|53098|45806|53098)\\d{11}$/g, cardType: &quot;CC&quot; }, { reg: /^(622210|622211|622212|622213|622214|622220|622223|622225|622229|622215|622224)\\d{10}$/g, cardType: &quot;SCC&quot; }, { reg: /^(620054|620142|620184|620030|620050|620143|620149|620124|620183|620094|620186|620148|620185)\\d{10}$/g, cardType: &quot;PC&quot; }, { reg: /^(620114|620187|620046)\\d{13}$/g, cardType: &quot;PC&quot; }] }, { bankName: &quot;中国农业银行&quot;, bankCode: &quot;ABC&quot;, patterns: [{ reg: /^(622841|622824|622826|622848|620059|621282|622828|622823|621336|621619|622821|622822|622825|622827|622845|622849|623018|623206|621671|622840|622843|622844|622846|622847|620501)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(95595|95596|95597|95598|95599)\\d{14}$/g, cardType: &quot;DC&quot; }, { reg: /^(103)\\d{16}$/g, cardType: &quot;DC&quot; }, { reg: /^(403361|404117|404118|404119|404120|404121|463758|519412|519413|520082|520083|552599|558730|514027|622836|622837|628268|625996|625998|625997|622838|625336|625826|625827|544243|548478|628269)\\d{10}$/g, cardType: &quot;CC&quot; }, { reg: /^(622820|622830)\\d{10}$/g, cardType: &quot;SCC&quot; }] }, { bankName: &quot;中国银行&quot;, bankCode: &quot;BOC&quot;, patterns: [{ reg: /^(621660|621661|621662|621663|621665|621667|621668|621669|621666|456351|601382|621256|621212|621283|620061|621725|621330|621331|621332|621333|621297|621568|621569|621672|623208|621620|621756|621757|621758|621759|621785|621786|621787|621788|621789|621790|622273|622274|622771|622772|622770|621741|621041)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(621293|621294|621342|621343|621364|621394|621648|621248|621215|621249|621231|621638|621334|621395|623040|622348)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(625908|625910|625909|356833|356835|409665|409666|409668|409669|409670|409671|409672|512315|512316|512411|512412|514957|409667|438088|552742|553131|514958|622760|628388|518377|622788|628313|628312|622750|622751|625145|622479|622480|622789|625140|622346|622347)\\d{10}$/g, cardType: &quot;CC&quot; }, { reg: /^(518378|518379|518474|518475|518476|524865|525745|525746|547766|558868|622752|622753|622755|524864|622757|622758|622759|622761|622762|622763|622756|622754|622764|622765|558869|625905|625906|625907|625333)\\d{10}$/g, cardType: &quot;SCC&quot; }, { reg: /^(53591|49102|377677)\\d{11}$/g, cardType: &quot;SCC&quot; }, { reg: /^(620514|620025|620026|620210|620211|620019|620035|620202|620203|620048|620515|920000)\\d{10}$/g, cardType: &quot;PC&quot; }, { reg: /^(620040|620531|620513|921000|620038)\\d{13}$/g, cardType: &quot;PC&quot; }] }, { bankName: &quot;中国建设银行&quot;, bankCode: &quot;CCB&quot;, patterns: [{ reg: /^(621284|436742|589970|620060|621081|621467|621598|621621|621700|622280|622700|623211|623668)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(421349|434061|434062|524094|526410|552245|621080|621082|621466|621488|621499|622966|622988|622382|621487|621083|621084|620107)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(436742193|622280193)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(553242)\\d{12}$/g, cardType: &quot;CC&quot; }, { reg: /^(625362|625363|628316|628317|356896|356899|356895|436718|436738|436745|436748|489592|531693|532450|532458|544887|552801|557080|558895|559051|622166|622168|622708|625964|625965|625966|628266|628366|622381|622675|622676|622677)\\d{10}$/g, cardType: &quot;CC&quot; }, { reg: /^(5453242|5491031|5544033)\\d{11}$/g, cardType: &quot;CC&quot; }, { reg: /^(622725|622728|436728|453242|491031|544033|622707|625955|625956)\\d{10}$/g, cardType: &quot;SCC&quot; }, { reg: /^(53242|53243)\\d{11}$/g, cardType: &quot;SCC&quot; }] }, { bankName: &quot;中国交通银行&quot;, bankCode: &quot;COMM&quot;, patterns: [{ reg: /^(622261|622260|622262|621002|621069|621436|621335)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(620013)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(405512|601428|405512|601428|622258|622259|405512|601428)\\d{11}$/g, cardType: &quot;DC&quot; }, { reg: /^(49104|53783)\\d{11}$/g, cardType: &quot;CC&quot; }, { reg: /^(434910|458123|458124|520169|522964|552853|622250|622251|521899|622253|622656|628216|622252|955590|955591|955592|955593|628218|625028|625029)\\d{10}$/g, cardType: &quot;CC&quot; }, { reg: /^(622254|622255|622256|622257|622284)\\d{10}$/g, cardType: &quot;SCC&quot; }, { reg: /^(620021|620521)\\d{13}$/g, cardType: &quot;PC&quot; }] }, { bankName: &quot;招商银行&quot;, bankCode: &quot;CMB&quot;, patterns: [{ reg: /^(402658|410062|468203|512425|524011|622580|622588|622598|622609|95555|621286|621483|621485|621486|621299)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(690755)\\d{9}$/g, cardType: &quot;DC&quot; }, { reg: /^(690755)\\d{12}$/g, cardType: &quot;DC&quot; }, { reg: /^(356885|356886|356887|356888|356890|439188|439227|479228|479229|521302|356889|545620|545621|545947|545948|552534|552587|622575|622576|622577|622578|622579|545619|622581|622582|545623|628290|439225|518710|518718|628362|439226|628262|625802|625803)\\d{10}$/g, cardType: &quot;CC&quot; }, { reg: /^(370285|370286|370287|370289)\\d{9}$/g, cardType: &quot;CC&quot; }, { reg: /^(620520)\\d{13}$/g, cardType: &quot;PC&quot; }] }, { bankName: &quot;中国民生银行&quot;, bankCode: &quot;CMBC&quot;, patterns: [{ reg: /^(622615|622616|622618|622622|622617|622619|415599|421393|421865|427570|427571|472067|472068|622620)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(545392|545393|545431|545447|356859|356857|407405|421869|421870|421871|512466|356856|528948|552288|622600|622601|622602|517636|622621|628258|556610|622603|464580|464581|523952|545217|553161|356858|622623|625912|625913|625911)\\d{10}$/g, cardType: &quot;CC&quot; }, { reg: /^(377155|377152|377153|377158)\\d{9}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;中国光大银行&quot;, bankCode: &quot;CEB&quot;, patterns: [{ reg: /^(303)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(90030)\\d{11}$/g, cardType: &quot;DC&quot; }, { reg: /^(620535)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(620085|622660|622662|622663|622664|622665|622666|622667|622669|622670|622671|622672|622668|622661|622674|622673|620518|621489|621492)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(356837|356838|486497|622657|622685|622659|622687|625978|625980|625981|625979|356839|356840|406252|406254|425862|481699|524090|543159|622161|622570|622650|622655|622658|625975|625977|628201|628202|625339|625976)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;中信银行&quot;, bankCode: &quot;CITIC&quot;, patterns: [{ reg: /^(433670|433680|442729|442730|620082|622690|622691|622692|622696|622698|622998|622999|433671|968807|968808|968809|621771|621767|621768|621770|621772|621773|622453|622456)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(622459)\\d{11}$/g, cardType: &quot;DC&quot; }, { reg: /^(376968|376969|376966)\\d{9}$/g, cardType: &quot;CC&quot; }, { reg: /^(400360|403391|403392|404158|404159|404171|404172|404173|404174|404157|433667|433668|433669|514906|403393|520108|433666|558916|622678|622679|622680|622688|622689|628206|556617|628209|518212|628208|356390|356391|356392|622916|622918|622919)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;华夏银行&quot;, bankCode: &quot;HXBANK&quot;, patterns: [{ reg: /^(622630|622631|622632|622633|999999|621222|623020|623021|623022|623023)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(523959|528709|539867|539868|622637|622638|628318|528708|622636|625967|625968|625969)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;深发/平安银行&quot;, bankCode: &quot;SPABANK&quot;, patterns: [{ reg: /^(621626|623058)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(602907|622986|622989|622298|627069|627068|627066|627067|412963|415752|415753|622535|622536|622538|622539|998800|412962|622983)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(531659|622157|528020|622155|622156|526855|356869|356868|625360|625361|628296|435744|435745|483536|622525|622526|998801|998802)\\d{10}$/g, cardType: &quot;CC&quot; }, { reg: /^(620010)\\d{10}$/g, cardType: &quot;PC&quot; }] }, { bankName: &quot;兴业银行&quot;, bankCode: &quot;CIB&quot;, patterns: [{ reg: /^(438589)\\d{12}$/g, cardType: &quot;DC&quot; }, { reg: /^(90592)\\d{11}$/g, cardType: &quot;DC&quot; }, { reg: /^(966666|622909|438588|622908)\\d{12}$/g, cardType: &quot;DC&quot; }, { reg: /^(461982|486493|486494|486861|523036|451289|527414|528057|622901|622902|622922|628212|451290|524070|625084|625085|625086|625087|548738|549633|552398|625082|625083|625960|625961|625962|625963)\\d{10}$/g, cardType: &quot;CC&quot; }, { reg: /^(620010)\\d{10}$/g, cardType: &quot;PC&quot; }] }, { bankName: &quot;上海银行&quot;, bankCode: &quot;SHBANK&quot;, patterns: [{ reg: /^(621050|622172|622985|622987|620522|622267|622278|622279|622468|622892|940021)\\d{12}$/g, cardType: &quot;DC&quot; }, { reg: /^(438600)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(356827|356828|356830|402673|402674|486466|519498|520131|524031|548838|622148|622149|622268|356829|622300|628230|622269|625099|625953)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;浦东发展银行&quot;, bankCode: &quot;SPDB&quot;, patterns: [{ reg: /^(622516|622517|622518|622521|622522|622523|984301|984303|621352|621793|621795|621796|621351|621390|621792|621791)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(84301|84336|84373|84385|84390|87000|87010|87030|87040|84380|84361|87050|84342)\\d{11}$/g, cardType: &quot;DC&quot; }, { reg: /^(356851|356852|404738|404739|456418|498451|515672|356850|517650|525998|622177|622277|628222|622500|628221|622176|622276|622228|625957|625958|625993|625831)\\d{10}$/g, cardType: &quot;CC&quot; }, { reg: /^(622520|622519)\\d{10}$/g, cardType: &quot;SCC&quot; }, { reg: /^(620530)\\d{13}$/g, cardType: &quot;PC&quot; }] }, { bankName: &quot;广发银行&quot;, bankCode: &quot;GDB&quot;, patterns: [{ reg: /^(622516|622517|622518|622521|622522|622523|984301|984303|621352|621793|621795|621796|621351|621390|621792|621791)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(622568|6858001|6858009|621462)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(9111)\\d{15}$/g, cardType: &quot;DC&quot; }, { reg: /^(406365|406366|428911|436768|436769|436770|487013|491032|491033|491034|491035|491036|491037|491038|436771|518364|520152|520382|541709|541710|548844|552794|493427|622555|622556|622557|622558|622559|622560|528931|558894|625072|625071|628260|628259|625805|625806|625807|625808|625809|625810)\\d{10}$/g, cardType: &quot;CC&quot; }, { reg: /^(685800|6858000)\\d{13}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;渤海银行&quot;, bankCode: &quot;BOHAIB&quot;, patterns: [{ reg: /^(621268|622684|622884|621453)\\d{10}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;广州银行&quot;, bankCode: &quot;GCB&quot;, patterns: [{ reg: /^(603445|622467|940016|621463)\\d{13}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;金华银行&quot;, bankCode: &quot;JHBANK&quot;, patterns: [{ reg: /^(622449|940051)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(622450|628204)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;温州银行&quot;, bankCode: &quot;WZCB&quot;, patterns: [{ reg: /^(621977)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(622868|622899|628255)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;徽商银行&quot;, bankCode: &quot;HSBANK&quot;, patterns: [{ reg: /^(622877|622879|621775|623203)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(603601|622137|622327|622340|622366)\\d{11}$/g, cardType: &quot;DC&quot; }, { reg: /^(628251|622651|625828)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;江苏银行&quot;, bankCode: &quot;JSBANK&quot;, patterns: [{ reg: /^(621076|622173|622131|621579|622876)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(504923|622422|622447|940076)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(628210|622283|625902)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;南京银行&quot;, bankCode: &quot;NJCB&quot;, patterns: [{ reg: /^(621777|622305|621259)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(622303|628242|622595|622596)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;宁波银行&quot;, bankCode: &quot;NBBANK&quot;, patterns: [{ reg: /^(621279|622281|622316|940022)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(621418)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(625903|622778|628207|512431|520194|622282|622318)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;北京银行&quot;, bankCode: &quot;BJBANK&quot;, patterns: [{ reg: /^(623111|421317|422161|602969|422160|621030|621420|621468)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(522001|622163|622853|628203|622851|622852)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;北京农村商业银行&quot;, bankCode: &quot;BJRCB&quot;, patterns: [{ reg: /^(620088|621068|622138|621066|621560)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(625526|625186|628336)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;汇丰银行&quot;, bankCode: &quot;HSBC&quot;, patterns: [{ reg: /^(622946)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(622406|621442)\\d{11}$/g, cardType: &quot;DC&quot; }, { reg: /^(622407|621443)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(622360|622361|625034|625096|625098)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;渣打银行&quot;, bankCode: &quot;SCB&quot;, patterns: [{ reg: /^(622948|621740|622942|622994)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(622482|622483|622484)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;花旗银行&quot;, bankCode: &quot;CITI&quot;, patterns: [{ reg: /^(621062|621063)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(625076|625077|625074|625075|622371|625091)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;东亚银行&quot;, bankCode: &quot;HKBEA&quot;, patterns: [{ reg: /^(622933|622938|623031|622943|621411)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(622372|622471|622472|622265|622266|625972|625973)\\d{10}$/g, cardType: &quot;CC&quot; }, { reg: /^(622365)\\d{11}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;广东华兴银行&quot;, bankCode: &quot;GHB&quot;, patterns: [{ reg: /^(621469|621625)\\d{13}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;深圳农村商业银行&quot;, bankCode: &quot;SRCB&quot;, patterns: [{ reg: /^(622128|622129|623035)\\d{10}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;广州农村商业银行股份有限公司&quot;, bankCode: &quot;GZRCU&quot;, patterns: [{ reg: /^(909810|940035|621522|622439)\\d{12}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;东莞农村商业银行&quot;, bankCode: &quot;DRCBCL&quot;, patterns: [{ reg: /^(622328|940062|623038)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(625288|625888)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;东莞市商业银行&quot;, bankCode: &quot;BOD&quot;, patterns: [{ reg: /^(622333|940050)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(621439|623010)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(622888)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;广东省农村信用社联合社&quot;, bankCode: &quot;GDRCC&quot;, patterns: [{ reg: /^(622302)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(622477|622509|622510|622362|621018|621518)\\d{13}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;大新银行&quot;, bankCode: &quot;DSB&quot;, patterns: [{ reg: /^(622297|621277)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(622375|622489)\\d{11}$/g, cardType: &quot;DC&quot; }, { reg: /^(622293|622295|622296|622373|622451|622294|625940)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;永亨银行&quot;, bankCode: &quot;WHB&quot;, patterns: [{ reg: /^(622871|622958|622963|622957|622861|622932|622862|621298)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(622798|625010|622775|622785)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;星展银行香港有限公司&quot;, bankCode: &quot;DBS&quot;, patterns: [{ reg: /^(621016|621015)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(622487|622490|622491|622492)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(622487|622490|622491|622492|621744|621745|621746|621747)\\d{11}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;恒丰银行&quot;, bankCode: &quot;EGBANK&quot;, patterns: [{ reg: /^(623078)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(622384|940034)\\d{11}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;天津市商业银行&quot;, bankCode: &quot;TCCB&quot;, patterns: [{ reg: /^(940015|622331)\\d{12}$/g, cardType: &quot;DC&quot; }, { reg: /^(6091201)\\d{11}$/g, cardType: &quot;DC&quot; }, { reg: /^(622426|628205)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;浙商银行&quot;, bankCode: &quot;CZBANK&quot;, patterns: [{ reg: /^(621019|622309|621019)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(6223091100|6223092900|6223093310|6223093320|6223093330|6223093370|6223093380|6223096510|6223097910)\\d{9}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;南洋商业银行&quot;, bankCode: &quot;NCB&quot;, patterns: [{ reg: /^(621213|621289|621290|621291|621292|621042|621743)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(623041|622351)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(625046|625044|625058|622349|622350)\\d{10}$/g, cardType: &quot;CC&quot; }, { reg: /^(620208|620209|625093|625095)\\d{10}$/g, cardType: &quot;PC&quot; }] }, { bankName: &quot;厦门银行&quot;, bankCode: &quot;XMBANK&quot;, patterns: [{ reg: /^(622393|940023)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(6886592)\\d{11}$/g, cardType: &quot;DC&quot; }, { reg: /^(623019|621600|)\\d{13}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;福建海峡银行&quot;, bankCode: &quot;FJHXBC&quot;, patterns: [{ reg: /^(622388)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(621267|623063)\\d{12}$/g, cardType: &quot;DC&quot; }, { reg: /^(620043|)\\d{12}$/g, cardType: &quot;PC&quot; }] }, { bankName: &quot;吉林银行&quot;, bankCode: &quot;JLBANK&quot;, patterns: [{ reg: /^(622865|623131)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(940012)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(622178|622179|628358)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;汉口银行&quot;, bankCode: &quot;HKB&quot;, patterns: [{ reg: /^(990027)\\d{12}$/g, cardType: &quot;DC&quot; }, { reg: /^(622325|623105|623029)\\d{10}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;盛京银行&quot;, bankCode: &quot;SJBANK&quot;, patterns: [{ reg: /^(566666)\\d{12}$/g, cardType: &quot;DC&quot; }, { reg: /^(622455|940039)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(623108|623081)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(622466|628285)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;大连银行&quot;, bankCode: &quot;DLB&quot;, patterns: [{ reg: /^(603708)\\d{11}$/g, cardType: &quot;DC&quot; }, { reg: /^(622993|623069|623070|623172|623173)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(622383|622385|628299)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;河北银行&quot;, bankCode: &quot;BHB&quot;, patterns: [{ reg: /^(622498|622499|623000|940046)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(622921|628321)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;乌鲁木齐市商业银行&quot;, bankCode: &quot;URMQCCB&quot;, patterns: [{ reg: /^(621751|622143|940001|621754)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(622476|628278)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;绍兴银行&quot;, bankCode: &quot;SXCB&quot;, patterns: [{ reg: /^(622486)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(603602|623026|623086)\\d{12}$/g, cardType: &quot;DC&quot; }, { reg: /^(628291)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;成都商业银行&quot;, bankCode: &quot;CDCB&quot;, patterns: [{ reg: /^(622152|622154|622996|622997|940027|622153|622135|621482|621532)\\d{13}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;抚顺银行&quot;, bankCode: &quot;FSCB&quot;, patterns: [{ reg: /^(622442)\\d{11}$/g, cardType: &quot;DC&quot; }, { reg: /^(940053)\\d{12}$/g, cardType: &quot;DC&quot; }, { reg: /^(622442|623099)\\d{13}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;郑州银行&quot;, bankCode: &quot;ZZBANK&quot;, patterns: [{ reg: /^(622421)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(940056)\\d{11}$/g, cardType: &quot;DC&quot; }, { reg: /^(96828)\\d{11}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;宁夏银行&quot;, bankCode: &quot;NXBANK&quot;, patterns: [{ reg: /^(621529|622429|621417|623089|623200)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(628214|625529|622428)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;重庆银行&quot;, bankCode: &quot;CQBANK&quot;, patterns: [{ reg: /^(9896)\\d{12}$/g, cardType: &quot;DC&quot; }, { reg: /^(622134|940018|623016)\\d{10}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;哈尔滨银行&quot;, bankCode: &quot;HRBANK&quot;, patterns: [{ reg: /^(621577|622425)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(940049)\\d{12}$/g, cardType: &quot;DC&quot; }, { reg: /^(622425)\\d{11}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;兰州银行&quot;, bankCode: &quot;LZYH&quot;, patterns: [{ reg: /^(622139|940040|628263)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(621242|621538|621496)\\d{13}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;青岛银行&quot;, bankCode: &quot;QDCCB&quot;, patterns: [{ reg: /^(621252|622146|940061|628239)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(621419|623170)\\d{13}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;秦皇岛市商业银行&quot;, bankCode: &quot;QHDCCB&quot;, patterns: [{ reg: /^(62249802|94004602)\\d{11}$/g, cardType: &quot;DC&quot; }, { reg: /^(621237|623003)\\d{13}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;青海银行&quot;, bankCode: &quot;BOQH&quot;, patterns: [{ reg: /^(622310|940068)\\d{11}$/g, cardType: &quot;DC&quot; }, { reg: /^(622817|628287|625959)\\d{10}$/g, cardType: &quot;CC&quot; }, { reg: /^(62536601)\\d{8}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;台州银行&quot;, bankCode: &quot;TZCB&quot;, patterns: [{ reg: /^(622427)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(940069)\\d{11}$/g, cardType: &quot;DC&quot; }, { reg: /^(623039)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(622321|628273)\\d{10}$/g, cardType: &quot;CC&quot; }, { reg: /^(625001)\\d{10}$/g, cardType: &quot;SCC&quot; }] }, { bankName: &quot;长沙银行&quot;, bankCode: &quot;CSCB&quot;, patterns: [{ reg: /^(694301)\\d{12}$/g, cardType: &quot;DC&quot; }, { reg: /^(940071|622368|621446)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(625901|622898|622900|628281|628282|622806|628283)\\d{10}$/g, cardType: &quot;CC&quot; }, { reg: /^(620519)\\d{13}$/g, cardType: &quot;PC&quot; }] }, { bankName: &quot;泉州银行&quot;, bankCode: &quot;BOQZ&quot;, patterns: [{ reg: /^(683970|940074)\\d{12}$/g, cardType: &quot;DC&quot; }, { reg: /^(622370)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(621437)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(628319)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;包商银行&quot;, bankCode: &quot;BSB&quot;, patterns: [{ reg: /^(622336|621760)\\d{11}$/g, cardType: &quot;DC&quot; }, { reg: /^(622165)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(622315|625950|628295)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;龙江银行&quot;, bankCode: &quot;DAQINGB&quot;, patterns: [{ reg: /^(621037|621097|621588|622977)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(62321601)\\d{11}$/g, cardType: &quot;DC&quot; }, { reg: /^(622860)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(622644|628333)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;上海农商银行&quot;, bankCode: &quot;SHRCB&quot;, patterns: [{ reg: /^(622478|940013|621495)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(625500)\\d{10}$/g, cardType: &quot;SCC&quot; }, { reg: /^(622611|622722|628211|625989)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;浙江泰隆商业银行&quot;, bankCode: &quot;ZJQL&quot;, patterns: [{ reg: /^(622717)\\d{10}$/g, cardType: &quot;SCC&quot; }, { reg: /^(628275|622565|622287)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;内蒙古银行&quot;, bankCode: &quot;H3CB&quot;, patterns: [{ reg: /^(622147|621633)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(628252)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;广西北部湾银行&quot;, bankCode: &quot;BGB&quot;, patterns: [{ reg: /^(623001)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(628227)\\d{10}$/g, cardType: &quot;CC&quot; }, { reg: /^(622335)\\d{10}$/g, cardType: &quot;CC&quot; } ] }, { bankName: &quot;桂林银行&quot;, bankCode: &quot;GLBANK&quot;, patterns: [{ reg: /^(621456)\\d{11}$/g, cardType: &quot;DC&quot; }, { reg: /^(621562)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(628219)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;龙江银行&quot;, bankCode: &quot;DAQINGB&quot;, patterns: [{ reg: /^(621037|621097|621588|622977)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(62321601)\\d{11}$/g, cardType: &quot;DC&quot; }, { reg: /^(622475|622860)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(625588)\\d{10}$/g, cardType: &quot;SCC&quot; }, { reg: /^(622270|628368|625090|622644|628333)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;成都农村商业银行&quot;, bankCode: &quot;CDRCB&quot;, patterns: [{ reg: /^(623088)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(622829|628301|622808|628308)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;福建省农村信用社联合社&quot;, bankCode: &quot;FJNX&quot;, patterns: [{ reg: /^(622127|622184|621701|621251|621589|623036)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(628232|622802|622290)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;天津农村商业银行&quot;, bankCode: &quot;TRCB&quot;, patterns: [{ reg: /^(622531|622329)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(622829|628301)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;江苏省农村信用社联合社&quot;, bankCode: &quot;JSRCU&quot;, patterns: [{ reg: /^(621578|623066|622452|622324)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(622815|622816|628226)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;湖南农村信用社联合社&quot;, bankCode: &quot;SLH&quot;, patterns: [{ reg: /^(622906|628386|625519|625506)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;江西省农村信用社联合社&quot;, bankCode: &quot;JXNCX&quot;, patterns: [{ reg: /^(621592)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(628392)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;商丘市商业银行&quot;, bankCode: &quot;SCBBANK&quot;, patterns: [{ reg: /^(621748)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(628271)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;华融湘江银行&quot;, bankCode: &quot;HRXJB&quot;, patterns: [{ reg: /^(621366|621388)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(628328)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;衡水市商业银行&quot;, bankCode: &quot;HSBK&quot;, patterns: [{ reg: /^(621239|623068)\\d{13}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;重庆南川石银村镇银行&quot;, bankCode: &quot;CQNCSYCZ&quot;, patterns: [{ reg: /^(621653004)\\d{10}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;湖南省农村信用社联合社&quot;, bankCode: &quot;HNRCC&quot;, patterns: [{ reg: /^(622169|621519|621539|623090)\\d{13}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;邢台银行&quot;, bankCode: &quot;XTB&quot;, patterns: [{ reg: /^(621238|620528)\\d{13}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;临汾市尧都区农村信用合作联社&quot;, bankCode: &quot;LPRDNCXYS&quot;, patterns: [{ reg: /^(628382|625158)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;东营银行&quot;, bankCode: &quot;DYCCB&quot;, patterns: [{ reg: /^(621004)\\d{12}$/g, cardType: &quot;DC&quot; }, { reg: /^(628217)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;上饶银行&quot;, bankCode: &quot;SRBANK&quot;, patterns: [{ reg: /^(621416)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(628217)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;德州银行&quot;, bankCode: &quot;DZBANK&quot;, patterns: [{ reg: /^(622937)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(628397)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;承德银行&quot;, bankCode: &quot;CDB&quot;, patterns: [{ reg: /^(628229)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;云南省农村信用社&quot;, bankCode: &quot;YNRCC&quot;, patterns: [{ reg: /^(622469|628307)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;柳州银行&quot;, bankCode: &quot;LZCCB&quot;, patterns: [{ reg: /^(622292|622291|621412)\\d{12}$/g, cardType: &quot;DC&quot; }, { reg: /^(622880|622881)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(62829)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;威海市商业银行&quot;, bankCode: &quot;WHSYBANK&quot;, patterns: [{ reg: /^(623102)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(628234)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;湖州银行&quot;, bankCode: &quot;HZBANK&quot;, patterns: [{ reg: /^(628306)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;潍坊银行&quot;, bankCode: &quot;BANKWF&quot;, patterns: [{ reg: /^(622391|940072)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(628391)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;赣州银行&quot;, bankCode: &quot;GZB&quot;, patterns: [{ reg: /^(622967|940073)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(628233)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;日照银行&quot;, bankCode: &quot;RZGWYBANK&quot;, patterns: [{ reg: /^(628257)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;南昌银行&quot;, bankCode: &quot;NCB&quot;, patterns: [{ reg: /^(621269|622275)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(940006)\\d{11}$/g, cardType: &quot;DC&quot; }, { reg: /^(628305)\\d{11}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;贵阳银行&quot;, bankCode: &quot;GYCB&quot;, patterns: [{ reg: /^(622133|621735)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(888)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(628213)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;锦州银行&quot;, bankCode: &quot;BOJZ&quot;, patterns: [{ reg: /^(622990|940003)\\d{11}$/g, cardType: &quot;DC&quot; }, { reg: /^(628261)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;齐商银行&quot;, bankCode: &quot;QSBANK&quot;, patterns: [{ reg: /^(622311|940057)\\d{11}$/g, cardType: &quot;DC&quot; }, { reg: /^(628311)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;珠海华润银行&quot;, bankCode: &quot;RBOZ&quot;, patterns: [{ reg: /^(622363|940048)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(628270)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;葫芦岛市商业银行&quot;, bankCode: &quot;HLDCCB&quot;, patterns: [{ reg: /^(622398|940054)\\d{10}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;宜昌市商业银行&quot;, bankCode: &quot;HBC&quot;, patterns: [{ reg: /^(940055)\\d{11}$/g, cardType: &quot;DC&quot; }, { reg: /^(622397)\\d{11}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;杭州商业银行&quot;, bankCode: &quot;HZCB&quot;, patterns: [{ reg: /^(603367|622878)\\d{12}$/g, cardType: &quot;DC&quot; }, { reg: /^(622397)\\d{11}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;苏州市商业银行&quot;, bankCode: &quot;JSBANK&quot;, patterns: [{ reg: /^(603506)\\d{13}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;辽阳银行&quot;, bankCode: &quot;LYCB&quot;, patterns: [{ reg: /^(622399|940043)\\d{11}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;洛阳银行&quot;, bankCode: &quot;LYB&quot;, patterns: [{ reg: /^(622420|940041)\\d{11}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;焦作市商业银行&quot;, bankCode: &quot;JZCBANK&quot;, patterns: [{ reg: /^(622338)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(940032)\\d{10}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;镇江市商业银行&quot;, bankCode: &quot;ZJCCB&quot;, patterns: [{ reg: /^(622394|940025)\\d{10}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;法国兴业银行&quot;, bankCode: &quot;FGXYBANK&quot;, patterns: [{ reg: /^(621245)\\d{10}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;大华银行&quot;, bankCode: &quot;DYBANK&quot;, patterns: [{ reg: /^(621328)\\d{13}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;企业银行&quot;, bankCode: &quot;DIYEBANK&quot;, patterns: [{ reg: /^(621651)\\d{13}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;华侨银行&quot;, bankCode: &quot;HQBANK&quot;, patterns: [{ reg: /^(621077)\\d{10}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;恒生银行&quot;, bankCode: &quot;HSB&quot;, patterns: [{ reg: /^(622409|621441)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(622410|621440)\\d{11}$/g, cardType: &quot;DC&quot; }, { reg: /^(622950|622951)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(625026|625024|622376|622378|622377|625092)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;临沂商业银行&quot;, bankCode: &quot;LSB&quot;, patterns: [{ reg: /^(622359|940066)\\d{13}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;烟台商业银行&quot;, bankCode: &quot;YTCB&quot;, patterns: [{ reg: /^(622886)\\d{10}$/g, cardType: &quot;DC&quot; }] }, { bankName: &quot;齐鲁银行&quot;, bankCode: &quot;QLB&quot;, patterns: [{ reg: /^(940008|622379)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(628379)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;BC卡公司&quot;, bankCode: &quot;BCCC&quot;, patterns: [{ reg: /^(620011|620027|620031|620039|620103|620106|620120|620123|620125|620220|620278|620812|621006|621011|621012|621020|621023|621025|621027|621031|620132|621039|621078|621220|621003)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(625003|625011|625012|625020|625023|625025|625027|625031|621032|625039|625078|625079|625103|625106|625006|625112|625120|625123|625125|625127|625131|625032|625139|625178|625179|625220|625320|625111|625132|625244)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;集友银行&quot;, bankCode: &quot;CYB&quot;, patterns: [{ reg: /^(622355|623042)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(621043|621742)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(622352|622353|625048|625053|625060)\\d{10}$/g, cardType: &quot;CC&quot; }, { reg: /^(620206|620207)\\d{10}$/g, cardType: &quot;PC&quot; }] }, { bankName: &quot;大丰银行&quot;, bankCode: &quot;TFB&quot;, patterns: [{ reg: /^(622547|622548|622546)\\d{13}$/g, cardType: &quot;DC&quot; }, { reg: /^(625198|625196|625147)\\d{10}$/g, cardType: &quot;CC&quot; }, { reg: /^(620072)\\d{13}$/g, cardType: &quot;PC&quot; }, { reg: /^(620204|620205)\\d{10}$/g, cardType: &quot;PC&quot; }] }, { bankName: &quot;AEON信贷财务亚洲有限公司&quot;, bankCode: &quot;AEON&quot;, patterns: [{ reg: /^(621064|622941|622974)\\d{10}$/g, cardType: &quot;DC&quot; }, { reg: /^(622493)\\d{10}$/g, cardType: &quot;CC&quot; }] }, { bankName: &quot;澳门BDA&quot;, bankCode: &quot;MABDA&quot;, patterns: [{ reg: /^(621274|621324)\\d{13}$/g, cardType: &quot;DC&quot; }] }] //验证银行卡号 $(&quot;input[name='bankNum']&quot;).blur(function () { var num = $(this).val(); //去掉空格，因为input框中设置了自动空格，如果input框中没有设置自动空格可省略这句代码 num = num.replace(/\\s/g, &quot;&quot;); //判断卡号是否正确 for (var i = 0,len=bankcardList.length; i &lt; len; i++) { for (var j = 0,regLen = bankcardList[i].patterns.length; j &lt; regLen; j++) { var reg = bankcardList[i].patterns[j].reg.test(num); if(reg){ alert(&quot;输入的是&quot;+bankcardList[i].bankName+&quot;的卡号&quot;) return ; } } } }); ","link":"https://faded.auspicious.space/post/regular-expression-band-card/"},{"title":"正则表达式——简介","content":"在自然语言处理中，很多时候我们都需要从文本或字符串中抽取出想要的信息，并进一步做语义理解或其它处理。 常用正则表达式网站 Regex Dictionary https://visca.com/regexdict/ RegExr https://regexr.com/ RegExper https://regexper.com/ Regular Expressions 101 https://regex101.com/ 基本语句 ♍锚点：^ 和 $ ^The 匹配任何以“The”开头的字符串。 end$ 匹配以“end”为结尾的字符串。 ^The end$ 匹配从“The”开始到“end”结束的字符串。 roar 匹配任何带有文本“roar”的字符串。 ♍数量符：* 和 + 和 ? 和 {} abc* 匹配在“ab”后面跟着 0 个或多个“c”的字符串。 abc+ 匹配在“ab”后面跟着 1 个或多个“c”的字符串。 abc? 匹配在“ab”后面跟着 0 个或 1 个“c”的字符串。 abc{2} 匹配在“ab”后面跟着 2 个“c”的字符串。 abc{2,} 匹配在“ab”后面跟着 2 个或更多“c”的字符串。 abc{2,5} 匹配在“ab”后面跟着 2 到 5 个“c”的字符串。 a(bc)* 匹配在“a”后面跟着 0 个或更多“bc”序列的字符串。 a(bc){2,5} 匹配在“a”后面跟着 2 到 5 个“bc”序列的字符串。 ♍或运算符：| 和 [] a(b|c) 匹配在“a”后面跟着“b”或“c”的字符串。 a[bc] 匹配在“a”后面跟着“b”或“c”的字符串。 ♍字符类：\\d 和 \\w 和 \\s 和 . \\d 匹配数字型的单个字符。 \\w 匹配单个词字（字母加下划线）。 \\s 匹配单个空格字符（包括制表符和换行符）。 . 匹配任意字符。 使用 . 运算符需要非常小心，因为常见类或排除型字符类都要更快与精确。\\d、\\w 和 \\s 同样有它们各自的排除型字符类，即 \\D、\\W 和 \\S。例如 \\D 将执行与 \\d 完全相反的匹配方法： \\D 匹配单个非数字型的字符。 为了正确地匹配，我们必须使用转义符反斜杠 \\ 定义我们需要匹配的符号 ^.[$()|*+?{\\，因为我们可能认为这些符号在原文本中有特殊的含义。 \\$\\d 匹配在单个数字前有符号“$”的字符串。 注意我们同样能匹配 non-printable 字符，例如 Tab 符 \\t、换行符 \\n 和回车符 \\r。 ♍Flags 模式的结尾我们通常可以指定以下 flag 配置或它们的组合： g（global）在第一次完成匹配后并不会返回结果，它会继续搜索剩下的文本。 m（multi line）允许使用^和$匹配一行的开始和结尾，而不是整个序列。 i（insensitive）令整个表达式不区分大小写（例如/aBc/i 将匹配 AbC）。 中级语句 ♍分组和捕获：() a(bc) 圆括弧会创建一个捕获性分组，它会捕获匹配项“bc”。 a(?:bc)* 使用“?: ”会使捕获分组失效，只需要匹配前面的“a”。 a(?&lt;foo&gt;bc) 使用“?&lt;foo&gt;”会为分组配置一个名称 。 捕获性圆括号 () 和非捕获性圆括弧 (?:) 对于从字符串或数据中抽取信息非常重要，我们可以使用 Python 等不同的编程语言实现这一功能。从多个分组中捕获的多个匹配项将以经典的数组形式展示：我们可以使用匹配结果的索引访问它们的值。如果需要为分组添加名称（使用 (?&lt;foo&gt;...)），我们就能如字典那样使用匹配结果检索分组的值，其中字典的键为分组的名称。 ♍方括弧表达式：[] [abc] 匹配带有一个“a”、“ab”或“ac”的字符串。 [a-c] 匹配带有一个“a”、“ab”或“ac”的字符串。 [a-fA-F0-9] 匹配一个代表 16 进制数字的字符串，不区分大小写。 [0-9]% 匹配在 % 符号前面带有 0 到 9 这几个字符的字符串。 [^a-zA-Z] 匹配不带 a 到 z 或 A 到 Z 的字符串，其中 ^ 为否定表达式。 记住在方括弧内，所有特殊字符（包括反斜杠 \\ ）都会失去它们应有的意义。 ♍Greedy 和 Lazy 匹配 数量符（* + {}）是一种贪心运算符，所以它们会遍历给定的文本，并尽可能匹配。例如，&lt;.+&gt; 可以匹配文本 “This is a &lt;div&gt; simple div&lt;/div&gt; test” 中的 “&lt;div&gt;simple div&lt;/div&gt;&quot;。为了仅捕获 div 标签，我们需要使用 ? 令贪心搜索变得 Lazy 一点： &lt;.+?&gt; 一次或多次匹配“&lt;”和“&gt;”里面的任何字符，可按需扩展。 注意更好的解决方案应该需要避免使用 .，这有利于实现更严格的正则表达式： &lt;[^&lt;&gt;]+&gt; 一次或多次匹配“&lt;”和“&gt;”里面的任何字符，除去“&lt;”或“&gt;”字符。 高级语句 ♍边界符：\\b 和 \\B \\babc\\b 执行整词匹配搜索。 \\b 如插入符号那样表示一个锚点（它与 $ 和 ^ 相同）来匹配位置，其中一边是一个单词符号（如 \\w），另一边不是单词符号（例如它可能是字符串的起始点或空格符号）。 它同样能表达相反的非单词边界 \\B，它会匹配 \\b 不会匹配的位置，如果我们希望找到被单词字符环绕的搜索模式，就可以使用它。 \\Babc\\B 只要是被单词字符环绕的模式就会匹配。 ♍前向匹配和后向匹配：(?=) 和 (?&lt;=) d(?=r) 只有在后面跟着“r”的时候才匹配“d”，但是“r”并不会成为整个正则表达式匹配的一部分。 (?&lt;=r)d 只有在前面跟着“r”时才匹配“d”，但是“r”并不会成为整个正则表达式匹配的一部分。 我们同样能使用否定运算子： d(?!r) 只有在后面不跟着“r”的时候才匹配“d”，但是“r”并不会成为整个正则表达式匹配的一部分。 (?&lt;!r)d 只有在前面不跟着“r”时才匹配“d”，但是“r”并不会成为整个正则表达式匹配的一部分。 结语 正如上文所示，正则表达式的应用领域非常广，很可能各位读者在开发的过程中已经遇到了它，下面是正则表达式常用的领域： 数据验证，例如检查时间字符串是否符合格式； 数据抓取，以特定顺序抓取包含特定文本或内容的网页； 数据包装，将数据从某种原格式转换为另外一种格式； 字符串解析，例如捕获所拥有 URL 的 GET 参数，或捕获一组圆括弧内的文本； 字符串替代，将字符串中的某个字符替换为其它字符。 ","link":"https://faded.auspicious.space/post/regular-expression-introduction/"},{"title":"有趣的网站","content":"一些平时遇到的有用有趣的网站 网站 网址 在浏览器中运行 Linux https://bellard.org/jslinux/ 一个修改漫画的小工具 https://moeka.me/mangaEditor/ AI 人工智能图片放大 https://bigjpg.com/ 黑白照片上色 https://colourise.sg/ 在线 Photoshop https://ps.gaoding.com/ GIF 加字幕 http://www.yingjingtu.com/index 证件照换底色 https://www.gaoding.com/koutu 在线图片编辑器_AI智能抠图_Aipix https://aipix.net/ 图片背景消除 https://www.remove.bg/zh 快速去掉背景色 https://bgeraser.com/index.html 也许是世界上最快且最智能的在线批量裁剪图片工具 https://www.smartresize.com/zh-cn 网版图制作 https://xoihazard.com/tools/halftone/ 双色套效果 https://duotones.co/ SVG在线压缩合并 https://www.zhangxinxu.com/sp/svgo/ Emoji 马赛克 https://ericandrewlewis.github.io/emoji-mosaic/ CSS 动画制作 https://animista.net/ SQL 语句在线格式化 https://sqlfum.pt/ 实时在线分享代码 https://codeshare.io/ gif 制作 https://gifs.com/ 生成漂亮的代码截图 https://carbon.now.sh/ 在线文件转换 https://cn.office-converter.com/ File Converter https://cloudconvert.com/ ToolFk 在线程序员开发工具 https://www.toolfk.com/ 在线工具 https://tool.lu/ 一个工具箱 http://www.atoolbox.net/ 爱资料工具 https://www.toolnb.com/ 孟坤工具箱 http://tool.mkblog.cn/ OKTools https://oktools.net/ 艾特网 - 程序员导航站、程序员之家 https://iiter.cn/ 在线工具 https://helloacm.com/tools/ JS/HTML格式化 https://www.zxgj.cn/g/jshtmlformat 甜言蜜语 API https://api.tryto.cn/saylove/text 甜言蜜语 API https://api.tryto.cn/djt/text 码灵程序员网址导航 https://nav.imaring.com/ CSS 剪切路径生成器 https://bennettfeely.com/clippy/ 文章生成器 https://suulnnka.github.io/BullshitGenerator/index.html 在线屏幕录制 https://www.p2hp.com/screenrecord.html 高手工具 https://c.p2hp.com/ 在线加密算法 https://www.ssleye.com/ JSON在线格式化,JSON在线解析 https://json.im/ 哈希 https://haxi.im/ 图片隐写术加密、图片隐写术解密 https://c.p2hp.com/yinxietu/ 在线代码运行时 Labstack https://code.labstack.com/ 在线文件加密 https://hat.sh/ 在线检测浏览器版本 https://liulanmi.com/labs/core.html 糖果短语视频生成器 https://hattemi.com/ 前端网址导航 http://www.daqianduan.com/nav 背景生成器 https://bggenerator.com/zh-cn.php “爱古典”数据库 http://www.iloveclassics.icoc.cc/ Compare package download counts over time https://www.npmtrends.com/ IP反查域名 https://dns.aizhan.com/ Breathe Relaxer - Calm down your mind, relax your body https://works.yangerxiao.com/breathe-relaxer/ ","link":"https://faded.auspicious.space/post/you-qu-de-wang-zhan/"},{"title":"正则表达式——常用案例","content":"正则大全 https://any86.github.io/any-rule/ 火车车次 /^[GCDZTSPKXLY1-9]\\d{1,4}$/ 手机机身码(IMEI) /^\\d{15,17}$/ 必须带端口号的网址(或ip) /^(((ht|f)tps?):\\/\\/)?[\\w\\-]+(\\.[\\w\\-]+)+:\\d{0,5}\\/?/ 网址(支持端口和&quot;?+参数&quot;和&quot;#+参数) /^(((ht|f)tps?):\\/\\/)?[\\w\\-]+(\\.[\\w\\-]+)+([\\w\\-.,@?^=%&amp;:\\/~+#]*[\\w\\-@?^=%&amp;\\/~+#])?$/ 统一社会信用代码 /^[0-9A-HJ-NPQRTUWXY]{2}\\d{6}[0-9A-HJ-NPQRTUWXY]{10}$/ 迅雷链接 /^thunderx?:\\/\\/[a-zA-Z\\d]+=$/ ed2k链接(宽松匹配) /^ed2k:\\/\\/\\|file\\|.+\\|\\/$/ 磁力链接(宽松匹配) /^magnet:\\?xt=urn:btih:[0-9a-fA-F]{40,}.*$/ 子网掩码 /^(?:\\d{1,2}|1\\d\\d|2[0-4]\\d|25[0-5])(?:\\.(?:\\d{1,2}|1\\d\\d|2[0-4]\\d|25[0-5])){3}$/ Linux&quot;文件夹&quot;路径 /^\\/(\\w+\\/?)+$/ Linux&quot;文件&quot;路径 /^\\/(\\w+\\/)+\\w+\\.\\w+$/ Window下&quot;文件夹&quot;路径 /^[a-zA-Z]:\\\\(?:\\w+\\\\?)*$/ Window下&quot;文件&quot;路径 /^[a-zA-Z]:\\\\(?:\\w+\\\\)*\\w+\\.\\w+$/ A股代码 /^(s[hz]|S[HZ])(000[\\d]{3}|002[\\d]{3}|300[\\d]{3}|600[\\d]{3}|60[\\d]{4})$/ 考卷分数 /^150$|^(?:\\d|[1-9]\\d|1[0-4]\\d)(?:.5)?$/ 大于等于0, 小于等于150, 支持小数位出现5, 如145.5 校验密码强度 ^(?=.*\\\\d)(?=.*[a-z])(?=.*[A-Z]).{8,10}$ 密码的强度必须是包含大小写字母和数字的组合，不能使用特殊字符，长度在8-10之间。 /^(?=.*[0-9])(?=.*[a-z])(?=.*[A-Z])(?=.*\\_)\\w{8,20}$/ 只允许字母数字下划线，必须含有大小写和数字和下划线 校验中文 ^[\\\\u4e00-\\\\u9fa5]{0,}$ 由数字、26个英文字母或下划线组成的字符串 ^\\\\w+$ 校验E-Mail 地址 [\\\\w!#$%&amp;'*+/=?^_{|}~-]+(?:\\.[\\w!#$%&amp;'*+/=?^_{|}~-]+)*@(?:[\\\\w](?:[\\\\w-]*[\\\\w])?\\\\.)+[\\\\w](?:[\\\\w-]*[\\\\w])? 校验日期 ^(?:(?!0000)[0-9]{4}-(?:(?:0[1-9]|1[0-2])-(?:0[1-9]|1[0-9]|2[0-8])|(?:0[13-9]|1[0-2])-(?:29|30)|(?:0[13578]|1[02])-31)|(?:[0-9]{2}(?:0[48]|[2468][048]|[13579][26])|(?:0[48]|[2468][048]|[13579][26])00)-02-29)$ “yyyy-mm-dd“ 格式的日期校验，已考虑平闰年。 校验金额 ^[0-9]+(.[0-9]{2})?$ 金额校验，精确到2位小数。 判断IE的版本 ^.*MSIE [5-8](?:\\\\.[0-9]+)?(?!.*Trident\\\\/[5-9]\\\\.0).*$ 校验IPv4地址 \\\\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\\\b 校验IPv6地址 (([0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,7}:|([0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,5}(:[0-9a-fA-F]{1,4}){1,2}|([0-9a-fA-F]{1,4}:){1,4}(:[0-9a-fA-F]{1,4}){1,3}|([0-9a-fA-F]{1,4}:){1,3}(:[0-9a-fA-F]{1,4}){1,4}|([0-9a-fA-F]{1,4}:){1,2}(:[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:((:[0-9a-fA-F]{1,4}){1,6})|:((:[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(:[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(ffff(:0{1,4}){0,1}:){0,1}((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])|([0-9a-fA-F]{1,4}:){1,4}:((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])) 提取URL链接 ^(f|ht){1}(tp|tps):\\\\/\\\\/([\\\\w-]+\\\\.)+[\\\\w-]+(\\\\/[\\\\w- ./?%&amp;=]*)? 文件路径及扩展名校验 ^([a-zA-Z]\\\\:|\\\\\\\\)\\\\\\\\([^\\\\\\\\]+\\\\\\\\)*[^\\\\/:*?&quot;&lt;&gt;|]+\\\\.txt(l)?$ 验证windows下文件路径和扩展名（以 .txt 文件为例） 提取Color Hex Codes ^#([A-Fa-f0-9]{6}|[A-Fa-f0-9]{3})$ 提取网页图片 \\\\&lt; *[img][^\\\\\\\\&gt;]*[src] *= *[\\\\&quot;\\\\']{0,1}([^\\\\&quot;\\\\'\\\\ &gt;]*) 提取页面超链接 (&lt;a\\\\s*(?!.*\\\\brel=)[^&gt;]*)(href=&quot;https?:\\\\/\\\\/)((?!(?:(?:www\\\\.)?'.implode('|(?:www\\\\.)?', $follow_list).'))[^&quot;]+)&quot;((?!.*\\\\brel=)[^&gt;]*)(?:[^&gt;]*)&gt; 查找CSS属性 ^\\\\s*[a-zA-Z\\\\-]+\\\\s*[:]{1}\\\\s[a-zA-Z0-9\\\\s.#]+[;]{1} 抽取注释 &lt;!--(.*?)--&gt; 匹配HTML标签 &lt;\\\\/?\\\\w+((\\\\s+\\\\w+(\\\\s*=\\\\s*(?:&quot;.*?&quot;|'.*?'|[\\\\^'&quot;&gt;\\\\s]+))?)+\\\\s*|\\\\s*)\\\\/?&gt; 银行卡四位一空格 str.replace(/\\s/g, '').replace(/(.{4})/g, &quot;$1 &quot;); 用户名正则 /^[a-zA-Z0-9_-]{4,16}$/ 4到16位（字母，数字，下划线，减号） 密码正则 ^[a-zA-Z]\\w{5,17}$ 以字母开头，长度在6~18之间，只能包含字母、数字和下划线 强密码正则 /^.*(?=.{6,})(?=.*\\d)(?=.*[A-Z])(?=.*[a-z])(?=.*[!@#$%^&amp;*? ]).*$/ 最少6位，包括至少1个大写字母，1个小写字母，1个数字，1个特殊字符 QQ 号正则 /^[1-9][0-9]{4,10}$/ 微信号正则 /^[a-zA-Z]([-_a-zA-Z0-9]{5,19})+$/ 6至20位，以字母开头，字母，数字，减号，下划线 特殊字符正则 /[&quot;'&lt;&gt;%;)(&amp;+]+-!！@#$~/ 域名正则 [a-zA-Z0-9][-a-zA-Z0-9]{0,62}(/.[a-zA-Z0-9][-a-zA-Z0-9]{0,62})+/.? 车牌号正则 /^[京津沪渝冀豫云辽黑湘皖鲁新苏浙赣鄂桂甘晋蒙陕吉闽贵粤青藏川宁琼使领A-Z]{1}[A-Z]{1}[A-Z0-9]{4}[A-Z0-9挂学警港澳]{1}$/ 护照正则 /^(P\\d{7}|G\\d{7,8}|TH\\d{7,8}|S\\d{7,8}|A\\d{7,8}|L\\d{7,8}|\\d{9}|D\\d+|1[4,5]\\d{7})$/ 固定电话正则 (\\(\\d{3,4}\\)|\\d{3,4}-|\\s)?\\d{8} 邮政编码正则 [1-9]{1}(\\d+){5} 经度正则 /^(\\-|\\+)?(((\\d|[1-9]\\d|1[0-7]\\d|0{1,3})\\.\\d{0,6})|(\\d|[1-9]\\d|1[0-7]\\d|0{1,3})|180\\.0{0,6}|180)$/ 维度正则 /^(\\-|\\+)?([0-8]?\\d{1}\\.\\d{0,6}|90\\.0{0,6}|[0-8]?\\d{1}|90)$/ ","link":"https://faded.auspicious.space/post/regular-expression-regular-cases/"},{"title":"Hello Gridea","content":"👏 欢迎使用 Gridea ！ ✍️ Gridea 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ... Github Gridea 主页 示例网站 特性👇 📝 你可以使用最酷的 Markdown 语法，进行快速创作 🌉 你可以给文章配上精美的封面图和在文章任意位置插入图片 🏷️ 你可以对文章进行标签分组 📋 你可以自定义菜单，甚至可以创建外部链接菜单 💻 你可以在 Windows，MacOS 或 Linux 设备上使用此客户端 🌎 你可以使用 𝖦𝗂𝗍𝗁𝗎𝖻 𝖯𝖺𝗀𝖾𝗌 或 Coding Pages 向世界展示，未来将支持更多平台 💬 你可以进行简单的配置，接入 Gitalk 或 DisqusJS 评论系统 🇬🇧 你可以使用中文简体或英语 🌁 你可以任意使用应用内默认主题或任意第三方主题，强大的主题自定义能力 🖥 你可以自定义源文件夹，利用 OneDrive、百度网盘、iCloud、Dropbox 等进行多设备同步 🌱 当然 Gridea 还很年轻，有很多不足，但请相信，它会不停向前 🏃 未来，它一定会成为你离不开的伙伴 尽情发挥你的才华吧！ 😘 Enjoy~ ","link":"https://faded.auspicious.space/post/hello-gridea/"}]}